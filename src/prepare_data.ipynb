{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "from data_loader import get_pdf_filepaths, load_page_and_line_indexes, load_split_data, load_raw_indexes_list\n",
    "from pdf_reader import parse_pdf\n",
    "from data_cleaner import clean_initial_indexes, add_split_data\n",
    "from data_saver import save_page_and_line_indexes, save_split_data, save_raw_indexes_list\n",
    "from data_transformer import (\n",
    "  get_candidates_and_frequencies, \n",
    "  add_frequencies_column, \n",
    "  add_is_in_toc, \n",
    "  add_importance,\n",
    "  add_position_in_context,\n",
    "  add_is_named_entity,\n",
    "  add_length_of_word,\n",
    "  add_is_named_author,\n",
    "  add_tfidf,\n",
    "  get_raw_indexes_list,\n",
    "  add_is_in_index,\n",
    "  aggregate_by_candidate\n",
    ")\n",
    "\n",
    "\n",
    "#InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration vars\n",
    "DATA_DIR_PATH = \"../data/\"\n",
    "PDF_SOURCE_DIR_PATH = DATA_DIR_PATH + \"pdf/\"\n",
    "PROCESSED_DATA_DIR_PATH = DATA_DIR_PATH + \"processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/2017_Book_TheDataScienceDesignManual.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/morris_07.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/lyons_96.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/2015_Book_LinearAlgebra.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/christensen_04.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/keefe_00.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/linnebo_18.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/oliver_13.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/jenkins_08.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/zimmermann_13.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/shapiro_97.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/jacobson_14.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/vajjala_20.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/winter_16.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/lycan_08.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/linnebo_21.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/2009_Book_TheElementsOfStatisticalLearni.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/williamson_94.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/heather_17.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/partee_04.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/2014_Book_Microeconomics.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/2007BoolosComputability and Logic Fifth Edition.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/heine_10.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/2017_Book_CosmologyForTheCurious.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/2017_Book_RoboticsVisionAndControl.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/peregrin_17.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/moltmann_97.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/2018_Book_PhilosophicalAndMathematicalLo.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/stenning_08.pdf',\n",
       " '/Users/bertagrim/dev_projects/final_project/index_generator/data/pdf/nussbaumer_15.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = get_pdf_filepaths(PDF_SOURCE_DIR_PATH)\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw line and page data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Does not work with file_paths[0] and file_paths[2]\n",
    "for file_path in file_paths[3:4]:\n",
    "    raw_line_and_page_indexes = parse_pdf(file_path)\n",
    "\n",
    "    line_and_page_indexes = clean_initial_indexes(raw_line_and_page_indexes)\n",
    "\n",
    "  # line_and_page_indexes['file_name']\n",
    "  # line_and_page_indexes['by_line'].head()\n",
    "  # line_and_page_indexes['by_page'].head()\n",
    "\n",
    "    save_page_and_line_indexes(\n",
    "     processed_data_dir_path=PROCESSED_DATA_DIR_PATH, \n",
    "     line_and_page_indexes=line_and_page_indexes\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in file_paths[3:4]:\n",
    "  line_and_page_indexes = load_page_and_line_indexes(\n",
    "    processed_data_dir_path=PROCESSED_DATA_DIR_PATH,\n",
    "    pdf_filepath=file_path\n",
    "  )\n",
    "  with_split_data = add_split_data(\n",
    "    file_path=file_path,\n",
    "    line_and_page_indexes=line_and_page_indexes\n",
    "  )\n",
    "\n",
    "  # with_split_data['by_page_body'].head()\n",
    "  \n",
    "  save_split_data(\n",
    "    processed_data_dir_path=PROCESSED_DATA_DIR_PATH,\n",
    "    split_data=with_split_data  \n",
    "  )\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare input data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_keyword</th>\n",
       "      <th>clean_context</th>\n",
       "      <th>raw_context</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chapter</td>\n",
       "      <td>[chapter]</td>\n",
       "      <td>chapter</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linear</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>algebra</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>every</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>life</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear algebra</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>algebra every</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>every day</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>day life</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>one</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>familiarize</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>student</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>actual</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>question</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>application</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>learns</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>deal</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>real</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>world</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>one familiarize</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>familiarize student</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>student actual</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>actual question</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>question application</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>application learns</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>learns deal</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>deal real</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>real world</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lothar</td>\n",
       "      <td>[lothar, collatz]</td>\n",
       "      <td>lothar collatz</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       candidate_keyword  \\\n",
       "0                chapter   \n",
       "1                 linear   \n",
       "2                algebra   \n",
       "3                  every   \n",
       "4                    day   \n",
       "5                   life   \n",
       "6         linear algebra   \n",
       "7          algebra every   \n",
       "8              every day   \n",
       "9               day life   \n",
       "10                   one   \n",
       "11           familiarize   \n",
       "12               student   \n",
       "13                actual   \n",
       "14              question   \n",
       "15           application   \n",
       "16                learns   \n",
       "17                  deal   \n",
       "18                  real   \n",
       "19                 world   \n",
       "20       one familiarize   \n",
       "21   familiarize student   \n",
       "22        student actual   \n",
       "23       actual question   \n",
       "24  question application   \n",
       "25    application learns   \n",
       "26           learns deal   \n",
       "27             deal real   \n",
       "28            real world   \n",
       "29                lothar   \n",
       "\n",
       "                                                                            clean_context  \\\n",
       "0                                                                               [chapter]   \n",
       "1                                                     [linear, algebra, every, day, life]   \n",
       "2                                                     [linear, algebra, every, day, life]   \n",
       "3                                                     [linear, algebra, every, day, life]   \n",
       "4                                                     [linear, algebra, every, day, life]   \n",
       "5                                                     [linear, algebra, every, day, life]   \n",
       "6                                                     [linear, algebra, every, day, life]   \n",
       "7                                                     [linear, algebra, every, day, life]   \n",
       "8                                                     [linear, algebra, every, day, life]   \n",
       "9                                                     [linear, algebra, every, day, life]   \n",
       "10  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "11  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "12  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "13  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "14  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "15  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "16  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "17  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "18  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "19  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "20  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "21  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "22  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "23  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "24  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "25  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "26  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "27  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "28  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "29                                                                      [lothar, collatz]   \n",
       "\n",
       "                                                                                                          raw_context  \\\n",
       "0                                                                                                             chapter   \n",
       "1                                                                                    linear algebra in every day life   \n",
       "2                                                                                    linear algebra in every day life   \n",
       "3                                                                                    linear algebra in every day life   \n",
       "4                                                                                    linear algebra in every day life   \n",
       "5                                                                                    linear algebra in every day life   \n",
       "6                                                                                    linear algebra in every day life   \n",
       "7                                                                                    linear algebra in every day life   \n",
       "8                                                                                    linear algebra in every day life   \n",
       "9                                                                                    linear algebra in every day life   \n",
       "10  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "11  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "12  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "13  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "14  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "15  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "16  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "17  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "18  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "19  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "20  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "21  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "22  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "23  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "24  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "25  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "26  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "27  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "28  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "29                                                                                                     lothar collatz   \n",
       "\n",
       "      POS  \n",
       "0    NOUN  \n",
       "1   PROPN  \n",
       "2   PROPN  \n",
       "3     DET  \n",
       "4    NOUN  \n",
       "5    NOUN  \n",
       "6   CHUNK  \n",
       "7   CHUNK  \n",
       "8   CHUNK  \n",
       "9   CHUNK  \n",
       "10    NUM  \n",
       "11   NOUN  \n",
       "12   NOUN  \n",
       "13    ADJ  \n",
       "14   NOUN  \n",
       "15   NOUN  \n",
       "16   VERB  \n",
       "17   NOUN  \n",
       "18    ADJ  \n",
       "19   NOUN  \n",
       "20  CHUNK  \n",
       "21  CHUNK  \n",
       "22  CHUNK  \n",
       "23  CHUNK  \n",
       "24  CHUNK  \n",
       "25  CHUNK  \n",
       "26  CHUNK  \n",
       "27  CHUNK  \n",
       "28  CHUNK  \n",
       "29  PROPN  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = file_paths[3]\n",
    "\n",
    "split_data = load_split_data(\n",
    "    processed_data_dir_path=PROCESSED_DATA_DIR_PATH,\n",
    "    pdf_filepath=file_path\n",
    "  )\n",
    "# split_data['by_line_toc'].head()\n",
    "\n",
    "(candidates_df, freq_ngrams) = get_candidates_and_frequencies(split_data)\n",
    "\n",
    "# for file_path in file_paths[3:4]:\n",
    "#   split_data = load_split_data(\n",
    "#     processed_data_dir_path=PROCESSED_DATA_DIR_PATH,\n",
    "#     pdf_filepath=file_path\n",
    "#   )\n",
    "#   split_data['by_line_toc'].head()\n",
    "\n",
    "#   (candidates_df, freq_ngrams) = get_candidates_and_frequencies(split_data)\n",
    "\n",
    "#   candidates_df.head()\n",
    "\n",
    "#   with_frequencies = add_frequencies_column(\n",
    "#     by_pages_body_df=split_data['by_page_body'],\n",
    "#     candidates_df=candidates_df,\n",
    "#     freq_ngrams=freq_ngrams\n",
    "#   )\n",
    "\n",
    "#   with_frequencies.head()\n",
    "\n",
    "candidates_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_keyword</th>\n",
       "      <th>clean_context</th>\n",
       "      <th>raw_context</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [candidate_keyword, clean_context, raw_context, POS]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# candidates_df[candidates_df['candidate_keyword'] == 'socio-cultural']\n",
    "#candidates_df[candidates_df['candidate_keyword'] == 'socio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_frequencies = add_frequencies_column(\n",
    "  by_pages_body_df=split_data['by_page_body'],\n",
    "  candidates_df=candidates_df,\n",
    "  freq_ngrams=freq_ngrams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_frequencies[500:510]\n",
    "# len(freq_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_keyword</th>\n",
       "      <th>clean_context</th>\n",
       "      <th>raw_context</th>\n",
       "      <th>POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>is_in_toc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chapter</td>\n",
       "      <td>[chapter]</td>\n",
       "      <td>chapter</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linear</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>algebra</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>every</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>DET</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>life</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear algebra</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>algebra every</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>every day</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>day life</td>\n",
       "      <td>[linear, algebra, every, day, life]</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>one</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NUM</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>familiarize</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>student</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>actual</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>question</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>application</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>learns</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>VERB</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>deal</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>real</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>world</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>one familiarize</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>familiarize student</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>student actual</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>actual question</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>question application</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>application learns</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>learns deal</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>deal real</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>real world</td>\n",
       "      <td>[one, familiarize, student, actual, question, application, learns, deal, real, world]</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lothar</td>\n",
       "      <td>[lothar, collatz]</td>\n",
       "      <td>lothar collatz</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       candidate_keyword  \\\n",
       "0                chapter   \n",
       "1                 linear   \n",
       "2                algebra   \n",
       "3                  every   \n",
       "4                    day   \n",
       "5                   life   \n",
       "6         linear algebra   \n",
       "7          algebra every   \n",
       "8              every day   \n",
       "9               day life   \n",
       "10                   one   \n",
       "11           familiarize   \n",
       "12               student   \n",
       "13                actual   \n",
       "14              question   \n",
       "15           application   \n",
       "16                learns   \n",
       "17                  deal   \n",
       "18                  real   \n",
       "19                 world   \n",
       "20       one familiarize   \n",
       "21   familiarize student   \n",
       "22        student actual   \n",
       "23       actual question   \n",
       "24  question application   \n",
       "25    application learns   \n",
       "26           learns deal   \n",
       "27             deal real   \n",
       "28            real world   \n",
       "29                lothar   \n",
       "\n",
       "                                                                            clean_context  \\\n",
       "0                                                                               [chapter]   \n",
       "1                                                     [linear, algebra, every, day, life]   \n",
       "2                                                     [linear, algebra, every, day, life]   \n",
       "3                                                     [linear, algebra, every, day, life]   \n",
       "4                                                     [linear, algebra, every, day, life]   \n",
       "5                                                     [linear, algebra, every, day, life]   \n",
       "6                                                     [linear, algebra, every, day, life]   \n",
       "7                                                     [linear, algebra, every, day, life]   \n",
       "8                                                     [linear, algebra, every, day, life]   \n",
       "9                                                     [linear, algebra, every, day, life]   \n",
       "10  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "11  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "12  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "13  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "14  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "15  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "16  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "17  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "18  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "19  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "20  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "21  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "22  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "23  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "24  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "25  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "26  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "27  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "28  [one, familiarize, student, actual, question, application, learns, deal, real, world]   \n",
       "29                                                                      [lothar, collatz]   \n",
       "\n",
       "                                                                                                          raw_context  \\\n",
       "0                                                                                                             chapter   \n",
       "1                                                                                    linear algebra in every day life   \n",
       "2                                                                                    linear algebra in every day life   \n",
       "3                                                                                    linear algebra in every day life   \n",
       "4                                                                                    linear algebra in every day life   \n",
       "5                                                                                    linear algebra in every day life   \n",
       "6                                                                                    linear algebra in every day life   \n",
       "7                                                                                    linear algebra in every day life   \n",
       "8                                                                                    linear algebra in every day life   \n",
       "9                                                                                    linear algebra in every day life   \n",
       "10  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "11  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "12  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "13  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "14  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "15  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "16  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "17  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "18  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "19  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "20  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "21  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "22  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "23  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "24  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "25  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "26  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "27  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "28  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "29                                                                                                     lothar collatz   \n",
       "\n",
       "      POS      freq  is_in_toc  \n",
       "0    NOUN  0.000518          0  \n",
       "1   PROPN  0.002786          1  \n",
       "2   PROPN  0.000648          1  \n",
       "3     DET  0.001888          1  \n",
       "4    NOUN  0.000046          1  \n",
       "5    NOUN  0.000046          1  \n",
       "6   CHUNK  0.000435          0  \n",
       "7   CHUNK  0.000037          0  \n",
       "8   CHUNK  0.000046          0  \n",
       "9   CHUNK  0.000046          0  \n",
       "10    NUM  0.001324          0  \n",
       "11   NOUN  0.000009          0  \n",
       "12   NOUN  0.000009          0  \n",
       "13    ADJ  0.000028          0  \n",
       "14   NOUN  0.000019          0  \n",
       "15   NOUN  0.000305          0  \n",
       "16   VERB  0.000019          0  \n",
       "17   NOUN  0.000019          0  \n",
       "18    ADJ  0.001046          0  \n",
       "19   NOUN  0.000037          0  \n",
       "20  CHUNK  0.000009          0  \n",
       "21  CHUNK  0.000009          0  \n",
       "22  CHUNK  0.000009          0  \n",
       "23  CHUNK  0.000009          0  \n",
       "24  CHUNK  0.000009          0  \n",
       "25  CHUNK  0.000009          0  \n",
       "26  CHUNK  0.000009          0  \n",
       "27  CHUNK  0.000009          0  \n",
       "28  CHUNK  0.000037          0  \n",
       "29  PROPN  0.000009          0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_is_in_toc = add_is_in_toc(\n",
    "  candidates_df=with_frequencies, \n",
    "  by_line_toc=split_data['by_line_toc']\n",
    ")\n",
    "\n",
    "with_is_in_toc.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_importance = add_importance(with_is_in_toc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_keyword</th>\n",
       "      <th>clean_context</th>\n",
       "      <th>raw_context</th>\n",
       "      <th>POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>is_in_toc</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chapter</td>\n",
       "      <td>chapter</td>\n",
       "      <td>chapter</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linear</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>algebra</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>every</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>DET</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>life</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear algebra</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0</td>\n",
       "      <td>0.541475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>algebra every</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0</td>\n",
       "      <td>0.541475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>every day</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0</td>\n",
       "      <td>0.541475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>day life</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0</td>\n",
       "      <td>0.541475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>one</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NUM</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>familiarize</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>student</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>actual</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>question</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>application</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>learns</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>VERB</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>deal</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>real</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>world</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>one familiarize</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>familiarize student</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>student actual</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>actual question</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>question application</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>application learns</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>learns deal</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>deal real</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>real world</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lothar</td>\n",
       "      <td>lothar collatz</td>\n",
       "      <td>lothar collatz</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.057172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       candidate_keyword  \\\n",
       "0                chapter   \n",
       "1                 linear   \n",
       "2                algebra   \n",
       "3                  every   \n",
       "4                    day   \n",
       "5                   life   \n",
       "6         linear algebra   \n",
       "7          algebra every   \n",
       "8              every day   \n",
       "9               day life   \n",
       "10                   one   \n",
       "11           familiarize   \n",
       "12               student   \n",
       "13                actual   \n",
       "14              question   \n",
       "15           application   \n",
       "16                learns   \n",
       "17                  deal   \n",
       "18                  real   \n",
       "19                 world   \n",
       "20       one familiarize   \n",
       "21   familiarize student   \n",
       "22        student actual   \n",
       "23       actual question   \n",
       "24  question application   \n",
       "25    application learns   \n",
       "26           learns deal   \n",
       "27             deal real   \n",
       "28            real world   \n",
       "29                lothar   \n",
       "\n",
       "                                                                 clean_context  \\\n",
       "0                                                                      chapter   \n",
       "1                                                linear algebra every day life   \n",
       "2                                                linear algebra every day life   \n",
       "3                                                linear algebra every day life   \n",
       "4                                                linear algebra every day life   \n",
       "5                                                linear algebra every day life   \n",
       "6                                                linear algebra every day life   \n",
       "7                                                linear algebra every day life   \n",
       "8                                                linear algebra every day life   \n",
       "9                                                linear algebra every day life   \n",
       "10  one familiarize student actual question application learns deal real world   \n",
       "11  one familiarize student actual question application learns deal real world   \n",
       "12  one familiarize student actual question application learns deal real world   \n",
       "13  one familiarize student actual question application learns deal real world   \n",
       "14  one familiarize student actual question application learns deal real world   \n",
       "15  one familiarize student actual question application learns deal real world   \n",
       "16  one familiarize student actual question application learns deal real world   \n",
       "17  one familiarize student actual question application learns deal real world   \n",
       "18  one familiarize student actual question application learns deal real world   \n",
       "19  one familiarize student actual question application learns deal real world   \n",
       "20  one familiarize student actual question application learns deal real world   \n",
       "21  one familiarize student actual question application learns deal real world   \n",
       "22  one familiarize student actual question application learns deal real world   \n",
       "23  one familiarize student actual question application learns deal real world   \n",
       "24  one familiarize student actual question application learns deal real world   \n",
       "25  one familiarize student actual question application learns deal real world   \n",
       "26  one familiarize student actual question application learns deal real world   \n",
       "27  one familiarize student actual question application learns deal real world   \n",
       "28  one familiarize student actual question application learns deal real world   \n",
       "29                                                              lothar collatz   \n",
       "\n",
       "                                                                                                          raw_context  \\\n",
       "0                                                                                                             chapter   \n",
       "1                                                                                    linear algebra in every day life   \n",
       "2                                                                                    linear algebra in every day life   \n",
       "3                                                                                    linear algebra in every day life   \n",
       "4                                                                                    linear algebra in every day life   \n",
       "5                                                                                    linear algebra in every day life   \n",
       "6                                                                                    linear algebra in every day life   \n",
       "7                                                                                    linear algebra in every day life   \n",
       "8                                                                                    linear algebra in every day life   \n",
       "9                                                                                    linear algebra in every day life   \n",
       "10  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "11  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "12  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "13  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "14  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "15  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "16  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "17  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "18  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "19  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "20  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "21  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "22  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "23  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "24  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "25  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "26  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "27  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "28  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "29                                                                                                     lothar collatz   \n",
       "\n",
       "      POS      freq  is_in_toc  importance  \n",
       "0    NOUN  0.000518          0   -0.162285  \n",
       "1   PROPN  0.002786          1    0.541475  \n",
       "2   PROPN  0.000648          1    0.541475  \n",
       "3     DET  0.001888          1    0.541475  \n",
       "4    NOUN  0.000046          1    0.541475  \n",
       "5    NOUN  0.000046          1    0.541475  \n",
       "6   CHUNK  0.000435          0    0.541475  \n",
       "7   CHUNK  0.000037          0    0.541475  \n",
       "8   CHUNK  0.000046          0    0.541475  \n",
       "9   CHUNK  0.000046          0    0.541475  \n",
       "10    NUM  0.001324          0    0.422510  \n",
       "11   NOUN  0.000009          0    0.422510  \n",
       "12   NOUN  0.000009          0    0.422510  \n",
       "13    ADJ  0.000028          0    0.422510  \n",
       "14   NOUN  0.000019          0    0.422510  \n",
       "15   NOUN  0.000305          0    0.422510  \n",
       "16   VERB  0.000019          0    0.422510  \n",
       "17   NOUN  0.000019          0    0.422510  \n",
       "18    ADJ  0.001046          0    0.422510  \n",
       "19   NOUN  0.000037          0    0.422510  \n",
       "20  CHUNK  0.000009          0    0.422510  \n",
       "21  CHUNK  0.000009          0    0.422510  \n",
       "22  CHUNK  0.000009          0    0.422510  \n",
       "23  CHUNK  0.000009          0    0.422510  \n",
       "24  CHUNK  0.000009          0    0.422510  \n",
       "25  CHUNK  0.000009          0    0.422510  \n",
       "26  CHUNK  0.000009          0    0.422510  \n",
       "27  CHUNK  0.000009          0    0.422510  \n",
       "28  CHUNK  0.000037          0    0.422510  \n",
       "29  PROPN  0.000009          0    0.057172  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_importance.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_position_in_context = add_position_in_context(with_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_is_named_entity = add_is_named_entity(\n",
    "  candidates_df=candidates_df,\n",
    "  df_pages_body=split_data['by_page_body']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_keyword</th>\n",
       "      <th>clean_context</th>\n",
       "      <th>raw_context</th>\n",
       "      <th>POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>is_in_toc</th>\n",
       "      <th>importance</th>\n",
       "      <th>position_in_context</th>\n",
       "      <th>is_named_entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chapter</td>\n",
       "      <td>chapter</td>\n",
       "      <td>chapter</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linear</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>algebra</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541475</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>every</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>DET</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541475</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541475</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>life</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear algebra</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0</td>\n",
       "      <td>0.541475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>algebra every</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0</td>\n",
       "      <td>0.541475</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>every day</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0</td>\n",
       "      <td>0.541475</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>day life</td>\n",
       "      <td>linear algebra every day life</td>\n",
       "      <td>linear algebra in every day life</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0</td>\n",
       "      <td>0.541475</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>one</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NUM</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>familiarize</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>student</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>actual</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>question</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>application</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>learns</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>VERB</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>deal</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>real</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>world</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>one familiarize</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>familiarize student</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>student actual</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>actual question</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>question application</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>application learns</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>learns deal</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>deal real</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>real world</td>\n",
       "      <td>one familiarize student actual question application learns deal real world</td>\n",
       "      <td>one ha to familiarize the student with actual question from application so that he learns to deal with real world</td>\n",
       "      <td>CHUNK</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422510</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lothar</td>\n",
       "      <td>lothar collatz</td>\n",
       "      <td>lothar collatz</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.057172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       candidate_keyword  \\\n",
       "0                chapter   \n",
       "1                 linear   \n",
       "2                algebra   \n",
       "3                  every   \n",
       "4                    day   \n",
       "5                   life   \n",
       "6         linear algebra   \n",
       "7          algebra every   \n",
       "8              every day   \n",
       "9               day life   \n",
       "10                   one   \n",
       "11           familiarize   \n",
       "12               student   \n",
       "13                actual   \n",
       "14              question   \n",
       "15           application   \n",
       "16                learns   \n",
       "17                  deal   \n",
       "18                  real   \n",
       "19                 world   \n",
       "20       one familiarize   \n",
       "21   familiarize student   \n",
       "22        student actual   \n",
       "23       actual question   \n",
       "24  question application   \n",
       "25    application learns   \n",
       "26           learns deal   \n",
       "27             deal real   \n",
       "28            real world   \n",
       "29                lothar   \n",
       "\n",
       "                                                                 clean_context  \\\n",
       "0                                                                      chapter   \n",
       "1                                                linear algebra every day life   \n",
       "2                                                linear algebra every day life   \n",
       "3                                                linear algebra every day life   \n",
       "4                                                linear algebra every day life   \n",
       "5                                                linear algebra every day life   \n",
       "6                                                linear algebra every day life   \n",
       "7                                                linear algebra every day life   \n",
       "8                                                linear algebra every day life   \n",
       "9                                                linear algebra every day life   \n",
       "10  one familiarize student actual question application learns deal real world   \n",
       "11  one familiarize student actual question application learns deal real world   \n",
       "12  one familiarize student actual question application learns deal real world   \n",
       "13  one familiarize student actual question application learns deal real world   \n",
       "14  one familiarize student actual question application learns deal real world   \n",
       "15  one familiarize student actual question application learns deal real world   \n",
       "16  one familiarize student actual question application learns deal real world   \n",
       "17  one familiarize student actual question application learns deal real world   \n",
       "18  one familiarize student actual question application learns deal real world   \n",
       "19  one familiarize student actual question application learns deal real world   \n",
       "20  one familiarize student actual question application learns deal real world   \n",
       "21  one familiarize student actual question application learns deal real world   \n",
       "22  one familiarize student actual question application learns deal real world   \n",
       "23  one familiarize student actual question application learns deal real world   \n",
       "24  one familiarize student actual question application learns deal real world   \n",
       "25  one familiarize student actual question application learns deal real world   \n",
       "26  one familiarize student actual question application learns deal real world   \n",
       "27  one familiarize student actual question application learns deal real world   \n",
       "28  one familiarize student actual question application learns deal real world   \n",
       "29                                                              lothar collatz   \n",
       "\n",
       "                                                                                                          raw_context  \\\n",
       "0                                                                                                             chapter   \n",
       "1                                                                                    linear algebra in every day life   \n",
       "2                                                                                    linear algebra in every day life   \n",
       "3                                                                                    linear algebra in every day life   \n",
       "4                                                                                    linear algebra in every day life   \n",
       "5                                                                                    linear algebra in every day life   \n",
       "6                                                                                    linear algebra in every day life   \n",
       "7                                                                                    linear algebra in every day life   \n",
       "8                                                                                    linear algebra in every day life   \n",
       "9                                                                                    linear algebra in every day life   \n",
       "10  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "11  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "12  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "13  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "14  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "15  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "16  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "17  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "18  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "19  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "20  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "21  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "22  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "23  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "24  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "25  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "26  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "27  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "28  one ha to familiarize the student with actual question from application so that he learns to deal with real world   \n",
       "29                                                                                                     lothar collatz   \n",
       "\n",
       "      POS      freq  is_in_toc  importance  position_in_context  \\\n",
       "0    NOUN  0.000518          0   -0.162285             0.000000   \n",
       "1   PROPN  0.002786          1    0.541475             0.000000   \n",
       "2   PROPN  0.000648          1    0.541475             0.200000   \n",
       "3     DET  0.001888          1    0.541475             0.600000   \n",
       "4    NOUN  0.000046          1    0.541475             0.800000   \n",
       "5    NOUN  0.000046          1    0.541475             1.000000   \n",
       "6   CHUNK  0.000435          0    0.541475             0.000000   \n",
       "7   CHUNK  0.000037          0    0.541475             0.200000   \n",
       "8   CHUNK  0.000046          0    0.541475             0.600000   \n",
       "9   CHUNK  0.000046          0    0.541475             0.800000   \n",
       "10    NUM  0.001324          0    0.422510             0.000000   \n",
       "11   NOUN  0.000009          0    0.422510             0.157895   \n",
       "12   NOUN  0.000009          0    0.422510             0.263158   \n",
       "13    ADJ  0.000028          0    0.422510             0.368421   \n",
       "14   NOUN  0.000019          0    0.422510             0.421053   \n",
       "15   NOUN  0.000305          0    0.422510             0.526316   \n",
       "16   VERB  0.000019          0    0.422510             0.736842   \n",
       "17   NOUN  0.000019          0    0.422510             0.842105   \n",
       "18    ADJ  0.001046          0    0.422510             0.947368   \n",
       "19   NOUN  0.000037          0    0.422510             1.000000   \n",
       "20  CHUNK  0.000009          0    0.422510             0.000000   \n",
       "21  CHUNK  0.000009          0    0.422510             0.157895   \n",
       "22  CHUNK  0.000009          0    0.422510             0.263158   \n",
       "23  CHUNK  0.000009          0    0.422510             0.368421   \n",
       "24  CHUNK  0.000009          0    0.422510             0.421053   \n",
       "25  CHUNK  0.000009          0    0.422510             0.526316   \n",
       "26  CHUNK  0.000009          0    0.422510             0.736842   \n",
       "27  CHUNK  0.000009          0    0.422510             0.842105   \n",
       "28  CHUNK  0.000037          0    0.422510             0.947368   \n",
       "29  PROPN  0.000009          0    0.057172             0.000000   \n",
       "\n",
       "    is_named_entity  \n",
       "0                 0  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 1  \n",
       "5                 1  \n",
       "6                 0  \n",
       "7                 0  \n",
       "8                 0  \n",
       "9                 0  \n",
       "10                0  \n",
       "11                0  \n",
       "12                0  \n",
       "13                0  \n",
       "14                0  \n",
       "15                0  \n",
       "16                0  \n",
       "17                0  \n",
       "18                1  \n",
       "19                0  \n",
       "20                0  \n",
       "21                0  \n",
       "22                0  \n",
       "23                0  \n",
       "24                0  \n",
       "25                0  \n",
       "26                0  \n",
       "27                0  \n",
       "28                0  \n",
       "29                0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_is_named_entity.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_length_of_word = add_length_of_word(candidates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_is_named_author = add_is_named_author(\n",
    "  candidates_df=with_length_of_word,\n",
    "  df_pages_biblio=split_data['by_page_biblio']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_tfidf = add_tfidf(\n",
    "  candidates_df=with_is_named_author,\n",
    "  df_pages_body=split_data['by_page_body']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>83537.0</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.005065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>0.020430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_in_toc</th>\n",
       "      <td>83537.0</td>\n",
       "      <td>0.122425</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>importance</th>\n",
       "      <td>83537.0</td>\n",
       "      <td>0.266748</td>\n",
       "      <td>0.171825</td>\n",
       "      <td>-0.277741</td>\n",
       "      <td>0.152840</td>\n",
       "      <td>0.278597</td>\n",
       "      <td>0.390791</td>\n",
       "      <td>0.732903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>position_in_context</th>\n",
       "      <td>83537.0</td>\n",
       "      <td>0.390919</td>\n",
       "      <td>0.304635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_named_entity</th>\n",
       "      <td>83537.0</td>\n",
       "      <td>0.086477</td>\n",
       "      <td>0.281068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>83537.0</td>\n",
       "      <td>6.868406</td>\n",
       "      <td>5.357519</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_named_author</th>\n",
       "      <td>83537.0</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.045852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>83537.0</td>\n",
       "      <td>0.069366</td>\n",
       "      <td>0.145401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066423</td>\n",
       "      <td>0.850750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       count      mean       std       min       25%  \\\n",
       "freq                 83537.0  0.002614  0.005065  0.000000  0.000037   \n",
       "is_in_toc            83537.0  0.122425  0.327778  0.000000  0.000000   \n",
       "importance           83537.0  0.266748  0.171825 -0.277741  0.152840   \n",
       "position_in_context  83537.0  0.390919  0.304635  0.000000  0.125000   \n",
       "is_named_entity      83537.0  0.086477  0.281068  0.000000  0.000000   \n",
       "length               83537.0  6.868406  5.357519  1.000000  3.000000   \n",
       "is_named_author      83537.0  0.002107  0.045852  0.000000  0.000000   \n",
       "tfidf                83537.0  0.069366  0.145401  0.000000  0.000000   \n",
       "\n",
       "                          50%        75%        max  \n",
       "freq                 0.000343   0.002175   0.020430  \n",
       "is_in_toc            0.000000   0.000000   1.000000  \n",
       "importance           0.278597   0.390791   0.732903  \n",
       "position_in_context  0.333333   0.631579   1.000000  \n",
       "is_named_entity      0.000000   0.000000   1.000000  \n",
       "length               5.000000  10.000000  35.000000  \n",
       "is_named_author      0.000000   0.000000   1.000000  \n",
       "tfidf                0.000000   0.066423   0.850750  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_tfidf.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content              0\n",
       "page_number          0\n",
       "real_page_num        0\n",
       "section_level_1      0\n",
       "section_level_2     26\n",
       "section_level_3    300\n",
       "clean_content        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data['by_page_body'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data['by_page_body'].fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content            0\n",
       "page_number        0\n",
       "real_page_num      0\n",
       "section_level_1    0\n",
       "section_level_2    0\n",
       "section_level_3    0\n",
       "clean_content      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data['by_page_body'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section_level_1</th>\n",
       "      <th>section_level_2</th>\n",
       "      <th>section_level_3</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 Linear Algebra in Every Day Life</td>\n",
       "      <td>1.1 The PageRank Algorithm</td>\n",
       "      <td></td>\n",
       "      <td>chapter linear algebra every day life one familiarize student actual question application learns deal real world lothar collatz pagerank algorithm pagerank algorithm method ass importance document mutual link web page basis link structure developed sergei brin larry page founder google stanford university late basic idea algorithm following instead counting link pagerank essentially interprets link page page b vote page page pagerank ass importance page number received vote pagerank also considers importance page cast vote since vote page higher value thus also assign higher value page point important page rated higher thus lead higher position search let u describe model idea mathematically presentation us idea article given set web page every page k assigned importance value xk page k important page j xk x j page k link page j say page j backlink page description backlinks vote example consider following link structure man mus den lernenden mit konkreten fragestellungen au den anwendungen vertraut machen das er lernt konkrete fragen zu text found http translation springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi linear algebra every day life page link page backlink page easiest approach define importance web page count backlinks vote cast page important page example give importance value page thus important page equally important however intuition also description google suggests backlinks important page important value page le important page idea modeled defining xk sum importance value backlinks page example result four equation satisfied simultaneously disadvantage approach consider number link page thus would possible significantly increase importance page adding link page order avoid importance value backlinks pagerank algorithm divided number link corresponding page creates kind internet democracy every page vote page total cast one vote example give equation four equation four unknown equation unknown occur first power chap see write equation form linear system equation analyzing solving system one important task linear algebra example pagerank algorithm show linear algebra present powerful modeling term linear originates latin word linea mean straight line linearis mean consisting straight line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 Linear Algebra in Every Day Life</td>\n",
       "      <td>1.2 No Claim Discounting in Car Insurances</td>\n",
       "      <td></td>\n",
       "      <td>pagerank algorithm tool turned real world problem assessing importance web page problem linear algebra problem examined sect completeness mention solution four unknown computed matlab rounded second significant digit given thus page important one possible multiply solution importance value xk positive constant multiplication scaling often advantageous computational method visual display result example scaling could used give important page value scaling allowed since change ranking page essential information provided pagerank algorithm claim discounting car insurance insurance company compute premium customer basis insured risk higher risk higher premium therefore important identify factor lead higher risk case car insurance factor include number mile driven per year distance home work marital status engine power age driver using information company calculates initial premium usually best indicator future accident hence future insurance claim number accident individual customer past claim history order incorporate information premium rate insurer establish system risk class divide customer homogeneous risk group respect previous claim history customer fewer accident past get discount premium approach called claim discounting scheme mathematical model scheme need set risk class transition rule moving class end policy year customer may move different class depending claim made year discount given percent premium initial class simple example consider four risk class discount following transition rule accident step one class stay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 Linear Algebra in Every Day Life</td>\n",
       "      <td>1.3 Production Planning in a Plant</td>\n",
       "      <td></td>\n",
       "      <td>linear algebra every day life one accident step back one class stay one accident step back class stay next insurance company estimate probability customer class ci year move class c j probability denoted pi j let u assume simplicity probability exactly one accident every customer probability two accident every customer course practice insurance company determine probability dependence class example customer class stay case least one accident happens probability customer accident probability chance move next year way obtain value pi j j arrange matrix follows entry matrix nonnegative real number sum entry row equal matrix called row-stochastic analysis matrix property central topic linear algebra developed throughout book example pagerank algorithm translated practical problem language linear algebra study using linear algebra technique example premium rate discussed example production planning plant production planning plant consider many different factor particular commodity price labor cost available capital order determine production plan consider simple example company produce product xi unit product pi produced pair called production plan suppose raw material labor production one unit product pi cost euro respectively euro available purchase raw material euro payment labor cost production plan must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 Linear Algebra in Every Day Life</td>\n",
       "      <td>1.4 Predicting Future Profits</td>\n",
       "      <td></td>\n",
       "      <td>production planning plant satisfy constraint inequality production plan satisfies constraint called feasible let pi profit selling one unit product pi goal determine production plan maximizes profit function find maximum two equation describe straight line coordinate system variable ax two line form boundary line feasible production plan line see figure note also must xi since produce negative unit product planned profit yi equation yi describe parallel straight line coordinate system see dashed line figure satisfy yi yi profit maximization problem solved moving dashed line one reach corner maximal case variable draw simple figure obtain solution graphically general idea finding corner maximum profit still example linear optimization problem formulated real world problem language linear algebra use mathematical method solution predicting future profit prediction profit loss company central planning instrument economics analogous problem arise many area political decision making</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 Linear Algebra in Every Day Life</td>\n",
       "      <td>1.5 Circuit Simulation</td>\n",
       "      <td></td>\n",
       "      <td>linear algebra every day life example budget planning tax estimate planning new infrastructure consider specific example four quarter year company profit million euro board want predict future profit development basis value evidence suggests profit behave linearly true profit would form straight line αt β connects point coordinate system time profit ax however neither hold example practice therefore one try find straight line deviate little possible given point one possible approach choose parameter α β order minimize sum squared distance given point straight line parameter α β determined resulting line used estimating predicting future profit illustrated following figure determination parameter α β minimize sum square called least square problem solve least square problem using method linear algebra example approach sometimes called parameter identification statistic modeling given data company profit using linear predictor function αt β known linear regression circuit simulation current development electronic device rapid short interval nowadays often le year new model laptop mobile phone issued market achieve continuously new generation computer chip developed typically become smaller powerful naturally use little energy possible important factor development plan simulate chip virtually computer without producing physical prototype model-based planning optimization product central method many high technology area based modern mathematics circuit simulation usually switching behavior chip modeled mathematical system consisting differential algebraic equation describe relation current voltage without going detail consider following circuit circuit description v given input current time characteristic value component r resistor l inductor c capacitor function potential difference three component denoted vr vl vc current applying kirchhoff electrical engineering lead following system linear equation differential equation model dynamic behavior circuit vl dt c vc dt r vr l vl vc vr v example easy solve last two equation vl vr hence obtain system differential equation r vc v dt l l l vc dt c function und vc discus solve system example simple example demonstrates simulation circuit system linear differential equation algebraic equation solved modern computer chip industrial practice require solving system million differential-algebraic equation linear algebra one central tool theoretical analysis system well development efficient solution method gustav robert kirchhoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.1 Basic Definitions and Properties of Linear Maps</td>\n",
       "      <td></td>\n",
       "      <td>chapter linear map chapter study map vector space compatible two vector space operation addition scalar multiplication map called linear map homomorphism first investigate important property show case finite dimensional vector space every linear map represented matrix base respective space chosen base chosen clever way read important property linear map matrix representation central idea arise frequently later chapter basic definition property linear map start investigation definition linear map vector space definition let v w k space map f v w called linear f λv λ f v f v w f v f w hold v w v λ k set map denoted l v w linear map f v w also called linear transformation vector space homomorphism bijective linear map called isomorphism exists isomorphism v w space v w called isomorphic denote map f l v v called endomorphism bijective endomorphism called automorphism springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi linear map easy exercise show condition definition hold f λv μw λ f v μ f w hold λ μ k v w example every matrix k n defines map k k x ax map linear since λx λax x k λ k x ax ay x k cp lemma aii linear cp map trace k n n k ai j trace exercise map f q q linear show exercise map g q q linear example g g g set linear map vector space form vector space lemma let v w k space f g l v w λ k define f g λ f f g v f v g v λ f v λ f v v l v w k space proof cp exercise next result deal existence uniqueness linear map theorem let v w k space let vm basis v let wm exists unique linear map f l v w f vi wi basic definition property linear map v proof every v v exist unique coordinate λ v λm v v λi vi cp lemma define map f v w f v λi v wi v definition f vi wi λ λi v vi next show f linear every λ k λv hence λ λi v wi λ λi v wi λ f v f λv u λi u vi v v u f v u λi v λi u wi v λi λi v wi λi u vi hence λi u wi f v f u thus f l v w suppose g l v w also satisfies g vi wi v every v λi vi f v f λi v vi λi v f vi λi v wi λi v g vi g λi v vi g v hence f g f indeed uniquely determined theorem show map f l v w uniquely determined image f given basis vector note image vector wm w may linearly dependent w may infinite dimensional definition introduced image pre-image map next recall definition completeness introduce kernel linear map definition v w k space f l v w kernel image f defined ker f v v f v im f f v v v w w pre-image w space v defined f w f w v v f v w kernel linear map sometimes called null space nullspace map author use notation null f instead ker f linear map note pre-image f w set f mean inverse map f cp definition particular f ker f w im f f w ø example k n corresponding map l k k example ker x k ax im ax x k note ker l cp definition let j k denote jth column j x xm k write ax xjaj clearly ker moreover see representation ax ker column linearly independent set im given linear combination column im span lemma v w k space every f l v w following assertion hold f f f v v f isomorphism f l w v ker f subspace v im f subspace f surjective im f f injective ker f f injective vm v linearly independent f f vm w linearly independent vm v linearly dependent f f vm w linearly dependent equivalently f f vm w linearly independent vm v linearly independent w im f u f w arbitrary f w u ker f u v v ker f proof f f k k f well f v f f v f v existence inverse map f w v guaranteed theorem show f linear w exist uniquely determined v f f hence f f f f f f f f basic definition property linear map moreover every λ k f f λ f f f λ f obvious corresponding definition let f injective v ker f f v know f since f v f injectivity f yield v suppose ker f let u v v f u f v f u v u v ker f implies u v u λi f vi linearity f yield let f λi vi λi vi ker f λi vi hence since f injective λm due linear independence vm thus f f vm linearly independent λi vi λm vm linearly dependent k equal zero applying f side using linearity λi f vi hence f f vm linearly dependent yield let w im f u f w v f w f v f u thus f v u v u ker f v u ker f show f w u ker f hand v u f f v f u w v f w show u ker f f w example consider matrix k n corresponding map l k k example given b k b l b b im l b ø case corollary suppose b im let x l b arbitrary lemma yield l b x ker assertion lemma ker column linearly independent b case corollary ker column linearly dependent b case corollary basis ker λi wi k l b thus solution ax b depend parameter linear map following result give important dimension formula linear map also known rank-nullity theorem dimension image f equal rank matrix associated f cp theorem dimension kernel null space f sometimes called f theorem let v w k space let v finite dimensional every f l v w dimension formula dim v dim im f dim ker f proof let vn f f vn w linearly independent lemma also vn linearly independent thus dim im f dim v since ker f v dim ker f dim v im f ker f finite dimensional let wr vk base im f ker f respectively let u f u r f wr show u u r vk basis v implies assertion v v lemma unique coordinate μr k f v μi wi let v μi u f v f v hence v v ker f give v v λi vi unique coordinate λk k therefore k r λi vi μi u k λi vi thus v span u u r vk since u u r vk v v span u u r vk remains show u u r vk linearly independent r αi u k βi vi f f r αi u k βi vi r αi f u r αi wi thus αr wr linearly independent finally linear independence vk implies βk term introduced james joseph sylvester basic definition property linear map example linear map f α q α q ker f im f α hence dim im f dim ker f indeed dim im f dim ker f dim k n l k k example dim k dim ker dim im thus dim im dim ker hold ker column linearly independent cp example hand dim im dim ker dim im thus ker case column linearly dependent since exists x k ax corollary v w k space dim v dim w n f l v w following statement equivalent f injective f surjective f bijective proof hold hold definition show implied well f injective ker f cp lemma dimension formula theorem yield dim w dim v dim im f thus im f w cp lemma f also surjective f surjective im f w dimension formula dim w dim v yield dim ker f dim v dim im f dim w dim im f thus ker f f also injective using theorem also characterize two finite dimensional vector space isomorphic linear map corollary two finite dimensional k space v w isomorphic dim v dim w proof v w exists bijective map f l v w lemma im f w ker f dimension formula theorem yield dim v dim im f dim ker f dim w dim dim w let dim v dim w need show exists bijective f l v w let vn wn base v theorem exists unique f l v w f vi wi v λn vn ker f f v f λn vn f λn f vn λ λn wn since wn linearly independent λn hence v ker f thus f injective moreover dimension formula yield dim v dim im f dim w therefore im f w cp lemma f also surjective example vector space k n k n dimension therefore isomorphic isomorphism given linear map r-vector space c x iy x r dimension therefore isomorphic isomorphism given linear map x x iy vector space q dimension therefore isomorphic isomorphism given linear map although mathematics formal exact science smallest detail matter one sometimes us abuse notation order simplify presentation used example inductive existence proof echelon form theorem kept simplicity index larger matrix smaller matrix ai course entry position j matrix keeping index entry denoted rather induction made argument much le technical proof remained formally correct abuse notation always justified confused misuse notation field linear algebra justification often given isomorphism identifies vector space example constant polynomial field k polynomial form αt α k often written simply α element field justified since</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.2 Linear Maps and Matrices</td>\n",
       "      <td></td>\n",
       "      <td>basic definition property linear map k k isomorphic k space dimension already used identification similarly identified vector space v v written v instead v sect another common example literature notation k n text denotes set n-tuples element k often used matrix set column vector k row vector k n actual meaning clear context attentive reader significantly benefit simplification due abuse notation linear map matrix let v w finite dimensional k space base vm wn respectively let f l v w lemma every f v j w j exist unique coordinate ai j k n f v j j j wn define ai j k n write similarly equation vector f v j f f vm wn matrix determined uniquely f given base v v λm vm v f v f λm vm f λm f vm f f vm λm wn λm wn λm coordinate f v respect given basis w therefore given λm linear map thus compute coordinate f v simply multiplying coordinate v motivates following definition definition uniquely determined matrix called matrix representation f l v w respect base vm v wn denote matrix f construction matrix representation definition consistently extended case least one k space dimension zero instance dim v n w f v j every basis vector v j thus every vector f v j empty linear combination vector basis ø matrix representation f empty matrix size also v matrix representation f empty matrix size many different notation matrix representation linear map literature notation reflect matrix depends linear map f given base example alternative notation f f mean matrix important special case obtained v w hence particular n f idv identity obtain vn wn idv idv exactly matrix p coordinate transformation matrix theorem hand wn vn idv thus idv idv example consider vector space q base linear map f q q matrix representation f f f vector space k basis b n linear map linear map matrix f k k αn n n αn f j j j n f b b k thus f b b permutation matrix theorem let v w finite dimensional k space base vm wn respectively map l v w k n f f isomorphism hence l v w k n dim l v w dim k n n proof proof denote map f f mat mat f f first show map linear let f g l v w mat f f j mat g gi j j f g v j f v j g v j n f j wi n gi j wi n f j gi j wi thus mat f g f j gi j f j gi j mat f mat g λ k j λ f v j λ f v j λ n f j wi n λ f j wi thus mat λ f λ f j λ f j λ mat f remains show mat bijective f ker mat mat f k n f v j j thus f v v v f zero map mat injective cp lemma hand ai j k n arbitrary define linear map f v w via f v j n ai j wi j cp proof theorem mat f hence mat also surjective cp lemma corollary show dim l v w dim k n n cp also example linear map theorem show particular f g l v w satisfy f g f g hold given base v thus prove equality linear map via equality matrix representation consider map element finite dimensional vector space coordinate respect given basis lemma b vn basis k space v map v k v λn vn b v λn isomorphism called coordinate map v respect basis b proof linearity b clear moreover obviously b v k b surjective v ker b λn v ker b b also injective cp lemma example vector space k basis b n b αn n k αn hand basis b n yield αn b αn n k base finite dimensional vector space v w respectively illustrate meaning construction matrix representation f f l v w following commutative diagram v f f k k see different composition map yield result particular f f linear map matrix matrix f k n interpreted linear map k k use coordinate map bijective hence invertible way obtain f f f v f v v word coordinate f v respect basis w given product f coordinate v respect basis next show consecutive application linear map corresponds multiplication matrix representation theorem let v w x k space f l v w g l w x g f l v x moreover v w x finite dimensional respective base g f g f proof let h g f show first h l v x u v v λ μ k h λu μv g f λu μv g λ f u μ f v λg f u μg f v λh u μh v let vm wn x f f j g gi j j h v j g f v j g n f k j wk n f k j gik xi n n f k j g wk gik f k j h j n fk j gik xi xi thus h h j gi j f j g f using theorem study change base affect matrix representation linear map linear map corollary let v w finite dimensional k space base v f l v w f idw f idv particular matrix f f equivalent proof applying theorem twice identity f idw f idv yield f idw f idv idw f idv idw f idv matrix f f equivalent since idw idv invertible v w becomes f idv f idv idv f idv thus matrix representation f f endomorphism f l v v similar cp definition following commutative diagram illustrates corollary f k bee ee ee ee idv b yv yy k k yyy f w f b yy yy idw b ee ee ee ee k analogously f f b f example following base vector space linear map matrix coordinate transformation matrix idv idv idv coordinate map one easily verify idv theorem let v w k space dim v dim w n respectively exist base v w f ir k n r dim im f min n furthermore r rank f f matrix representation f respect arbitrary base v w define rank f rank f dim im f proof let vm wn two arbitrary base v w respectively let r rank f theorem exist invertible matrix q k n n z k q f z ir linear map r rank f min n let u introduce two new base vm wn v w via vm vm z wn wn q hence wn wn q construction z idv q idw corollary obtain ir idw f idv f thus found base yield desired matrix representation f every choice base lead corollary equivalent matrix therefore also rank r remains show r dim im f structure matrix f show f v j w j j r r j therefore vr vm ker f implies dim ker f r hand w j im f thus dim im f r theorem yield dim v dim im f dim ker f hence dim ker f r dim im f r example k n corresponding map l k k example im span thus rank equal number linearly independent column since rank rank cp theorem number equal number linearly independent row theorem first example general strategy use several time following chapter choosing appropriate base matrix representation reveal desired information linear map efficient way theorem information rank linear map f dimension image dimension formula linear map generalized composition map follows linear map matrix theorem v w x finite dimensional k space f l v w g l w x dim im g f dim im f dim im f ker g proof let g f restriction g image f map g l im f x v g v applying theorem g yield dim im f dim im g dim ker g im g g v x v im f im g f ker g v im f g v im f ker g imply assertion note theorem v w f idv g l v x give dim im g dim v dim ker g equivalent theorem interpret matrix k n b k n linear map theorem implies equation rank b rank dim im ker b special case k r b following result corollary rn rank rank proof let w ωn im ker w ay vector multiplying equation left using w ker obtain w ay implies ay w w n ω since hold w im ker linear map exercise following exercise k arbitrary field consider linear map given matrix determine ker dim ker dim im construct map f l v w linearly independent vector vr v image f f vr w linearly dependent map f r r αn n nαn n called derivative polynomial p r respect variable show f determine ker f im f base let f l r r matrix representation f determine f base b determine coordinate f respect basis construct map f l k k following property f pq f p q p f q p q k f map uniquely determined property map property let α k k n n show map k k p p α k k p p linear justify name evaluation homomorphism map let g l n k show map f k n n k n n isomorphism linear map matrix let k field let k n n consider map f k k x x ax f linear map show f let v q-vector space basis vn let f l v v defined f v j v j v j n j vn determine f b let wn w j j j show basis determine coordinate transformation matrix idv idv well matrix representation f f extend theorem consistently case w property matrix g f g f consider map f r r αn n αn n n show f linear determine ker f im f b choose base two vector space verify choice rank f dim im f hold let αn r n pairwise distinct number let n polynomial r defined pj n j αk α j αk j show set b pn basis r basis called lagrange r b show corresponding coordinate map given joseph-louis de lagrange linear map b r p p p αn hint use exercise b verify different path commutative diagram vector space base linear map f q q f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.1 Linear Forms and Dual Spaces</td>\n",
       "      <td></td>\n",
       "      <td>chapter linear form bilinear form chapter study different class map one two k space one dimensional k space defined field k map play important role many area mathematics including analysis functional analysis solution differential equation also essential development book using bilinear sesquilinear form introduced chapter define study euclidean unitary vector space chap linear form dual space used existence proof jordan canonical form chap linear form dual space start set linear map k space vector space k definition v k space f l v k called linear form k space v l v k called dual space linear form sometimes called linear functional one-form stress linearly map one dimensional vector space example v r-vector space continuous real valued function real interval α β γ α β two map f v r g g γ β f v r g g x x α linear form springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi linear form bilinear form dim v n dim v n theorem let vn basis v let basis k space k f v f vi αi αi k n f αn k n element v n λi vi v f v f n λi vi λi f vi λi αi αn λn n n n f v identified isomorphic vector space k k given basis finite dimensional vector space v construct special uniquely determined basis dual space v theorem v k space basis b vn exists unique basis b v v j δi j j n called dual basis b proof theorem unique linear map v k constructed prescribing image given basis b thus n exists unique map l v k v j δi j j remains show b basis v λn k n λi v v j n λi v j λ j j thus linearly independent dim v n implies b basis v cp exercise example consider v k canonical basis b en en dual basis b ei e j δi j show ei b eit k n n linear form dual space definition let v w k space respective dual space v w let f l v w f w v h f h h f called dual map f next derive property dual map lemma v w x k space following assertion hold f l v w dual map f linear hence f l w v f l v w g l w x g f l x v g f f f l v w bijective f l w v bijective f f proof h h w k f h h h h f h f h f h f h f f h f h exercise following theorem show concept dual map transposed matrix closely related theorem let v w finite dimensional k space base respectively let corresponding dual base f l v w f f proof vm wn let let f ai j k n f v j n ai j wi j f bi j k n f w bi j j n linear form bilinear form every pair k k n n wi n wi f f bik bik used definition dual map well wi δki close relationship transposed matrix dual map author call dual map f transpose linear map f applied matrix lemma theorem yield following rule known chap ab b k n b k g l n k example two base element corresponding dual base given r r r r matrix representation map linear map α f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.2 Bilinear Forms</td>\n",
       "      <td></td>\n",
       "      <td>linear form dual space f f bilinear form consider special map pair k space k space definition let v w k space map β v w k called bilinear form v w β w β w β w β v β v β v β λv w β v λw λβ v w hold v v w w λ k bilinear form β called non-degenerate first variable β v w w w implies v analogously called non-degenerate second variable β v w v v implies w β non-degenerate variable β called non-degenerate space v w called dual pair respect β v w β called bilinear form additionally β v w β w v hold v w v β called symmetric otherwise β called nonsymmetric example k n β k k k v w w av bilinear form k k non-degenerate n g l n k cp exercise bilinear form β r x x degenerate variable x β x β x x set r x β x x equal solution set quadratic equation two variable geometrically set given two straight line cartesian coordinate system linear form bilinear form v k space β v v k v f f v bilinear form v v since β f f f f β f β f β v f f f f v f v f v β v f β v f β λv f f λv λ f v λβ v f λ f v β v λ f hold v v f f f v λ k bilinear form non-degenerate thus v v dual pair respect β cp exercise case dim v n definition let v w k space base vm wn respectively β bilinear form v w β bi j k n bi j β v j wi called matrix representation β respect base v λ j v j v w μi wi w β v w n λ j μi β v j wi n μi bi j λ j w β v used coordinate map lemma example em n en n canonical base k k respectively β bilinear form example ai j k n β bi j n ai j ei n ae bi j β e j ei j hence β following result show symmetric bilinear form symmetric matrix representation lemma bilinear form β finite dimensional vector space v following statement equivalent β symmetric every basis b v matrix β symmetric exists basis b v β symmetric bilinear form proof exercise analyze effect basis change matrix representation bilinear form theorem let v w finite dimensional k space base v β bilinear form v w β idw β idv proof let vm vm wn n w vm p p pi j idv vm wn w n q q qi j idw β vj w bi j bi j β β v j wi β pk j vk n w n n β vk w pk j pk j j β qni pm j implies β q β p hence assertion follows v w two base v obtain following special case theorem β idv β idv two matrix representation β β β case congruent formally define follows definition two matrix b k n n exists matrix z g l n k b z az b called congruent lemma congruence equivalence relation set k n n proof exercise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.3 Sesquilinear Forms</td>\n",
       "      <td></td>\n",
       "      <td>linear form bilinear form sesquilinear form complex vector space introduce another special class form definition let v w c-vector space map v w c called sesquilinear form v w w w w λv w λs v w v v v v λw λs v w hold v v w w λ v w called sesquilinear form additionally v w w v hold v w v called prefix sesqui latin mean one half note sesquilinear form linear first variable semilinear half linear second variable following result characterizes hermitian sesquilinear form lemma sesquilinear form c-vector space v hermitian v v r v proof hermitian particular v v v v v v thus v v hand v w v definition v w v w v v v w w v w w v iw v iw v v w v v w w w first equation implies v w w v r since v w v w v v w w r assumption second equation implies analogously w v v w therefore v w w v v w w v v w w v v w w v multiplying second equation adding resulting equation first obtain v w w v corollary sesquilinear form c-vector space v v w v w v w v iw v iw v v w w v w charles hermite sesquilinear form proof result follows multiplication adding result corollary show sesquilinear form c-vector space v uniquely determined value v v v definition hermitian transpose ai j cn matrix h j cm n h called hermitian matrix real entry obviously h thus real symmetric matrix also hermitian ai j cn n hermitian particular aii ii n hermitian matrix real diagonal entry hermitian transposition satisfies similar rule usual transposition cp lemma cn b cm λ c following assertion lemma hold h h h ah λ h λ h ab h b h h proof exercise example cn map c v w w h av sesquilinear form matrix representation sesquilinear form defined analogously matrix representation bilinear form cp definition definition let v w c-vector space base vm wn respectively sesquilinear form v w bi j cn bi j v j wi called matrix representation respect base example em n en n canonical base respectively sesquilinear form example ai j cn bi j linear form bilinear form n bi j e ei n ae ai j ei n ae j ei j j hence exercise following exercise k arbitrary field let v finite dimensional k space v show f v f v v consider basis b vector space r compute dual basis b b let v n-dimensional k space let basis v prove disprove exists unique basis vn v v j δi j let v finite dimensional k space let f g v f show g λ f λ k hold ker f ker g possible omit assumption f let v k space let u subspace set u f v f u u u called annihilator u show following assertion u subspace v b subspace v c w k space f l v w ker f im f prove lemma let v w k space show set bilinear form v w operation v w v w v w λ β v w λ β v w k space let v w k space base vm wn corresponding dual base respectively j n let βi j v w k v w v w w show βi j bilinear form v w sesquilinear form b show set βi j j n basis k space bilinear form v w cp exercise determine dimension space let v r-vector space continuous real valued function real interval α β show β f x g x x β v v r f g α symmetric bilinear form β degenerate show map β example bilinear form show non-degenerate n g l n k let v finite dimensional k space show v v dual pair respect bilinear form β example bilinear form β non-degenerate let v finite dimensional k space let u v w v subspace dim u dim w prove disprove space u w form dual pair respect bilinear form β u w k v h h v let v w finite dimensional k space base respectively let β bilinear form v show following statement equivalent β invertible β degenerate second variable β degenerate first variable b conclude β non-degenerate β invertible prove lemma prove lemma bilinear form β k space v map qβ v k v β v v called quadratic form induced β show following assertion k β symmetric β v w qβ v v qβ w hold v w show sesquilinear form c-vector space v satisfies polarization identity v w v v v v v v v v v w consider following map c x x b x c x linear form bilinear form x bilinear form sesquilinear form test whether bilinear form symmetric sesquilinear form hermitian derive corresponding matrix representation respect canonical basis basis prove lemma let cn n hermitian show v w w h av hermitian sesquilinear form let v finite dimensional c-vector space basis b let sesquilinear form show hermitian hermitian show following assertion b cn n h eigenvalue purely imaginary b h trace trace c h b h b trace ab trace b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.1 Scalar Products and Norms</td>\n",
       "      <td></td>\n",
       "      <td>chapter euclidean unitary vector space chapter study vector space field r using definition bilinear sesquilinear form introduce scalar product vector space scalar product allow extension well-known concept elementary geometry length angle abstract real complex vector space particular lead idea orthogonality orthonormal base vector space example importance concept many application study least-squares approximation scalar product norm start definition scalar product euclidean unitary vector space definition let v k space either k r k map v v k v w called scalar product v following property hold k r symmetric bilinear form k c hermitian sesquilinear form positive definite hold v v equality v r-vector space scalar product called euclidean vector c-vector space scalar product called unitary vector space scalar product sometimes called inner product note nonnegative real also v c-vector space easy see subspace u euclid alexandria approx bc springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi euclidean unitary vector space euclidean unitary vector space v euclidean unitary vector space respectively scalar product space v restricted subspace u example scalar product given w called standard scalar product scalar product given w h called standard scalar product k r k c spur b h scalar product k n scalar product vector space continuous real valued function real interval α β given f α β f x g x x show use euclidean unitary structure vector space order introduce geometric concept length vector angle vector motivation general concept length absolute value real number map r r x map following property λ x x r equality x x property generalized real complex vector space follows definition let v k space either k r k map v r v v scalar product norm called norm v v w v λ k following property hold λv v v equality v v w v w triangle inequality k space norm defined called normed space example standard scalar product v v v defines norm called euclidean norm standard scalar product v v h v defines norm called euclidean norm common terminology although space unitary euclidean k r k c f trace h n j norm k n called frobenius k n frobenius norm equal euclidean norm k moreover frobenius norm k n equal euclidean norm k k nm identify vector space via isomorphism obviously f f h f k n v vector space continuous real valued function real interval α β β f x x f f f α norm v called l let k r k c let p r p given v νn k p-norm k defined v p n ferdinand georg frobenius p p euclidean unitary vector space p euclidean norm k norm typically omit index write instead taking limit p obtain k given v max following figure illustrate unit circle respect p-norm set v v p p p p k r k c p-norm k n defined p sup av p v p use p-norm k denominator p-norm k numerator notation sup mean supremum least upper bound known analysis one show supremum attained vector v thus may write max instead sup definition particular ai j k n max max n j j norm called maximum column sum maximum row sum norm k n respectively easily see h h however matrix thus matrix satisfies matrix considered chap scalar product norm norm example form v given scalar product show map v always defines norm proof based following theorem theorem v euclidean unitary vector space scalar product v w v equality v w linearly dependent proof inequality trivial w thus let w let λ λw v λ implies v w linearly dependent v λw scalar λ hence λλ hand let w v w linearly dependent w define λ get λw v since scalar product positive definite v λw thus v w linearly dependent inequality called cauchy-schwarz important tool analysis particular estimation approximation interpolation error augustin louis cauchy hermann amandus schwarz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.2 Orthogonality</td>\n",
       "      <td></td>\n",
       "      <td>euclidean unitary vector space corollary v euclidean unitary vector space scalar product map v r v v norm v called norm induced scalar product proof prove three defining property norm since positive definite v equality v v v λ k euclidean case k r unitary case k c λv hence λv v order show triangle inequality use cauchy-schwarz inequality fact z every complex number z v w v w v v w v w v v w w v w thus v w v w orthogonality use scalar product introduce angle vector motivation consider euclidean vector space standard scalar product induced euclidean norm v cauchy-schwarz inequality show v w v w v w angle v w uniquely determined real number ϕ π co ϕ v w orthogonality vector v w orthogonal ϕ co ϕ thus v w orthogonal elementary calculation lead cosine theorem triangle w v v w v w co ϕ v w orthogonal cosine theorem implies pythagorean v w following figure illustrate cosine theorem pythagorean theorem vector following definition generalize idea angle orthogonality definition let v euclidean unitary vector space scalar product euclidean case angle two vector v w v uniquely determined real number ϕ π co ϕ v w two vector v w v called orthogonal basis vn v called orthogonal basis v j j n j furthermore vi n v norm induced scalar product vn called orthonormal basis orthonormal basis therefore v j δi j pythagoras samos approx bc euclidean unitary vector space note term defined respect given scalar product different scalar product yield different angle vector particular orthogonality two given vector may lost consider different scalar product example standard basis vector orthogonal orthonormal basis respect standard scalar product cp example consider symmetric invertible matrix defines symmetric non-degenerate bilinear form v w w av cp example bilinear form positive definite since v v av bilinear form therefore scalar product denote denote induced norm respect scalar product vector satisfy clearly orthonormal basis r respect also note hand vector satisfy hence orthonormal basis respect scalar product show every finite dimensional euclidean unitary vector space orthonormal basis theorem let v euclidean unitary vector space basis vn exists orthonormal basis u u n v span u u k span vk k n orthogonality proof give proof induction dim v n set u u u orthonormal basis v span u span let assertion hold n let dim v n let basis vn span vn n-dimensional subspace induction hypothesis exists orthonormal basis u u n vn span u u k span vk k define u n u k k u u u since vn span u u n must u lemma yield span u u span j n u u j u u j u u n u k k u j u j u j u j u finally u u u completes proof proof theorem show given basis vn orthonormalized transformed orthonormal basis u u n span u u k span vk k resulting algorithm called gram-schmidt method algorithm given basis vn set u j n set u v u u jørgen j u k k u pedersen gram erhard schmidt euclidean unitary vector space slight reordering combination step gram-schmidt method yield u u vn u u u n u n n n un upper triangular matrix right hand side coordinate transformation matrix basis vn basis u u n v cp theorem thus shown following result theorem v finite dimensional euclidean unitary vector space given basis gram-schmidt method applied yield orthonormal basis v idv invertible upper triangular matrix consider m-dimensional subspace standard scalar product write vector orthonormal basis qm column matrix q qm obtain real case q q qit q j j qi δ ji im analogously complex case q h q qih q j j qi δ ji im hand q q im q h q im matrix q rn q cn respectively column q form orthonormal basis respect standard scalar product m-dimensional subspace respectively matrix version theorem therefore formulated follows corollary let k r k c let vm k linearly independent exists matrix q k n column orthonormal respect standard scalar product k q q im k r q h q im k c upper triangular matrix r g l k vm q factorization called q r-decomposition matrix vm q r-decomposition many application numerical mathematics cp example lemma let k r k c let q k n matrix orthonormal column respect standard scalar product k v qv hold v k euclidean norm k k orthogonality proof k c v v h v v h q h q v qv proof k r analogous introduce two important class matrix definition matrix q rn n whose column form orthonormal basis respect standard scalar product called orthogonal matrix q cn n whose column form orthonormal basis respect standard scalar product called unitary matrix q qn rn n therefore orthogonal q q qit q j j qi δ ji particular orthogonal matrix q invertible q q cp corollary equation q q mean n row q form orthonormal basis n respect scalar product wv analogously unitary matrix q cn n invertible q q h h q q q q h n column q form orthonormal basis n lemma set n orthogonal u n unitary n matrix form subgroup g l n r g l n c respectively proof consider n proof u n analogous since every orthogonal matrix invertible n g l n r identity matrix orthogonal hence n ø q n also q q n since q q q q finally q q n q q q q q q q q q q thus q q n example many application measurement sample lead data set represented tuples τi μi τm pairwise distinct measurement point μm corresponding measurement order approximate given data set simple model one try construct polynomial p small degree value p p τm close possible measurement μm simplest case real polynomial degree geometrically corresponds construction straight line minimal distance euclidean unitary vector space given point shown figure cp sect many possibility measure distance following describe one detail use gram-schmidt method q r-decomposition construction straight line statistic method called linear regression real polynomial degree form p αt β looking coefficient α β r p τi ατi β μi using matrix write problem α v v α β β μm τm mentioned different possibility interpreting symbol particular different norm measure distance given value μm polynomial value p p τm use euclidean norm consider minimization problem α min β α vector linearly independent since entry pairwise distinct entry equal let r q r-decomposition extend vector orthonormal basis qm q qm rm orthogonal matrix orthogonality α min v min v α β α r α β r α min q β α r α q min q β α r α β min α qt used q q im qv v v upper triangular matrix r invertible thus minimization problem solved q α r β using definition euclidean norm write minimizing property polynomial p αt β α p τi μi β min α ατi β μi since polynomial p minimizes sum square distance measurement μi polynomial value p τi polynomial yield least square approximation measurement value consider example sect four quarter year company profit million euro assumption profit grows linearly like straight line goal estimate profit last quarter following year given data lead approximation problem α α β β euclidean unitary vector space numerical computation q r-decomposition yield α β resulting profit estimate last quarter following year p million euro matlab-minute example one could imagine profit grows quadratically instead linearly determine analogously procedure example polynomial p αt βt γ solves least square problem p τi μi min α β βτi γ μi use matlab command qr computing q r-decomposition determine estimated profit last quarter following year analyze property orthonormal base detail lemma v euclidean unitary vector space scalar product orthonormal basis u u n n u v proof every v v exist uniquely determined coordinate λn n λi u every j n u j λi u j v λj coordinate u n v respect orthonormal basis u u n often called n fourier coefficient v respect basis representation v u called abstract fourier expansion v given orthonormal basis jean baptiste joseph fourier orthogonality corollary v euclidean unitary vector space scalar product orthonormal basis u u n following assertion hold n n u u u v w v parseval n u v v bessel proof v n u thus n n n u w u u u special case v bessel identity every vector v v satisfies v n u max u norm induced scalar product absolute value coordinate v respect orthonormal basis v therefore bounded norm property hold general basis example consider v standard scalar product euclidean norm every real ε set ε basis every vector v v ε ε ε moderate number small large numerical algorithm situation lead significant problem due roundoff error avoided orthonormal base used marc-antoine friedrich parseval wilhelm bessel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.3 The Vector Product in mathbbR3,1</td>\n",
       "      <td></td>\n",
       "      <td>euclidean unitary vector space definition let v euclidean unitary vector space scalar product let u v subspace u v v u u called orthogonal complement u v lemma orthogonal complement u subspace proof exercise lemma v n-dimensional euclidean unitary vector space u v m-dimensional subspace dim u n v u u proof know n cp lemma n u v thus u v v v u v assertion trivial thus let n let u u orthonormal basis u extend basis basis v apply gram-schmidt method order obtain orthonormal basis u u u u n span u u n u therefore v u u w u u hence w since scalar product positive definite thus u u implies v u u dim u n cp theorem particular u span u u n vector product section consider product vector space frequently used physic electrical engineering definition vector product cross product map v w v w contrast scalar product vector product two element vector space scalar vector using canonical basis vector vector product write vector product ω ω det det v w det ω ω ω lemma vector product linear component v w following property hold v w v vector product anti commutative alternating v w v w standard scalar product euclidean norm v v standard scalar product proof exercise cauchy-schwarz inequality follows v w hold v w linearly dependent obtain μw v v v arbitrary λ μ v w linearly independent product v w orthogonal plane origin spanned v w v w λv μw λ μ r geometrically two possibility position three vector v w v left side figure correspond right-handed orientation usual coordinate system canonical basis vector associated thumb index finger middle finger right hand motivates name right-hand rule order explain detail one need introduce concept orientation omit ϕ π angle vector v w v w co ϕ euclidean unitary vector space cp definition write lemma v w v w ϕ v w ϕ v w sin ϕ geometric interpretation equation following norm vector product v w equal area parallelogram spanned v interpretation illustrated following figure exercise let v finite dimensional real complex vector space show exists scalar product show map defined example scalar product corresponding vector space let arbitrary scalar product show exists matrix rn n w av v w let v finite dimensional c-vector space let scalar product v following property v w v satisfy v w also v w prove disprove exists real scalar λ v w v w v w show map defined example norm corresponding vector space show n max j max j ai j k n k r k c cp example sketch matrix example p set av v v p let v euclidean unitary vector space let norm induced scalar product show satisfies parallelogram identity v w v w vector product let v k space k r k c scalar product induced norm show v w v orthogonal respect v λw v λw λ k exist scalar product cp example induced norm scalar product show inequality n αi βi n γi αi n βi γi hold arbitrary real number αn βn positive real number γn let v finite dimensional euclidean unitary vector space scalar product let f v v map f v f w v w show f isomorphism let v unitary vector space suppose f l v v satisfies f v v prove disprove f statement also hold euclidean vector space let diag dn rn n dn show w dv scalar product analyze property scalar product violated least one di zero di nonzero different sign orthonormalize following basis vector space respect scalar product trace b h let q rn n orthogonal let q cn n unitary matrix possible value det q let u let h u ut u uu rn n show n column h u form orthonormal basis respect standard scalar product matrix form called householder study detail example prove lemma alston scott householder pioneer numerical linear algebra euclidean unitary vector space let analyze whether vector orthonormal respect standard scalar product compute orthogonal complement span let v euclidean unitary vector space scalar product let u u k v let u span u u k show v v v u u j j unitary vector space standard scalar product let given determine orthonormal basis span prove lemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13 Adjoints of Linear Maps</td>\n",
       "      <td>13.1 Basic Definitions and Properties</td>\n",
       "      <td></td>\n",
       "      <td>chapter adjoints linear map chapter introduce adjoints linear map sense represent generalization hermitian transpose matrix matrix symmetric hermitian equal hermitian transpose analogous way endomorphism selfadjoint equal adjoint endomorphism set symmetric hermitian matrix selfadjoint endomorphisms form certain vector space play key role proof fundamental theorem algebra chap special property selfadjoint endomorphisms studied chap basic definition property chap considered euclidean unitary vector space hence vector space field r let v w vector space general field k let β bilinear form v every fixed vector v v map βv w k w β v w linear form thus assign every v v vector βv w defines map β v w v βv analogously define map β w v w βw βw v k defined v β v w every w springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi adjoints linear map lemma map β β defined respectively linear β l v w β l w v dim v dim w n β non-degenerate cp definition β β bijective thus isomorphism proof prove assertion map β proof β analogous first show linearity let v k every w w β w β w β w β w β w β w β β w hence β β β therefore β l v w let dim v dim w n let β non-degenerate show β l v w injective lemma hold ker β v ker β β v βv w thus βv w β v w w since β non-degenerate v finally dim v dim w dim w dim w imply dim v dim w β bijective cp corollary next discus existence adjoint map theorem v w k space dim v dim w n β non-degenerate bilinear form v w following assertion hold every f l v v exists uniquely determined g l w w β f v w β v g w v v w map g called right adjoint f respect β every h l w w exists uniquely determined k l v v β v h w β k v w v v w map k called left adjoint h respect β proof show proof analogous let v dual space v let f l v v dual map f let β l w v since β non-degenerate β bijective lemma define g β f β l w w basic definition property v v w w β v g w β v β f β w β β f β w v β β f β w v β β β w f v β w f v β f v w recall dual map satisfies f β w β w f remains show uniqueness let g l w w β v g w β f v w v v w β v g w β v g w hence β v g g w v v w since β non-degenerate second variable g g w w w g example let v w k β v w w bv matrix b g l n k β non-degenerate cp example consider linear map f v v v fv matrix f k n n linear map h w w w h w matrix h k n n βv w k w w bv β v w v bv β w v w w b identified isomorphic vector space w k n respectively v k n g l w w right adjoint f respect β β f v w w b f v w b fv β v g w g w bv v v w represent linear map g via multiplication matrix g k n n g w gw w b fv w g bv v w k hence b f g b since b invertible unique right adjoint given g b f b b f b analogously left adjoint k l v v h respect β obtain equation β v h w h w bv w h bv β k v w w bk v v v w k v lv matrix l k n n obtain h b b l hence l b h b adjoints linear map v finite dimensional β non-degenerate bilinear form v theorem every f l v v unique right adjoint g unique left adjoint k β f v w β v g w β v f w β k v w v w β symmetric β v w β w v hold v w v yield β v g w β f v w β w f v β k w v β v k w therefore β v g k w v w v hence g k since β non-degenerate thus proved following result corollary β symmetric non-degenerate bilinear form finite dimensional k space v every f l v v exists unique g l v v β f v w β v g w β v f w β g v w v w definition scalar product euclidean vector space symmetric nondegenerate bilinear form cp definition lead following corollary corollary v finite dimensional euclidean vector space scalar product every f l v v exists unique f ad l v v f v w v f ad w v f w f ad v w v w map f ad called adjoint f respect order determine whether given map g l v v unique adjoint f l v v one two condition verified f g l v v equation f v w v g w hold v w v also v f w f w v w g v g v w v w v used symmetry scalar product similarly v f w g v w hold v w v also f v w v g w v w v basic definition property example consider euclidean vector space scalar product v w w dv linear map f v fv f v w f v w w fv w f dv f w dv v f ad w thus f ad v f dv v used symmetric show uniquely determined adjoint map also exist unitary case however conclude directly corollary since scalar product c-vector space symmetric bilinear form hermitian sesquilinear form order show existence adjoint map unitary case construct explicitly construction work also euclidean case let v unitary vector space scalar product let u u n orthonormal basis given f l v v define map n v f u u g v v v v w v λ μ c n g λv μw n λv μw f u u λg v μg w λ v f u u μ v f u u adjoints linear map hence g l v v let v n λi u v w v n n w f u j u j λi u v g w n n λi w f u λi f u w f v w furthermore v f w f w v w g v g v w v w g l v v satisfies f v w v g w v w v g g since scalar product positive definite therefore formulate following result analogously corollary corollary v finite dimensional unitary vector space scalar product every f l v v exists unique f ad l v v f v w v f ad w v f w f ad v w v w map f ad called adjoint f respect euclidean case validity one two equation v w v implies validity v w example consider unitary vector space scalar product v w w h dv linear map f v fv f v w f v w w h fv w h f dv f h h w h dv v f ad w thus f ad v f h dv v basic definition property used real symmetric next investigate property adjoint map lemma let v finite dimensional euclidean unitary vector space f f l v v k k r euclidean k c unitary case f f ad f f euclidean case map f f ad therefore linear unity case semilinear idv ad idv every f l v v f ad ad f f f l v v f f ad f f proof v w v k f f v w f v w f v w v f w v f w v f w f w v f f w thus f f ad f f v w v idv v w v w v idv w thus idv ad idv v w v f ad v w v f w thus f ad ad f v w v f f v w f f v w f v f w v f f w v f f w thus f f ad f f following result show relation image kernel endomorphism adjoint theorem v finite dimensional euclidean unitary vector space f l v v following assertion hold adjoints linear map ker f ad im f ker f im f ad proof w ker f ad f ad w v f ad w f v w v v hence w im f hand w im f f v w v f ad w v since non-degenerate f ad w hence w ker f ad using f ad ad f get ker f ker f ad ad im f ad example consider unitary vector space standard scalar product linear map f v fv f f ad v f h v f h matrix f f h rank therefore dim ker f dim ker f ad simple calculation show ker f span ker f ad span dimension formula linear map implies dim im f dim im f ad matrix f f h see im f span im f ad span equation ker f ad im f ker f im f ad verified direct computation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13 Adjoints of Linear Maps</td>\n",
       "      <td>13.2 Adjoint Endomorphisms and Matrices</td>\n",
       "      <td></td>\n",
       "      <td>adjoint endomorphisms matrix adjoint endomorphisms matrix study relation matrix representation endomorphism adjoint let v finite dimensional unitary vector space scalar product let f l v v orthonormal basis b u u n v let f b b ai j cn n n f u j ak j u k j n hence n ak j u k u ai j j f u j u f ad b b bi j cn n n f ad u j bk j u k j n bi j f ad u j u u j f u f u u j ji thus f ad b b f b b h hold finite dimensional euclidean vector space omit complex conjugation therefore shown following result theorem v finite dimensional euclidean unitary vector space orthonormal basis b f l v v f ad b b f b b h euclidean case f b b h f b b important special class selfadjoint endomorphisms definition let v finite dimensional euclidean unitary vector space endomorphism f l v v called selfadjoint f f ad trivial example selfadjoint endomorphism l v v f idv corollary v finite dimensional euclidean vector space f l v v selfadjoint b orthonormal basis v f b b symmetric matrix adjoints linear map v finite dimensional unitary vector space f l v v selfadjoint b orthonormal basis v f b b hermitian matrix selfadjoint endomorphisms form vector space however one careful use appropriate field vector space defined particular set selfadjoint endomorphisms unitary vector space v form c-vector space f f ad l v v f ad f ad f f cp lemma similarly hermitian matrix cn n form c-vector space h cn n hermitian h h lemma v n-dimensional euclidean vector space set selfadjoint endomorphisms f l v v f f ad form r-vector space dimension n n v n-dimensional unitary vector space set selfadjoint endomorphisms f l v v f f ad form r-vector space dimension proof exercise matrix cn n called complex symmetric unlike hermitian matrix complex symmetric matrix form c-vector space lemma set complex symmetric matrix cn n form c-vector space dimension n n proof exercise lemma used chap proof fundamental theorem algebra exercise let β v w w bv b diag defined v w consider linear map f v fv h w h w h determine βv β β well right adjoint f left adjoint h respect β let v v w w two finite dimensional euclidean vector space let f l v w show exists unique g l w v f v w w v g w v v v w let v w w bv v w adjoint endomorphisms matrix show v w w bv scalar product b using scalar product determine adjoint map f ad f v fv f c investigate property f need satisfy f selfadjoint let n f xn determine adjoint f ad f respect standard scalar product let v finite dimensional euclidean unitary vector space let f l v v show ker f ad f ker f im f ad f im f ad let v finite dimensional euclidean unitary vector space let u v subspace let f l v v f u u show f ad u u let v finite dimensional euclidean unitary vector space let f l v v v show v im f v ker f ad matrix version cn n b linear system equation ax b solution b l h let v finite dimensional euclidean unitary vector space let f g l v v selfadjoint show f g selfadjoint f g commute f g g f let v finite dimensional unitary vector space let f l v v show f selfadjoint f v v r hold v let v finite dimensional euclidean unitary vector space let f l v v projection f satisfies f f show f selfadjoint ker f im f v w hold v ker f w im f let v finite dimensional euclidean unitary vector space let f g l v v show g ad f l v v v w hold v im f w im g two polynomial p q r let p q dt p q show defines scalar product r b consider map n f r r n αi iαi determine f ad ker f ad im f ker f ad im f prove lemma prove lemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.1 Basic Definitions and Properties</td>\n",
       "      <td></td>\n",
       "      <td>chapter eigenvalue endomorphisms previous chapter already studied eigenvalue eigenvectors matrix chapter generalize concept endomorphisms investigate endomorphisms finite dimensional vector space represented diagonal matrix upper triangular matrix representation easily read important information endomorphism particular eigenvalue basic definition property first consider arbitrary vector space concentrate finite dimensional case definition let v k space f l v v λ k v v satisfy f v λv λ called eigenvalue f v called eigenvector f corresponding λ definition v eigenvector eigenvalue λ may occur cp example following definition equation f v λv written λv f v λidv f v hence λ k eigenvalue f ker λidv f springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi eigenvalue endomorphisms already know kernel endomorphism v form subspace v cp lemma hold particular ker λidv f definition v k space λ k eigenvalue f l v v subspace v f λ ker λidv f called eigenspace f corresponding λ g λ f dim v f λ called geometric multiplicity eigenvalue λ definition eigenspace v f λ spanned eigenvectors f corresponding eigenvalue λ v f λ finite dimensional g λ f dim v f λ equal maximal number linearly independent eigenvectors f corresponding λ definition let v k space let u v subspace let f l v v f u u f u u hold u u u called f subspace important example f subspace eigenspaces f lemma v k space λ k eigenvalue f l v v v f λ f subspace proof every v v f λ f v λv v f λ consider finite dimensional vector space discus relationship eigenvalue f eigenvalue matrix representation f respect given basis lemma v finite dimensional k space f l v v following statement equivalent λ k eigenvalue f λ k eigenvalue matrix f b b every basis b proof let λ k eigenvalue f let b vn arbitrary basis v v eigenvector f corresponding eigenvalue λ f v λv exist unique coordinate μn k equal zero v μ j v j using obtain f b b b f v b λv b v λ μn μn basic definition property thus λ eigenvalue f b b hand f b b μn λ μn given arbitrary basis b vn v set μn v μ j v j v f v μ j f v j f f vn vn f b b μn μn vn λv n μn λ eigenvalue f lemma implies eigenvalue f root characteristic polynomial matrix f b b cp theorem however hold general matrix representation form f b b b b two different base general two matrix f b b idv b b f b b f b b eigenvalue example consider vector space base endomorphism f v fv f matrix representation f b b f b b det f b b thus f eigenvalue hand characteristic polynomial f b b matrix eigenvalue two different base b b v matrix f b b f b b similar cp discussion following corollary theorem shown eigenvalue endomorphisms similar matrix characteristic polynomial justifies following definition definition n n v n-dimensional k space basis b f l v v p f det f b b k called characteristic polynomial f characteristic polynomial p f always monic polynomial deg p f n dim v discussed p f independent choice basis scalar λ k eigenvalue f λ root p f p f λ shown example real vector space dimension least two exist endomorphisms eigenvalue λ root p f p f λ q monic polynomial q k linear factor λ divide polynomial p f show formally corollary also q λ q λ q monic polynomial q continue p f λ g q k thus p f λ g k g λ lead following definition definition let v finite dimensional k space let f l v v eigenvalue λ k characteristic polynomial f form p f λ g g k g λ called algebraic multiplicity eigenvalue λ f denoted λ f λk pairwise distinct eigenvalue f corresponding algebraic multiplicity f λk f dim v n f λk f n since deg p f dim v example endomorphism f v fv f characteristic polynomial p f real root p f f dim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.2 Diagonalizability</td>\n",
       "      <td></td>\n",
       "      <td>basic definition property lemma v finite dimensional k space f l v v g λ f λ f every eigenvalue λ f proof let λ k eigenvalue f geometric multiplicity g λ f exist linear independent eigenvectors vm v f corresponding eigenvalue λ dim v eigenvectors form basis b dim v n extend eigenvectors basis b vm vn f v j λv j j therefore f b b λim z two matrix z k z k using lemma obtain p f det f b b λ det z implies λ f g λ f following try find basis v eigenvalue given endomorphism f read easily matrix representation easiest form matrix sense diagonal triangular matrix since eigenvalue diagonal entry diagonalizability section analyze given endomorphism diagonal matrix representation formally define property follows definition let v finite dimensional k space endomorphism f l v v called diagonalizable exists basis b v f b b diagonal matrix accordingly matrix k n n diagonalizable exists matrix g l n k d diagonal matrix k n n order analyze diagonalizablility begin sufficient condition linear independence eigenvectors condition also hold v infinite dimensional lemma let v k space f l v v λk k k pairwise distinct eigenvalue f corresponding eigenvectors vk v vk linearly independent eigenvalue endomorphisms proof prove assertion induction let k let eigenvectors f corresponding eigenvalue let k applying f side equation well multiplying equation yield two equation subtracting second equation first get since also obtain since thus linearly independent proof inductive step analogous assume assertion hold k let pairwise distinct eigenvalue f corresponding eigenvectors let k satisfy μk vk applying f equation yield μk λk vk multiplication give μk vk subtracting equation previous one get μk λk vk since pairwise distinct vk linearly independent induction hypothesis obtain μk implies also linearly independent using result next show sum eigenspaces corresponding pairwise distinct eigenvalue direct cp theorem lemma let v k space f l v v λk k k pairwise distinct eigenvalue f corresponding eigenspaces satisfy k v f λi v f λ j k diagonalizability proof let fixed let k v v f λi v f λ j v j v j v f λ j j particular v v linear independence eigenvectors corresponding pairwise j distinct eigenvalue cp lemma implies v following theorem give necessary sufficient condition diagonalizability endomorphism finite dimensional vector space theorem v finite dimensional k space f l v v following statement equivalent f diagonalizable exists basis v consisting eigenvectors f characteristic polynomial p f decomposes n dim v linear factor k p f λn eigenvalue λn k f every eigenvalue λ j g λ j f λ j f proof f l v v diagonalizable exists basis b vn v scalar λn k f b b λn hence f v j λ j v j j scalar λn thus eigenvalue f corresponding eigenvectors vn hand exists basis b vn v consisting eigenvectors f f v j λ j v j j n scalar λn k corresponding eigenvalue hence f b b form let b vn basis v consisting eigenvectors f let λn k corresponding eigenvalue f b b form hence p f λn p f decomposes linear factor k eigenvalue endomorphisms still show g λ j f λ j f every eigenvalue λ j eigenvalue λ j algebraic multiplicity j λ j f λ j occurs j time diagonal diagonal matrix f b b hold exactly j vector basis b eigenvectors f corresponding eigenvalue λ j j linearly independent vector element eigenspace v f λ j hence dim v f λ j g λ j f j λ j f lemma know g λ j f λ j f thus g λ j f λ j f λk pairwise distinct eigenvalue f correspond let λ j f j k ing geometric algebraic multiplicity g λ j f respectively since p f decomposes linear factor k λ j f n dim v g λ j f λ j f j k implies k g λ j f n dim v lemma obtain cp also theorem v f v f λk λ j j k select base respective eigenspaces v f get basis v consists eigenvectors f theorem lemma imply important sufficient condition diagonalizability corollary v n-dimensional k space f l v v n pairwise distinct eigenvalue f diagonalizable condition n dim v pairwise distinct eigenvalue however necessary diagonalizability endomorphism simple counterexample identity idv n-fold eigenvalue idv b b hold every basis b hand exist endomorphisms multiple eigenvalue diagonalizable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.3 Triangulation and Schur's Theorem</td>\n",
       "      <td></td>\n",
       "      <td>diagonalizability example endomorphism f r r v fv f characteristic polynomial thus eigenvalue ker v f span thus g f f theorem f diagonalizable triangulation schur theorem property g λ j f λ j f hold every eigenvalue λ j f f diagonalizable however long characteristic polynomial p f decomposes linear factor find special basis b f b b triangular matrix theorem v finite dimensional k space f l v v following statement equivalent characteristic polynomial p f decomposes linear factor k exists basis b v f b b upper triangular f triangulated proof n dim v f b b ri j k n n upper triangular p f rnn show assertion induction n dim v case n trivial since f b b k suppose assertion hold n let dim v n assumption p f k eigenvalue f let v eigenvector corresponding eigenvalue extend vector basis b bw w span bw v span w f b b eigenvalue endomorphisms define h l w span g l w w h w j j g w j k j wk j n f w h w g w w w f b b h bw g bw bw consequently pg p f hence pg dim w n characteristic polynomial g l w w decomposes linear factor w w induction hypothesis exists basis bw upper triangular thus basis b v w g w bw bw matrix f upper triangular matrix version theorem read follows characteristic polynomial pa k n n decomposes linear factor k triangulated exists matrix g l n k r upper triangular matrix r k n n corollary let v finite dimensional euclidian unitary vector space f l v v p f decomposes r euclidian case case c unitary case linear factor exists orthonormal basis b v f b b upper triangular proof p f decomposes linear factor theorem exists basis v f upper triangular applying gram-schmidt method basis obtain orthonormal basis v idv upper triangular cp theorem f idv f idv idv f idv invertible upper triangular matrix form group respect matrix multiplication cp theorem thus matrix product right hand side upper triangular hence f upper triangular example consider euclidian vector space r scalar product p p q dt endomorphism triangulation schur theorem f r r f f polynomial eigenvectors f corresponding distinct eigenvalue thus b basis diagonal matrix note b orthonormal basis r f b b since particular since p f decomposes linear factor corollary guarantee existence orthonormal basis b f b b upper triangular proof implication theorem one chooses eigenvector f proceeds inductively order obtain triangulation f example let u use first vector vector eigenvector f norm corresponding eigenvalue r vector norm b orthonormal basis f b b upper triangular matrix construct vector orthogonalizing using gram-schmidt method lead triangulation f b b could also choose eigenvector f norm corresponding eigenvalue orthogonalizing vector lead second basis vector corresponding basis obtain triangulation f example show triangulation f element diagonal different different orthonormal base diagonal element except order uniquely determined since eigenvalue f detailed statement uniqueness given lemma next chapter prove fundamental theorem algebra state every non-constant polynomial c decomposes linear factor result following corollary known schur issai schur eigenvalue endomorphisms corollary v finite dimensional unitary vector space every endomorphism v unitarily triangulated f l v v exists orthonormal basis b v f b b upper triangular matrix f b b called schur form f v unitary vector space standard scalar product obtain following matrix version corollary corollary cn n exists unitary matrix q cn n q r q h upper triangular matrix r cn n matrix r called schur form following result show schur form matrix cn n n pairwise distinct eigenvalue almost unique lemma let cn n n pairwise distinct eigenvalue let cn n two schur form diagonal equal u u h unitary diagonal matrix u proof exercise survey result unitary similarity matrix found article matlab-minute consider n matrix n n n cn n n n compute schur form using command u r schur n eigenvalue formulate conjecture rank general prove conjecture exercise following exercise k arbitrary field let v vector space let f l v v eigenvalue λ show im λidv f f subspace let v finite dimensional vector space let f l v v bijective show f f invariant subspace triangulation schur theorem let v n-dimensional k space let f l v v let u m-dimensional f subspace show basis b v exists f b b matrix k k k let k r c f k k v fv f compute p f determine k r k c eigenvalue f algebraic geometric multiplicity well associated eigenspaces consider vector space r standard basis n endomorphism n f r r n αi αi dt compute p f eigenvalue f algebraic geometric multiplicity examine whether f diagonalizable change one considers map kth derivative k n examine whether following matrix b q c diagonalizable set diagonalizable invertible matrix subgroup g l n k let n consider r-vector space r map f r r p p p show f linear n f diagonalizable let v r-vector space basis vn examine whether following endomorphisms diagonalizable f v j v j v j n f vn vn eigenvalue endomorphisms b f v j jv j v j n f vn nvn let v finite dimensional euclidian vector space let f l v v f f ad l v v show f f diagonalizable let v c-vector space let f l v v f determine possible eigenvalue f let v finite dimensional vector space f l v v show p f f l v v let v finite dimensional k space let f l v v p μm k show p f bijective μm eigenvalue determine condition entry matrix αβ γ δ diagonalizable triangulated determine endomorphism r diagonalizable triangulated let v vector space dim v show f l v v triangulated exist subspace vn v v j v j n b dim v j j j n c v j f j prove lemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.1 Polynomials</td>\n",
       "      <td></td>\n",
       "      <td>chapter polynomial fundamental theorem algebra chapter discus polynomial detail consider division polynomial derive classical result polynomial algebra including factorization irreducible factor also prove fundamental theorem algebra state every non-constant polynomial complex number least one complex root implies every complex matrix every endomorphism finite dimensional complex vector space least one eigenvalue polynomial let u recall important term context polynomial k field p αn n n αn k polynomial k variable set k polynomial form commutative ring unit cp example αn deg p n called degree αn p called monic p deg p deg p p called constant lemma two polynomial p q k following assertion hold deg p q max deg p deg q deg p q deg p deg q proof exercise introduce concept associated division polynomial springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi polynomial fundamental theorem algebra definition let k field two polynomial p k exists polynomial q k p q called divisor p write p read divide p two polynomial p k called coprime p q k always imply q constant non-constant polynomial p k called irreducible k p q two polynomial q k implies q constant exist two non-constant polynomial q k p q p called reducible k note property irreducibility defined polynomial degree least polynomial degree always irreducible whether polynomial degree least irreducible may depend underlying field example polynomial q irreducible factorization show r reducible polynomial r irreducible using imaginary unit c reducible next result concern division remainder polynomial theorem p k k exist uniquely defined polynomial q r k p q r deg r deg proof show first existence polynomial q r k hold deg k follows q p r deg r deg assume deg deg p deg set q r p q r deg r deg let n deg p deg prove induction n hence p therefore p q r q r deg r deg polynomial suppose assertion hold n let two polynomial p n deg p deg given let sm highest coefficient p h p k deg h deg p n induction hypothesis exist polynomial q r k h q r deg r deg follows p q r q q deg r deg remains show uniqueness suppose hold exist polynomial q r k p q r deg r deg r r q q r r q q thus deg r r deg q q deg deg q q deg hand also deg r r max deg r deg r deg contradiction show indeed r r q q theorem important consequence root polynomial first known theorem corollary λ k root p k p λ exists uniquely determined polynomial q k p λ q proof apply theorem polynomial p λ get uniquely determined polynomial q r deg r deg p λ q polynomial r constant evaluating λ give p λ λ λ q λ r λ r λ paolo ruffini polynomial fundamental theorem algebra yield r p λ q polynomial p k least degree root λ k linear factor λ divisor p particular p reducible converse statement hold instance polynomial q reducible root corollary motivates following definition definition λ k root p k multiplicity uniquely determined nonnegative integer p λ q polynomial q k q λ recursive application corollary given polynomial p k lead following result corollary λk k pairwise distinct root p k corresponding multiplicity k exists unique polynomial q k p λk k q q λ j j particular sum multiplicity pairwise distinct root p deg p next result known lemma lemma p k coprime exist polynomial k p proof may assume without loss generality deg p deg proceed induction deg deg k thus p suppose assertion hold polynomial p k deg n n let p k deg p deg n given theorem exist polynomial q r p q r deg r deg r since assumption p coprime suppose exists non-constant polynomial h k divide r h also divide p contradiction assumption p coprime thus polynomial r coprime since deg r deg étienne bézout polynomial apply induction hypothesis polynomial r k hence k exist polynomial r r p q get p q p q completes proof using lemma bézout easily prove following result lemma p k irreducible divisor product h two polynomial h k p divide least one factor proof every polynomial divisor zero polynomial p divisor p coprime since p irreducible lemma exist polynomial k p hence h h h p h polynomial p divide term right hand side thus also recursive application lemma obtain euclidean theorem describes prime factor decomposition ring polynomial theorem every polynomial p αn n k unique ordering factor decomposition p μ pk μ k monic irreducible polynomial pk k proof deg p thus p assertion hold k μ let deg p p irreducible assertion hold p μ αn p reducible p two non-constant polynomial either irreducible decompose every multiplicative decomposition p obtained way deg p n non-constant factor suppose p μ pk β q q k k n μ β k well monic irreducible polynomial pk k p hence j j since polynomial q j irreducible must q j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.2 The Fundamental Theorem of Algebra</td>\n",
       "      <td></td>\n",
       "      <td>polynomial fundamental theorem algebra may assume without loss generality j cancel polynomial identity give μ pk β q q proceeding analogously polynomial pk finally obtain k μ β p j q j j fundamental theorem algebra seen existence root polynomial depends field considered field c special sense since fundamental theorem guarantee every non-constant polynomial root order use theorem context first present equivalent formulation language linear algebra theorem following statement equivalent every non-constant polynomial p c root v finite dimensional c-vector space every endomorphism f l v v eigenvector proof v f l v v characteristic polynomial p f c non-constant since deg p f dim v thus p f root c eigenvalue f f indeed eigenvector let p αn n c non-constant polynomial αn root p equal root monic polynomial p pa p cp p let cn n companion matrix lemma v n-dimensional c-vector space b arbitrary basis v exists uniquely determined f l v v f b b cp theorem assumption f eigenvector hence also eigenvalue p pa root fundamental theorem algebra proven without tool analysis particular one need polynomial continuous use following standard result based continuity polynomial lemma every polynomial p r odd degree real root numerous proof important result exist carl friedrich gauß alone gave four different proof starting one dissertation contained however gap history result described detail book fundamental theorem algebra proof let highest coefficient p positive lim p lim p since real function p continuous intermediate value theorem analysis implies existence root argument case negative leading coefficient analogous proof fundamental theorem algebra follows presentation article proof induction dimension however use usual consecutive order dim v order based set j j odd n j instance odd lemma v r-vector space dim v odd dim v every f l v v eigenvector let k field j every k space v dim v j every f l v v eigenvector two commuting f f l v v common eigenvector f f f f exists vector v v two scalar k f v v f v v r-vector space dim v odd two commuting f f l v v common eigenvector proof every f l v v degree p f r odd hence lemma implies p f root therefore f eigenvector proceed induction dim v dim v run element j increasing order set j proper subset n consisting natural number divisible j particular smallest element j dim v j assumption two arbitrary f f l v v eigenvector f f since dim v α k thus f f α f polynomial fundamental theorem algebra common eigenvector f f let dim v j let assertion proven k space whose dimension element j smaller dim v let f f l v v f f f f assumption f eigenvector corresponding eigenvalue let u im idv f w v ker idv f subspace u w v f f u u f w space w shown lemma space u easily shown well cp exercise subspace u w also f u u u idv f v v since f f commute f u f idv f v idv f f v idv f f v u w w idv f f w idv f f w f idv f w f idv f w f hence f w dim v dim u dim w since dim v divisible j either dim u dim w divisible j hence either dim u j dim w j corresponding subspace proper subspace v dimension element j smaller dim v induction hypothesis f f common eigenvector subspace thus f f common eigenvector corresponding subspace equal v must subspace w since dim w v w every vector v eigenvector f assumption also f eigenvector exists least one common eigenvector f f follows assumption hold k r j mean hold well prove fundamental theorem algebra formulation theorem theorem v finite dimensional c-vector space every f l v v eigenvector fundamental theorem algebra proof prove assertion induction j dim v j start j thus showing assertion c-vector space odd dimension let v arbitrary c-vector space n dim v let f l v v consider arbitrary scalar product v scalar product always exists cp exercise well set self-adjoint map respect scalar product h g l v v g g ad lemma set h form r-vector space dimension n define h h l h h h g f g g f ad h g f g g f ad g h h h h h cp exercise since n odd also n odd lemma h h common eigenvector hence exists g h g g h g g h h ih g f g g h therefore particular g f g h ih since g exists v v g v g v f g v show g v v eigenvector f proof j complete assume j every c-vector space v dim v j every f l v v eigenvector lemma implies every two commuting f f l v v common eigenvector show every c-vector space v dim v every f l v v eigenvector since j j q q odd prove c-vector space v n dim v j q odd q let v vector space let f l v v given choose arbitrary basis v denote matrix representation f respect basis cn n let polynomial fundamental theorem algebra b cn n b b set complex symmetric n n matrix define h h l h b ab b h b ab b h h h h cp exercise lemma set form c-vector space dimension n n n j q odd natural number q thus n n j q j q q j q j induction hypothesis commuting endomorphisms h h common eigenvector hence exists b b b b h b particular b b multiplying equation left yield b b b b b b b b n factorize α β used every complex number square root αin β b bv β bv bv since b exists v bv eigenvector corresponding eigenvalue β β bv eigenvector corresponding eigenvalue α since β eigenvector also f eigenvector fundamental theorem algebra matlab-minute compute eigenvalue matrix using command eig definition real matrix real eigenvalue reason occurrence complex eigenvalue matlab interprets every matrix complex matrix mean within matlab every matrix unitarily triangulated since every complex polynomial degree least decomposes linear factor direct corollary fundamental theorem algebra lemma following result corollary v finite dimensional c-vector space two commuting f f l v v common eigenvector example two complex matrix b commute eigenvalue b hence b common eigenvalue common eigenvectors b using corollary schur theorem corollary generalized follows theorem v finite dimensional unitary vector space f f l v v commute f f simultaneously unitarily triangulated exists orthonormal basis b v f b b f b b upper triangular proof exercise polynomial fundamental theorem algebra exercise following exercise k arbitrary field prove lemma show following assertion k b c imply imply exists c k examine whether following polynomial irreducible q r c q r c determine decomposition irreducible factor decompose polynomial irreducible factor field k q k r k show following assertion p k deg p p irreducible b deg p p root p irreducible c deg p p irreducible p root let g l n c n let adj cn n adjunct show exist n matrix j cn n det j det j n aj adj hint use pa construct polynomial p c adj p express p product linear factor show two polynomial p q c common root exist polynomial c deg deg p deg deg q p q let v finite dimensional unitary vector space f l v v h g l v v g g ad let h h l v v g f g g f ad fundamental theorem algebra h h l v v g f g g f ad show h h l h h h h h h let cn n b cn n b b let h cn n b ab b h cn n b ab show h h l h h h h let v c-vector space f l v v let u finite dimensional f subspace show u contains least one eigenvector f let v k space let f l v v show following statement k c exists f subspace u v dim u b k r exists f subspace u v dim u prove theorem construct example showing condition f g g f theorem sufficient necessary simultaneous unitary triangulation f let k n n diagonal matrix pairwise distinct diagonal entry b k n n ab b show case b diagonal matrix say b diagonal entry pairwise distinct show matrix commute determine unitary matrix q q h aq q h b q upper triangular show following statement p k k n n g l n k p sp b b c k n n ab c ap b p c c k c cn n exists unitary matrix q q h aq q h p q upper triangular let v finite dimensional unitary vector space let f l v v normal f satisfies f f ad f ad f polynomial fundamental theorem algebra show λ c eigenvalue f v f λ f subspace b show using f diagonalizable hint show induction dim v v direct sum eigenspaces f c show using b f even unitarily diagonalizable exists orthonormal basis b v f b b diagonal matrix let g l v v unitarily diagonalizable show g normal show endomorphism finite dimensional unitary vector space normal unitarily diagonalizable give different proof result theorem let v finite dimensional k space f l v v v f subspace let furthermore f j f j l u j u j j every v v exist unique u u v u show also f v f u f u f u f u write f f f call f direct sum f f respect decomposition v b show rank f rank f rank f p f p p c show λ f λ f λ f λ k set λ h λ eigenvalue h l v v show g λ f g λ f g λ f λ k set g λ h dim ker λidv even λ eigenvalue h l v v e show p f p f p f p k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.1 Cyclic f-invariant Subspaces and Duality</td>\n",
       "      <td></td>\n",
       "      <td>chapter cyclic subspace duality jordan canonical form chapter use duality theory analyze property endomorphism f finite dimensional vector space v detail particularly interested algebraic geometric multiplicity eigenvalue f characterization corresponding eigenspaces strategy analysis decompose vector space v direct sum f subspace appropriately chosen base essential property f obvious matrix representation matrix representation derive called jordan canonical form f great importance many different derivation form using different mathematical tool approach using duality theory based article vlastimil pták cyclic f subspace duality let v finite dimensional k space f l v v v exists uniquely defined smallest number n vector f f linearly independent vector f f f linearly dependent obviously dim v since dim v vector v linearly independent number called grade respect f denote grade f vector linearly dependent thus grade respect f springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi cyclic subspace duality jordan canonical form f vector f linearly dependent hold eigenvector f eigenvector f f every j n define subspace k j f span f f space k j f called jth krylov f lemma v finite dimensional k space f l v v v following assertion hold f km f f subspace v span f f km f j f j f u v f subspace contains vector km f u thus among f subspace v contain vector krylov subspace km f one smallest dimension f f n dim k j f j j proof exercise assertion trivial thus let f let u v f subspace contains u also contains vector f f km f u particular dim u dim km f let k f apply f side f thus since f apply inductively f k obtain thus vector f linearly independent implies dim k j f j j vector f f form construction basis krylov subspace km f application f vector f k basis yield next basis vector f k application f last vector f yield linear combination basis vector since f km f due special structure subspace km f called cyclic f subspace aleksey nikolaevich krylov cyclic f subspace duality definition let v k space endomorphism f l v v called nilpotent f hold time f f called nilpotent index zero map f nilpotent endomorphism index v zero map endomorphism map nilpotent index case omit requirement f f f nilpotent index v vector f v f f v f v f v hence f v eigenvector f corresponding eigenvalue construction sect show eigenvalue nilpotent endomorphism also cp exercise lemma v k space f l v v nilpotent index dim v proof f nilpotent index exists v f f lemma vector f linearly independent implies dim v example vector space k endomorphism f k k nilpotent index since f f f u f subspace v f l u u f u u u f u restriction f subspace u cp definition theorem let v finite dimensional k space f l v v exist f subspace v v v f l bijective f l nilpotent proof v ker f f v f f v f thus v ker f therefore ker f ker f proceeding inductively see ker f ker f ker f since v finite dimensional exists smallest number ker f ker f j j number let im f ker f cyclic subspace duality jordan canonical form f bijective v show space satisfy assertion first observe f v v f w w v therefore f v f f w f f w v f f v f f v f therefore f v application dimension formula linear map cp theorem f give dim v dim dim v v f w w v since v hence f v f f w f w first equation hold since v definition ker f ker f implies f w therefore v f w obtain v let v ker f given since v exists vector w v v f w implies f v f f w f w definition ker f ker f thus w ker f therefore v f w implies ker f f injective thus also bijective cp corollary hand v definition f v f v thus f zero map l f nilpotent development recall term result chap let v finite dimensional k space let v dual space u v w v two subspace bilinear form β u w k v h h v non-degenerate u w called dual pair respect β requires dim u dim w f l u u dual map f l u u defined f u u h h v u h u f h v h f v furthermore f k f k k set u h v h u u u called annihilator u set subspace v cp exercise analogously set w v v h v h w called annihilator set subspace v cyclic f subspace duality lemma let v finite dimensional k space f l v v v dual space v f l v v dual map f let u v w v two subspace following assertion hold dim v dim w dim w dim u dim u f nilpotent index f nilpotent index w v f subspace w v f subspace u w dual pair respect bilinear form defined v u w proof exercise v v f v hence h f v f h v f h v every h v v v f nilpotent index f f h h v therefore f h v h f v v implies f contradiction assumption f nilpotent index thus f nilpotent index let w w every h w f h w thus f h w h f w hence f w w u u w h u h w since u w since u w dual pair respect bilinear form defined u moreover dim u dim w using obtain dim v dim w dim w dim u dim w u w obtain v u w example consider vector space v canonical basis b subspace v u span w h v h b α α α r v u h v h b α α r v w span cyclic subspace duality jordan canonical form example easily see dim v dim w dim w dim u dim u u w form dual pair respect bilinear form defined k moreover v u w following theorem present given nilpotent f decomposition v f subspace idea decomposition construct dual pair subspace u v w v u f w f lemma w f lemma follows v u w theorem let v finite dimensional k space let f l v v nilpotent index let v satisfy f let h v satisfy h f f f h f f subspace km f v km f h v respectively dual pair respect bilinear form defined furthermore v km f km f h km f h f subspace proof let v vector f since f space km f m-dimensional f subspace v cp lemma let h v vector h f f h particular f h l v v since f nilpotent index also f nilpotent index cp lemma f h l v v therefore km f h m-dimensional f subspace v cp lemma remains show km f km f h dual pair let γ j f j km f vector h β h h km f h show inductively thus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.2 The Jordan Canonical Form</td>\n",
       "      <td></td>\n",
       "      <td>cyclic f subspace duality using f h km f h assumption vector yield f h h f γ j h f j h f last equation hold since f j j f h f obtain suppose k k using f h km f h assumption vector yield f h h f γ j h f γk h f last equation hold since γ j j k f j k asserted therefore bilinear form defined space km f km f h non-degenerate first variable analogously bilinear form non-degenerate second variable hence km f km f h dual pair using lemma v km f km f h space km f h lemma f subspace jordan canonical form let v finite dimensional k space f l v v exists basis b v consisting eigenvectors f f b b diagonal matrix f diagonalizable necessary sufficient condition characteristic polynomial p f decomposes linear factor k addition g f λ j f λ j every eigenvalue λ j cp theorem p f decomposes linear factor g f λ j f λ j hold least one eigenvalue λ j f diagonalizable still triangulated exists basis b v f b b upper triangular matrix cp theorem triangular matrix read algebraic usually geometric multiplicity eigenvalue goal following construction determine basis b v f b b upper triangular addition algebraic also reveals geometric multiplicity eigenvalue assumption p f decomposes linear factor k construct basis b v f b b block diagonal matrix form cyclic subspace duality jordan canonical form f b b jdm λm diagonal block form λj jd j λ j k j j λj λ j k j n j matrix form called jordan block size j corresponding eigenvalue λ j following construction first assume p f decomposes linear factor assume existence single eigenvalue k f using eigenvalue define endomorphism g f idv l v v theorem exist g-invariant subspace u v w v v u w nilpotent bijective u since otherwise w v g would bijective contradicts assumption eigenvalue f let nilpotent index construction dim u let u vector since vector eigenvector corresponding eigenvalue lemma vector linearly independent subspace u consider basis matrix representation respect basis given k jordan canonical form show particular characteristic polynomial given monomial hence eigenvalue moreover construction dim u construction complete moment hand dim u applying theorem l u u show u u consider exists subspace u map nilpotent index carry construction g g determine vector u eigenvector subspace u basis u k construction k dim u step procedure terminates found decomposition u form u kdk gk wk g kdk g wk second equation used kd j g j w j kd j g wk j combine constructed base bk basis b u b b bk bk jdk thus nilpotent endomorphism characteristic polynomial eigenvalue transfer result f g idv every g-invariant subspace f one observes easily kd j f w j kd j g w j j k cyclic subspace duality jordan canonical form cp exercise hence follows u f kdk f wk every j k j f g w j g g w j g w j g w j g w j g j w j matrix representation f respect basis b u therefore given f b b f f bk bk jdk map f idw bijective construction eigenvalue f therefore f dim u dk order determine g f let v u arbitrary vector exist scalar α j k j k α j g w j using obtain k j f v k j k j α j f g w j α j g w j α j g w j k j v α j g w j vector last sum linearly independent hence f v v α j j k j show every eigenvector f corresponding eigenvalue form k α j g j w j least one α j nonzero v f span g g dk wk jordan canonical form since g g dk wk linearly independent follows g f geometric multiplicity eigenvalue therefore equal number jordan block corresponding eigenvalue matrix representation furthermore observe every subspace kd j f w j endomorphism f exactly one linear independent eigenvector corresponding eigenvalue summarize result following theorem theorem let v finite dimensional k space let f l v v k eigenvalue f following assertion hold exist f subspace u v w v v u map f idu nilpotent map f idw bijective particular eigenvalue f subspace u written u f kdk f wk vector wk u kd j f w j j f invariant subspace v j called cyclic decomposition u exists basis b u f b b jdk f dk g f f eigenvalue eigenvalue restriction f l w w apply theorem f vector space w direct sum form w x f idx nilpotent f idy bijective space x cyclic decomposition analogous theorem exists matrix representation f analogous construction carried eigenvalue f characteristic polynomial p f decomposes linear factor k finally obtain cyclic decomposition entire space v give following theorem theorem let v finite dimensional k space let f l v v characteristic polynomial p f decomposes linear factor k exists basis b v f b b jdm λm λm k necessarily pairwise distinct eigenvalue f every eigenvalue λ j f f λ j equal sum size cyclic subspace duality jordan canonical form jordan block corresponding λ j g f λ j equal number jordan block corresponding λ j matrix representation form called jordan canonical f theorem know f l v v diagonalizable p f decomposes linear factor k g f λ j f λ j hold every eigenvalue λ j f p f decomposes linear factor jordan canonical form show g f λ j f λ j every jordan block corresponding λ j size fundamental theorem algebra yield following corollary theorem corollary v finite dimensional c-vector space every f l v v jordan canonical form following uniqueness result justifies name canonical form theorem let v finite dimensional k space f l v v jordan canonical form unique order jordan block diagonal proof let dim v n let two base v f well f k n n jdm λm k n n jck μk given eigenvalue λ j j define r λ j rank λ j λ j r λ j d λ j equal number jordan block λ j k diagonal number jordan block corresponding eigenvalue λ j exact size therefore given λ j λ j λ j λ j d λ j marie ennemond camille jordan derived form two year earlier karl weierstraß proved result implies jordan canonical form jordan canonical form cp example matrix similar therefore eigenvalue λm μk furthermore rank αin rank αin α k particular every λ j exists μi μk μi λ j μi matrix get r μi rank μi r λ j show matrix reordering jordan block diagonal matrix example example illustrates construction proof theorem get r d d cyclic subspace duality jordan canonical form consider power jordan block jd λ k since id jd commute k j jd j j k jd λ k λid jd k k p j λ jd j j every k p j jth derivative polynomial p k respect p k k p j k j k k k j j j easily show following result lemma p k polynomial degree k k p jd λ p j λ jd j j proof exercise considered linear map k k matrix jd represents upshift since jd αd clearly k αd jd jd hence linear map jd nilpotent index sum right hand side therefore term even deg p moreover right hand side show p jd λ upper triangular matrix constant entry diagonal matrix constant diagonal called toeplitz particular main diagonal entry p λ see p jd λ hold p λ p λ p λ thus shown following result otto toeplitz jordan canonical form lemma let p k polynomial jd λ k jordan block matrix p jd λ invertible λ root p jd λ k λ d-fold root p linear factor λ divisor let v finite dimensional k space let f l v v assume p f decomposes linear factor cayley-hamilton theorem theorem know p f f l v v exists monic polynomial degree dim v annihilates endomorphism f let k two monic polynomial smallest possible degree f f f since monic k polynomial deg deg deg minimality assumption deg deg implies thus every f l v v exists uniquely determined monic polynomial minimal degree annihilates f justifies following definition definition v finite dimensional k space f l v v uniquely determined monic polynomial minimal degree annihilates f called minimal polynomial f denote polynomial f construction always deg f deg p f dim v lemma v finite dimensional k space f l v v minimal polynomial f divide every polynomial annihilates f particular divisor characteristic polynomial p f proof p p f f divide p k polynomial p f deg f deg p using division remainder cp theorem exist uniquely determined polynomial q r k p q f r deg r deg f thus p f q f f f r f r f minimality deg f implies r hence f divide p f decomposes linear factor explicitly construct f using jordan canonical form f lemma let v finite dimensional k space f l v v jordan canonical form pairwise distinct eigenvalue λk respective maximal size corresponding jordan block mf k λ j j cyclic subspace duality jordan canonical form proof know lemma f divisor p f therefore mf k λ j j exponent jdm λm jordan canonical form f f f l v v equivalent f k n n n dim v f f jd j λ j j necessary sufficient f λ j j lemma hold every linear factor λ j j j k divisor f therefore f desired form example f endomorphism jordan canonical form p f f f show f f f l v v jordan canonical form great importance theoretical linear algebra practical application however usually matrix k r k c considered relevant since numerically stable method computing jordan canonical form general matrix finite precision arithmetic reason lack method entry jordan canonical form depend continuously entry given matrix example consider matrix ε ε</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.3 Computation of the Jordan Canonical Form</td>\n",
       "      <td></td>\n",
       "      <td>jordan canonical form every given ε matrix ε two distinct eigenvalue ε hence diagonal matrix j ε jordan canonical form ε however ε obtain ε j ε thus j ε converge jordan canonical form ε similar example given matrix exercise jordan block size n corresponding eigenvalue every ε obtain diagonalizable matrix ε cn n n pairwise distinct eigenvalue matlab-minute let random matrix constructed command rand construct several matrix always compute eigenvalue using command eig display eigenvalue format long one observes two eigenvalue real complex conjugate always error starting digit decimal point error order happen chance due behavior eigenvalue perturbation arise rounding error computer computation jordan canonical form derive method computation jordan canonical form endomorphism f finite dimensional k space assume p f decomposes linear factor k root p f eigenvalue f known construction follows important step existence proof jordan canonical form sect suppose λ eigenvalue f f corresponding jordan block size exist linearly independent vector t f b b j λ cyclic subspace duality jordan canonical form b t writing id instead idv simplicity notation f λid f λid f λid t hence j f λid j t j vector t form sequence one constructed context krylov subspace span t k f λid t reverse sequence called jordan chain f corresponding eigenvalue λ vector eigenvector f corresponding λ vector f f λid f λid hence ker f λid ker f λid general j ker f λid j ker f λid j motivates following definition definition let v finite dimensional k space let f l v v eigenvalue λ k let k vector v v v ker f λid k ker f λid called principal vector level k f corresponding eigenvalue λ principal vector level one eigenvectors principal vector higher level considered generalization eigenvectors therefore sometimes called generalized eigenvectors computation jordan canonical form f thus need know number length jordan chain corresponding different eigenvalue f correspond number size jordan block f f matrix representation f respect arbitrary basis cp proof theorem computation jordan canonical form d λ rank f λi rank f λi dim im f λid dim im f λid dim v dim ker f λid dim v dim ker f λid dim ker f λid dim ker f λid number jordan block corresponding λ size least implies particular d λ λ d λ λ number jordan block exact size corresponding λ exists smallest number n ker f λid ker f λid ker f λid ker f λid hence d λ jordan block corresponding λ size larger order compute jordan canonical form therefore proceed follows determine eigenvalue f every eigenvalue λ f carry following step determine smallest number n ker f ker f ker f ker f dim ker f λid λ f b determine d λ dim ker f λid dim ker f λid d λ λ dim ker f λid g λ f number jordan block corresponding λ c simplify notation write d d λ determine jordan chain follows since dm dm exist dm jordan block size block determine jordan chain dm principal vector level vector tdm ker f λid ker f λid cyclic subspace duality jordan canonical form following property dm αdm k αi ti ker f λid αdm first index ti j indicates number chain second indicates level principal vector ker f λid j ker f λid ii j proceed follows determined j principal vector level j say j j td j j apply f λid vector hence ti f λid ti j j order determine principal vector level j dj αi ti ker f λid αd j k dj f λid dj dj αi ti f λid thus αi ti j αi ti j ker f λid giving αd j j exist j jordan block size j need jordan chain length j thus extend already computed td j ker f λid ker f λid principal vector level j j via td ker f λid ker f λid following must hold αd k αi ti ker f λid αd completing step j obtained linearly independent vector ker f λid since dim ker f λid found basis ker f λid way determined different jordan chain combine follows tλ computation jordan canonical form chain begin eigenvector followed principal vector increasing level use convention chain ordered decreasingly according length jordan chain linearly independent first vector eigenvectors linearly independent show exercise thus pairwise distinct eigenvalue f basis f jordan canonical form example interpret matrix f endomorphism eigenvalue f root pf particular pf decomposes linear factor f jordan canonical form consider different eigenvalue f eigenvalue obtain ker f ker span dim ker f f eigenvalue obtain ker f ker span cyclic subspace duality jordan canonical form ker f ker span dim ker f f b dim ker f dim ker f dim ker f dim ker f c computation jordan chain principal vector level one choose form basis ker f r finished choose principal vector level two say vector r span compute f since add another principal vector level one choose since vector linearly independent ker f implies way get coordinate transformation matrix jordan canonical form f f computation jordan canonical form exercise following exercise k arbitrary field prove lemma prove lemma let v k space f l v v λ k prove disprove subspace u v f f λidv let v finite dimensional k space f l v v v v λ k show k j f v k j f λidv v j conclude grade v respect f equal grade v respect f λidv prove lemma let v finite dimensional euclidean unitary vector space let f l v v selfadjoint nilpotent show f let v finite dimensional k space let f l v v nilpotent index suppose p f decomposes linear factor show following assertion p f n n dim v b f c exists vector v v grade respect f every λ k f λ let v finite dimensional k space f l v v show following assertion ker f j ker f j exists ker f ker f ker f ker f j j b im f j im f j exists im f im f im f im f j j c minimal ker f ker f im f im f theorem implies v ker f im f decomposition v f subspace let v finite dimensional k space let f l v v projection cp exercise show following assertion v im f implies f v b v im f ker f c exists basis b v f b b ik k dim im f n dim v particular p f k λ every eigenvalue λ f cyclic subspace duality jordan canonical form map g idv f projection ker g im f im g ker f let v finite dimensional k space let u w v two subspace v u show exists uniquely determined projection f l v v im f u ker f determine jordan canonical form matrix r using method presented sect determine also minimal polynomial determine jordan canonical form minimal polynomial linear map f determine order block matrix j jordan canonical form pj j let v finite dimensional k space f l v v suppose p f decomposes linear factor show following assertion p f f hold g λ f eigenvalue λ f b f diagonalizable f simple root root multiplicity one c root λ k f simple ker f λidv ker f λidv let v k space dimension let f l v v p f decomposing linear factor show jordan canonical form f uniquely determined p f f hold longer dim v let k n n matrix characteristic polynomial decomposes linear factor show exists diagonalizable matrix nilpotent matrix n n n n let k n n matrix jordan canonical form define λ inr δi j jnr λ k n n λ computation jordan canonical form show following assertion b c inr jn λ inr jn λ similar jn λ inr jnr λ written product two symmetric matrix determine matrix two symmetric matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.1 Matrix Functions and the Matrix Exponential Function</td>\n",
       "      <td></td>\n",
       "      <td>chapter matrix function system differential equation chapter give introduction area matrix function first define general matrix function derive important property using example network analysis chemical reaction illustrate matrix function arise naturally application network analysis example involves exponential function matrix study property important function detail analysis chemical reaction kinetics lead system ordinary differential equation whose solution based matrix exponential function matrix function matrix exponential function following study function yield given n n matrix n n matrix possible definition function given entrywise application one could define scalar function matrix instance ai j cn n function sin sin sin ai j however definition compatible matrix multiplication since general already following definition primary matrix function definition turn consistent matrix multiplication since definition based jordan canonical form assume simplicity cn n consideration also apply square matrix r long jordan canonical form definition let cn n jordan canonical form j diag jdm λm springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi matrix function system differential equation let c λm function f c said defined spectrum value f j λi j di exist f j λi j di jth derivative function f λ respect λ evaluated λi λi r real derivative λi c r complex derivative moreover assume equal eigenvalue occur different jordan block mapped value f defined spectrum primary matrix function f defined f f j f j diag f f jdm λm f jdi λi di λi f di f λi f λi f λi f λi f λi f λi f λi f λi note definition f existence value required example let let f z z square root function set f f definition choose branch square root function f f matrix primary square root taking different branch function different jordan block corresponding eigenvalue incompatible definition instance matrix x incompatible definition despite fact x x solution x cn n matrix equation x called square root matrix cn n example show may primary square root according definition following f always mean primary matrix function according definition usually omit term primary shown polynomial p c degree k matrix function matrix exponential function p jdi λi k p j λi jdi j j simple comparison show formula agrees f mean computation p jdi λi lead result definition p jdi λi generally following result hold lemma let cn n p αk k c f p yield matrix function f satisfies f αk ak proof exercise consider particular polynomial f resulting f equal product show definition primary matrix function f consistent matrix multiplication following theorem great practical theoretical importance show matrix f always written polynomial theorem let cn n minimal polynomial let f definition exists uniquely determined polynomial p c degree deg f p particular f f f f well f v av v f v v g l n c proof present proof since requires advanced result interpolation theory detail found chap using theorem show primary matrix function f definition independent choice jordan canonical form already know theorem jordan canonical form unique order jordan block j diag jdm λm λm diag two jordan canonical form p j p permutation matrix p rn n matrix j order diagonal block hence f j diag f f jdm λm p p diag f f jdm λm p p p diag f f λm p p f p matrix function system differential equation theorem applied matrix j yield existence polynomial p f j p j thus get f f j sp j p p p p j p p f j p f let u consider exponential function f z e z infinitely often complex differentiable throughout particular e z defined sense definition spectrum every given matrix sdiag jdm λm cn n c arbitrary fixed derivative function et z respect variable z given j tz e j et z dz j j use notation exp instead e exponential function matrix every jordan block jd λ f z e z exp jd λ etλ tλ jd k e k matrix exponential function exp given exp sdiag exp exp jdm λm parameter used next section context linear differential equation analysis shown every z c function e z represented absolutely convergent series ez zj j using series equation jd obtain matrix function matrix exponential function j tλ tλ exp jd λ e jd jd j j tλ jd j j tj j λi jd j tj λid jd j j jd λ j j derivation used absolute convergence exponential series finiteness series matrix jd allows application cauchy product absolutely convergent series also proven analysis lemma cn n c exp matrix exponential function j exp j proof shown already jordan block assertion follows j j j j j j representation matrix exponential function immediately see lemma matrix rn n every real matrix exponential function exp real matrix following result present important property matrix exponential function lemma two matrix b cn n commute exp b exp exp b every matrix cn n exp g l n c exp exp augustin louis cauchy matrix function system differential equation proof b commute cauchy product formula yield j j exp exp b b b j j j j b b j j j exp b used binomial formula commuting matrix cp exercise since commute exp exp exp exp j j hence exp g l n c exp exp non-commuting matrix statement lemma general hold cp exercise matlab-minute compute matrix exponential function exp matrix r using command look help expm also compute diagonalization using command form matrix exponential function exp compare matrix compute relative error norm look help norm example let ai j cn n symmetric matrix aii ai j j identify matrix graph g v e consisting set n vertex v n set edge e v v n row identified vertex e every entry ai j identified edge j e due symmetry ai j ji therefore consider following element matrix function matrix exponential function e unordered pair j j following example illustrates identification identified g v e e v graph g displayed follows path length vertex vertex ordered list vertex ki v closed path length example path given length respectively mathematical field graph theory one usually assumes vertex path pairwise distinct deviation convention motivated following interpretation matrix power entry ai j matrix mean exists path length vertex vertex j vertex j adjacent ai j path exists matrix therefore called adjacency matrix graph g square adjacency matrix entry j position given j n sum right hand side obtain given e j e sum right side therefore equal number vertex adjacent j hence j entry equal number pairwise distinct path j j pairwise distinct closed path length g generally one show following cp exercise let ai j cn n symmetric adjacency matrix aii ai j j n let g graph identified n j entry equal number pairwise distinct path j j pairwise distinct closed path length g matrix function system differential equation matrix obtain pairwise distinct closed path length pairwise distinct path length numerous real world application involve network modeled mathematically using graph example include social biological telecommunication airline network property network studied interdisciplinary area network science important task identify participant network central sense functionality significant impact entire network network modeled graph study centrality vertex example vertex considered central connected large part graph via many short closed path longer connection usually le important thus path scaled according length use scaling factor path length vertex graph g adjacency matrix obtain centrality measure form ii relative ordering vertex according formula changed add constant obtain centrality vertex exp ii ii another important quantity so-called communicability vertex j j given weighted sum pairwise distinct path j exp j ij matrix matlab function expm yield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.2 Systems of Linear Ordinary Differential Equations</td>\n",
       "      <td></td>\n",
       "      <td>matrix function matrix exponential function exp vertex largest centrality followed would define centrality vertex number adjacent vertex example could distinguish vertex largest communicability example exists vertex information concerning analysis network using adjacency matrix matrix function found article system linear ordinary differential equation differential equation describes relationship desired function derivative equation used area science engineering modeling physical phenomenon ordinary differential equation involve function one variable derivative partial differential equation involve function several variable partial derivative section focus ordinary differential equation first order function first derivative occur simple example modeling ordinary differential equation first order increase decrease biological population bacteria petri dish let size population time enough food external condition temperature pressure constant population grows real rate k proportional current number individual described equation dt clearly one also take k population shrink looking function r r satisfies general solution given exponential function cetk c r arbitrary constant unique solution need know size population given initial time way obtain initial value problem ky matrix function system differential equation show solved uniquely function e k example chemical reaction certain initial substance called educts reactant transformed substance called product reaction distinguished concerning order discus reaction first order reaction rate determined one educt reaction second higher order one typically obtains nonlinear differential equation beyond focus chapter example educt transformed product rate write reaction symbolically model mathematically ordinary differential equation value concentration substance time concentration product grows rate corresponding equation may happen reaction first order develops direction transforms rate transforms rate model reaction mathematically system linear ordinary differential equation combining function vector valued function write system ay system linear ordinary differential equation derivative function always considered entrywise reaction also several step example reaction form lead differential equation thus system ay sum entry column equal zero since every decrease substance certain rate substance increase rate summary chemical reaction first order lead system linear ordinary differential equation first order written ay real square matrix derive general theory system linear real complex ordinary differential equation first order form ay g k n n given matrix given positive real number g k given function k desired solution assume k r k g k system called homogeneous otherwise called non-homogeneous given system form system ay called associated homogeneous system matrix function system differential equation lemma solution homogeneous system form subspace infinite dimensional k space continuously differentiable function interval k proof show required property according lemma function w continuously differentiable solves homogeneous system thus solution set system empty k continuously differentiable solution k w continuously differentiable aw function w solution homogeneous system following characterization solution non-homogeneous system analogous characterization solution set non-homogeneous linear system equation lemma also cp lemma lemma k solution non-homogeneous system every solution written solution associated homogeneous system proof solution ay g g difference thus solution associated homogeneous system order describe solution system ordinary differential equation consider given matrix k n n matrix exponential function exp lemma consider real variable power series matrix exponential function lemma converges differentiated termwise respect variable derivative matrix respect variable considered entrywise yield exp dt dt exp result obtained entrywise differentiation matrix exp respect system linear ordinary differential equation obtain tλ exp jd λ e dt dt λetλ etλ λetλ etλ jd λid jd etλ jd λ exp jd λ also give dt exp exp theorem unique solution homogeneous differential equation system given initial condition k given function exp set solution homogeneous differential equation system form n-dimensional k space basis exp exp en proof exp exp exp exp dt dt exp ay exp hence solution satisfies initial condition w another solution u exp w exp w exp w exp dt exp aw k show function u constant entry particular u u w w exp used exp exp cp lemma matrix function system differential equation function exp e j exp en k j n solves homogeneous system ay since matrix exp k n n invertible every cp lemma function linearly independent arbitrary solution ay k unique solution initial value problem linear combination function exp consequence exp exp en describe solution non-homogeneous system need integral function form w k wn every fixed define w d d k wn d apply integral entrywise function definition dt w d w determine explicit solution formula system linear differential equation based so-called duhamel theorem unique solution non-homogeneous differential equation system initial condition k given exp exp exp g d proof derivative function defined exp exp g d exp g d exp exp g exp exp exp dt dt jean-marie constant duhamel system linear ordinary differential equation exp exp exp g d g ay furthermore exp exp exp g d also satisfies initial condition let another solution satisfies initial condition lemma w w solves homogeneous system therefore w exp c c k cp theorem obtain c c hence theorem shown explicit solution system linear ordinary differential equation first order compute matrix exponential function introduced function using jordan canonical form given matrix numerical computation based jordan canonical form advisable cp example significant practical relevance numerous different algorithm computing matrix exponential function proposed shown article existing algorithm completely satisfactory example example circuit simulation presented sect lead system ordinary differential equation r vc v dt l l l vc dt c using initial value vc obtain solution exp vc v exp d example let u also consider example mechanic weight mass attached spring spring constant μ let distance weight equilibrium position illustrated following figure matrix function system differential equation want determine position x weight time x extension spring described hooke corresponding ordinary differential equation second order μ x x dt initial condition x initial velocity weight write differential equation second order x system first order introducing velocity v new variable velocity given derivative position respect time v thus acceleration yield system ay mμ x v initial condition theorem unique solution homogeneous initial value problem given function exp consider element eigenvalue two complex non-real number iρ ρ mμ corresponding eigenvectors iρ thus exp sir itρ e iρ robert hooke system linear ordinary differential equation exercise construct matrix ai j determine solution x matrix equation x classify solution primary square root determine matrix x real entry x prove lemma prove following assertion cn n det exp exp trace b h exp unitary c exp e e let diag jdm λm cn n rank determine primary matrix function f f z z function also exist rank n let log z r eiϕ r ϕ π c r eiϕ ln r iϕ principle branch complex logarithm ln denotes real natural logarithm show function defined spectrum compute log well exp log compute exp exp sin π construct two matrix b exp b exp exp b prove assertion entry ad example let compute exp r solve homogeneous system differential equation ay initial condition compute matrix exp example explicitly thus show exp r despite fact eigenvalue eigenvectors real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.1 Normal Endomorphisms</td>\n",
       "      <td></td>\n",
       "      <td>chapter special class endomorphisms chapter discus class endomorphisms square matrix whose eigenvalue eigenvectors special property property exist assumption chapter assumption concern relationship given endomorphism adjoint endomorphism thus focus euclidean unitary vector space lead class normal orthogonal unitary selfadjoint endomorphisms class natural counterpart set square real complex matrix normal endomorphisms start definition endomorphism matrix definition let v finite dimensional euclidean unitary vector space endomorphism f l v v called normal f f ad f ad f matrix rn n cn n called normal h h respectively z c zz zz property normality therefore interpreted generalization property complex number first study property normal endomorphisms finite dimensional unitary vector space recall following result b orthonormal basis v f l v v f b b h f ad b b cp theorem every f l v v unitarily triangulated cp corollary schur theorem hold general euclidean case since every real polynomial decomposes linear factor term introduced otto toeplitz context bilinear form springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi special class endomorphisms using result obtain following characterization normal endomorphisms unitary vector space theorem v finite dimensional unitary vector space f l v v normal exists orthonormal basis b v f b b diagonal matrix f unitarily diagonalizable proof let f l v v normal let b orthonormal basis v r f b b upper triangular matrix r h f ad b b f f ad f ad f obtain r r h f f ad b b f ad f b b r h show induction n dim v r diagonal obvious n let assertion hold n let r upper triangular r r h r h write r r cn n upper triangular rrh rh r obtain hence induction hypothesis cn n diagonal therefore diagonal well conversely suppose exists orthonormal basis b v f b b diagonal f ad b b f b b h diagonal since diagonal matrix commute f f ad b b f b b f ad b b f ad b b f b b f ad f b b implies f f ad f ad f hence f normal application theorem unitary vector space v standard scalar product matrix cn n viewed element l v v yield following matrix version corollary matrix cn n normal exists orthonormal basis consisting eigenvectors unitarily diagonalizable normal endomorphisms following theorem present another characterization normal endomorphisms unitary vector space theorem v finite dimensional unitary vector space f l v v normal exists polynomial p c p f f ad proof p f f ad polynomial p c f f ad f p f p f f f ad f hence f normal conversely f normal exists orthonormal basis b v f b b diag λn furthermore f ad b b f b b h diag λn let p c polynomial p λ j λ j j polynomial explicitly constructed using lagrange basis c cp exercise f ad b b diag λn diag p p λn p diag λn p f b b p f b b hence also f ad p f several characterization normal endomorphisms finite dimensional unitary vector space normal matrix cn n found article see also exercise consider euclidean case focus real square matrix result formulated analogously normal endomorphisms finite dimensional euclidean vector space let rn n normal also satisfies h h considered element cn n unitarily diagonalizable d h hold unitary matrix cn n diagonal matrix cn n despite fact real entry neither real general since element rn n may diagonalizable instance normal matrix diagonalizable r considered element eigenvalue unitarily diagonalizable discus case real normal matrix detail first prove real version schur theorem special class endomorphisms theorem every matrix rn n exists orthogonal matrix u rn n n n u au r r rmm every j either r j j rjj j j j j j r second case r j j considered complex matrix pair complex conjugate eigenvalue form α j iβ j α j r β j r matrix r called real schur form proof proceed via induction n r u suppose assertion hold n let given consider element eigenvalue λ c α β r corresponding eigenvector v x iy x av λv dividing equation real imaginary part obtain two real equation ax αx β ay βx αy two case case β two equation ax αx ay αy thus least one real vector x eigenvector corresponding real eigenvalue α without loss generality assume vector x x extend x vector orthonormal basis respect standard scalar product matrix x orthogonal satisfies α matrix rn n induction hypothesis exists orthogonal matrix rn n desired form matrix u orthogonal satisfies normal endomorphisms u au α r r desired form case β first show x linearly independent x using β first equation implies also possible since eigenvector v x iy must nonzero thus x using β second equation implies also x linearly dependent exists μ r x μy two equation written ax α βμ x ax β αμ x μ implies β since μ r implies β contradicts assumption β consequently x linearly independent combine two equation system αβ x x α rank x applying gram-schmidt method respect standard scalar product matrix x yield x q q q g l r follows aq x x αβ αβ q α α real matrix αβ α considered element pair complex conjugate eigenvalue α iβ β particular nonzero since otherwise would two real eigenvalue extend vector orthonormal basis respect standard scalar product n list empty q orthogonal aq q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.2 Orthogonal and Unitary Endomorphisms</td>\n",
       "      <td></td>\n",
       "      <td>special class endomorphisms matrix analogously first case application induction hypothesis matrix yield desired matrix r u theorem implies following result real normal matrix corollary matrix rn n normal exists orthogonal matrix u rn n u au diag rm every j either r j rj αj βj j α j β j second case matrix r j considered complex matrix pair complex conjugate eigenvalue form α j iβ j proof exercise example matrix considered complex matrix eigenvalue therefore neither diagonalizable triangulated orthogonal matrix u transformed matrix u au real schur form orthogonal unitary endomorphisms section extend concept orthogonal unitary matrix endomorphisms orthogonal unitary endomorphisms definition let v finite dimensional euclidean unitary vector space endomorphism f l v v called orthogonal unitary respectively f ad f idv f ad f idv f ad f bijective hence f injective cp exercise corollary implies f bijective hence f ad unique inverse f also f f ad idv cp remark following definition note orthogonal unitary endomorphism f normal therefore result previous section also apply f lemma let v finite dimensional euclidean unitary vector space let f l v v orthogonal unitary respectively b orthonormal basis v f b b orthogonal unitary matrix respectively proof let dim v every orthonormal basis b v idv b b f ad f b b f ad b b f b b f b b h f b b thus f b b orthogonal unitary respectively euclidean case f b b h f b b following theorem show orthogonal unitary endomorphism characterized fact change scalar product arbitrary vector lemma let v finite dimensional euclidean unitary vector space scalar product f l v v orthogonal unitary respectively f v f w v w v w proof f orthogonal unitary v w v v w idv v w f ad f v w f v f w hand suppose v w f v f w v w v w f v f w v w v f ad f w v idv f ad f w since scalar product non-degenerate v chosen arbitrarily idv f ad f w w v hence idv f ad f following corollary cp lemma corollary v finite dimensional euclidean unitary vector space scalar product f l v v orthogonal unitary respectively norm induced scalar product f v v v v special class endomorphisms vector space v standard scalar product induced norm v v h v well unitary matrix cn n av v v thus av sup v cp example hold analogously orthogonal matrix rn n study eigenvalue eigenvectors orthogonal unitary endomorphisms lemma let v finite dimensional euclidean unitary vector space let f l v v orthogonal unitary respectively λ eigenvalue f proof let scalar product f v λv v v v idv v v f ad f v v f v f v λv λv v v v v implies statement lemma hold particular unitary orthogonal matrix however one keep mind orthogonal matrix orthogonal endomorphism may eigenvalue example orthogonal matrix characteristic polynomial pa real root considered element matrix eigenvalue theorem cn n unitary exists unitary matrix u cn n u h au diag λn j j rn n orthogonal exists orthogonal matrix u rn n u au diag rm every j either r j λ j λ j cj sj rj j c j j orthogonal unitary endomorphisms proof unitary matrix cn n normal hence unitarily diagonalizable cp corollary lemma eigenvalue absolute value orthogonal matrix normal hence corollary exists orthogonal matrix u rn n u au diag rm either r j αj βj rj j α j β j first case r j λ j j lemma since u orthogonal also u au orthogonal hence every diagonal block r j orthogonal well r tj r j obtain r j desired form study two important class orthogonal matrix example let j n n j n let α define ri j α j sin α co α sin α co α j matrix ri j α ri j rn n equal identity matrix except entry rii co α ri j sin α r ji sin α r j j co α n matrix α co α sin α sin α co α special class endomorphisms satisfies α α co α sin α sin α co α co α sin α sin α co α co α α α α α α one easily see matrix ri j α rn n orthogonal multiplication vector v matrix ri j α result counterclockwise rotation v angle α j plane numerical mathematics matrix ri j α called given illustrated figure vector v matrix represent rotation degree respectively example u define householder matrix h u ut u uu rn n u set h every u h u orthogonal matrix cp exercise multiplication vector v matrix h u describes reflection v hyperplane span u u hyperplane vector orthogonal u respect standard scalar product illustrated figure vector v householder matrix h u corresponds u wallace given pioneer numerical linear algebra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.3 Selfadjoint Endomorphisms</td>\n",
       "      <td></td>\n",
       "      <td>orthogonal unitary endomorphisms matlab-minute let u apply command norm u compute euclidean norm u form householder matrix u check orthogonality h via computation norm h form vector compare euclidean norm u selfadjoint endomorphisms already studied selfadjoint endomorphisms f finite dimensional euclidean unitary vector space defining property class endomorphisms f f ad cp definition obviously selfadjoint endomorphisms normal hence result sect hold strengthen result lemma finite dimensional euclidean unitary vector space v f l v v following statement equivalent f selfadjoint every orthonormal basis b v f b b f b b h exists orthonormal basis b v f b b f b b h euclidean case f b b h f b b proof corollary already shown implies obviously implies hold f b b f b b h f ad b b cp theorem hence f f ad hold following strong result diagonalizability selfadjoint endomorphisms euclidean unitary case theorem v finite dimensional euclidean unitary vector space f l v v selfadjoint exists orthonormal basis b v f b b real diagonal matrix special class endomorphisms proof consider first unitary case f selfadjoint f normal hence unitarily diagonalizable cp theorem let b orthonormal basis v f b b diagonal matrix f b b f ad b b f b b h implies diagonal entry f b b eigenvalue f real let v n-dimensional euclidean vector space b vn symmetric particular normal corolorthonormal basis v f b b lary exists orthogonal matrix u u j rn n u f b b u diag rm j either r j αj βj rj j α j β j since u f b b u symmetric block r j β j occur thus u real diagonal matrix u f b b define basis b wn v wn vn idv construction u idv b b hence u u b b therefore u f scalar product v vi v j δi j u f b b b b j u u get wi w j n u ki vk n n n n u u ki u vk u ki u k j δi j hence b orthonormal basis theorem following matrix version corollary rn n symmetric exist orthogonal matrix u rn n diagonal matrix rn n u du cn n hermitian exist unitary matrix u cn n diagonal matrix rn n u du h statement corollary known principal ax transformation briefly discus background name theory bilinear form application geometry symmetric matrix ai j rn n defines symmetric bilinear form via selfadjoint endomorphisms β r x ax n n ai j xi j map q r x β x x x ax called quadratic form associated symmetric bilinear form since symmetric exists orthogonal matrix u u u n u au real diagonal matrix en β set u u n form orthonormal basis respect standard scalar product u u n en u hence u change base obtain β β u au cp theorem thus real diagonal matrix represents bilinear form β defined respect basis quadratic form q associated β also transformed simpler form change base since analogously q x x ax x u du x dy n λi q u yn thus quadratic form q turned sum square defined quadratic form q principal ax transformation given change base canonical basis basis given pairwise orthonormal eigenvectors n pairwise orthogonal subspace span u j j n form n principal ax geometric interpretation term illustrated following example example symmetric matrix u au special class endomorphisms orthogonal matrix u u u c c number rounded fourth significant digit associated quadratic form q x define set e x q x described principal ax transformation consists transformation canonical coordinate system coordinate system given orthonormal basis eigenvectors carry transformation replace q quadratic form q get set ed r q set form ellipse centered origin two dimensional cartesian coordinate system spanned canonical basis vector ax length illustrated left part following figure element x e given x u e orthogonal matrix c c selfadjoint endomorphisms given rotation rotates ellipse e counterclockwise angle c approximately degree hence e rotated version e right part figure show ellipse e cartesian coordinate system dashed line indicate respective span vector u u eigenvectors principal ax ellipse e let rn n symmetric given vector v scalar α r q x x ax v x α x quadratic function n variable entry vector x set zero function set x q x called hypersurface degree quadric example already seen quadric case n v next give example example let n v α corresponding quadric surface ball radius around origin v α corresponding quadric let n parabola special class endomorphisms let n v α corresponding quadric parabolic cylinder corollary motivates following definition definition rn n symmetric cn n hermitian n positive n negative n zero eigenvalue counted corresponding multiplicity triple n n n called inertia let u first consider simplicity case real symmetric matrix lemma rn n symmetric inertia n n n diag congruent proof let rn n symmetric let u orthogonal matrix u rn n diag λn rn n inertia n n n assume without loss generality diag diagonal matrix contain positive negative eigenvalue respectively rn n diag rn n diag g l n r selfadjoint endomorphisms diag μm diag μm thus u u u u result used proof sylvester law theorem inertia symmetric matrix rn n invariant congruence every matrix g g l n r matrix g ag inertia proof assertion trivial let inertia n n n n n equal zero assume without loss generality n n following argument applied n lemma exist g g l n r diag g g let g g l n r arbitrary set b g ag b n n therefore b g b g b symmetric inertia n n diag matrix g g l n r show n n also n bg g b g g g g b g g g g g g l n r implies rank rank b rank b hence n set g u u n vn wn w w g w w since n let span u u n span dim x α j u j g αn αn r zero implies x ax james joseph sylvester proved result quadratic form also coined name law inertia according expressing fact existence invariable number inseparably attached bilinear form special class endomorphisms hand x analogous argument show x ax hence dimension formula subspace cp theorem yield dim v dim dim dim dim n n repeat construction interchanging role thus n n n n thus n n proof complete n following result transfer lemma theorem complex hermitian matrix theorem let cn n hermitian inertia n n n exists matrix g g l n c g h diag moreover every matrix g g l n c matrix g h ag inertia proof exercise finally discus special class symmetric hermitian matrix definition real symmetric complex hermitian n n matrix called positive semidefinite v h av v resp v positive definite v h av v resp v reverse inequality hold corresponding matrix called negative semidefinite negative definite respectively selfadjoint endomorphisms define analogously v finite dimensional euclidean unitary vector space scalar product f l v v selfadjoint f called positive semidefinite positive definite f v v v v resp f v v v v following theorem characterizes symmetric positive definite matrix see exercise exercise transfer result positive semidefinite matrix resp positive definite endomorphisms theorem rn n symmetric following statement equivalent positive definite eigenvalue real positive exists lower triangular matrix l g l n r l l selfadjoint endomorphisms proof symmetric matrix diagonalizable real eigenvalue cp corollary λ eigenvalue associated eigenvector v av λv λv v v av v v implies λ let u diag λn u diagonalization orthogonal matrix u rn n cp corollary λ j j let v arbitrary let w u w v u w v av u w u diag λn u u w w diag λn w n λ j w l l l g l n r every v v av v l l v l v since l invertible note need l lower triangular let u diag λn u diagonalization orthogonal matrix u rn n cp corollary since positive definite know λ j j set diag λn u u b b let b q r q rdecomposition invertible matrix b cp corollary q rn n orthogonal r rn n invertible upper triangular matrix b b q r q r l l l r one easily see analogous result hold complex hermitian matrix cn n case assertion lower triangular matrix l g l n c l l h factorization l l called cholesky special case lu theorem fact theorem show lu real symmetric positive definite matrix computed without row permutation order compute cholesky factorization symmetric positive definite matrix ai j rn n consider equation cholesky special class endomorphisms l l lnn lnn first row obtain j l j j analogously row n obtain aii li j li j lii aii ai j n lik l jk lik l jk lik l jk lii l ji ai j lik l jk j lii l ji symmetric hermitian positive definite matrix closely related positive definite bilinear form euclidian unitary vector space theorem v finite dimensional euclidian unitary vector space β symmetric hermitian bilinear form v respectively following statement equivalent β positive definite β v v v v every basis b v matrix representation β symmetric hermitian positive definite exists basis b v matrix representation β symmetric hermitian positive definite proof exercise exercise let rn n normal show α every α r ak every k p every p r normal let b rn n normal b ab normal well let normal symmetric show αβ α selfadjoint endomorphisms α r β r prove corollary using theorem show real skew-symmetric matrix matrix rn n complex skew-hermitian matrix matrix h cn n normal let v finite dimensional unitary vector space let f l v v normal show following assertion f f f selfadjoint b f f f f c f nilpotent f let v finite dimensional real complex vector space let f l v v diagonalizable show exists scalar product v f normal respect scalar product let cn n show following assertion normal exists normal matrix b n distinct eigenvalue commute b normal normal every c let h h hermitian h skew-hermitian part show h h h h h show furthermore normal h commute show cn n normal f z ad bc defined spectrum f bi c map f z called möbius transformation play important role function theory many area mathematics let v finite dimensional euclidian unitary vector space let f l v v orthogonal unitary respectively show f exists orthogonal unitary respectively let u let householder matrix h u defined show following assertion u matrix h u en orthogonally similar exists orthogonal matrix q rn n q h u q en implies h u eigenvalue algebraic multiplicity n respectively b every orthogonal matrix rn n written product n householder matrix exist u u n h u h u n august ferdinand möbius special class endomorphisms let v satisfy v v show exists orthogonal matrix u rn n u v transfer proof lemma theorem complex hermitian matrix thus show theorem determine symmetric matrix orthogonal matrix u u au diagonal positive definite let k r c let vn basis k prove disprove matrix h k n n positive definite v hj av j j use definition test whether symmetric matrix positive definite determine case inertia let rn n g l r rm called schur complement matrix r show positive definite positive definite schur complement see also exercise show cn n hermitian positive definite x h ax defines scalar product prove following version theorem positive semidefinite matrix rn n symmetric following statement equivalent positive semidefinite eigenvalue real nonnegative exists upper triangular matrix l rn n l l let v finite dimensional euclidian unitary vector space let f l v v selfadjoint show f positive definite eigenvalue f real positive let rn n matrix x rn n x called square root cp sect issai schur selfadjoint endomorphisms show symmetric positive definite matrix rn n symmetric positive definite square root b show matrix symmetric positive definite compute symmetric positive definite square root c show matrix jn n square root show matrix positive definite compute cholesky factorization using let b cn n hermitian let b furthermore positive definite show polynomial det b c exactly n real root prove theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19 The Singular Value Decomposition</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>chapter singular value decomposition matrix decomposition introduced chapter important many practical application since yield best possible approximation certain sense given matrix matrix low rank low rank approximation considered compression data represented given matrix illustrate example image processing first prove existence decomposition theorem let cn n given exist unitary matrix v cn n w cm h rn diag σr v r σr r rank proof set v cn w im finished let r rank since n r since h cm hermitian exists unitary matrix w wm cm w h h w diag λm rm cp corollary without loss generality assume λm every j h aw j λ j w j hence λ j w hj w j w hj h aw j j λ j j rank h rank r see modify proof lemma complex case therefore matrix h exactly r positive eigenvalue λr r time eigenvalue springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi singular value decomposition define σ j λ j j r σr let g l r x xm aw vr xr z xr xm vr vrh vr vrh z ir h h h v z x x w aw r zh z h vr z h z implies particular z vrh vr ir extend vector xr xn respect xr orthonormal basis xr standard scalar product matrix xr xn cn n v vr unitary x aw x vr z vr finally obtain vr dw h v h proof show theorem formulated analogously real matrix rn n case two matrix v w orthogonal n apply theorem h resp real case definition decomposition form called singular value decomposition short matrix diagonal entry matrix called singular value column v resp w called left resp right singular vector obtain unitary diagonalization matrix h ah h h h v w v singular value therefore uniquely determined positive square root positive eigenvalue h h unitary matrix v w singular value decomposition however eigenvectors general uniquely determined development decomposition special case middle century current general form many important player history linear algebra played role historical note concerning singular value decomposition one find contribution jordan sylvester schmidt current form shown carl henry eckart gale young singular value decomposition write svd form im wh w w h u p v h v u cn orthonormal column u h u im p p h cm positive semidefinite inertia r r factorization u p called polar decomposition viewed generalization polar representation complex number z eiϕ lemma suppose matrix cn rank r svd form v vn w wm considering element l im span vr ker span wr wm proof j r aw j v h w j v j σ j v j since σ j hence r linear independent vector satisfy vr im r rank dim im implies im span vr j r aw j hence linear independent vector satisfy wr wm ker dim ker dim im r implies ker span wr wm svd form written r σ j v j w hj thus written sum r matrix form σ j v j w hj rank σ j v j w hj let ak k σ j v j w hj k k rank ak k using matrix unitarily invariant cp exercise get ak σr hence approximated matrix ak rank approximating matrix approximation error matrix explicitly known singular value decomposition furthermore yield best possible approximation matrix rank k respect matrix theorem ak ak every matrix b cn rank b k singular value decomposition proof assertion clear k rank since ak ak let k rank let b cn rank b k given dim ker b k consider b element l wm right singular vector u span dimension k since ker b u subspace dim ker b dim u ker b u let v ker b u given exist c v α j w j j hence b v av α j aw j αjσjvj therefore α j σ j v j max b b j σ j since pairwise orthonormal j since ak completes proof matlab-minute command n generates n n n matrix entry n row column diagonal sum equal entry therefore magic square compute svd using command v w said singular value rank form ak k rank verify numerically equation svd one important practical mathematical tool almost area science engineering social science medicine even psychology great importance due fact svd allows distinguish important non-important information given data practice latter singular value decomposition corresponds measurement error noise transmission data fine detail signal image play important role often important information corresponds large singular value non-important information small one many application one see furthermore singular value given matrix decay rapidly exist large many small singular value case matrix approximated well matrix low rank since already small k approximation error ak small low rank approximation ak requires little storage capacity computer k scalar vector stored make svd powerful tool application data compression interest example illustrate use svd image compression picture obtained research center matheon mathematics key greyscale picture shown left figure consists pixel pixel given value value stored real matrix full rank compute svd v using command v w matlab diagonal entry matrix singular value ordered decreasingly matlab theorem k compute matrix ak rank k using command k k k matrix represent approximation original picture based k largest singular value corresponding singular vector three approximation shown next original picture quality approximation decrease decreasing k even approximation k show essential feature matheon bear another important application svd arises solution linear system equation cn svd form define matrix w v h n rm n thank falk ebert help original bear seen front mathematics building tu berlin information matheon found singular value decomposition one easily see ir w h rm r n invertible right hand side equation equal identity matrix case matrix therefore viewed generalized inverse case invertible matrix equal inverse definition matrix called moore-penrose pseudoinverse let cn b given linear system equation ax b x close possible solution try find x b using moore-penrose inverse obtain best possible approximation respect euclidean norm theorem let cn n b given v h x b satisfies svd x r h v b j x σj x proof let given let z ξm w h v v h b h b r n h h b σ j ξ j j j n h b j equality hold ξ j v hj b j j r satisfied z w h v h b last equation hold w v h b b eliakim hastings moore sir roger penrose singular value decomposition vector x therefore attains lower bound equation r h vj x σj easily checked every vector attains lower bound must form h vh b b r yr ym σr yr ym c implies x minimization problem vector x written x min τm pairwise distinct τm r minimization problem corresponds problem linear regression least square approximation example solved q r-decomposition q r decomposition h h cp exercise r h q h q r r h q h r r h r h q h r q h thus solution least-squares approximation example identical solution minimization problem using svd exercise show frobenius norm matrix unitarily invariant f f cn unitary matrix p cn n q cm hint frobenius norm one use trace h use result exercise show f σr singular value cn show h h cn show cn singular value decomposition let cn let moore-penrose inverse show following assertion rank h h b matrix x uniquely determined matrix satisfies following four matrix equation ax x ax x ax h ax x h x let b compute moore-penrose inverse vector x x x b x prove following theorem let cn b n h b h b b u matrix u n u h u b real u also chosen real hint one direction trivial direction consider unitary diagonalization h b h b yield matrix w svd b show assertion using two decomposition theorem application found article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.1 Sets and Mathematical Logic</td>\n",
       "      <td></td>\n",
       "      <td>chapter basic mathematical concept chapter introduce mathematical concept form basis development following chapter begin set basic mathematical logic consider map set important property finally discus relation particular equivalence relation set set mathematical logic begin development concept set use following definition definition set collection well determined distinguishable object x perception thinking object called element object x definition well determined therefore uniquely decide whether x belongs set write x x element set otherwise write x furthermore element distinguishable mean element pairwise distinct two object x equal write x otherwise x mathematical object usually give formal definition equality example consider equality set see definition describe set curly bracket contain either list element example red yellow green georg cantor one founder set theory cantor published definition journal mathematische annalen springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi basic mathematical concept defining property example x x positive even number x x person owning bike well known set number denoted follows n natural number natural number including zero z integer q x x z b n rational number r x x real number real number construction characterization real number r usually done introductory course real analysis describe set via defining property formally write x p x p predicate may hold object x p x assertion p hold x general assertion statement classified either true false instance statement set n infinitely many element true sentence tomorrow weather good assertion since meaning term good weather unclear weather prediction general uncertain negation assertion assertion denote assertion true false false true instance negation true assertion set n infinitely many element given set n infinitely many element set n finitely many element false two assertion b combined via logical composition new assertion following list common logical composition together mathematical short hand notation composition conjunction disjunction implication notation equivalence wording b b implies b b sufficient condition b b necessary condition b equivalent true b true necessary sufficient b b necessary sufficient set mathematical logic example write assertion x real number x negative x r x whether assertion composed two assertion b true false depends logical value b following table logical value f denote true false respectively f f b f f f f f f f f f example assertion b true b true assertion b false true b false particular false b true independent logical value b thus true since true false since false hand assertion true since false following often prove certain implication b true table logical value show example illustrates prove assumption true assertion b true well instead assume true often write let hold easy see b exercise create table logical value compare table b truth b therefore proved showing truth implies truth b false implies false assertion called contraposition assertion b conclusion b called proof contraposition together assertion also often use so-called quantifier quantifier universal existential notation wording exists return set theory introduce subset equality set definition let n set called subset n denoted n every element also element n write n hold n called equal denoted n n n write n hold basic mathematical concept called proper subset n denoted n n n hold using notation mathematical logic write definition follows n n n x x x n n n n n assertion right side equivalence read follows object x truth x implies truth x n shorter x x hold x n hold special set set element define formally follows definition set ø x x x called empty set notation mean defined introduced empty set defining property every object x x x element ø hold object hence ø contain element set contains least one element called nonempty theorem every set following assertion hold ø ø ø proof show assertion x x ø x true since x ø assertion x ø false therefore x ø x true every x cp remark implication b let ø know ø hence ø follows definition theorem let n l set following assertion hold subset relation reflexivity n n l l transitivity proof show assertion x x x true x true x x implication two true assertion hence true show assertion x x x l true x true also x n true since n truth x n implies x l true since n hence assertion x x l true set mathematical logic definition let n set n n x x x n intersection n n x x x n difference n n x x x n n ø set n called disjoint set operation union intersection extended two set ø set set mi mi x x mi mi x x mi set called index set n n write union intersection set mn n mi n mi theorem let n two set n following equivalent n n ø proof show hold since n exists x n x thus x n n ø hold exists x n x hence n since n hold see n hold theorem let n l set following assertion hold n n commutativity n n n n associativity n l n l n l n distributivity n l n l n l n l n n l n l n l n l proof exercise notation n n union intersection set n introduced giuseppe peano one founder formal logic notation smallest common multiple n largest common divisor n set n suggested georg cantor catch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.2 Maps</td>\n",
       "      <td></td>\n",
       "      <td>basic mathematical concept definition let set cardinality denoted number element power set denoted p set subset p n n empty set ø cardinality zero p ø ø thus ø cardinality p ø hence one show every set finitely many element finite cardinality hold map section discus map set definition let x nonempty set map f x rule assigns x x exactly one f x write f x x f x instead x f x also write f x set x called domain codomain f two map f x g x called equal f x g x hold x x write f definition assumed x nonempty since otherwise rule assigns element element x one set empty one define empty map however following always assume always explicitly state set given map act nonempty example two map x r r given f x f x x x g x x x analyze property map need terminology map definition let x nonempty set map id x x x x x called identity x let f x map let x n f f x x called image f f n x x f x n called pre-image n f f x x f x map ø x f x f x called restriction f one note definition f n set hence symbol f mean inverse map f map introduced definition example map domain x r following property f x x r x f f ø g x g g x r x definition let x nonempty set map f x called injective x equality f f implies surjective f x bijective f injective surjective every nonempty set x simplest example bijective map x x id x identity x example let x r x f r r f x x neither injective surjective f r f x x surjective injective f r f x x injective surjective f f x x bijective assertion used continuity map f x x discussed basic course analysis particular used fact continuous function map real interval real interval assertion also show important include domain codomain definition map theorem map f x bijective every exists exactly one x x f x proof let f bijective let since f surjective exists x f x also satisfies f basic mathematical concept follows injectivity f therefore exists unique x f since exists unique x x f x follows f x thus f surjective let x f f assumption implies f also injective one show two set x finite cardinality exists bijective map lemma set x n exist exactly pairwise distinct bijective map x proof exercise definition let f x x f x g z g map composition f g map g f x z x g f x expression g f read g f stress order composition first f applied x g f x one immediately see f id x f idy f every map f x theorem let f w x g x h z map h g f h g f composition map associative f g g f proof exercise theorem map f x bijective exists map g x g f id x f g idy proof f bijective theorem every exists x x x f x define map g g x g x let given hence f g idy f g f g f hand x x given f x theorem x exists unique x f x g f x g f g f g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.3 Relations</td>\n",
       "      <td></td>\n",
       "      <td>map g f id x assumption g f id x thus g f injective thus also f injective see exercise moreover f g idy thus f g surjective hence also f surjective see exercise therefore f bijective map g x characterized theorem unique another map h x h f id x f h idy h id x h g f h g f h g idy lead following definition definition f x bijective map unique map g x theorem called inverse inverse map f denote inverse f f show given map g x unique inverse bijective map f x sufficient show one equation g f id x f g idy indeed f bijective g f id x g g idy g f f g f f id x f f way g f follows assumption f g idy theorem f x g z bijective map following assertion hold f bijective f f g f bijective g f f g proof exercise know theorem g f x z bijective therefore exists unique inverse g f map f g f g g f f g g f f g g f f idy f f f id x hence f g inverse g f relation first introduce cartesian two set named rené descartes founder analytic geometry georg cantor used name connection set n notation n basic mathematical concept definition n nonempty set set n x x n cartesian product n element x n called ordered pair easily generalize definition n n nonempty set mn mn xn xi mi n element xn mn called ordered n-tuple n-fold cartesian product single nonempty set n xn xi n n time definition least one set empty resulting cartesian product empty set well definition n nonempty set set r n called relation n n r called relation instead x r also write x r x clear relation considered definition least one set n empty every relation n also empty set since n ø instance n n q r x n x relation n expressed r n n n definition relation r set called reflexive x x hold x symmetric x x hold x transitive x z x z hold x z r reflexive transitive symmetric called equivalence relation example let r x x r reflexive since x hold x x also hence r symmetric finally r transitive example x r z r x z r relation relation r x x reflexive transitive symmetric f r r map r x f x f equivalence relation definition let r equivalence relation set x set x r x r x called equivalence class x respect set equivalence class x r x called quotient set respect equivalence class x r element x never empty set since always x x reflexivity therefore x x r clear equivalence relation r meant often write x instead oft x r also skip additional respect r theorem r equivalence relation set x following equivalent x x ø x proof since x x follows x x x follows x thus x x since x ø exists z x element z x z z thus x z z symmetry therefore x transitivity let x z x x z using symmetry transitivity obtain z hence z mean x analogous way one show x hence x hold theorem show two equivalence class x either x x ø thus every x contained exactly one equivalence class namely x equivalence relation r yield partitioning decomposition mutually disjoint subset every element x called representative equivalence class x useful general approach often use book partition set object set matrix equivalence class find class representative particularly simple structure representative called normal form respect given equivalence relation basic mathematical concept example given number n n set rn b b divisible n without remainder equivalence relation z since following property hold reflexivity divisible n without remainder symmetry b divisible n without remainder also b transitivity let b b c divisible n without remainder write c b b c summands right divisible n without remainder hence also hold z equivalence class called residue class modulo n nz nz z z equivalence relation rn yield partitioning z n mutually disjoint subset particular n set residue class modulo n quotient set respect rn often denoted thus n set play important role mathematical field number theory exercise let b c assertion show following assertion true associative law b c b c b c b c hold b commutative law b b b b hold c distributive law b c c b c b c c b c hold let b c assertion show following assertion true b b b b b relation c e f b b b b c c b c c assertion c called de morgan law prove theorem show two set n following hold n let x nonempty set u v nonempty subset let f x map show f u v f u f v let u v x nonempty check whether f u v f u f v hold following map injective surjective bijective f r r x b f r x x c f r x x n n even f n z n n odd show two map f x g z following assertion hold g f surjective g surjective b g f injective f injective let z given show map f z z f x x bijective prove lemma prove theorem prove theorem find two map f g n n simultaneously f surjective b g injective c g f bijective determine equivalence relation set determine symmetric transitive relation set b c reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>20 The Kronecker Product and Linear Matrix Equations</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>chapter kronecker product linear matrix equation many application particular stability analysis differential equation lead linear matrix equation ax x b matrix b c given goal determine matrix x solves equation give formal definition description solution equation kronecker another product matrix useful chapter develop important property product study application context linear matrix equation many result topic found book definition k field ai j k b k n n b b b ai j b b amm b called kronecker product b kronecker product sometimes called tensor product matrix product defines map k k n n k mn mn definition extended non-square matrix simplicity consider case square matrix following lemma present basic computational rule kronecker product lemma square matrix b c k following computational rule hold b c b leopold kronecker said used product lecture berlin defined formally first time johann georg zehfuss springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi kronecker product linear matrix equation μa b μb μ b μ k b c c b c whenever b defined b c b c whenever b c defined b b therefore kronecker product two symmetric matrix symmetric proof exercise particular contrast standard matrix multiplication order factor kronecker product change transposition following result describes matrix multiplication two kronecker product lemma c k b k n n b c ac b hence particular b im b im b b b b invertible proof since b ai j b c ci j block fi j k n n block matrix fi j b c given fi j aik b ck j aik ck j b aik ck j b block matrix g j ac b g j k n n obtain g j gi j b gi j aik ck j show b c ac b easily follow equation general kronecker product non-commutative cp exercise following relationship b b lemma k b k n n exists permutation matrix p k mn mn p b p b proof exercise computation determinant trace rank kronecker product exist simple formula kronecker product linear matrix equation theorem k b k n n following rule hold det b det n det b det b trace b trace trace b trace b rank b rank rank b rank b proof lemma multiplication theorem determinant cp theorem get det b det im b det det im b lemma exists permutation matrix p p p implies det det p p det det n since det im b det b follows det b det n det b therefore also det b det b b ai j b obtain trace b n aii b j j aii n b j j trace trace b trace b trace trace b exercise matrix k n column j k j n define vec k application vec turn matrix column vector thus vectorizes lemma map vec k n k isomorphism particular ak k n linearly independent vec vec ak k linearly independent proof exercise consider relationship kronecker product vec map kronecker product linear matrix equation theorem k b k n n c k n vec ac b b vec c hence particular vec ac vec c vec c b b im vec c vec ac c b b im vec c proof j n jth column ac b given ac b e j ac j n bk j ac ek n bk j cek j j bn j vec c implies vec ac b b vec c b resp im obtain linearity vec yield order study relationship eigenvalue matrix b kronecker product use bivariate polynomial polynomial two variable cp exercise p l j αi j k polynomial k b k n n define matrix p b l αi j ai b j careful order factor since general ai b j b j ai cp exercise example rm b rn n p r get matrix p b b following result known stephanos named cyparissos stephanos showed besides result also assertion lemma kronecker product linear matrix equation theorem let k b k n n two matrix jordan normal form eigenvalue λm k μn k respectively p b defined following assertion hold eigenvalue p b p λk k eigenvalue b λk k eigenvalue λk k proof let g l k g l n k j bt j b jordan canonical form matrix j j b upper triangular thus j j j matrix j ai j b j ai j b upper triangular eigenvalue j j j j j b λm μn respectively thus p λk k n diagonal entry matrix p j j b using lemma obtain p b l αi j j j b j l l αi j j ai j b j αi j j ai j b j l αi j j ai j b j l αi j j ai j b j p j j b implies assertion follow p p respectively following result matrix exponential function kronecker product helpful application involve system linear differential equation lemma cm b cn n c im b exp c exp exp b proof lemma know matrix im b commute using lemma obtain kronecker product linear matrix equation exp c exp im b exp exp im b j im b j j im b j j b j exp exp b used property matrix exponential series cp sect given matrix j k b j k n n j q c k n equation form x x aq x bq c called linear matrix equation unknown matrix x k n theorem matrix x k n solves x vec x solves linear system equation k gx vec c g q b tj j proof exercise consider two special case theorem cm b cn n c cm n sylvester ax x b c unique solution common eigenvalue eigenvalue b negative real part unique solution given x exp c exp b dt sect integral defined entrywise james joseph sylvester kronecker product linear matrix equation proof analogous representation theorem write sylvester equation b im x vec c b eigenvalue λm μn respectively g b im theorem eigenvalue λk k thus g invertible sylvester equation uniquely solvable λk k let b matrix eigenvalue negative real part common eigenvalue unique solution let j j b bt jordan canonical form b consider linear differential equation dz az z b z c dt solved function z cm n z exp c exp b cp exercise function satisfies lim z lim exp c exp b lim exp j exp j b constant integration equation yield z lim z z z dt z dt b use without proof existence infinite integral implies x z dt exp c exp b dt unique solution theorem also give solution another important matrix equation corollary c cn n lyapunov ax x h alexandr mikhailovich lyapunov also ljapunov liapunov kronecker product linear matrix equation unique solution x cn n eigenvalue negative real part furthermore c hermitian positive definite also x hermitian positive definite proof since assumption h common eigenvalue unique solvability follows theorem solution given matrix x exp exp h dt exp c exp h dt c hermitian positive definite x hermitian x h x xx x exp c exp dt x x h exp c exp h x dt last inequality follows monotonicity integral fact x also exp h x since exp h invertible every real exercise prove lemma construct two square matrix b b b prove lemma prove theorem prove lemma show b normal cm b cn n normal true b unitary b unitary use singular value decomposition v w ah cm b vb b w bh cn n derive singular value decomposition b show cm b cn n matrix equation b b hold prove theorem let cm b cn n c cm n show z exp c exp b solution matrix differential equation ddtz az z b initial condition z c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.1 Groups</td>\n",
       "      <td></td>\n",
       "      <td>chapter algebraic structure algebraic structure set operation element follow certain rule example structure consider integer operation property addition already elementary school one learns sum b two integer b another integer moreover number every integer every integer exists integer analysis property concrete example lead definition abstract concept built simple axiom integer operation addition lead algebraic structure group principle abstraction concrete example one strength basic working principle mathematics extracting completely exposing mathematical kernel david hilbert also simplify work every proved assertion abstract concept automatically hold concrete example moreover combining defined concept move generalization way extend mathematical theory step step hermann günther graßmann described procedure mathematical method move forward simplest concept combination gain via combination new general group begin set operation specific property definition group set g map called operation g g g b b die mathematische methode hingegen schreitet von den einfachsten begriffen zu den zusammengesetzteren fort gewinnt durch verknüpfung de besonderen neue allgemeinere springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi algebraic structure satisfies following operation associative b c b c hold b c exists element e g called neutral element e g b every g exists g called inverse element b b hold b g group called commutative short hand notation group use g g clear operation used theorem every group g following assertion hold e g neutral element g e also e g neutral element g also e g contains exactly one neutral element every g exists unique inverse element proof let e g neutral element let g satisfy definition exists element g thus e e let e g neutral element let exists g also e follows e e let e g two neutral element e e since neutral element since e also neutral element follows e e second identity used assertion hence e let g two inverse element g let e g unique neutral element follows e e named niels henrik abel founder group theory group example z q r commutative group group neutral element number zero inverse number instead usually write b since operation addition group also called additive group natural number n addition form group since neutral element consider set includes also number zero inverse element hence also addition form group set q r usual multiplication form commutative group multiplicative group neutral element number one inverse element number instead also write ab integer z multiplication form group set z includes number z z inverse element z definition let g group h h group called subgroup g next theorem give alternative characterization subgroup theorem h subgroup group g following property hold ø h b h b h every h also inverse element satisfies proof exercise following definition characterizes map two group compatible respective group operation definition let g g group map ϕ g g g ϕ g called group homomorphism ϕ b ϕ ϕ b b g bijective group homomorphism called group isomorphism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.2 Rings and Fields</td>\n",
       "      <td></td>\n",
       "      <td>algebraic structure ring field section extend concept group discus mathematical structure characterized two operation motivating example consider integer addition group z multiply element z multiplication associative b b c z furthermore addition multiplication satisfy distributive law b c b c b c c b c integer b property make z addition multiplication ring definition ring set r two operation r r r b b addition r r r b b multiplication satisfy following r commutative group call neutral element group zero write denote inverse element r write b instead multiplication associative b c b c b c distributive law hold b c r b c b c b c c b ring called commutative b b b element r called unit case r called ring unit right hand side two distributive law omitted parenthesis since multiplication supposed bind stronger addition b c b useful illustration purpose nevertheless use parenthesis sometimes write b c instead b c analogous notation group denote ring r r operation clear context ring unit unit element unique e r satisfy e e r particular e e r use following abbreviation sum product element n j n j ring field moreover n empty sum r n k define k j ring unit also define k empty product k j theorem every ring r following assertion hold b b b b proof every r adding left right hand side equality obtain way show since b b follows unique additive inverse b b way show b b furthermore b thus b immediately clear z commutative ring unit standard example concept ring modeled example let nonempty set let r set map f r operation r r r r r r f g f g f g f g f g x f x g x f g x f x g x commutative ring unit f x g x f x g x sum product two real number zero ring map r r x unit map r r x real number zero one definition ring additive inverse element occur formally define concept multiplicative inverse algebraic structure definition let r ring unit element b r called inverse r respect b b element r inverse called invertible clear definition b r inverse r r inverse b general however every element ring must invertible element invertible unique inverse shown following theorem theorem let r ring unit r invertible inverse unique denote b r invertible b r invertible b proof b b r inverse r b b b b b b b since b invertible r well defined b b b way show b thus b algebraic point view difference integer one hand rational real number set q r every element except number zero invertible additional structure make q r field definition commutative ring r unit called field every r invertible definition every field commutative ring unit converse hold one also introduce concept field based concept group cp exercise definition field set k two operation k k k k k k b b b b addition multiplication ring field satisfy following k commutative group call neutral element group zero write denote inverse element k write b instead k commutative group call neutral element group unit write denote inverse element k distributive law hold b c k b c b c b c c b show useful property field lemma every field k following assertion hold k least two element k b c imply b c b c k b imply b b k proof follows definition since k already shown ring cp theorem since know exists multiplying side b c left yield b suppose b finished exists multiplying side b left yield b ring r element r called zero b r exists b element called trivial zero divisor property lemma mean field contain trivial zero divisor also ring property hold instance ring integer z later chapter encounter ring matrix contain non-trivial zero divisor see proof theorem following definition analogous concept subgroup cp definition subring cp excercise definition let k field l k l field called subfield k two important example algebraic concept discussed discus field complex number ring polynomial concept zero divisor introduced karl theodor wilhelm weierstraß algebraic structure example set complex number defined c x x r r set define following operation addition multiplication c c c c c c right hand side use addition multiplication field c field neutral element respect addition multiplication given inverse element respect addition multiplication given x x c x x c x x x multiplicative inverse element written ab instead common notation considering subset l x x r c identify every x r element set l via bijective map x x particular thus interpret r subfield c although r really subset c distinguish zero unit element r special complex number imaginary unit satisfies identified real number complex number imaginary unit denoted hence write using identification x r x c write z x c x x x x iy z im z ring field last expression z x im z abbreviation real part imaginary part complex number z x since iy yi justified write complex number x iy x yi given complex number z x z x iy number z x respectively z x iy called associated complex conjugate number using real square root modulus absolute value complex number defined zz x iy x iy x ix iyx x simplification omitted multiplication sign two complex number equation show absolute value complex number nonnegative real number property complex number stated exercise end chapter example let r commutative ring unit polynomial r indeterminate variable expression form p αn n αn r coefficient polynomial instead α j j often write α j j set polynomial r denoted r let p αn n q βm two polynomial r n n set β j j n call p q equal written p q α j β j j particular αn n αn n n degree polynomial p αn n denoted deg p defined largest index j α j index exists polynomial zero polynomial p set deg p let p q r degree n respectively n n set β j j define following operation r algebraic structure p q αn βn n αi β j p q γk operation r commutative ring unit zero given polynomial p unit p r field since every polynomial p r invertible even r field example p polynomial q βm r p q βm hence p invertible polynomial substitute variable object resulting expression evaluated algebraically example may substitute λ r interpret addition multiplication corresponding operation ring defines map r r λ p λ αn λn λk λ λ k n k time r empty product one confuse ring element p λ polynomial p rather think p λ evaluation p λ study property polynomial detail later also evaluate polynomial object matrix endomorphisms exercise determine following whether form group x r x b b b r b ab let b r map f b r r r r x ax ay set g f b b r given show g commutative group operation g g g defined composition two map cp definition let x ø set let x f x x f bijective show x group let g group g denote g unique inverse element show following rule element g b c b b b ring field prove theorem let g group fixed g let z g g g g g show z g subgroup subgroup element g commute called centralizer let ϕ g h group homomorphism show following assertion u g subgroup also ϕ u h subgroup furthermore g commutative also ϕ u commutative even h commutative b v h subgroup also v g subgroup let ϕ g h group homomorphism let eg e h neutral element group g h respectively show ϕ eg e h b let ker ϕ g g ϕ g e h show ϕ injective ker ϕ eg show property definition r example order show r commutative ring unit suppose example replace codomain r map commutative ring unit r still commutative ring unit let r ring n show following assertion n even r n n n odd b exists unit r n r invertible element r n n n called nilpotent let r ring unit show r let r ring unit let r denote set invertible element show r group called group unit r b determine set k k k field fixed n n let nz nk k z n example show nz subgroup z b define b b b b b b addition multiplication addition multiplication z show following assertion algebraic structure well defined ii commutative ring unit iii field n prime number let r ring subset r called subring r ring show subring r following property hold r r also r r r also show definition field describe mathematical structure let k field show l subfield k cp definition following property hold l k k b l b l b l l l show field hold let r commutative ring contain non-trivial zero divisor ring called integral domain define r r relation x x x x show equivalence relation b denote equivalence class x xy show following map well defined x x x x x x x x denotes quotient set respect cp definition c show field field called quotient field associated field r z exercise consider r k ring polynomial field k construct way field rational function ring field let c b determine b b b ab ba show following rule complex number z z z z z z z z z z b z z z z z c show absolute value complex number satisfies following property z z z b z c equality z c z z z c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.1 Basic Definitions and Operations</td>\n",
       "      <td></td>\n",
       "      <td>chapter matrix chapter define matrix important operation study several group ring matrix james joseph sylvester coined term matrix described matrix oblong arrangement term matrix operation defined chapter introduced arthur cayley article memoir theory matrix first consider matrix independent algebraic object book matrix form central approach theory linear algebra basic definition operation begin formal definition matrix definition let r commutative ring unit let n array form ai j anm latin word matrix mean womb sylvester considered matrix object may form various system determinant cp chap interestingly english writer charles lutwidge dodgson better known pen name lewis carroll objected sylvester term wrote aware word matrix already use express meaning use word block surely former word mean rather mould form algebraic quantity may introduced actual assemblage quantity dodgson also objected notation ai j matrix entry space occupied number wholly superfluous important part notation reduced minute subscript alike difficult writer springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi matrix ai j r n j called matrix size n ai j called entry coefficient matrix set matrix denoted r n following usually assume without explicitly mentioning excludes trivial case ring contains zero element cp exercise formally definition n obtain empty matrix size n denote matrix used technical reason proof analyze algebraic property matrix however always consider n zero matrix r n denoted matrix entry equal matrix size n n called square matrix square entry aii n called diagonal entry identity matrix r n n matrix δi j j δi j kronecker delta-function.2 clear n considered write instead n set ith row r n aim r n use comma optical separation entry jth column j j r j j thus row column matrix matrix matrix ai aim r n given combine matrix n r leopold kronecker anm basic definition operation write square bracket around row way combine n matrix j j j r j j matrix n r anm n n ai j r ni j j combine four matrix matrix r n matrix ai j called block block matrix introduce four operation matrix begin addition r n r n r n b b ai j bi j addition r n operates entrywise based addition note addition defined matrix equal size multiplication two matrix defined follows r n r r n b b ci j ci j aik bk j thus entry ci j product b constructed successive multiplication summing entry ith row jth column b clearly order define product b number column must equal number row b definition entry ci j matrix b written multiplication symbol element follows usual convention omitting multiplication sign clear multiplication considered eventually also omit multiplication sign matrix matrix illustrate multiplication rule cij equal ith row time jth column b follows j b b b mj m aim ci j anm important note matrix multiplication general commutative example matrix b hand b although b b defined obviously b b case one recognizes non-commutativity matrix multiplication fact b b different size even b b defined size general b b example yield two product b matrix multiplication however associative distributive respect matrix addition basic definition operation lemma r n b b r c r k following assertion hold b c b b b b b b b b im proof show property others exercise let r n b r c r k well b c di j b c di j definition matrix multiplication using associative distributive law r get di j ait bts c j ait bts c j c j ait bts ait bts c j di j n j k implies b c b c right hand side lemma written parenthesis since use common convention multiplication matrix bind stronger addition r n n define k n ak k time another multiplicative operation matrix multiplication defined follows r r n r n λ λ λai j easily see r n addition scalar multiplication following property lemma b r n c r λ μ r following assertion hold λμ λ μ λ μ λ μ term scalar introduced sir william rowan hamilton originates latin word scale mean ladder matrix λ b λ λ b λ c λ c λ c proof exercise fourth matrix operation introduce transposition r n r n ai j bi j bi j ji example matrix called transpose definition r n n satisfies called symmetric called skew-symmetric transposition following property lemma r n b r λ r following assertion hold λ λ b b proof property exercise proof let b ci j ai j b bi j b ci j ci j aik bk j ci j c ji jk bki ak j bik ak j see b b matlab-minute carry following command order get used matrix operation chapter matlab notation order see matlab output put semicolon end command basic definition operation example consider example car insurance premium chap recall pi j denotes probability customer class ci year move class c j example consists four class probability associated row-stochastic matrix cp denote suppose insurance company following distribution customer four class class class class class matrix describes initial customer distribution using matrix multiplication compute p contains distribution customer next year example consider entry p position computed customer class year move class thus respective initial percentage multiplied probability customer class class probability respectively yield two product continuing way obtain k year distribution pk k k formula also hold k since p insurance company use formula compute revenue payment premium rate coming year assume full premium rate class euro per year rate class euro discount customer initially revenue first year euro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.2 Matrix Groups and Rings</td>\n",
       "      <td></td>\n",
       "      <td>matrix customer cancel contract model yield revenue year k pk p k example revenue next year rounded full euro number decrease annually rate decrease seems slow exists stationary state state revenue changing significantly property model guarantee existence state important practical question insurance company existence stationary state guarantee significant revenue long-time future since formula depends essentially entry matrix p k reached interesting problem linear algebra analysis property row-stochastic matrix analyze property sect matrix group ring section study algebraic structure formed certain set matrix matrix operation introduced begin addition r n theorem r n commutative group neutral element r n zero matrix ai j r n inverse element j r n write b instead proof using associativity addition r arbitrary b c r n obtain b c ai j bi j ci j ai j bi j ci j ai j bi j ci j ai j bi j ci j b c thus addition r n associative zero matrix r n satisfies ai j ai j ai j given ai j r n j r n j ai j j ai j finally commutativity addition r implies b ai j bi j ai j bi j bi j ai j b note lemma implies transposition homomorphism even isomorphism group r n r n cp definition matrix group ring theorem r n n ring unit given identity matrix ring commutative n proof already shown r n n commutative group cp theorem property ring associativity distributivity existence unit element follow lemma commutativity n hold commutativity multiplication ring example show ring r n n commutative n example proof theorem show n ring r n n non-trivial zero-divisors exist matrix b r n n b exist even r field let u consider invertibility matrix ring r n n respect r n n must satisfy matrix multiplication given matrix r n n inverse cp definition inverse two equation r n n exists invertible inverse unique denoted cp theorem invertible matrix sometimes called non-singular non-invertible matrix called singular show corollary existence inverse already implied one two equation one hold invertible correct check validity equation matrix r n n invertible simple example non-invertible matrix r r another non-invertible matrix however considered element unique inverse given lemma b r n n invertible following assertion hold invertible also write matrix b invertible b b matrix proof using lemma int int thus inverse already shown theorem general ring unit thus hold particular ring r n n next result show invertible matrix form multiplicative group theorem set invertible matrix r form group respect matrix multiplication denote group g l n r gl abbreviates general linear group proof associativity multiplication g l n r clear shown lemma product two invertible matrix invertible matrix neutral element g l n r identity matrix since every g l n r assumed invertible exists g l n r introduce important class matrix definition let ai j r n n called upper triangular ai j j called lower triangular ai j j upper triangular called diagonal ai j j upper lower triangular write diagonal matrix diag ann next investigate set matrix respect group property beginning invertible upper lower triangular matrix theorem set invertible upper triangular n n matrix invertible lower triangular n n matrix r form subgroup g l n r proof show result upper triangular matrix proof lower triangular matrix analogous order establish subgroup property prove three property theorem since invertible upper triangular matrix set invertible upper triangular matrix nonempty subset g l n r next show two invertible upper triangular matrix b r n n product c b invertible upper triangular matrix invertibility c ci j follows lemma j matrix group ring n ci j aik bk j bk j k j aik bk j aik k j since j j therefore c upper triangular remains prove inverse invertible upper triangular matrix upper triangular matrix n assertion hold trivially assume n let ci j equation written system n equation j j ann cn j δn j j δi j kronecker delta-function defined prove inductively n n diagonal entry aii invertible cii n ci j δi j j formula implies particular ci j j n last row given ann cn j δn j j j n ann cnn cnn ann second equation use cnn commutativity multiplication therefore ann invertible ann thus δn j j cn j ann equivalent note n sum empty thus equal zero particular cn j j n assume assertion hold n k k n particular ci j k n j word row n k upper triangular order prove assertion k consider kth row given matrix akk ck j ak j akn cn j δk j j j k n obtain akk ckk ak k akn cnk induction hypothesis k cn k implies akk ckk ckk akk used commutativity multiplication hence ckk get akk invertible akk δk j ak j akn cn j ck j akk j n hence hold k j δk j j cn j give ck j point represents recursive formula computing entry inverse invertible upper triangular matrix using formula entry computed bottom top right left process sometimes called backward substitution following frequently partition matrix block make use block multiplication every k n write r n n r k k r b r n n partitioned like product b evaluated blockwise particular g l k r g l r g l n r direct computation show matrix group ring matlab-minute create block matrix matlab carrying following command tridiag zero k investigate meaning command full compute product well inverse inv inv b compute inverse b matlab formula corollary set invertible diagonal n n matrix r form commutative subgroup respect matrix multiplication invertible upper lower triangular n n matrix proof since invertible diagonal matrix invertible diagonal n matrix form nonempty subset invertible upper lower triangular n n matrix diag ann b diag bnn invertible b invertible cp lemma diagonal since b diag ann diag bnn diag ann bnn moreover diag ann invertible aii r invertible n cp proof theorem inverse given ann finally commutativity property invertible diagonal matrix diag b b follows directly commutativity definition matrix p r n n called permutation matrix every row every column p exactly one unit entry zero term permutation mean exchange matrix r n n multiplied permutation matrix left right row column respectively exchanged permuted example p matrix p p theorem set n n permutation matrix r form subgroup g l n r particular p r n n permutation matrix p invertible p p proof exercise omit multiplication sign matrix multiplication write ab instead b exercise following exercise r commutative ring unit consider following matrix z b c determine possible matrix c bc b c c b ac c b consider matrix ai j r n x r ym r xn following expression well defined n n x b x c yx yx e x ay f x ay g x ay h x ay x j x k ax l x show following computational rule r n r r matrix group ring prove lemma prove lemma prove lemma let determine n n let p αn n r polynomial cp example r define p r p αn im b fixed matrix r consider map f r r p p show f p q f p f q f pq f p f q p q r map f ring homomorphism ring r r c show f r p p r commutative subring r f r subring r cp exercise multiplication subring commutative map f surjective determine p p z let k field show every matrix k n n written symmetric matrix k n n skew-symmetric matrix k n n also hold field give proof counterexample show binomial formula commuting matrix b r n n k j j k k ab b b j b kj j j n n invertible show let r matrix j hold every let r n n matrix n exists let smallest natural number property investigate whether invertible give particularly simple representation inverse b determine cardinality set ak k n let ai j r n n j j n show subring r n n b show r n n subring property called left ideal r n n c determine analogous subring b r n n b b r n n b b subring property called left ideal r n n examine whether g matrix co α sin α α r sin α co α subgroup g l r generalize block multiplication matrix r n b r determine invertible upper triangular matrix r n n let r n n r n n r n n r n n let b let r n n g l n r show invertible invertible derive case formula g l n r show invertible invertible derive case formula let g l n r u r n v r n show following assertion u v g l n r hold im v u g l r b im v u g l r u v u im v u v last equation called sherman-morrison-woodbury formula named jack sherman winifred morrison max woodbury show set block upper triangular matrix invertible diagonal block set matrix amm aii g l r group respect matrix multiplication prove theorem group permutation matrix commutative show following equivalence relation r n n exists permutation matrix p p b company produce four raw material five intermediate product z z z z z three final product e e e following table show many unit ri z j required producing one unit z k e respectively matrix group ring instance five unit one unit required producing one unit z determine help matrix operation corresponding table show many unit ri required producing one unit e b determine many unit four raw material required producing unit e unit e unit e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.1 Elementary Matrices</td>\n",
       "      <td></td>\n",
       "      <td>chapter echelon form rank matrix chapter develop systematic method transforming matrix entry field special form called echelon form transformation consists sequence multiplication left certain elementary matrix invertible echelon form identity matrix inverse product inverse elementary matrix non-invertible matrix echelon form sense closest possible matrix identity matrix form motivates concept rank matrix introduce chapter use frequently later elementary matrix let r commutative ring unit n n j n let r n n identity matrix let ei ith column en define e j ei e tj ei r n n column j entry j e j entry n j define pi j e j e ei e en r n n thus pi j permutation matrix cp definition obtained exchanging column j multiplication r n left pi j mean exchange ofthe row j example springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi echelon form rank matrix λ r define mi λ λei en r n n thus mi λ diagonal matrix obtained replacing ith column λei multiplication r n left mi λ mean multiplication ith row λ example n j λ r define g j λ λe ji ei λe j en r n n thus lower triangular matrix g j λ obtained replacing ith column ei λe j multiplication r n left g j λ mean λ time ith row added jth row similarly multiplication r n left upper triangular matrix g j λ mean λ time jth row added ith row example g g g lemma elementary matrix pi j mi λ invertible λ r g j λ defined respectively invertible following inverse j pi j pi j mi λ mi g j λ g j proof invertibility pi j j pi j already shown theorem symmetry pi j easily seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.2 The Echelon Form and Gaussian Elimination</td>\n",
       "      <td></td>\n",
       "      <td>elementary matrix since λ r invertible matrix mi well defined straightforward computation show mi mi λ mi λ mi since e tj ei j e ei e tj ei e tj therefore g j λ g j λe ji e ji λe ji e ji e similar computation show g j g j λ echelon form gaussian elimination constructive proof following theorem relies gaussian elimination given matrix k n k field algorithm construct matrix g l n k c quasi-upper triangular obtain special form left-multiplication elementary matrix pi j mi j λ g j λ left-multiplications corresponds application one so-called elementary row operation matrix pi j exchange two row mi λ multiply row invertible scalar g j λ add multiple one row another row assume entry field rather ring proof theorem require nonzero entry invertible generalization result hold certain ring integer z given hermite normal play important role number theory theorem let k field let k n exist invertible matrix st k n n product elementary matrix c st echelon form either c named carl friedrich gauß similar method already described chap rectangular array nine chapter mathematical art text developed ancient china several decade bc stated problem every day life gave practical mathematical solution method detailed commentary analysis written liu hui approx ad around ad charles hermite echelon form rank matrix c denotes arbitrary zero nonzero entry precisely c ci j either zero matrix exists sequence natural number jr called step echelon form jr r min n ci j r j ji ci j r n j ci ji r entry column ji zero n k n n invertible c case st proof set c done let let index first column ai j consist zero let ai first entry column nonzero form proceed follows first permute row finally eliminate normalize new first row multiply ai j nonzero element first entry column permuting normalizing lead echelon form gaussian elimination j set order eliminate column left matrix multiply g g n g g n ai j n j keep index larger matrix smaller matrix finished since c echelon form case r least one entry nonzero apply step described matrix k define matrix sk recursively sk k k sk jk matrix constructed analogous first identify first column jk k completely zero well first nonzero entry ai k k jk column permuting normalizing yield matrix k ai k j mk k aik jk pk ik k echelon form rank matrix k k set pk k k k k pk ik jk g k jk mk aik jk sk g k n sk indeed product elementary matrix form elementary matrix size n k n k continue procedure inductively end r min n step either r r r step sr construction entry position r jr r echelon form see discussion beginning proof r still eliminate nonzero entry column jr denote matrix r ri j form k r recursively r k ri k j sr r g k sr g k jk jk c st echelon form suppose n c st echelon form invertible c product invertible matrix thus invertible invertible matrix row containing zero r n hence c hand c invertibility elementary matrix echelon form gaussian elimination implies product invertible matrix invertible st literature echelon form sometimes called reduced row echelon form example transformation matrix echelon form via left multiplication elementary matrix g g g g matlab-minute echelon form computed matlab command rref reduced row echelon form apply rref eye order compute inverse matrix gallery tridiag cp exercise formulate conjecture general form prove conjecture proof theorem lead so-called lu square matrix theorem every matrix k n n exists permutation matrix p k n n lower triangular matrix l gln k one diagonal upper triangular matrix u k n n plu matrix u invertible invertible echelon form rank matrix u upper proof k n n eq form sn u triangular r n set sn sr since matrix invertible invertible sn invertible follows u n every matrix si form pi j si si sn ji n pi ji permutation necessary therefore sn sn n sn j j sn j j form permutation matrix k n k implies pk j pk jk k sn sn echelon form gaussian elimination hold certain j k j hence sn sn n sn sn n sn invertible lower triangular matrix permutation matrix form group respect matrix multiplication cp theorem thus permutasn l p l invertible lower triangular p lnn invertible tion matrix since l li j invertible also diag l p l u u obtain p lu p p construction diagonal entry l equal one example computation lu matrix g g u hence p l g g thus p p diag echelon form rank matrix l u u gln k lu yield u l p hence computing lu one obtains inverse essentially inverting two triangular matrix since achieved efficient recursive formula lu popular method scientific computing application require inversion matrix solution linear system equation cp chap context however alternative strategy choice permutation matrix used example instead first nonzero entry column one chooses entry large largest absolute value row exchange subsequent elimination strategy influence rounding error computation reduced matlab-minute hilbert matrix ai j qn n entry ai j j j generated matlab command hilb n carry command l u p hilb order compute lu matrix hilb matrix p l u look like compute also lu matrix full gallery tridiag study corresponding matrix p l u show given matrix matrix c theorem uniquely determined certain sense need following definition definition c k n echelon form theorem position r jr called pivot position also need following result lemma z gln k x k z x x proof exercise theorem let b k n echelon form z b matrix z gln k b david hilbert echelon form gaussian elimination proof b zero matrix zb hence b let b let b respective column ai bi furthermore let r jr r pivot position b show every matrix z gln k z b form ir z z k since b echelon form entry b row r zero follows b z b since first pivot position b bi k b first column z b implies ai k z b z since z invertible lemma implies k since echelon form b furthermore z z n z z k cp exercise r done r proceed pivot position analogous way since b echelon form kth pivot position give b jk ek jk z b jk invertibility z obtain jk b jk z z z k result yield uniqueness echelon form matrix invariance left-multiplication invertible matrix corollary k n following assertion hold unique matrix c k n echelon form transformed elementary row operation left-multiplication elementary matrix matrix c called echelon form gln k matrix c also echelon form echelon form matrix invariant left-multiplication invertible matrix proof echelon form invertible theorem give gln k echelon form get theorem give</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.3 Rank and Equivalence of Matrices</td>\n",
       "      <td></td>\n",
       "      <td>echelon form rank matrix rank equivalence matrix seen corollary echelon form k n unique particular every matrix k n exists unique number pivot position cp definition echelon form justifies following definition definition number r pivot position echelon form k n called denoted rank see immediately k n always rank min n rank moreover theorem show k n n invertible rank property rank summarized following theorem theorem k n following assertion hold exist matrix q gln k z glm k q az ir r rank r q gln k z glm k rank rank q az bc b k n c k rank rank b b rank rank c rank rank exist matrix b k n c k bc rank proof let q g l n k q b echelon form q q bc matrix q bc first rank b row contain nonzero entry corollary echelon form q equal echelon form thus normal echelon form also first rank b row nonzero implies rank rank b rank r ir assertion hold arbitrary matrix q g l n k z g l k r exists matrix q g l n k q echelon form r pivot position exists permutation matrix p k product elementary permutation matrix pi j concept rank introduced context bilinear form first ferdinand georg frobenius rank equivalence matrix p q ir v matrix v k r r v following simplicity omit size zero matrix matrix invertible thus ir ir v k k ir ypa q z p g l k obtain ir q az suppose hold k n matrix q g l n k z g l k obtain rank rank az z rank az rank thus particular rank rank az due invariance echelon form hence rank left-multiplication invertible matrix cp corollary get rank rank az rank q az rank ir k q gln k z glm k invariance rank left-multiplication invertible matrix used showing rank rank q az z rank q az rank az rank hence particular rank rank q az rank r exist matrix q g l n k z g l k q az r therefore echelon form rank matrix rank rank q az rank ir rank ir rank q az rank z q rank using obtain rank rank rank c b rank c rank c let bc b k n c k rank rank bc rank b let hand rank r exist matrix q ir thus obtain g l n k z g l k q az q ir r ir r z bc b k n c k example matrix example echelon form since two pivot position rank multiplying right q yield ab hence rank ab rank assertion theorem motivates following definition rank equivalence matrix definition two matrix b k n called equivalent exist matrix q g l n k z g l k q b z name suggests defines equivalence relation set k n since following property hold reflexivity q az q z im symmetry q b z b q az transitivity q b z b q c z q q c z z equivalence class k n given q az q g l n k z g l k rank r theorem ir r therefore ir ir consequently rank fully determines equivalence class matrix ir k n called equivalence normal form obtain ir k r ir ø r n min n hence min n pairwise distinct equivalence class ir n r min n complete set representative proof theorem know k n n n noncommutative ring unit contains non-trivial zero divisor using equivalence normal form characterized follows k n n invertible zero divisor since ab implies b echelon form rank matrix k n n zero divisor invertible hence rank r n equivalence normal form identity matrix let q z g l n k given q az r every matrix v r r r b z v ab q ir k n n r v b since z invertible exercise following exercise k arbitrary field compute echelon form matrix b c e simplicity element denoted k instead k state elementary matrix carry transformation one matrix invertible compute inverse product elementary matrix αβ let k αδ βγ determine echelon form γ δ k n n k b k show let b g l n k b g l k consider matrix k rank equivalence matrix k field rational function cp exercise examine whether invertible determine possible verify result computing show g l n k echelon form k given inverse invertible matrix thus computed via transformation echelon form two matrix b k n called left equivalent exists matrix q g l n k q b show defines equivalence relation k n determine simple representative equivalence class prove lemma determine lu cp theorem matrix r one matrix invertible determine inverse using lu decomposition let hilbert matrix cp matlab-minute definition determine rank lu theorem p determine rank matrix αβ γ dependence α β γ let b k n n given show rank rank b rank ac b c k n n examine inequality strict let b c determine rank ba b let b ba ab show following assertion b b b c b c c b ii λa μb c λm c μm b c λ μ r iii rank b exist λ μ r λ μ λa μb iv rank b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6 Linear Systems of Equations</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>chapter linear system equation solving linear system equation central problem linear algebra discus introductory way chapter system arise numerous application engineering natural social science major source linear system equation discretization differential equation linearization nonlinear equation chapter analyze solution set linear system equation characterize number solution using echelon form chap also develop algorithm computation solution definition linear system equation field k n equation unknown xm form xm xm anm xm bn ax b coefficient matrix ai j k n right hand side b bi k given b linear system called homogeneous otherwise x b called solution linear non-homogeneous every x k system x form solution set linear system denote l b next result characterizes solution set l b linear system ax b using solution set l associated homogeneous linear system ax springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi linear system equation lemma let k n b k l b ø given x l b l b x l x z z l proof z l thus x z x l x z x z b b hence x z l b show x l l b z x let l b let x b b z x z x l show l b z l hence x l closer look set l clearly l ø z l λ k z λ z λ hence z l furthermore z z l z z z z z l thus l nonempty subset k hence z closed scalar multiplication addition lemma k n b k k n n l b l sb moreover invertible l b l sb proof x l b also x sb thus x l sb show l b l sb invertible l sb sb multiplying left yield b since l b l sb l b consider linear system equation ax b theorem find matrix g l n k echelon form let b bi sb l b l b lemma linear system ax b take form x bn linear system equation suppose rank r let jr pivot column using rightmultiplication permutation matrix move r pivot column first r column achieved p e e jr e e e e e jr e jr em k yield p ir r k r r permutation lead simplification following presentation usually omitted practical computation b p p x b ay b since p p im write ax form ir yr br yr b r r bn ym apt x b left-multiplication x p mean different ordering unknown xm thus solution ax b easily recovered solution ay b vice versa l b x p l b l b solution determined using extended coefficient matrix b k n note rank obtained attaching b extra column rank b equality br bn rank bn nonzero rank b least one br solution since entry row r n zero rank bn hand rank b br written yr yr ym br linear system equation representation implies particular br l b b order determine lemma know l b b l set br yield l ym yr ym arbitrary yr yr ym l thus r b solution ay b uniquely determined example extended coefficient matrix b rank rank b l b ø b written ay b hence b l b l arbitrary summarizing consideration following algorithm solving linear system equation algorithm let k n b k given determine g l n k echelon form define b sb rank rank b l b l b ø p r rank rank b define l b l b l b b l l b determined well l b p rank b rank since rank rank rank b rank b discussion also yield following result different case solvability linear system equation linear system equation corollary k n b k following assertion hold rank rank b l b ø rank rank b b exists unique solution rank rank b exist many solution field k infinitely many element k q k r k c exist infinitely many pairwise distinct solution different case corollary studied example example let k q consider linear system equation ax b b form b apply gaussian elimination algorithm order transform echelon form b rank rank b hence exist solution pivot column ji p p ax b written linear system equation consequently b l b l b b l l x arbitrary exercise find field k matrix k n k n n b k l b l sb determine l b following b b b r b let α q bα α determine l l bα dependence α let k n b k n denote bi ith column b show linear system equation ax b least one solution x k rank rank rank rank b find condition solution unique linear system equation let n n βn bn αn given βi αi determine recursive formula entry solution linear system ax b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.1 Definition of the Determinant</td>\n",
       "      <td></td>\n",
       "      <td>chapter determinant matrix determinant map assigns every square matrix r n n r commutative ring unit element map interesting important property instance yield necessary sufficient condition invertibility r n n moreover form basis definition characteristic polynomial matrix chap definition determinant several different approach define determinant matrix use constructive approach via permutation definition let n n given bijective map σ n n j σ j called permutation number n denote set map sn permutation σ sn written form σ σ σ n example lemma know n springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi determinant matrix set sn composition map form group cp exercise sometimes called symmetric group neutral element group permutation n commutative group group sn n noncommutative example consider n permutation definition let n σ sn pair σ σ j j n σ σ j called inversion σ k number inversion σ sgn σ k called sign σ n define sgn short inversion permutation σ pair order term inversion confused inverse map σ exists since σ bijective sign permutation sometimes also called signature example permutation inversion sgn permutation inversion sgn define determinant map definition let r commutative ring unit let n map det r n n r ai j det sgn σ n ai σ called determinant ring element det called determinant formula det called signature formula term sgn σ definition interpreted element ring r either sgn σ r sgn σ r r unique additive inverse unit example n thus det sgn n get det det sgn sgn gottfried wilhelm leibniz definition determinant n sarrus det order compute det using signature formula leibniz form n product n factor large n costly even modern computer see corollary efficient way computing det signature formula mostly theoretical relevance since represents determinant explicitly term entry considering n entry variable interpret det polynomial variable r r r c standard technique analysis show det continuous function entry study group permutation detail permutation σ inversion sgn σ moreover σ σ σ σ σ σ σ j σ j sgn σ observation generalized follows lemma σ sn sgn σ σ j σ j proof n left hand side empty product defined cp sect hold n let n σ sn sgn σ k k number pair σ σ j j σ σ j σ j σ k j σ k j last equation used fact two product factor except possibly order pierre frédéric sarrus determinant matrix theorem sn sgn sgn sgn particular sgn σ sgn σ σ sn proof lemma sgn j j j j σ j σ j j sgn j j j sgn j sgn sgn σ sn sgn n sgn σ σ sgn σ sgn σ sgn σ sgn σ theorem show map sgn homomorphism group sn operation second group standard multiplication integer definition transposition permutation τ sn n exchange exactly two distinct element k n τ k τ k τ j j j n k obviously τ τ every transposition τ sn lemma let τ sn transposition exchange k k τ exactly inversion hence sgn τ proof k j j thus τ given τ k k j k k j k n point denote value τ increasing thus correct order simple counting argument show τ exactly j k inversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.2 Properties of the Determinant</td>\n",
       "      <td></td>\n",
       "      <td>property determinant property determinant section prove important property determinant map lemma r n n following assertion hold λ r det λ det λ n λ det ai j upper lower triangular det aii zero row column det n two equal row two equal column det det det proof exercise follows application upper lower triangular matrix zero row column every σ sn least one factor n ai σ equal zero thus det product let row k k ai j equal ak j j let τ sn transposition exchange element k let tn σ sn σ k σ since set tn contains permutation σ sn σ k σ sn tn σ τ σ tn moreover ai σ k ak σ k σ k ak σ σ σ k ak σ k thus using theorem lemma obtain sgn σ n ai σ sgn σ τ ai σ n n ai sgn σ n ai σ determinant matrix implies det sgn σ n ai σ sgn σ n ai σ sgn σ n ai σ proof case two equal column analogous observe first σ n σ n every σ sn see let n fixed σ j σ j thus σ j element first set j σ j j element second set since σ bijective two set equal let ai j bi j bi j ji det sgn σ bi σ sgn σ n aσ sgn σ n sgn σ n n aσ sgn σ n ai ai σ det used sgn σ cp theorem fact sgn σ n ai factor two product aσ example matrix b c obtain det lemma det b det c lemma may also compute determinant using sarrus rule example item lemma show particular det identity matrix en r n n reason determinant map called normalized property determinant σ sn matrix pσ eσ eσ eσ n called permutation matrix associated σ map group sn group permutation matrix r n n bijective inverse permutation matrix transpose cp theorem easily check pσt r n n j r jth column pσ aσ aσ aσ n right-multiplication pσ exchange column according permutation σ hand ai r n ith row aσ pσt aσ n left-multiplication pσt exchange row according permutation σ next study determinant elementary matrix lemma σ sn associated permutation matrix pσ r n n sgn σ det pσ n pi j defined det pi j mi λ g j λ defined respectively det mi λ λ det g j λ proof σ sn ai j r n n j j j n entry zero hence det σ det σ sgn σ n aσ j j sgn σ n σ σ j j sgn σ σ permutation matrix pi j associated transposition exchange j hence det pi j follows lemma since mi λ g j λ lower triangular matrix assertion follows lemma determinant matrix result lead important computational rule determinant lemma r n n n λ r following assertion hold multiplication row λ lead multiplication det λ det mi λ λ det det mi λ det addition row another row change det det g j λ det det g j λ det det g j λ det det g j λ det exchanging two row change sign det det pi j det det pi j det proof mi λ amk amk amk λamk amk hence det n sgn σ σ sgn σ ai σ n σ σ σ λ det g j λ amk amk amk amk j jk λaik j hence det sgn σ j σ j λai σ j n σ j sgn σ n σ λ sgn σ ai σ j n σ j first term equal det second equal determinant matrix two equal column thus equal zero proof matrix g j λ analogous property determinant permutation matrix pi j exchange row j j exchange expressed following four elementary row operation multiply row j add row row j add row j row add row row j therefore pi j g j g j g j j one may verify also carrying matrix multiplication using obtain det pi j det g j g j g j j det g j det g j det g j det j det det since det det cp lemma result lemma row formulated analogously column example consider matrix b simple calculation show det since b obtained exchanging first two column det b det determinant map interpreted map r n r map n column matrix r n n ring ai j r two column ai j det det j ai lemma due property determinant map called alternating map column analogously determinant map alternating map row j form λa μa λ μ r kth row j j n akn r j determinant matrix n det det μa sgn σ λak σ k μak σ k ai σ sgn σ ak σ k n ak σ k μ sgn σ ak σ k λ det μ det n ai σ property called linearity determinant map respect row analogously linearity respect column linear map studied detail later chapter next result called multiplication theorem determinant theorem k field b k n n det ab det det b moreover invertible det det proof theorem know k n n exist invertible elementary st echelon form lemma matrix st det det det det well det ab det ab det det det ab thus also ab zero two case invertible det ab row det implies det hence det ab det det b hand invertible echelon form det give det ab det det b since finally invertible det det det det hence det det since proof relies theorem valid matrix field k formulated theorem b k n n however multiplication theorem determinant also hold matrix commutative ring r unit direct proof based signature formula leibniz found example book advanced linear algebra loehr sect book also contains proof cauchy-binet formula det ab r n b r n n sometimes use det ab det det b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.3 Minors and the Laplace Expansion</td>\n",
       "      <td></td>\n",
       "      <td>property determinant hold b r n n although shown result theorem b k n n proof theorem suggests det easily computed transforming k n n echelon form using elementary row operation corollary k n n let st k n n elementary matrix zero row hence st echelon form either det hence det det det st shown theorem every matrix k n n factorized p lu hence det det p det l det u determinant matrix right hand side easily computed since permutation triangular matrix lu matrix therefore yield efficient way compute det matlab-minute look matrix wilkinson n matlab find general formula entry compute n l u p lu cp matlab-minute definition det l det u det p det p l u det permutation associated computed matrix p det integer odd n minor laplace expansion show determinant used deriving formula inverse invertible matrix solution linear system equation formula however theoretical practical relevance definition let r commutative ring unit let r n n n matrix j r obtained deleting jth row ith column called minor matrix adj bi j r n n bi j j det j called adjunct adjunct also called adjungate classical adjoint term introduced james joseph sylvester determinant matrix theorem r n n n adj adj det particular invertible det r invertible case det det det adj proof let b bi j entry bi j j det j c ci j adj satisfies ci j n bik ak j n det k ak j let column let ek r n n k ek kth column identity matrix exist permutation matrix p q perform k row column exchange respectively q p k k using lemma obtain q det p k k det q det p det k det k det det k det k linearity determinant respect column give ci j n ak j det k det j j det j δi j det thus adj det analogously show adj det minor laplace expansion det r invertible det adj det adj invertible det adj hand invertible det det det det det det used multiplication theorem determinant r cp comment following proof theorem thus det invertible det det det adj example det thus invertible invertible considered element since case det det z det matrix invertible since z invertible note r n n invertible theorem show obtained inverting one ring element det use theorem multiplication theorem matrix commutative ring unit prove result already announced sect order r n n unique inverse r n n one two show need checked equation r n n exists corollary let r n n matrix invertible multiplication theorem determinant yield proof det det det det det det thus also invertible det r invertible det det unique inverse n obvious n shown right theorem multiply equation get analogous proof starting determinant matrix let u summarize invertibility criterion square matrix field shown far g l n k theorem echelon form identity matrix definition clear rank n rank rank b n b k algorithm b b k theorem det alternatively obtain g l n k theorem echelon form least one zero row definition clear rank n rank n algorithm l theorem det field q r c usual absolute value number formulate following useful invertibility criterion matrix theorem k n n k q r c diagonally dominant n j n j det proof prove assertion contraposition showing det implies diagonally dominant det l homogeneous linear system xn let xm equation ax least one solution x entry x maximal absolute value xm x j j x given particular xm mth row amn xn amm xm n j xj j take absolute value side use triangle inequality yield minor laplace expansion xm n j j n j xm hence n j j j diagonally dominant converse theorem hold example matrix det diagonally dominant theorem obtain laplace determinant particularly useful contains many zero entry cp example corollary r n n n following assertion hold n det n j ai j det j laplace expansion det respect ith row j n det n j ai j det j laplace expansion det respect jth column proof two expansion det follow immediately comparison diagonal entry matrix equation det adj det adj laplace expansion allows recursive definition determinant r n n n let det defined corollary choose arbitrary row column formula det contains matrix size use laplace expansion expressing determinant term determinant n n matrix recursively matrix remain r define det finally state cramer give explicit formula solution linear system form determinant rule theoretical value order compute n component solution requires evaluation n determinant n n matrix pierre-simon gabriel laplace published expansion cramer determinant matrix corollary let k field g l n k b k unique solution linear system equation ax b given xn b det adj b x xi det b det example consider q b laplace expansion respect last column yield det det det thus invertible ax b unique solution x b cramer rule following entry det det det det det det det det minor laplace expansion exercise permutation σ sn called r exists subset ir n r element σ k k r σ ir σ ir write r σ ir particular transposition τ sn let n given compute b let n σ determine σ j j c show inverse cycle ir given ir show two cycle disjoint element ir j ir j ø commute e show every permutation σ sn written product disjoint cycle except order uniquely determined σ prove lemma using show group homomorphism sgn sn satisfies following assertion set σ sn sgn σ subgroup sn cp exercise b σ π sn π σ π compute determinant following matrix en zn n ei ith column identity matrix b b bi j zn n bi j c c eπ π e π π πe e determinant matrix wilkinson matrix cp matlab-minute end sect construct matrix b rn n n det b det det b let r commutative ring unit n r n n show following assertion hold b c e f g adj adj ab adj b adj b r n n invertible adj λa adj λ adj adj det adj det invertible adj adj det adj adj invertible one drop requirement invertibility b e let n ai j rn n ai j xi xn j yn hence particular xi j j matrix called cauchy show det x j x j x j yi b use derive formula determinant n n hilbert matrix cp matlab-minute definition let r commutative ring unit αn r n n n r αn vn called vandermonde show det vn james hardy wilkinson louis cauchy alexandre-théophile vandermonde augustin α j αi minor laplace expansion b let k field let k set polynomial variable degree n show two polynomial p q k equal exist pairwise distinct βn k p β j q β j show following assertion let k field let k n n n odd det b g l n r det let k field k n n k n n k n n k n n show following assertion g l n k det det det b g l n k det det det c det det det show also matrix defined commutative ring unit construct matrix rn n n det det det det det let ai j g l n r ai j z j show following assertion hold qn n b zn n det c linear system equation ax b unique solution x every b det show g zn n det subgroup g l n q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.1 The Characteristic Polynomial  and the Cayley-Hamilton Theorem</td>\n",
       "      <td></td>\n",
       "      <td>chapter characteristic polynomial eigenvalue matrix already characterized matrix using rank determinant chapter use determinant map order assign every square matrix unique polynomial called characteristic polynomial matrix polynomial contains important information matrix example one read determinant thus see whether matrix invertible even important root characteristic polynomial called eigenvalue matrix characteristic polynomial cayley-hamilton theorem let r commutative ring unit let r corresponding ring polynomial cp example ai j r n n set r n n n ann entry matrix element commutative ring unit r diagonal entry polynomial degree entry constant polynomial using definition form determinant matrix element r springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi characteristic polynomial eigenvalue matrix definition let r commutative ring unit r n n pa det r called characteristic polynomial example n pa det det n obtain pa det using definition see general form pa matrix r n n given n sgn σ δi σ ai σ pa following lemma present basic property characteristic polynomial lemma r n n pa pat pa n n aii det proof using lemma obtain pa det det det pat using pa see n pa n aii sgn σ n δi σ ai σ characteristic polynomial cayley-hamilton theorem first term right hand side form n aii polynomial degree n n second term polynomial degree n thus claimed moreover definition yield aii pa det n det det lemma show characteristic polynomial r n n always degree coefficient n polynomial called monic coefficient given sum diagonal entry quantity called trace n trace aii following lemma show every monic polynomial p r degree n exists matrix r n n pa lemma n n p n r p characteristic polynomial matrix r n n n matrix called companion matrix proof prove assertion induction n p pa det let assertion hold n consider p βn n r characteristic polynomial eigenvalue matrix using laplace expansion respect first row cp corollary induction hypothesis get pa det det βn det det βn n βn βn n example polynomial p z companion matrix identity matrix characteristic polynomial det pa thus different matrix may characteristic polynomial example seen evaluate polynomial p r scalar λ analogously evaluate p matrix r cp exercise p βn n r define p βn n im r multiplication right hand side scalar multiplication β j r j r j recall im evaluating given polynomial matrix r therefore defines map r r characteristic polynomial cayley-hamilton theorem particular using characteristic polynomial pa r n n satisfies n pa sgn σ δi σ ai σ im r note r n n pa det obvious equation pa det wrong definition pa r n n det r two expression even n following result called cayley-hamilton theorem every matrix r n n characteristic polynomial pa r pa r n n proof n pa pa let n let ei ith column identity matrix r n n aei ani en n equivalent n aii ei ji e j last n equation written bε ann en hence b r n n r p p r r n n set r form commutative ring unit given identity matrix cp exercise using theorem obtain adj b b det b claimed verified n feel necessary give proof general sir william rowan hamilton proved theorem case n context investigation quaternion one first proof general n given ferdinand georg frobenius james joseph sylvester coined name theorem calling no-little-marvelous hamilton-cayley theorem arthur cayley showed theorem n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.2 Eigenvalues and Eigenvectors</td>\n",
       "      <td></td>\n",
       "      <td>characteristic polynomial eigenvalue matrix det b r identity matrix r n n matrix n time identity matrix diagonal multiplying equation right ε yield adj b bε det b ε implies det b r n n finally using lemma give n det b δi σ aσ sgn σ n δσ aσ sgn σ pat pa completes proof eigenvalue eigenvectors section present introduction topic eigenvalue eigenvectors square matrix field k concept studied detail later chapter definition let k n n λ k v k satisfy av λv λ called eigenvalue v called eigenvector corresponding λ definition v never eigenvector matrix λ may eigenvalue example v eigenvector corresponding eigenvalue λ α k αv αv α av α λv λ αv thus also αv eigenvector corresponding λ eigenvalue eigenvectors theorem k n n following assertion hold λ eigenvalue λ root characteristic polynomial pa λ k λ eigenvalue det λ eigenvalue λ eigenvalue proof equation pa λ det λin hold matrix λin invertible cp equivalent l λin x however mean exists vector x λin x x λ eigenvalue pa assertion follows pa n det cp lemma follows pa pat cp lemma whether matrix k n n eigenvalue may depend field k considered example matrix characteristic polynomial pa r polynomial root since equation real solution consider element pa c root two complex number eigenvalue item theorem show eigenvalue eigenvector however may eigenvector example matrix characteristic polynomial pa hence eigenvalue λ characteristic polynomial eigenvalue matrix λ thus eigenvector corresponding eigenvalue eigenvector hand λ λ thus eigenvector corresponding eigenvalue eigenvector theorem implies criterion invertibility k n n cp g l n k eigenvalue root pa definition two matrix b k n n called similar exists matrix z g l n k z b z one easily show defines equivalence relation set k n n cp proof following definition theorem two matrix b k n n similar pa pb proof z b z multiplication theorem determinant yield pa det det z b z det z b z det z det b det z det b det z z pb cp remark theorem theorem theorem show two similar matrix eigenvalue condition b similar sufficient necessary pa pb example let pa pb every matrix z g l n k z b z thus pa pb although b similar cp also example</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.3 Eigenvectors of Stochastic Matrices</td>\n",
       "      <td></td>\n",
       "      <td>eigenvalue eigenvectors matlab-minute root polynomial p αn n computed approximated matlab using command root p p n matrix entry p n compute root p monic polynomial p r display output using format long exact root p large numerical error computation root using root p form matrix p compare structure one companion matrix lemma transfer proof lemma structure matrix compute eigenvalue command eig compare output one root p observe eigenvectors stochastic matrix consider eigenvalue problem presented sect context pagerank algorithm mathematical modeling lead equation written form ax x ai j rn n n number document satisfies n ai j ai j j matrix called column-stochastic note column-stochastic row-stochastic matrix also occurred car insurance application considered sect example want determine x xn ax x entry xi describes importance document importance value nonnegative xi thus want determine entrywise nonnegative eigenvector corresponding eigenvalue λ first check whether problem solution study whether solution unique presentation based article lemma column-stochastic matrix rn n eigenvector corresponding eigenvalue proof since column-stochastic eigenvalue theorem show also eigenvalue hence exists corresponding eigenvector characteristic polynomial eigenvalue matrix matrix real entry called positive entry positive lemma rn n positive column-stochastic x eigenvector corresponding eigenvalue either x positive proof x xn eigenvector ai j corresponding eigenvalue n xi ai j x j suppose entry x positive entry x negative exists least one index k n ak j x j n ak j j implies n n n n n ai j j n n n j ai j j ai j j impossible indeed x must positive prove following uniqueness result theorem rn n positive column-stochastic exists xi ax unique positive x xn proof lemma least one positive eigenvector corresponding eigenvalue suppose x xn x xn j two eigenvectors suppose normalized xi j assumption made without loss generality since every nonzero multiple eigenvector still eigenvector show x x α r define x α x αx ax α ax α ax x αx x α α equal zero thus α first entry x lemma x α eigenvector corresponding eigenvalue ax α x α implies x α hence α xi xi eigenvectors stochastic matrix summing n equation yield n xi α n xi α get xi xi n therefore x x unique positive eigenvector x theorem called perron eigenvector positive matrix theory eigenvalue eigenvectors positive general nonnegative matrix important area matrix theory since matrix arise many application construction matrix rn n pagerank algorithm columnstochastic positive since usually many entry ai j order obtain uniquely solvable problem one use following trick let si j rn n si j obviously positive columnstochastic real number α define matrix α α αs matrix positive column-stochastic hence unique positive eigenvector u corresponding eigenvalue thus α u α u α u u α u n large number document entire internet number small α u u therefore solution eigenvalue problem u α u small α potentially give good approximation u satisfies au u practical solution eigenvalue problem matrix α topic field numerical linear algebra matrix represents link structure document mutually linked thus document equally important matrix α α αs therefore model following internet surfing behavior user follows proposed link probability arbitrary link probability α originally google used value α oskar perron characteristic polynomial eigenvalue matrix exercise following exercise k arbitrary field determine characteristic polynomial following matrix q verify cayley-hamilton theorem case direct computation two matrix b c similar let r commutative ring unit n show every g l n r exists polynomial p r degree n adj p conclude q hold polynomial q r degree n b let r n n apply theorem matrix r n n derive alternative proof cayley-hamilton theorem formula det adj let k n n matrix ak k matrix called nilpotent show λ eigenvalue b determine pa show n hint may assume pa form λn k c show μin invertible μ k show determine eigenvalue corresponding eigenvectors following matrix r b c difference consider b c matrix c let n ε consider matrix ε ε eigenvectors stochastic matrix element cn n determine eigenvalue dependence ε many pairwise distinct eigenvalue ε determine eigenvalue corresponding eigenvectors b simplicity element denoted k instead k let k n n b k n c k n rank c ac c b show every eigenvalue b eigenvalue show following assertion trace λ μb λ trace μ trace b hold λ μ k b k n n b trace ab trace b hold b k n n c b k n n similar trace trace b prove disprove following statement exist matrix b k n n trace ab trace trace b b exist matrix b k n n ab b suppose matrix ai j cn n real entry ai j show λ eigenvalue corresponding eigenvector v νn also λ eigenvalue corresponding eigenvector v ν ν n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.1 Basic Definitions and Properties of Vector Spaces</td>\n",
       "      <td></td>\n",
       "      <td>chapter vector space previous chapter focussed matrix property defined algebraic operation matrix derived important concept associated including rank determinant characteristic polynomial eigenvalue chapter place concept abstract framework introducing idea vector space matrix form one important example vector space property certain namely finite dimensional vector space studied transparent way using matrix next chapter study linear map vector space connection matrix play central role well basic definition property vector space begin definition vector space field k definition let k field vector space k shortly k space set v two operation v v v v w v w addition k v v λ v λ v scalar multiplication satisfy following v commutative group v w v λ μ k following assertion hold b c λ μ v λμ v λ v w λ v λ λ μ v λ v μ springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi vector space element v v called element λ k called scalar usually omit sign scalar multiplication usually write λv instead λ clear context important field using often omit explicit reference k simply write vector space instead k space example set k n matrix addition scalar multiplication form k space obvious reason element k k sometimes called column row vector respectively set k form k space addition defined example usual addition polynomial scalar multiplication p αn n k defined λ p λ λ λ αn n continuous real valued function defined real interval α β pointwise addition scalar multiplication f g x f x g x λ f x λ f x form r-vector space shown using addition two continuous function well multiplication continuous function real number yield continuous function since definition v commutative group already know vector space property theory group cp chap particular every vector space contains unique neutral element respect addition called null vector every vector v v unique additive inverse v v v v usual write v w instead v lemma let v k space k neutral null element k v respectively following assertion hold k v v λ λ k λ v v λ v v λ k term introduced sir william rowan hamilton context quaternion motivated latin verb vehi vehor vectus sum mean ride drive also term scalar introduced hamilton see footnote scalar multiplication basic definition property vector space proof v v k v k k v k v k adding k v side identity give k λ k λ λ λ λ adding λ side identity give λ λ k v v λ v v λ λ v k v well λ v λ λ v v λ following write instead k clear null element meant group ring field identify substructure vector space vector space definition let v k space let u u k space called subspace v substructure must closed respect given operation addition scalar multiplication lemma u subspace k space v ø u v following assertion hold v w u v w u λv u λ k v u proof exercise example every vector space v trivial subspace u v u let k n u l k u solution set homogeneous linear system ax u u empty v w u v w av aw v w u furthermore λ k λ v λ av λ λv u hence u subspace k every n set k p k deg p n subspace k definition let v k space n n vn vector form n λi vi v λn vn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.2 Bases and Dimension of Vector Spaces</td>\n",
       "      <td></td>\n",
       "      <td>vector space called linear combination vn coefficient λn k linear span vn set span vn n λi vi λn k let set suppose every vector vm let set vector called system vector denoted vm linear span system vm denoted span vm defined set vector v v linear combination finitely many vector system definition consistently extended case n case vn list length zero empty list define empty sum vector v obtain span vn span ø following consider list vector vn set vector vn usually mean n case empty list associated zero vector space v sometimes discussed separately example vector space k k spanned vector set k form subspace k spanned vector lemma v vector space vn v span vn subspace proof clear ø span vn furthermore span vn definition closed respect addition scalar multiplication lemma satisfied base dimension vector space discus central theory base dimension vector space start concept linear independence definition let v k space vector vn v called linearly independent equation n λi vi λn k λi vi always implies λn otherwise hold scalar λn k equal zero vector vn called linearly dependent base dimension vector space empty list linear independent set every vector vm v corresponding system vm called linearly independent finitely many vector system always linearly independent sense otherwise system called linearly dependent vector vn linearly independent zero vector linearly combined trivial way vn consequently one vector zero vector vn linearly dependent single vector v linearly independent v following result give useful characterization linear independence finitely many least two given vector lemma vector vn n linearly independent vector vi n written linear combination others proof prove assertion contraposition vector vn linearly dependent n λi vi least one scalar λ j equivalently vj n j λi vi j v j linear combination vector using concept linear independence define concept basis vector space definition let v vector space set vn v called basis v vn linearly independent span vn set ø basis zero vector space v let set suppose every vector vm set vm called basis v corresponding system vm linearly independent span vm short basis linearly independent spanning set vector space example let e j k n matrix entry position j entry cp sect set vector space e j n j basis vector space k n cp example matrix e j k n n j linearly independent since n λi j e j λi j implies λi j n j ai j k n n ai j e j hence span e j n j k n basis called canonical standard basis vector space k n denote canonical basis vector k en vector also called unit vector n column identity matrix basis vector space k cp example given set since corresponding system linearly independent every polynomial p k linear combination finitely many vector system next result called basis extension theorem theorem let v vector space let vr v r vr linearly independent span vr v set vr extended basis v using vector set proof note r list vr empty hence linearly independent due definition prove assertion induction span vr v linear independence vr show set basis v base dimension vector space let assertion hold suppose vr v given vr linearly independent span vr vr already basis v done suppose therefore span vr exists least one j j span vr particular w j w j λw j r λi vi implies λ otherwise would w j span vr therefore λr due linear independence vr thus vr w j linearly independent induction hypothesis extend set vr w j basis v using vector set w j contains element example consider vector space v k cp example vector vector linearly independent basis v since span example vector element v span span extend get linearly independent vector indeed span thus basis basis extension theorem every vector space spanned finitely many vector basis consisting finitely many element central result theory vector space every basis number element order show result first prove following exchange lemma λi vi lemma let v vector space let vm v let w v span w vm span vm proof assumption λi vi span vm say γi vi λi vi w γi vi γi λi vi span w vm hand w αi vi span w vm vector space λi vi αi vi λi αi vi span vm thus span w vm span vm using lemma prove exchange theorem theorem let w wn u u u finite subset vector space let wn linearly independent w span u u n n element u numbered appropriately element u u n exchanged n element w way span wn u u span u u n u u λi u scalar λm proof assumption zero otherwise contradicts linear independence wn appropriate renumbering lemma yield span u u span u u u suppose r r n exchanged vector u u r wr span wr u r u span u u r u r u clear r assumption wr span u u thus wr r λi wi λi u scalar λm one scalar λr λm must nonzero otherwise wr span wr contradicts linear independence wm appropriate renumbering λr lemma yield span wr u r u span wr u r u continue construction r n obtain literature theorem sometimes called steinitz exchange theorem ernst steinitz result first proved hermann günther graßmann base dimension vector space span wn u u span u u n u u particular n using fundamental theorem following result unique number basis element simple corollary corollary vector space v spanned finitely many vector v basis consisting finitely many element two base v number element proof assertion clear v cp definition let v span vm theorem extend span using element vm basis thus v basis finitely many element let u u u w wk two base w v span u u theorem k u v span wk theorem k thus define dimension vector space definition exists basis k space v consists finitely many element v called finite dimensional unique number basis element called dimension denote dimension dim k v dim v clear field meant v spanned finitely many vector v called infinite dimensional write dim k v note zero vector space v basis ø thus dimension zero cp definition v finite dimensional vector space vm v dim v vector vm must linearly dependent vector linearly independent could extend via theorem basis v would contain dim v element example set form basis vector space k n basis n element hence dim k n n hand vector space k spanned finitely many vector cp example hence infinite dimensional example let v vector space continuous real valued function real interval cp example define n function f n v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.3 Coordinates and Changes of the Basis</td>\n",
       "      <td></td>\n",
       "      <td>vector space f n x j x n n x n x every linear combination k x n n x λ j f j continuous function value λ j thus equation k λ j f j v implies λ j must zero f f k v linearly independent k consequently dim v coordinate change basis study linear combination basis vector finite dimensional vector space particular study happens linear combination change another basis vector space lemma vn basis k space v every v v exist uniquely determined scalar λn k v λn vn scalar called coordinate v respect basis vn proof let v λi vi μi vi scalar λi μi k n n λi μi vi coordinate change basis linear independence vn implies λi μi definition coordinate vector depend given basis particular depend ordering numbering basis vector author distinguish basis set collection element without particular ordering ordered basis book keep set notation basis vn index indicate ordering basis vector let v k space vn v need linearly independent v λn vn coefficient λn k let u write vn λn vn λn vn n-tuple v vn v n v n time n v skip parenthesis write v instead v notation formally defines multiplication map v n k α k α v α α λn vn vn αλn μn k u μn vn vn μn v u λn μn vn vn λn μn vector space show vector given linear combination operation scalar multiplication addition correspond operation coefficient vector respect linear combination extend notation let ai j k n let j u j vn j j write linear combination u u system u u vn side equation element v right-multiplication arbitrary n-tuple vn v n matrix k n thus corresponds forming linear combination vector vn corresponding coefficient given entry formally defines multiplication map v n k n v lemma let v k space let vn v linearly independent let k n let u u vn vector u u linearly independent rank proof exercise consider also matrix b bi j k using obtain u u b vn lemma previous notation vn b vn ab proof exercise let vn wn base v let v lemma exist unique coordinate λn μn respectively v vn wn λn μn describe method transforming coordinate λn respect basis vn coordinate μn respect basis wn vice versa coordinate change basis every basis vector v j j n exist unique coordinate pi j n j v j wn j pn j defining p pi j k n n write n equation vector v j analogous vn wn way every basis vector w j j n exist unique coordinate qi j n j w j vn j qn j set q qi j k n n analogously get wn vn q thus wn vn q wn p q wn p q implies wn p q mean n linear combination basis vector wn corresponding coordinate given entry n column p q equal zero vector since basis vector linearly independent coordinate must zero hence p q k n n p q analogously obtain equation q p therefore matrix p k n n invertible p q furthermore v vn wn p wn p λn λn λn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.4 Relations Between Vector Spaces and Their Dimensions</td>\n",
       "      <td></td>\n",
       "      <td>vector space due uniqueness coordinate v respect basis wn obtain p p μn λn λn μn hence multiplication matrix p transforms coordinate v respect basis vn respect basis wn multiplication p yield inverse transformation therefore p p called coordinate transformation matrix summarize result obtained follows theorem let vn wn base k space uniquely determined matrix p k n n invertible yield coordinate transformation vn wn v vn vn λn μn p μn λn example consider vector space v r entrywise addition scalar multiplication basis v given set another basis v set corresponding coordinate transformation matrix obtained defining equation p q relation vector space dimension first result describes relation vector space subspace lemma v finite dimensional vector space u v subspace dim u dim v equality u v relation vector space dimension proof let u v let u u basis u u u ø u using theorem extend set basis u proper subset v least one basis vector need added hence dim u dim v u v every basis v also basis u thus dim u dim v subspace vector space v intersection given u v u u cp definition sum two subspace defined u u v u u lemma subspace vector space v following assertion hold subspace equality proof exercise important result following dimension formula subspace theorem finite dimensional subspace vector space v dim dim dim dim proof let vr basis extend set basis vr basis vr xk assume r k one list empty following argument easily modified suffices show vr xk basis obviously span vr xk hence suffices show vr xk linearly independent let r k λi vi μi wi γi xi vector space k γi xi r λi vi μi wi left hand side equation definition vector γi xi construction right hand side vector therefore vector vr however vr basis μi wi implies linearly independent therefore also r λi vi k γi xi hence λr γk due linear independence vr xk least one subspace theorem infinite dimensional assertion still formally correct since case dim dim dim example subspace k k k dim dim k dim k dim definition sum extended arbitrary finite number subspace uk k subspace vector space v define uk k u j k u j u j u j j k sum called direct ui k u j k case write direct sum relation vector space dimension uk k uj particular sum two subspace v direct following theorem present two equivalent characterization direct sum subspace theorem u uk sum k subspace vector space v following assertion equivalent sum u direct ui u j every vector u u representation form u u j uniquely determined u j u j j u j u j u j j k implies u j j proof let u u j u j u j u j u j j every k ui ui u j u j ui uj u hence u u ui u j implies u obvious u j u given let u u j u particular implies u j u j j hence j u thus ui u j exercise following exercise k arbitrary field following set usual addition scalar multiplication r-vector space determine possible basis dimension determine basis r-vector space c dimr c determine basis c-vector space c dimc c show k linearly independent det vector space let v k space nonempty set map v set map show map v operation map v map v map v f g f g f g x f x g x x k map v map v λ f λ f λ f x λ f x x k space show function sin co map r r linearly independent let v vector space n dim v n let vn show following statement equivalent vn linearly independent span vn vn basis show k n k space cp example find subspace k space show k k space cp example show k subspace k cp example determine dim k show polynomial linearly independent q extend basis q let n n n j αi j αi j k k element k called bivariate polynomial k unknown define scalar multiplication addition k becomes vector space determine basis k show lemma let k n b k solution set l b ax b subspace k let k n n let λ k eigenvalue show set v k av λv subspace k let k n n let two eigenvalue show two associated eigenvectors linearly independent show b c relation vector space dimension base vector space k determine corresponding coordinate transformation matrix examine element following set linear independence vector space k determine dimension subspace spanned element one set basis k show set sequence αi q n entrywise addition scalar multiplication form infinite dimensional vector space determine basis system prove lemma prove lemma prove lemma let finite dimensional subspace vector space show sum direct dim dim dim let uk k finite dimensional subspace vector space suppose ui u j j sum uk direct let u subspace finite dimensional vector space show u u subspace u called exists another subspace u complement u determine three subspace v v subspace v uniquely determined complement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Appendix A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>appendix short introduction matlab interactive software system numerical computation simulation visualization contains large number predefined function allows user implement program so-called m-files name matlab originates matrix laboratory indicates matrix orientation software indeed matrix major object matlab due simple intuitive use matrix consider matlab well suited teaching field linear algebra short introduction explain important way enter operate matrix matlab one learn essential matrix operation well important algorithm concept context matrix linear algebra general actively using matlab-minutes book use predefined function matrix matlab entered form list entry enclosed square bracket entry list ordered row natural order index top bottom left right new row start every semicolon example matrix entered matlab typing semicolon matrix suppresses output matlab omitted matlab writes entered computed quantity example entering registered trademark mathworks springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi appendix short introduction matlab matlab give output one access part matrix corresponding index list index k abbreviated k colon mean row given column index column given row index example matrix matrix matrix several predefined function produce matrix particular given positive integer n eye n identity matrix zero n one n n matrix zero n matrix one rand n n random matrix several matrix appropriate size combined new matrix example command b c lead e help function matlab started command help order get information specific function one add name function example appendix short introduction matlab input help ops information operation operator matlab particular addition multiplication transposition help matfun matlab function operate matrix help gallery collection example matrix help det determinant help expm matrix exponential function</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 section_level_1  \\\n",
       "0                             1 Linear Algebra in Every Day Life   \n",
       "1                             1 Linear Algebra in Every Day Life   \n",
       "2                             1 Linear Algebra in Every Day Life   \n",
       "3                             1 Linear Algebra in Every Day Life   \n",
       "4                             1 Linear Algebra in Every Day Life   \n",
       "5                                                 10 Linear Maps   \n",
       "6                                                 10 Linear Maps   \n",
       "7                             11 Linear Forms and Bilinear Forms   \n",
       "8                             11 Linear Forms and Bilinear Forms   \n",
       "9                             11 Linear Forms and Bilinear Forms   \n",
       "10                        12 Euclidean and Unitary Vector Spaces   \n",
       "11                        12 Euclidean and Unitary Vector Spaces   \n",
       "12                        12 Euclidean and Unitary Vector Spaces   \n",
       "13                                    13 Adjoints of Linear Maps   \n",
       "14                                    13 Adjoints of Linear Maps   \n",
       "15                               14 Eigenvalues of Endomorphisms   \n",
       "16                               14 Eigenvalues of Endomorphisms   \n",
       "17                               14 Eigenvalues of Endomorphisms   \n",
       "18         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "19         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "20    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "21    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "22    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "23    17 Matrix Functions and Systems  of Differential Equations   \n",
       "24    17 Matrix Functions and Systems  of Differential Equations   \n",
       "25                           18 Special Classes of Endomorphisms   \n",
       "26                           18 Special Classes of Endomorphisms   \n",
       "27                           18 Special Classes of Endomorphisms   \n",
       "28                           19 The Singular Value Decomposition   \n",
       "29                                 2 Basic Mathematical Concepts   \n",
       "30                                 2 Basic Mathematical Concepts   \n",
       "31                                 2 Basic Mathematical Concepts   \n",
       "32          20 The Kronecker Product and Linear Matrix Equations   \n",
       "33                                        3 Algebraic Structures   \n",
       "34                                        3 Algebraic Structures   \n",
       "35                                                    4 Matrices   \n",
       "36                                                    4 Matrices   \n",
       "37                   5 The Echelon Form and the Rank of Matrices   \n",
       "38                   5 The Echelon Form and the Rank of Matrices   \n",
       "39                   5 The Echelon Form and the Rank of Matrices   \n",
       "40                                 6 Linear Systems of Equations   \n",
       "41                                    7 Determinants of Matrices   \n",
       "42                                    7 Determinants of Matrices   \n",
       "43                                    7 Determinants of Matrices   \n",
       "44  8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "45  8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "46  8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "47                                               9 Vector Spaces   \n",
       "48                                               9 Vector Spaces   \n",
       "49                                               9 Vector Spaces   \n",
       "50                                               9 Vector Spaces   \n",
       "51                                                    Appendix A   \n",
       "\n",
       "                                                       section_level_2  \\\n",
       "0                                           1.1 The PageRank Algorithm   \n",
       "1                           1.2 No Claim Discounting in Car Insurances   \n",
       "2                                   1.3 Production Planning in a Plant   \n",
       "3                                        1.4 Predicting Future Profits   \n",
       "4                                               1.5 Circuit Simulation   \n",
       "5                 10.1 Basic Definitions and Properties of Linear Maps   \n",
       "6                                        10.2 Linear Maps and Matrices   \n",
       "7                                    11.1 Linear Forms and Dual Spaces   \n",
       "8                                                  11.2 Bilinear Forms   \n",
       "9                                             11.3 Sesquilinear Forms    \n",
       "10                                      12.1 Scalar Products and Norms   \n",
       "11                                                  12.2 Orthogonality   \n",
       "12                               12.3 The Vector Product in mathbbR3,1   \n",
       "13                               13.1 Basic Definitions and Properties   \n",
       "14                             13.2 Adjoint Endomorphisms and Matrices   \n",
       "15                               14.1 Basic Definitions and Properties   \n",
       "16                                              14.2 Diagonalizability   \n",
       "17                              14.3 Triangulation and Schur's Theorem   \n",
       "18                                                    15.1 Polynomials   \n",
       "19                             15.2 The Fundamental Theorem of Algebra   \n",
       "20                       16.1 Cyclic f-invariant Subspaces and Duality   \n",
       "21                                      16.2 The Jordan Canonical Form   \n",
       "22                       16.3 Computation of the Jordan Canonical Form   \n",
       "23           17.1 Matrix Functions and the Matrix Exponential Function   \n",
       "24              17.2 Systems of Linear Ordinary Differential Equations   \n",
       "25                                           18.1 Normal Endomorphisms   \n",
       "26                           18.2 Orthogonal and Unitary Endomorphisms   \n",
       "27                                      18.3 Selfadjoint Endomorphisms   \n",
       "28                                                                       \n",
       "29                                     2.1 Sets and Mathematical Logic   \n",
       "30                                                            2.2 Maps   \n",
       "31                                                       2.3 Relations   \n",
       "32                                                                       \n",
       "33                                                          3.1 Groups   \n",
       "34                                                3.2 Rings and Fields   \n",
       "35                                4.1 Basic Definitions and Operations   \n",
       "36                                         4.2 Matrix Groups and Rings   \n",
       "37                                             5.1 Elementary Matrices   \n",
       "38                       5.2 The Echelon Form and Gaussian Elimination   \n",
       "39                                5.3 Rank and Equivalence of Matrices   \n",
       "40                                                                       \n",
       "41                                   7.1 Definition of the Determinant   \n",
       "42                                   7.2 Properties of the Determinant   \n",
       "43                                7.3 Minors and the Laplace Expansion   \n",
       "44  8.1 The Characteristic Polynomial  and the Cayley-Hamilton Theorem   \n",
       "45                                    8.2 Eigenvalues and Eigenvectors   \n",
       "46                             8.3 Eigenvectors of Stochastic Matrices   \n",
       "47               9.1 Basic Definitions and Properties of Vector Spaces   \n",
       "48                            9.2 Bases and Dimension of Vector Spaces   \n",
       "49                            9.3 Coordinates and Changes of the Basis   \n",
       "50            9.4 Relations Between Vector Spaces and Their Dimensions   \n",
       "51                                                                       \n",
       "\n",
       "   section_level_3  \\\n",
       "0                    \n",
       "1                    \n",
       "2                    \n",
       "3                    \n",
       "4                    \n",
       "5                    \n",
       "6                    \n",
       "7                    \n",
       "8                    \n",
       "9                    \n",
       "10                   \n",
       "11                   \n",
       "12                   \n",
       "13                   \n",
       "14                   \n",
       "15                   \n",
       "16                   \n",
       "17                   \n",
       "18                   \n",
       "19                   \n",
       "20                   \n",
       "21                   \n",
       "22                   \n",
       "23                   \n",
       "24                   \n",
       "25                   \n",
       "26                   \n",
       "27                   \n",
       "28                   \n",
       "29                   \n",
       "30                   \n",
       "31                   \n",
       "32                   \n",
       "33                   \n",
       "34                   \n",
       "35                   \n",
       "36                   \n",
       "37                   \n",
       "38                   \n",
       "39                   \n",
       "40                   \n",
       "41                   \n",
       "42                   \n",
       "43                   \n",
       "44                   \n",
       "45                   \n",
       "46                   \n",
       "47                   \n",
       "48                   \n",
       "49                   \n",
       "50                   \n",
       "51                   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           clean_content  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           chapter linear algebra every day life one familiarize student actual question application learns deal real world lothar collatz pagerank algorithm pagerank algorithm method ass importance document mutual link web page basis link structure developed sergei brin larry page founder google stanford university late basic idea algorithm following instead counting link pagerank essentially interprets link page page b vote page page pagerank ass importance page number received vote pagerank also considers importance page cast vote since vote page higher value thus also assign higher value page point important page rated higher thus lead higher position search let u describe model idea mathematically presentation us idea article given set web page every page k assigned importance value xk page k important page j xk x j page k link page j say page j backlink page description backlinks vote example consider following link structure man mus den lernenden mit konkreten fragestellungen au den anwendungen vertraut machen das er lernt konkrete fragen zu text found http translation springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi linear algebra every day life page link page backlink page easiest approach define importance web page count backlinks vote cast page important page example give importance value page thus important page equally important however intuition also description google suggests backlinks important page important value page le important page idea modeled defining xk sum importance value backlinks page example result four equation satisfied simultaneously disadvantage approach consider number link page thus would possible significantly increase importance page adding link page order avoid importance value backlinks pagerank algorithm divided number link corresponding page creates kind internet democracy every page vote page total cast one vote example give equation four equation four unknown equation unknown occur first power chap see write equation form linear system equation analyzing solving system one important task linear algebra example pagerank algorithm show linear algebra present powerful modeling term linear originates latin word linea mean straight line linearis mean consisting straight line  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        pagerank algorithm tool turned real world problem assessing importance web page problem linear algebra problem examined sect completeness mention solution four unknown computed matlab rounded second significant digit given thus page important one possible multiply solution importance value xk positive constant multiplication scaling often advantageous computational method visual display result example scaling could used give important page value scaling allowed since change ranking page essential information provided pagerank algorithm claim discounting car insurance insurance company compute premium customer basis insured risk higher risk higher premium therefore important identify factor lead higher risk case car insurance factor include number mile driven per year distance home work marital status engine power age driver using information company calculates initial premium usually best indicator future accident hence future insurance claim number accident individual customer past claim history order incorporate information premium rate insurer establish system risk class divide customer homogeneous risk group respect previous claim history customer fewer accident past get discount premium approach called claim discounting scheme mathematical model scheme need set risk class transition rule moving class end policy year customer may move different class depending claim made year discount given percent premium initial class simple example consider four risk class discount following transition rule accident step one class stay  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           linear algebra every day life one accident step back one class stay one accident step back class stay next insurance company estimate probability customer class ci year move class c j probability denoted pi j let u assume simplicity probability exactly one accident every customer probability two accident every customer course practice insurance company determine probability dependence class example customer class stay case least one accident happens probability customer accident probability chance move next year way obtain value pi j j arrange matrix follows entry matrix nonnegative real number sum entry row equal matrix called row-stochastic analysis matrix property central topic linear algebra developed throughout book example pagerank algorithm translated practical problem language linear algebra study using linear algebra technique example premium rate discussed example production planning plant production planning plant consider many different factor particular commodity price labor cost available capital order determine production plan consider simple example company produce product xi unit product pi produced pair called production plan suppose raw material labor production one unit product pi cost euro respectively euro available purchase raw material euro payment labor cost production plan must  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       production planning plant satisfy constraint inequality production plan satisfies constraint called feasible let pi profit selling one unit product pi goal determine production plan maximizes profit function find maximum two equation describe straight line coordinate system variable ax two line form boundary line feasible production plan line see figure note also must xi since produce negative unit product planned profit yi equation yi describe parallel straight line coordinate system see dashed line figure satisfy yi yi profit maximization problem solved moving dashed line one reach corner maximal case variable draw simple figure obtain solution graphically general idea finding corner maximum profit still example linear optimization problem formulated real world problem language linear algebra use mathematical method solution predicting future profit prediction profit loss company central planning instrument economics analogous problem arise many area political decision making  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          linear algebra every day life example budget planning tax estimate planning new infrastructure consider specific example four quarter year company profit million euro board want predict future profit development basis value evidence suggests profit behave linearly true profit would form straight line αt β connects point coordinate system time profit ax however neither hold example practice therefore one try find straight line deviate little possible given point one possible approach choose parameter α β order minimize sum squared distance given point straight line parameter α β determined resulting line used estimating predicting future profit illustrated following figure determination parameter α β minimize sum square called least square problem solve least square problem using method linear algebra example approach sometimes called parameter identification statistic modeling given data company profit using linear predictor function αt β known linear regression circuit simulation current development electronic device rapid short interval nowadays often le year new model laptop mobile phone issued market achieve continuously new generation computer chip developed typically become smaller powerful naturally use little energy possible important factor development plan simulate chip virtually computer without producing physical prototype model-based planning optimization product central method many high technology area based modern mathematics circuit simulation usually switching behavior chip modeled mathematical system consisting differential algebraic equation describe relation current voltage without going detail consider following circuit circuit description v given input current time characteristic value component r resistor l inductor c capacitor function potential difference three component denoted vr vl vc current applying kirchhoff electrical engineering lead following system linear equation differential equation model dynamic behavior circuit vl dt c vc dt r vr l vl vc vr v example easy solve last two equation vl vr hence obtain system differential equation r vc v dt l l l vc dt c function und vc discus solve system example simple example demonstrates simulation circuit system linear differential equation algebraic equation solved modern computer chip industrial practice require solving system million differential-algebraic equation linear algebra one central tool theoretical analysis system well development efficient solution method gustav robert kirchhoff  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            chapter linear map chapter study map vector space compatible two vector space operation addition scalar multiplication map called linear map homomorphism first investigate important property show case finite dimensional vector space every linear map represented matrix base respective space chosen base chosen clever way read important property linear map matrix representation central idea arise frequently later chapter basic definition property linear map start investigation definition linear map vector space definition let v w k space map f v w called linear f λv λ f v f v w f v f w hold v w v λ k set map denoted l v w linear map f v w also called linear transformation vector space homomorphism bijective linear map called isomorphism exists isomorphism v w space v w called isomorphic denote map f l v v called endomorphism bijective endomorphism called automorphism springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi linear map easy exercise show condition definition hold f λv μw λ f v μ f w hold λ μ k v w example every matrix k n defines map k k x ax map linear since λx λax x k λ k x ax ay x k cp lemma aii linear cp map trace k n n k ai j trace exercise map f q q linear show exercise map g q q linear example g g g set linear map vector space form vector space lemma let v w k space f g l v w λ k define f g λ f f g v f v g v λ f v λ f v v l v w k space proof cp exercise next result deal existence uniqueness linear map theorem let v w k space let vm basis v let wm exists unique linear map f l v w f vi wi basic definition property linear map v proof every v v exist unique coordinate λ v λm v v λi vi cp lemma define map f v w f v λi v wi v definition f vi wi λ λi v vi next show f linear every λ k λv hence λ λi v wi λ λi v wi λ f v f λv u λi u vi v v u f v u λi v λi u wi v λi λi v wi λi u vi hence λi u wi f v f u thus f l v w suppose g l v w also satisfies g vi wi v every v λi vi f v f λi v vi λi v f vi λi v wi λi v g vi g λi v vi g v hence f g f indeed uniquely determined theorem show map f l v w uniquely determined image f given basis vector note image vector wm w may linearly dependent w may infinite dimensional definition introduced image pre-image map next recall definition completeness introduce kernel linear map definition v w k space f l v w kernel image f defined ker f v v f v im f f v v v w w pre-image w space v defined f w f w v v f v w kernel linear map sometimes called null space nullspace map author use notation null f instead ker f linear map note pre-image f w set f mean inverse map f cp definition particular f ker f w im f f w ø example k n corresponding map l k k example ker x k ax im ax x k note ker l cp definition let j k denote jth column j x xm k write ax xjaj clearly ker moreover see representation ax ker column linearly independent set im given linear combination column im span lemma v w k space every f l v w following assertion hold f f f v v f isomorphism f l w v ker f subspace v im f subspace f surjective im f f injective ker f f injective vm v linearly independent f f vm w linearly independent vm v linearly dependent f f vm w linearly dependent equivalently f f vm w linearly independent vm v linearly independent w im f u f w arbitrary f w u ker f u v v ker f proof f f k k f well f v f f v f v existence inverse map f w v guaranteed theorem show f linear w exist uniquely determined v f f hence f f f f f f f f basic definition property linear map moreover every λ k f f λ f f f λ f obvious corresponding definition let f injective v ker f f v know f since f v f injectivity f yield v suppose ker f let u v v f u f v f u v u v ker f implies u v u λi f vi linearity f yield let f λi vi λi vi ker f λi vi hence since f injective λm due linear independence vm thus f f vm linearly independent λi vi λm vm linearly dependent k equal zero applying f side using linearity λi f vi hence f f vm linearly dependent yield let w im f u f w v f w f v f u thus f v u v u ker f v u ker f show f w u ker f hand v u f f v f u w v f w show u ker f f w example consider matrix k n corresponding map l k k example given b k b l b b im l b ø case corollary suppose b im let x l b arbitrary lemma yield l b x ker assertion lemma ker column linearly independent b case corollary ker column linearly dependent b case corollary basis ker λi wi k l b thus solution ax b depend parameter linear map following result give important dimension formula linear map also known rank-nullity theorem dimension image f equal rank matrix associated f cp theorem dimension kernel null space f sometimes called f theorem let v w k space let v finite dimensional every f l v w dimension formula dim v dim im f dim ker f proof let vn f f vn w linearly independent lemma also vn linearly independent thus dim im f dim v since ker f v dim ker f dim v im f ker f finite dimensional let wr vk base im f ker f respectively let u f u r f wr show u u r vk basis v implies assertion v v lemma unique coordinate μr k f v μi wi let v μi u f v f v hence v v ker f give v v λi vi unique coordinate λk k therefore k r λi vi μi u k λi vi thus v span u u r vk since u u r vk v v span u u r vk remains show u u r vk linearly independent r αi u k βi vi f f r αi u k βi vi r αi f u r αi wi thus αr wr linearly independent finally linear independence vk implies βk term introduced james joseph sylvester basic definition property linear map example linear map f α q α q ker f im f α hence dim im f dim ker f indeed dim im f dim ker f dim k n l k k example dim k dim ker dim im thus dim im dim ker hold ker column linearly independent cp example hand dim im dim ker dim im thus ker case column linearly dependent since exists x k ax corollary v w k space dim v dim w n f l v w following statement equivalent f injective f surjective f bijective proof hold hold definition show implied well f injective ker f cp lemma dimension formula theorem yield dim w dim v dim im f thus im f w cp lemma f also surjective f surjective im f w dimension formula dim w dim v yield dim ker f dim v dim im f dim w dim im f thus ker f f also injective using theorem also characterize two finite dimensional vector space isomorphic linear map corollary two finite dimensional k space v w isomorphic dim v dim w proof v w exists bijective map f l v w lemma im f w ker f dimension formula theorem yield dim v dim im f dim ker f dim w dim dim w let dim v dim w need show exists bijective f l v w let vn wn base v theorem exists unique f l v w f vi wi v λn vn ker f f v f λn vn f λn f vn λ λn wn since wn linearly independent λn hence v ker f thus f injective moreover dimension formula yield dim v dim im f dim w therefore im f w cp lemma f also surjective example vector space k n k n dimension therefore isomorphic isomorphism given linear map r-vector space c x iy x r dimension therefore isomorphic isomorphism given linear map x x iy vector space q dimension therefore isomorphic isomorphism given linear map although mathematics formal exact science smallest detail matter one sometimes us abuse notation order simplify presentation used example inductive existence proof echelon form theorem kept simplicity index larger matrix smaller matrix ai course entry position j matrix keeping index entry denoted rather induction made argument much le technical proof remained formally correct abuse notation always justified confused misuse notation field linear algebra justification often given isomorphism identifies vector space example constant polynomial field k polynomial form αt α k often written simply α element field justified since  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    basic definition property linear map k k isomorphic k space dimension already used identification similarly identified vector space v v written v instead v sect another common example literature notation k n text denotes set n-tuples element k often used matrix set column vector k row vector k n actual meaning clear context attentive reader significantly benefit simplification due abuse notation linear map matrix let v w finite dimensional k space base vm wn respectively let f l v w lemma every f v j w j exist unique coordinate ai j k n f v j j j wn define ai j k n write similarly equation vector f v j f f vm wn matrix determined uniquely f given base v v λm vm v f v f λm vm f λm f vm f f vm λm wn λm wn λm coordinate f v respect given basis w therefore given λm linear map thus compute coordinate f v simply multiplying coordinate v motivates following definition definition uniquely determined matrix called matrix representation f l v w respect base vm v wn denote matrix f construction matrix representation definition consistently extended case least one k space dimension zero instance dim v n w f v j every basis vector v j thus every vector f v j empty linear combination vector basis ø matrix representation f empty matrix size also v matrix representation f empty matrix size many different notation matrix representation linear map literature notation reflect matrix depends linear map f given base example alternative notation f f mean matrix important special case obtained v w hence particular n f idv identity obtain vn wn idv idv exactly matrix p coordinate transformation matrix theorem hand wn vn idv thus idv idv example consider vector space q base linear map f q q matrix representation f f f vector space k basis b n linear map linear map matrix f k k αn n n αn f j j j n f b b k thus f b b permutation matrix theorem let v w finite dimensional k space base vm wn respectively map l v w k n f f isomorphism hence l v w k n dim l v w dim k n n proof proof denote map f f mat mat f f first show map linear let f g l v w mat f f j mat g gi j j f g v j f v j g v j n f j wi n gi j wi n f j gi j wi thus mat f g f j gi j f j gi j mat f mat g λ k j λ f v j λ f v j λ n f j wi n λ f j wi thus mat λ f λ f j λ f j λ mat f remains show mat bijective f ker mat mat f k n f v j j thus f v v v f zero map mat injective cp lemma hand ai j k n arbitrary define linear map f v w via f v j n ai j wi j cp proof theorem mat f hence mat also surjective cp lemma corollary show dim l v w dim k n n cp also example linear map theorem show particular f g l v w satisfy f g f g hold given base v thus prove equality linear map via equality matrix representation consider map element finite dimensional vector space coordinate respect given basis lemma b vn basis k space v map v k v λn vn b v λn isomorphism called coordinate map v respect basis b proof linearity b clear moreover obviously b v k b surjective v ker b λn v ker b b also injective cp lemma example vector space k basis b n b αn n k αn hand basis b n yield αn b αn n k base finite dimensional vector space v w respectively illustrate meaning construction matrix representation f f l v w following commutative diagram v f f k k see different composition map yield result particular f f linear map matrix matrix f k n interpreted linear map k k use coordinate map bijective hence invertible way obtain f f f v f v v word coordinate f v respect basis w given product f coordinate v respect basis next show consecutive application linear map corresponds multiplication matrix representation theorem let v w x k space f l v w g l w x g f l v x moreover v w x finite dimensional respective base g f g f proof let h g f show first h l v x u v v λ μ k h λu μv g f λu μv g λ f u μ f v λg f u μg f v λh u μh v let vm wn x f f j g gi j j h v j g f v j g n f k j wk n f k j gik xi n n f k j g wk gik f k j h j n fk j gik xi xi thus h h j gi j f j g f using theorem study change base affect matrix representation linear map linear map corollary let v w finite dimensional k space base v f l v w f idw f idv particular matrix f f equivalent proof applying theorem twice identity f idw f idv yield f idw f idv idw f idv idw f idv matrix f f equivalent since idw idv invertible v w becomes f idv f idv idv f idv thus matrix representation f f endomorphism f l v v similar cp definition following commutative diagram illustrates corollary f k bee ee ee ee idv b yv yy k k yyy f w f b yy yy idw b ee ee ee ee k analogously f f b f example following base vector space linear map matrix coordinate transformation matrix idv idv idv coordinate map one easily verify idv theorem let v w k space dim v dim w n respectively exist base v w f ir k n r dim im f min n furthermore r rank f f matrix representation f respect arbitrary base v w define rank f rank f dim im f proof let vm wn two arbitrary base v w respectively let r rank f theorem exist invertible matrix q k n n z k q f z ir linear map r rank f min n let u introduce two new base vm wn v w via vm vm z wn wn q hence wn wn q construction z idv q idw corollary obtain ir idw f idv f thus found base yield desired matrix representation f every choice base lead corollary equivalent matrix therefore also rank r remains show r dim im f structure matrix f show f v j w j j r r j therefore vr vm ker f implies dim ker f r hand w j im f thus dim im f r theorem yield dim v dim im f dim ker f hence dim ker f r dim im f r example k n corresponding map l k k example im span thus rank equal number linearly independent column since rank rank cp theorem number equal number linearly independent row theorem first example general strategy use several time following chapter choosing appropriate base matrix representation reveal desired information linear map efficient way theorem information rank linear map f dimension image dimension formula linear map generalized composition map follows linear map matrix theorem v w x finite dimensional k space f l v w g l w x dim im g f dim im f dim im f ker g proof let g f restriction g image f map g l im f x v g v applying theorem g yield dim im f dim im g dim ker g im g g v x v im f im g f ker g v im f g v im f ker g imply assertion note theorem v w f idv g l v x give dim im g dim v dim ker g equivalent theorem interpret matrix k n b k n linear map theorem implies equation rank b rank dim im ker b special case k r b following result corollary rn rank rank proof let w ωn im ker w ay vector multiplying equation left using w ker obtain w ay implies ay w w n ω since hold w im ker linear map exercise following exercise k arbitrary field consider linear map given matrix determine ker dim ker dim im construct map f l v w linearly independent vector vr v image f f vr w linearly dependent map f r r αn n nαn n called derivative polynomial p r respect variable show f determine ker f im f base let f l r r matrix representation f determine f base b determine coordinate f respect basis construct map f l k k following property f pq f p q p f q p q k f map uniquely determined property map property let α k k n n show map k k p p α k k p p linear justify name evaluation homomorphism map let g l n k show map f k n n k n n isomorphism linear map matrix let k field let k n n consider map f k k x x ax f linear map show f let v q-vector space basis vn let f l v v defined f v j v j v j n j vn determine f b let wn w j j j show basis determine coordinate transformation matrix idv idv well matrix representation f f extend theorem consistently case w property matrix g f g f consider map f r r αn n αn n n show f linear determine ker f im f b choose base two vector space verify choice rank f dim im f hold let αn r n pairwise distinct number let n polynomial r defined pj n j αk α j αk j show set b pn basis r basis called lagrange r b show corresponding coordinate map given joseph-louis de lagrange linear map b r p p p αn hint use exercise b verify different path commutative diagram vector space base linear map f q q f  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             chapter linear form bilinear form chapter study different class map one two k space one dimensional k space defined field k map play important role many area mathematics including analysis functional analysis solution differential equation also essential development book using bilinear sesquilinear form introduced chapter define study euclidean unitary vector space chap linear form dual space used existence proof jordan canonical form chap linear form dual space start set linear map k space vector space k definition v k space f l v k called linear form k space v l v k called dual space linear form sometimes called linear functional one-form stress linearly map one dimensional vector space example v r-vector space continuous real valued function real interval α β γ α β two map f v r g g γ β f v r g g x x α linear form springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi linear form bilinear form dim v n dim v n theorem let vn basis v let basis k space k f v f vi αi αi k n f αn k n element v n λi vi v f v f n λi vi λi f vi λi αi αn λn n n n f v identified isomorphic vector space k k given basis finite dimensional vector space v construct special uniquely determined basis dual space v theorem v k space basis b vn exists unique basis b v v j δi j j n called dual basis b proof theorem unique linear map v k constructed prescribing image given basis b thus n exists unique map l v k v j δi j j remains show b basis v λn k n λi v v j n λi v j λ j j thus linearly independent dim v n implies b basis v cp exercise example consider v k canonical basis b en en dual basis b ei e j δi j show ei b eit k n n linear form dual space definition let v w k space respective dual space v w let f l v w f w v h f h h f called dual map f next derive property dual map lemma v w x k space following assertion hold f l v w dual map f linear hence f l w v f l v w g l w x g f l x v g f f f l v w bijective f l w v bijective f f proof h h w k f h h h h f h f h f h f h f f h f h exercise following theorem show concept dual map transposed matrix closely related theorem let v w finite dimensional k space base respectively let corresponding dual base f l v w f f proof vm wn let let f ai j k n f v j n ai j wi j f bi j k n f w bi j j n linear form bilinear form every pair k k n n wi n wi f f bik bik used definition dual map well wi δki close relationship transposed matrix dual map author call dual map f transpose linear map f applied matrix lemma theorem yield following rule known chap ab b k n b k g l n k example two base element corresponding dual base given r r r r matrix representation map linear map α f  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             linear form dual space f f bilinear form consider special map pair k space k space definition let v w k space map β v w k called bilinear form v w β w β w β w β v β v β v β λv w β v λw λβ v w hold v v w w λ k bilinear form β called non-degenerate first variable β v w w w implies v analogously called non-degenerate second variable β v w v v implies w β non-degenerate variable β called non-degenerate space v w called dual pair respect β v w β called bilinear form additionally β v w β w v hold v w v β called symmetric otherwise β called nonsymmetric example k n β k k k v w w av bilinear form k k non-degenerate n g l n k cp exercise bilinear form β r x x degenerate variable x β x β x x set r x β x x equal solution set quadratic equation two variable geometrically set given two straight line cartesian coordinate system linear form bilinear form v k space β v v k v f f v bilinear form v v since β f f f f β f β f β v f f f f v f v f v β v f β v f β λv f f λv λ f v λβ v f λ f v β v λ f hold v v f f f v λ k bilinear form non-degenerate thus v v dual pair respect β cp exercise case dim v n definition let v w k space base vm wn respectively β bilinear form v w β bi j k n bi j β v j wi called matrix representation β respect base v λ j v j v w μi wi w β v w n λ j μi β v j wi n μi bi j λ j w β v used coordinate map lemma example em n en n canonical base k k respectively β bilinear form example ai j k n β bi j n ai j ei n ae bi j β e j ei j hence β following result show symmetric bilinear form symmetric matrix representation lemma bilinear form β finite dimensional vector space v following statement equivalent β symmetric every basis b v matrix β symmetric exists basis b v β symmetric bilinear form proof exercise analyze effect basis change matrix representation bilinear form theorem let v w finite dimensional k space base v β bilinear form v w β idw β idv proof let vm vm wn n w vm p p pi j idv vm wn w n q q qi j idw β vj w bi j bi j β β v j wi β pk j vk n w n n β vk w pk j pk j j β qni pm j implies β q β p hence assertion follows v w two base v obtain following special case theorem β idv β idv two matrix representation β β β case congruent formally define follows definition two matrix b k n n exists matrix z g l n k b z az b called congruent lemma congruence equivalence relation set k n n proof exercise  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     linear form bilinear form sesquilinear form complex vector space introduce another special class form definition let v w c-vector space map v w c called sesquilinear form v w w w w λv w λs v w v v v v λw λs v w hold v v w w λ v w called sesquilinear form additionally v w w v hold v w v called prefix sesqui latin mean one half note sesquilinear form linear first variable semilinear half linear second variable following result characterizes hermitian sesquilinear form lemma sesquilinear form c-vector space v hermitian v v r v proof hermitian particular v v v v v v thus v v hand v w v definition v w v w v v v w w v w w v iw v iw v v w v v w w w first equation implies v w w v r since v w v w v v w w r assumption second equation implies analogously w v v w therefore v w w v v w w v v w w v v w w v multiplying second equation adding resulting equation first obtain v w w v corollary sesquilinear form c-vector space v v w v w v w v iw v iw v v w w v w charles hermite sesquilinear form proof result follows multiplication adding result corollary show sesquilinear form c-vector space v uniquely determined value v v v definition hermitian transpose ai j cn matrix h j cm n h called hermitian matrix real entry obviously h thus real symmetric matrix also hermitian ai j cn n hermitian particular aii ii n hermitian matrix real diagonal entry hermitian transposition satisfies similar rule usual transposition cp lemma cn b cm λ c following assertion lemma hold h h h ah λ h λ h ab h b h h proof exercise example cn map c v w w h av sesquilinear form matrix representation sesquilinear form defined analogously matrix representation bilinear form cp definition definition let v w c-vector space base vm wn respectively sesquilinear form v w bi j cn bi j v j wi called matrix representation respect base example em n en n canonical base respectively sesquilinear form example ai j cn bi j linear form bilinear form n bi j e ei n ae ai j ei n ae j ei j j hence exercise following exercise k arbitrary field let v finite dimensional k space v show f v f v v consider basis b vector space r compute dual basis b b let v n-dimensional k space let basis v prove disprove exists unique basis vn v v j δi j let v finite dimensional k space let f g v f show g λ f λ k hold ker f ker g possible omit assumption f let v k space let u subspace set u f v f u u u called annihilator u show following assertion u subspace v b subspace v c w k space f l v w ker f im f prove lemma let v w k space show set bilinear form v w operation v w v w v w λ β v w λ β v w k space let v w k space base vm wn corresponding dual base respectively j n let βi j v w k v w v w w show βi j bilinear form v w sesquilinear form b show set βi j j n basis k space bilinear form v w cp exercise determine dimension space let v r-vector space continuous real valued function real interval α β show β f x g x x β v v r f g α symmetric bilinear form β degenerate show map β example bilinear form show non-degenerate n g l n k let v finite dimensional k space show v v dual pair respect bilinear form β example bilinear form β non-degenerate let v finite dimensional k space let u v w v subspace dim u dim w prove disprove space u w form dual pair respect bilinear form β u w k v h h v let v w finite dimensional k space base respectively let β bilinear form v show following statement equivalent β invertible β degenerate second variable β degenerate first variable b conclude β non-degenerate β invertible prove lemma prove lemma bilinear form β k space v map qβ v k v β v v called quadratic form induced β show following assertion k β symmetric β v w qβ v v qβ w hold v w show sesquilinear form c-vector space v satisfies polarization identity v w v v v v v v v v v w consider following map c x x b x c x linear form bilinear form x bilinear form sesquilinear form test whether bilinear form symmetric sesquilinear form hermitian derive corresponding matrix representation respect canonical basis basis prove lemma let cn n hermitian show v w w h av hermitian sesquilinear form let v finite dimensional c-vector space basis b let sesquilinear form show hermitian hermitian show following assertion b cn n h eigenvalue purely imaginary b h trace trace c h b h b trace ab trace b  \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               chapter euclidean unitary vector space chapter study vector space field r using definition bilinear sesquilinear form introduce scalar product vector space scalar product allow extension well-known concept elementary geometry length angle abstract real complex vector space particular lead idea orthogonality orthonormal base vector space example importance concept many application study least-squares approximation scalar product norm start definition scalar product euclidean unitary vector space definition let v k space either k r k map v v k v w called scalar product v following property hold k r symmetric bilinear form k c hermitian sesquilinear form positive definite hold v v equality v r-vector space scalar product called euclidean vector c-vector space scalar product called unitary vector space scalar product sometimes called inner product note nonnegative real also v c-vector space easy see subspace u euclid alexandria approx bc springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi euclidean unitary vector space euclidean unitary vector space v euclidean unitary vector space respectively scalar product space v restricted subspace u example scalar product given w called standard scalar product scalar product given w h called standard scalar product k r k c spur b h scalar product k n scalar product vector space continuous real valued function real interval α β given f α β f x g x x show use euclidean unitary structure vector space order introduce geometric concept length vector angle vector motivation general concept length absolute value real number map r r x map following property λ x x r equality x x property generalized real complex vector space follows definition let v k space either k r k map v r v v scalar product norm called norm v v w v λ k following property hold λv v v equality v v w v w triangle inequality k space norm defined called normed space example standard scalar product v v v defines norm called euclidean norm standard scalar product v v h v defines norm called euclidean norm common terminology although space unitary euclidean k r k c f trace h n j norm k n called frobenius k n frobenius norm equal euclidean norm k moreover frobenius norm k n equal euclidean norm k k nm identify vector space via isomorphism obviously f f h f k n v vector space continuous real valued function real interval α β β f x x f f f α norm v called l let k r k c let p r p given v νn k p-norm k defined v p n ferdinand georg frobenius p p euclidean unitary vector space p euclidean norm k norm typically omit index write instead taking limit p obtain k given v max following figure illustrate unit circle respect p-norm set v v p p p p k r k c p-norm k n defined p sup av p v p use p-norm k denominator p-norm k numerator notation sup mean supremum least upper bound known analysis one show supremum attained vector v thus may write max instead sup definition particular ai j k n max max n j j norm called maximum column sum maximum row sum norm k n respectively easily see h h however matrix thus matrix satisfies matrix considered chap scalar product norm norm example form v given scalar product show map v always defines norm proof based following theorem theorem v euclidean unitary vector space scalar product v w v equality v w linearly dependent proof inequality trivial w thus let w let λ λw v λ implies v w linearly dependent v λw scalar λ hence λλ hand let w v w linearly dependent w define λ get λw v since scalar product positive definite v λw thus v w linearly dependent inequality called cauchy-schwarz important tool analysis particular estimation approximation interpolation error augustin louis cauchy hermann amandus schwarz  \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           euclidean unitary vector space corollary v euclidean unitary vector space scalar product map v r v v norm v called norm induced scalar product proof prove three defining property norm since positive definite v equality v v v λ k euclidean case k r unitary case k c λv hence λv v order show triangle inequality use cauchy-schwarz inequality fact z every complex number z v w v w v v w v w v v w w v w thus v w v w orthogonality use scalar product introduce angle vector motivation consider euclidean vector space standard scalar product induced euclidean norm v cauchy-schwarz inequality show v w v w v w angle v w uniquely determined real number ϕ π co ϕ v w orthogonality vector v w orthogonal ϕ co ϕ thus v w orthogonal elementary calculation lead cosine theorem triangle w v v w v w co ϕ v w orthogonal cosine theorem implies pythagorean v w following figure illustrate cosine theorem pythagorean theorem vector following definition generalize idea angle orthogonality definition let v euclidean unitary vector space scalar product euclidean case angle two vector v w v uniquely determined real number ϕ π co ϕ v w two vector v w v called orthogonal basis vn v called orthogonal basis v j j n j furthermore vi n v norm induced scalar product vn called orthonormal basis orthonormal basis therefore v j δi j pythagoras samos approx bc euclidean unitary vector space note term defined respect given scalar product different scalar product yield different angle vector particular orthogonality two given vector may lost consider different scalar product example standard basis vector orthogonal orthonormal basis respect standard scalar product cp example consider symmetric invertible matrix defines symmetric non-degenerate bilinear form v w w av cp example bilinear form positive definite since v v av bilinear form therefore scalar product denote denote induced norm respect scalar product vector satisfy clearly orthonormal basis r respect also note hand vector satisfy hence orthonormal basis respect scalar product show every finite dimensional euclidean unitary vector space orthonormal basis theorem let v euclidean unitary vector space basis vn exists orthonormal basis u u n v span u u k span vk k n orthogonality proof give proof induction dim v n set u u u orthonormal basis v span u span let assertion hold n let dim v n let basis vn span vn n-dimensional subspace induction hypothesis exists orthonormal basis u u n vn span u u k span vk k define u n u k k u u u since vn span u u n must u lemma yield span u u span j n u u j u u j u u n u k k u j u j u j u j u finally u u u completes proof proof theorem show given basis vn orthonormalized transformed orthonormal basis u u n span u u k span vk k resulting algorithm called gram-schmidt method algorithm given basis vn set u j n set u v u u jørgen j u k k u pedersen gram erhard schmidt euclidean unitary vector space slight reordering combination step gram-schmidt method yield u u vn u u u n u n n n un upper triangular matrix right hand side coordinate transformation matrix basis vn basis u u n v cp theorem thus shown following result theorem v finite dimensional euclidean unitary vector space given basis gram-schmidt method applied yield orthonormal basis v idv invertible upper triangular matrix consider m-dimensional subspace standard scalar product write vector orthonormal basis qm column matrix q qm obtain real case q q qit q j j qi δ ji im analogously complex case q h q qih q j j qi δ ji im hand q q im q h q im matrix q rn q cn respectively column q form orthonormal basis respect standard scalar product m-dimensional subspace respectively matrix version theorem therefore formulated follows corollary let k r k c let vm k linearly independent exists matrix q k n column orthonormal respect standard scalar product k q q im k r q h q im k c upper triangular matrix r g l k vm q factorization called q r-decomposition matrix vm q r-decomposition many application numerical mathematics cp example lemma let k r k c let q k n matrix orthonormal column respect standard scalar product k v qv hold v k euclidean norm k k orthogonality proof k c v v h v v h q h q v qv proof k r analogous introduce two important class matrix definition matrix q rn n whose column form orthonormal basis respect standard scalar product called orthogonal matrix q cn n whose column form orthonormal basis respect standard scalar product called unitary matrix q qn rn n therefore orthogonal q q qit q j j qi δ ji particular orthogonal matrix q invertible q q cp corollary equation q q mean n row q form orthonormal basis n respect scalar product wv analogously unitary matrix q cn n invertible q q h h q q q q h n column q form orthonormal basis n lemma set n orthogonal u n unitary n matrix form subgroup g l n r g l n c respectively proof consider n proof u n analogous since every orthogonal matrix invertible n g l n r identity matrix orthogonal hence n ø q n also q q n since q q q q finally q q n q q q q q q q q q q thus q q n example many application measurement sample lead data set represented tuples τi μi τm pairwise distinct measurement point μm corresponding measurement order approximate given data set simple model one try construct polynomial p small degree value p p τm close possible measurement μm simplest case real polynomial degree geometrically corresponds construction straight line minimal distance euclidean unitary vector space given point shown figure cp sect many possibility measure distance following describe one detail use gram-schmidt method q r-decomposition construction straight line statistic method called linear regression real polynomial degree form p αt β looking coefficient α β r p τi ατi β μi using matrix write problem α v v α β β μm τm mentioned different possibility interpreting symbol particular different norm measure distance given value μm polynomial value p p τm use euclidean norm consider minimization problem α min β α vector linearly independent since entry pairwise distinct entry equal let r q r-decomposition extend vector orthonormal basis qm q qm rm orthogonal matrix orthogonality α min v min v α β α r α β r α min q β α r α q min q β α r α β min α qt used q q im qv v v upper triangular matrix r invertible thus minimization problem solved q α r β using definition euclidean norm write minimizing property polynomial p αt β α p τi μi β min α ατi β μi since polynomial p minimizes sum square distance measurement μi polynomial value p τi polynomial yield least square approximation measurement value consider example sect four quarter year company profit million euro assumption profit grows linearly like straight line goal estimate profit last quarter following year given data lead approximation problem α α β β euclidean unitary vector space numerical computation q r-decomposition yield α β resulting profit estimate last quarter following year p million euro matlab-minute example one could imagine profit grows quadratically instead linearly determine analogously procedure example polynomial p αt βt γ solves least square problem p τi μi min α β βτi γ μi use matlab command qr computing q r-decomposition determine estimated profit last quarter following year analyze property orthonormal base detail lemma v euclidean unitary vector space scalar product orthonormal basis u u n n u v proof every v v exist uniquely determined coordinate λn n λi u every j n u j λi u j v λj coordinate u n v respect orthonormal basis u u n often called n fourier coefficient v respect basis representation v u called abstract fourier expansion v given orthonormal basis jean baptiste joseph fourier orthogonality corollary v euclidean unitary vector space scalar product orthonormal basis u u n following assertion hold n n u u u v w v parseval n u v v bessel proof v n u thus n n n u w u u u special case v bessel identity every vector v v satisfies v n u max u norm induced scalar product absolute value coordinate v respect orthonormal basis v therefore bounded norm property hold general basis example consider v standard scalar product euclidean norm every real ε set ε basis every vector v v ε ε ε moderate number small large numerical algorithm situation lead significant problem due roundoff error avoided orthonormal base used marc-antoine friedrich parseval wilhelm bessel  \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            euclidean unitary vector space definition let v euclidean unitary vector space scalar product let u v subspace u v v u u called orthogonal complement u v lemma orthogonal complement u subspace proof exercise lemma v n-dimensional euclidean unitary vector space u v m-dimensional subspace dim u n v u u proof know n cp lemma n u v thus u v v v u v assertion trivial thus let n let u u orthonormal basis u extend basis basis v apply gram-schmidt method order obtain orthonormal basis u u u u n span u u n u therefore v u u w u u hence w since scalar product positive definite thus u u implies v u u dim u n cp theorem particular u span u u n vector product section consider product vector space frequently used physic electrical engineering definition vector product cross product map v w v w contrast scalar product vector product two element vector space scalar vector using canonical basis vector vector product write vector product ω ω det det v w det ω ω ω lemma vector product linear component v w following property hold v w v vector product anti commutative alternating v w v w standard scalar product euclidean norm v v standard scalar product proof exercise cauchy-schwarz inequality follows v w hold v w linearly dependent obtain μw v v v arbitrary λ μ v w linearly independent product v w orthogonal plane origin spanned v w v w λv μw λ μ r geometrically two possibility position three vector v w v left side figure correspond right-handed orientation usual coordinate system canonical basis vector associated thumb index finger middle finger right hand motivates name right-hand rule order explain detail one need introduce concept orientation omit ϕ π angle vector v w v w co ϕ euclidean unitary vector space cp definition write lemma v w v w ϕ v w ϕ v w sin ϕ geometric interpretation equation following norm vector product v w equal area parallelogram spanned v interpretation illustrated following figure exercise let v finite dimensional real complex vector space show exists scalar product show map defined example scalar product corresponding vector space let arbitrary scalar product show exists matrix rn n w av v w let v finite dimensional c-vector space let scalar product v following property v w v satisfy v w also v w prove disprove exists real scalar λ v w v w v w show map defined example norm corresponding vector space show n max j max j ai j k n k r k c cp example sketch matrix example p set av v v p let v euclidean unitary vector space let norm induced scalar product show satisfies parallelogram identity v w v w vector product let v k space k r k c scalar product induced norm show v w v orthogonal respect v λw v λw λ k exist scalar product cp example induced norm scalar product show inequality n αi βi n γi αi n βi γi hold arbitrary real number αn βn positive real number γn let v finite dimensional euclidean unitary vector space scalar product let f v v map f v f w v w show f isomorphism let v unitary vector space suppose f l v v satisfies f v v prove disprove f statement also hold euclidean vector space let diag dn rn n dn show w dv scalar product analyze property scalar product violated least one di zero di nonzero different sign orthonormalize following basis vector space respect scalar product trace b h let q rn n orthogonal let q cn n unitary matrix possible value det q let u let h u ut u uu rn n show n column h u form orthonormal basis respect standard scalar product matrix form called householder study detail example prove lemma alston scott householder pioneer numerical linear algebra euclidean unitary vector space let analyze whether vector orthonormal respect standard scalar product compute orthogonal complement span let v euclidean unitary vector space scalar product let u u k v let u span u u k show v v v u u j j unitary vector space standard scalar product let given determine orthonormal basis span prove lemma  \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               chapter adjoints linear map chapter introduce adjoints linear map sense represent generalization hermitian transpose matrix matrix symmetric hermitian equal hermitian transpose analogous way endomorphism selfadjoint equal adjoint endomorphism set symmetric hermitian matrix selfadjoint endomorphisms form certain vector space play key role proof fundamental theorem algebra chap special property selfadjoint endomorphisms studied chap basic definition property chap considered euclidean unitary vector space hence vector space field r let v w vector space general field k let β bilinear form v every fixed vector v v map βv w k w β v w linear form thus assign every v v vector βv w defines map β v w v βv analogously define map β w v w βw βw v k defined v β v w every w springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi adjoints linear map lemma map β β defined respectively linear β l v w β l w v dim v dim w n β non-degenerate cp definition β β bijective thus isomorphism proof prove assertion map β proof β analogous first show linearity let v k every w w β w β w β w β w β w β w β β w hence β β β therefore β l v w let dim v dim w n let β non-degenerate show β l v w injective lemma hold ker β v ker β β v βv w thus βv w β v w w since β non-degenerate v finally dim v dim w dim w dim w imply dim v dim w β bijective cp corollary next discus existence adjoint map theorem v w k space dim v dim w n β non-degenerate bilinear form v w following assertion hold every f l v v exists uniquely determined g l w w β f v w β v g w v v w map g called right adjoint f respect β every h l w w exists uniquely determined k l v v β v h w β k v w v v w map k called left adjoint h respect β proof show proof analogous let v dual space v let f l v v dual map f let β l w v since β non-degenerate β bijective lemma define g β f β l w w basic definition property v v w w β v g w β v β f β w β β f β w v β β f β w v β β β w f v β w f v β f v w recall dual map satisfies f β w β w f remains show uniqueness let g l w w β v g w β f v w v v w β v g w β v g w hence β v g g w v v w since β non-degenerate second variable g g w w w g example let v w k β v w w bv matrix b g l n k β non-degenerate cp example consider linear map f v v v fv matrix f k n n linear map h w w w h w matrix h k n n βv w k w w bv β v w v bv β w v w w b identified isomorphic vector space w k n respectively v k n g l w w right adjoint f respect β β f v w w b f v w b fv β v g w g w bv v v w represent linear map g via multiplication matrix g k n n g w gw w b fv w g bv v w k hence b f g b since b invertible unique right adjoint given g b f b b f b analogously left adjoint k l v v h respect β obtain equation β v h w h w bv w h bv β k v w w bk v v v w k v lv matrix l k n n obtain h b b l hence l b h b adjoints linear map v finite dimensional β non-degenerate bilinear form v theorem every f l v v unique right adjoint g unique left adjoint k β f v w β v g w β v f w β k v w v w β symmetric β v w β w v hold v w v yield β v g w β f v w β w f v β k w v β v k w therefore β v g k w v w v hence g k since β non-degenerate thus proved following result corollary β symmetric non-degenerate bilinear form finite dimensional k space v every f l v v exists unique g l v v β f v w β v g w β v f w β g v w v w definition scalar product euclidean vector space symmetric nondegenerate bilinear form cp definition lead following corollary corollary v finite dimensional euclidean vector space scalar product every f l v v exists unique f ad l v v f v w v f ad w v f w f ad v w v w map f ad called adjoint f respect order determine whether given map g l v v unique adjoint f l v v one two condition verified f g l v v equation f v w v g w hold v w v also v f w f w v w g v g v w v w v used symmetry scalar product similarly v f w g v w hold v w v also f v w v g w v w v basic definition property example consider euclidean vector space scalar product v w w dv linear map f v fv f v w f v w w fv w f dv f w dv v f ad w thus f ad v f dv v used symmetric show uniquely determined adjoint map also exist unitary case however conclude directly corollary since scalar product c-vector space symmetric bilinear form hermitian sesquilinear form order show existence adjoint map unitary case construct explicitly construction work also euclidean case let v unitary vector space scalar product let u u n orthonormal basis given f l v v define map n v f u u g v v v v w v λ μ c n g λv μw n λv μw f u u λg v μg w λ v f u u μ v f u u adjoints linear map hence g l v v let v n λi u v w v n n w f u j u j λi u v g w n n λi w f u λi f u w f v w furthermore v f w f w v w g v g v w v w g l v v satisfies f v w v g w v w v g g since scalar product positive definite therefore formulate following result analogously corollary corollary v finite dimensional unitary vector space scalar product every f l v v exists unique f ad l v v f v w v f ad w v f w f ad v w v w map f ad called adjoint f respect euclidean case validity one two equation v w v implies validity v w example consider unitary vector space scalar product v w w h dv linear map f v fv f v w f v w w h fv w h f dv f h h w h dv v f ad w thus f ad v f h dv v basic definition property used real symmetric next investigate property adjoint map lemma let v finite dimensional euclidean unitary vector space f f l v v k k r euclidean k c unitary case f f ad f f euclidean case map f f ad therefore linear unity case semilinear idv ad idv every f l v v f ad ad f f f l v v f f ad f f proof v w v k f f v w f v w f v w v f w v f w v f w f w v f f w thus f f ad f f v w v idv v w v w v idv w thus idv ad idv v w v f ad v w v f w thus f ad ad f v w v f f v w f f v w f v f w v f f w v f f w thus f f ad f f following result show relation image kernel endomorphism adjoint theorem v finite dimensional euclidean unitary vector space f l v v following assertion hold adjoints linear map ker f ad im f ker f im f ad proof w ker f ad f ad w v f ad w f v w v v hence w im f hand w im f f v w v f ad w v since non-degenerate f ad w hence w ker f ad using f ad ad f get ker f ker f ad ad im f ad example consider unitary vector space standard scalar product linear map f v fv f f ad v f h v f h matrix f f h rank therefore dim ker f dim ker f ad simple calculation show ker f span ker f ad span dimension formula linear map implies dim im f dim im f ad matrix f f h see im f span im f ad span equation ker f ad im f ker f im f ad verified direct computation  \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              adjoint endomorphisms matrix adjoint endomorphisms matrix study relation matrix representation endomorphism adjoint let v finite dimensional unitary vector space scalar product let f l v v orthonormal basis b u u n v let f b b ai j cn n n f u j ak j u k j n hence n ak j u k u ai j j f u j u f ad b b bi j cn n n f ad u j bk j u k j n bi j f ad u j u u j f u f u u j ji thus f ad b b f b b h hold finite dimensional euclidean vector space omit complex conjugation therefore shown following result theorem v finite dimensional euclidean unitary vector space orthonormal basis b f l v v f ad b b f b b h euclidean case f b b h f b b important special class selfadjoint endomorphisms definition let v finite dimensional euclidean unitary vector space endomorphism f l v v called selfadjoint f f ad trivial example selfadjoint endomorphism l v v f idv corollary v finite dimensional euclidean vector space f l v v selfadjoint b orthonormal basis v f b b symmetric matrix adjoints linear map v finite dimensional unitary vector space f l v v selfadjoint b orthonormal basis v f b b hermitian matrix selfadjoint endomorphisms form vector space however one careful use appropriate field vector space defined particular set selfadjoint endomorphisms unitary vector space v form c-vector space f f ad l v v f ad f ad f f cp lemma similarly hermitian matrix cn n form c-vector space h cn n hermitian h h lemma v n-dimensional euclidean vector space set selfadjoint endomorphisms f l v v f f ad form r-vector space dimension n n v n-dimensional unitary vector space set selfadjoint endomorphisms f l v v f f ad form r-vector space dimension proof exercise matrix cn n called complex symmetric unlike hermitian matrix complex symmetric matrix form c-vector space lemma set complex symmetric matrix cn n form c-vector space dimension n n proof exercise lemma used chap proof fundamental theorem algebra exercise let β v w w bv b diag defined v w consider linear map f v fv h w h w h determine βv β β well right adjoint f left adjoint h respect β let v v w w two finite dimensional euclidean vector space let f l v w show exists unique g l w v f v w w v g w v v v w let v w w bv v w adjoint endomorphisms matrix show v w w bv scalar product b using scalar product determine adjoint map f ad f v fv f c investigate property f need satisfy f selfadjoint let n f xn determine adjoint f ad f respect standard scalar product let v finite dimensional euclidean unitary vector space let f l v v show ker f ad f ker f im f ad f im f ad let v finite dimensional euclidean unitary vector space let u v subspace let f l v v f u u show f ad u u let v finite dimensional euclidean unitary vector space let f l v v v show v im f v ker f ad matrix version cn n b linear system equation ax b solution b l h let v finite dimensional euclidean unitary vector space let f g l v v selfadjoint show f g selfadjoint f g commute f g g f let v finite dimensional unitary vector space let f l v v show f selfadjoint f v v r hold v let v finite dimensional euclidean unitary vector space let f l v v projection f satisfies f f show f selfadjoint ker f im f v w hold v ker f w im f let v finite dimensional euclidean unitary vector space let f g l v v show g ad f l v v v w hold v im f w im g two polynomial p q r let p q dt p q show defines scalar product r b consider map n f r r n αi iαi determine f ad ker f ad im f ker f ad im f prove lemma prove lemma  \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             chapter eigenvalue endomorphisms previous chapter already studied eigenvalue eigenvectors matrix chapter generalize concept endomorphisms investigate endomorphisms finite dimensional vector space represented diagonal matrix upper triangular matrix representation easily read important information endomorphism particular eigenvalue basic definition property first consider arbitrary vector space concentrate finite dimensional case definition let v k space f l v v λ k v v satisfy f v λv λ called eigenvalue f v called eigenvector f corresponding λ definition v eigenvector eigenvalue λ may occur cp example following definition equation f v λv written λv f v λidv f v hence λ k eigenvalue f ker λidv f springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi eigenvalue endomorphisms already know kernel endomorphism v form subspace v cp lemma hold particular ker λidv f definition v k space λ k eigenvalue f l v v subspace v f λ ker λidv f called eigenspace f corresponding λ g λ f dim v f λ called geometric multiplicity eigenvalue λ definition eigenspace v f λ spanned eigenvectors f corresponding eigenvalue λ v f λ finite dimensional g λ f dim v f λ equal maximal number linearly independent eigenvectors f corresponding λ definition let v k space let u v subspace let f l v v f u u f u u hold u u u called f subspace important example f subspace eigenspaces f lemma v k space λ k eigenvalue f l v v v f λ f subspace proof every v v f λ f v λv v f λ consider finite dimensional vector space discus relationship eigenvalue f eigenvalue matrix representation f respect given basis lemma v finite dimensional k space f l v v following statement equivalent λ k eigenvalue f λ k eigenvalue matrix f b b every basis b proof let λ k eigenvalue f let b vn arbitrary basis v v eigenvector f corresponding eigenvalue λ f v λv exist unique coordinate μn k equal zero v μ j v j using obtain f b b b f v b λv b v λ μn μn basic definition property thus λ eigenvalue f b b hand f b b μn λ μn given arbitrary basis b vn v set μn v μ j v j v f v μ j f v j f f vn vn f b b μn μn vn λv n μn λ eigenvalue f lemma implies eigenvalue f root characteristic polynomial matrix f b b cp theorem however hold general matrix representation form f b b b b two different base general two matrix f b b idv b b f b b f b b eigenvalue example consider vector space base endomorphism f v fv f matrix representation f b b f b b det f b b thus f eigenvalue hand characteristic polynomial f b b matrix eigenvalue two different base b b v matrix f b b f b b similar cp discussion following corollary theorem shown eigenvalue endomorphisms similar matrix characteristic polynomial justifies following definition definition n n v n-dimensional k space basis b f l v v p f det f b b k called characteristic polynomial f characteristic polynomial p f always monic polynomial deg p f n dim v discussed p f independent choice basis scalar λ k eigenvalue f λ root p f p f λ shown example real vector space dimension least two exist endomorphisms eigenvalue λ root p f p f λ q monic polynomial q k linear factor λ divide polynomial p f show formally corollary also q λ q λ q monic polynomial q continue p f λ g q k thus p f λ g k g λ lead following definition definition let v finite dimensional k space let f l v v eigenvalue λ k characteristic polynomial f form p f λ g g k g λ called algebraic multiplicity eigenvalue λ f denoted λ f λk pairwise distinct eigenvalue f corresponding algebraic multiplicity f λk f dim v n f λk f n since deg p f dim v example endomorphism f v fv f characteristic polynomial p f real root p f f dim  \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  basic definition property lemma v finite dimensional k space f l v v g λ f λ f every eigenvalue λ f proof let λ k eigenvalue f geometric multiplicity g λ f exist linear independent eigenvectors vm v f corresponding eigenvalue λ dim v eigenvectors form basis b dim v n extend eigenvectors basis b vm vn f v j λv j j therefore f b b λim z two matrix z k z k using lemma obtain p f det f b b λ det z implies λ f g λ f following try find basis v eigenvalue given endomorphism f read easily matrix representation easiest form matrix sense diagonal triangular matrix since eigenvalue diagonal entry diagonalizability section analyze given endomorphism diagonal matrix representation formally define property follows definition let v finite dimensional k space endomorphism f l v v called diagonalizable exists basis b v f b b diagonal matrix accordingly matrix k n n diagonalizable exists matrix g l n k d diagonal matrix k n n order analyze diagonalizablility begin sufficient condition linear independence eigenvectors condition also hold v infinite dimensional lemma let v k space f l v v λk k k pairwise distinct eigenvalue f corresponding eigenvectors vk v vk linearly independent eigenvalue endomorphisms proof prove assertion induction let k let eigenvectors f corresponding eigenvalue let k applying f side equation well multiplying equation yield two equation subtracting second equation first get since also obtain since thus linearly independent proof inductive step analogous assume assertion hold k let pairwise distinct eigenvalue f corresponding eigenvectors let k satisfy μk vk applying f equation yield μk λk vk multiplication give μk vk subtracting equation previous one get μk λk vk since pairwise distinct vk linearly independent induction hypothesis obtain μk implies also linearly independent using result next show sum eigenspaces corresponding pairwise distinct eigenvalue direct cp theorem lemma let v k space f l v v λk k k pairwise distinct eigenvalue f corresponding eigenspaces satisfy k v f λi v f λ j k diagonalizability proof let fixed let k v v f λi v f λ j v j v j v f λ j j particular v v linear independence eigenvectors corresponding pairwise j distinct eigenvalue cp lemma implies v following theorem give necessary sufficient condition diagonalizability endomorphism finite dimensional vector space theorem v finite dimensional k space f l v v following statement equivalent f diagonalizable exists basis v consisting eigenvectors f characteristic polynomial p f decomposes n dim v linear factor k p f λn eigenvalue λn k f every eigenvalue λ j g λ j f λ j f proof f l v v diagonalizable exists basis b vn v scalar λn k f b b λn hence f v j λ j v j j scalar λn thus eigenvalue f corresponding eigenvectors vn hand exists basis b vn v consisting eigenvectors f f v j λ j v j j n scalar λn k corresponding eigenvalue hence f b b form let b vn basis v consisting eigenvectors f let λn k corresponding eigenvalue f b b form hence p f λn p f decomposes linear factor k eigenvalue endomorphisms still show g λ j f λ j f every eigenvalue λ j eigenvalue λ j algebraic multiplicity j λ j f λ j occurs j time diagonal diagonal matrix f b b hold exactly j vector basis b eigenvectors f corresponding eigenvalue λ j j linearly independent vector element eigenspace v f λ j hence dim v f λ j g λ j f j λ j f lemma know g λ j f λ j f thus g λ j f λ j f λk pairwise distinct eigenvalue f correspond let λ j f j k ing geometric algebraic multiplicity g λ j f respectively since p f decomposes linear factor k λ j f n dim v g λ j f λ j f j k implies k g λ j f n dim v lemma obtain cp also theorem v f v f λk λ j j k select base respective eigenspaces v f get basis v consists eigenvectors f theorem lemma imply important sufficient condition diagonalizability corollary v n-dimensional k space f l v v n pairwise distinct eigenvalue f diagonalizable condition n dim v pairwise distinct eigenvalue however necessary diagonalizability endomorphism simple counterexample identity idv n-fold eigenvalue idv b b hold every basis b hand exist endomorphisms multiple eigenvalue diagonalizable  \n",
       "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             diagonalizability example endomorphism f r r v fv f characteristic polynomial thus eigenvalue ker v f span thus g f f theorem f diagonalizable triangulation schur theorem property g λ j f λ j f hold every eigenvalue λ j f f diagonalizable however long characteristic polynomial p f decomposes linear factor find special basis b f b b triangular matrix theorem v finite dimensional k space f l v v following statement equivalent characteristic polynomial p f decomposes linear factor k exists basis b v f b b upper triangular f triangulated proof n dim v f b b ri j k n n upper triangular p f rnn show assertion induction n dim v case n trivial since f b b k suppose assertion hold n let dim v n assumption p f k eigenvalue f let v eigenvector corresponding eigenvalue extend vector basis b bw w span bw v span w f b b eigenvalue endomorphisms define h l w span g l w w h w j j g w j k j wk j n f w h w g w w w f b b h bw g bw bw consequently pg p f hence pg dim w n characteristic polynomial g l w w decomposes linear factor w w induction hypothesis exists basis bw upper triangular thus basis b v w g w bw bw matrix f upper triangular matrix version theorem read follows characteristic polynomial pa k n n decomposes linear factor k triangulated exists matrix g l n k r upper triangular matrix r k n n corollary let v finite dimensional euclidian unitary vector space f l v v p f decomposes r euclidian case case c unitary case linear factor exists orthonormal basis b v f b b upper triangular proof p f decomposes linear factor theorem exists basis v f upper triangular applying gram-schmidt method basis obtain orthonormal basis v idv upper triangular cp theorem f idv f idv idv f idv invertible upper triangular matrix form group respect matrix multiplication cp theorem thus matrix product right hand side upper triangular hence f upper triangular example consider euclidian vector space r scalar product p p q dt endomorphism triangulation schur theorem f r r f f polynomial eigenvectors f corresponding distinct eigenvalue thus b basis diagonal matrix note b orthonormal basis r f b b since particular since p f decomposes linear factor corollary guarantee existence orthonormal basis b f b b upper triangular proof implication theorem one chooses eigenvector f proceeds inductively order obtain triangulation f example let u use first vector vector eigenvector f norm corresponding eigenvalue r vector norm b orthonormal basis f b b upper triangular matrix construct vector orthogonalizing using gram-schmidt method lead triangulation f b b could also choose eigenvector f norm corresponding eigenvalue orthogonalizing vector lead second basis vector corresponding basis obtain triangulation f example show triangulation f element diagonal different different orthonormal base diagonal element except order uniquely determined since eigenvalue f detailed statement uniqueness given lemma next chapter prove fundamental theorem algebra state every non-constant polynomial c decomposes linear factor result following corollary known schur issai schur eigenvalue endomorphisms corollary v finite dimensional unitary vector space every endomorphism v unitarily triangulated f l v v exists orthonormal basis b v f b b upper triangular matrix f b b called schur form f v unitary vector space standard scalar product obtain following matrix version corollary corollary cn n exists unitary matrix q cn n q r q h upper triangular matrix r cn n matrix r called schur form following result show schur form matrix cn n n pairwise distinct eigenvalue almost unique lemma let cn n n pairwise distinct eigenvalue let cn n two schur form diagonal equal u u h unitary diagonal matrix u proof exercise survey result unitary similarity matrix found article matlab-minute consider n matrix n n n cn n n n compute schur form using command u r schur n eigenvalue formulate conjecture rank general prove conjecture exercise following exercise k arbitrary field let v vector space let f l v v eigenvalue λ show im λidv f f subspace let v finite dimensional vector space let f l v v bijective show f f invariant subspace triangulation schur theorem let v n-dimensional k space let f l v v let u m-dimensional f subspace show basis b v exists f b b matrix k k k let k r c f k k v fv f compute p f determine k r k c eigenvalue f algebraic geometric multiplicity well associated eigenspaces consider vector space r standard basis n endomorphism n f r r n αi αi dt compute p f eigenvalue f algebraic geometric multiplicity examine whether f diagonalizable change one considers map kth derivative k n examine whether following matrix b q c diagonalizable set diagonalizable invertible matrix subgroup g l n k let n consider r-vector space r map f r r p p p show f linear n f diagonalizable let v r-vector space basis vn examine whether following endomorphisms diagonalizable f v j v j v j n f vn vn eigenvalue endomorphisms b f v j jv j v j n f vn nvn let v finite dimensional euclidian vector space let f l v v f f ad l v v show f f diagonalizable let v c-vector space let f l v v f determine possible eigenvalue f let v finite dimensional vector space f l v v show p f f l v v let v finite dimensional k space let f l v v p μm k show p f bijective μm eigenvalue determine condition entry matrix αβ γ δ diagonalizable triangulated determine endomorphism r diagonalizable triangulated let v vector space dim v show f l v v triangulated exist subspace vn v v j v j n b dim v j j j n c v j f j prove lemma  \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              chapter polynomial fundamental theorem algebra chapter discus polynomial detail consider division polynomial derive classical result polynomial algebra including factorization irreducible factor also prove fundamental theorem algebra state every non-constant polynomial complex number least one complex root implies every complex matrix every endomorphism finite dimensional complex vector space least one eigenvalue polynomial let u recall important term context polynomial k field p αn n n αn k polynomial k variable set k polynomial form commutative ring unit cp example αn deg p n called degree αn p called monic p deg p deg p p called constant lemma two polynomial p q k following assertion hold deg p q max deg p deg q deg p q deg p deg q proof exercise introduce concept associated division polynomial springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi polynomial fundamental theorem algebra definition let k field two polynomial p k exists polynomial q k p q called divisor p write p read divide p two polynomial p k called coprime p q k always imply q constant non-constant polynomial p k called irreducible k p q two polynomial q k implies q constant exist two non-constant polynomial q k p q p called reducible k note property irreducibility defined polynomial degree least polynomial degree always irreducible whether polynomial degree least irreducible may depend underlying field example polynomial q irreducible factorization show r reducible polynomial r irreducible using imaginary unit c reducible next result concern division remainder polynomial theorem p k k exist uniquely defined polynomial q r k p q r deg r deg proof show first existence polynomial q r k hold deg k follows q p r deg r deg assume deg deg p deg set q r p q r deg r deg let n deg p deg prove induction n hence p therefore p q r q r deg r deg polynomial suppose assertion hold n let two polynomial p n deg p deg given let sm highest coefficient p h p k deg h deg p n induction hypothesis exist polynomial q r k h q r deg r deg follows p q r q q deg r deg remains show uniqueness suppose hold exist polynomial q r k p q r deg r deg r r q q r r q q thus deg r r deg q q deg deg q q deg hand also deg r r max deg r deg r deg contradiction show indeed r r q q theorem important consequence root polynomial first known theorem corollary λ k root p k p λ exists uniquely determined polynomial q k p λ q proof apply theorem polynomial p λ get uniquely determined polynomial q r deg r deg p λ q polynomial r constant evaluating λ give p λ λ λ q λ r λ r λ paolo ruffini polynomial fundamental theorem algebra yield r p λ q polynomial p k least degree root λ k linear factor λ divisor p particular p reducible converse statement hold instance polynomial q reducible root corollary motivates following definition definition λ k root p k multiplicity uniquely determined nonnegative integer p λ q polynomial q k q λ recursive application corollary given polynomial p k lead following result corollary λk k pairwise distinct root p k corresponding multiplicity k exists unique polynomial q k p λk k q q λ j j particular sum multiplicity pairwise distinct root p deg p next result known lemma lemma p k coprime exist polynomial k p proof may assume without loss generality deg p deg proceed induction deg deg k thus p suppose assertion hold polynomial p k deg n n let p k deg p deg n given theorem exist polynomial q r p q r deg r deg r since assumption p coprime suppose exists non-constant polynomial h k divide r h also divide p contradiction assumption p coprime thus polynomial r coprime since deg r deg étienne bézout polynomial apply induction hypothesis polynomial r k hence k exist polynomial r r p q get p q p q completes proof using lemma bézout easily prove following result lemma p k irreducible divisor product h two polynomial h k p divide least one factor proof every polynomial divisor zero polynomial p divisor p coprime since p irreducible lemma exist polynomial k p hence h h h p h polynomial p divide term right hand side thus also recursive application lemma obtain euclidean theorem describes prime factor decomposition ring polynomial theorem every polynomial p αn n k unique ordering factor decomposition p μ pk μ k monic irreducible polynomial pk k proof deg p thus p assertion hold k μ let deg p p irreducible assertion hold p μ αn p reducible p two non-constant polynomial either irreducible decompose every multiplicative decomposition p obtained way deg p n non-constant factor suppose p μ pk β q q k k n μ β k well monic irreducible polynomial pk k p hence j j since polynomial q j irreducible must q j  \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                polynomial fundamental theorem algebra may assume without loss generality j cancel polynomial identity give μ pk β q q proceeding analogously polynomial pk finally obtain k μ β p j q j j fundamental theorem algebra seen existence root polynomial depends field considered field c special sense since fundamental theorem guarantee every non-constant polynomial root order use theorem context first present equivalent formulation language linear algebra theorem following statement equivalent every non-constant polynomial p c root v finite dimensional c-vector space every endomorphism f l v v eigenvector proof v f l v v characteristic polynomial p f c non-constant since deg p f dim v thus p f root c eigenvalue f f indeed eigenvector let p αn n c non-constant polynomial αn root p equal root monic polynomial p pa p cp p let cn n companion matrix lemma v n-dimensional c-vector space b arbitrary basis v exists uniquely determined f l v v f b b cp theorem assumption f eigenvector hence also eigenvalue p pa root fundamental theorem algebra proven without tool analysis particular one need polynomial continuous use following standard result based continuity polynomial lemma every polynomial p r odd degree real root numerous proof important result exist carl friedrich gauß alone gave four different proof starting one dissertation contained however gap history result described detail book fundamental theorem algebra proof let highest coefficient p positive lim p lim p since real function p continuous intermediate value theorem analysis implies existence root argument case negative leading coefficient analogous proof fundamental theorem algebra follows presentation article proof induction dimension however use usual consecutive order dim v order based set j j odd n j instance odd lemma v r-vector space dim v odd dim v every f l v v eigenvector let k field j every k space v dim v j every f l v v eigenvector two commuting f f l v v common eigenvector f f f f exists vector v v two scalar k f v v f v v r-vector space dim v odd two commuting f f l v v common eigenvector proof every f l v v degree p f r odd hence lemma implies p f root therefore f eigenvector proceed induction dim v dim v run element j increasing order set j proper subset n consisting natural number divisible j particular smallest element j dim v j assumption two arbitrary f f l v v eigenvector f f since dim v α k thus f f α f polynomial fundamental theorem algebra common eigenvector f f let dim v j let assertion proven k space whose dimension element j smaller dim v let f f l v v f f f f assumption f eigenvector corresponding eigenvalue let u im idv f w v ker idv f subspace u w v f f u u f w space w shown lemma space u easily shown well cp exercise subspace u w also f u u u idv f v v since f f commute f u f idv f v idv f f v idv f f v u w w idv f f w idv f f w f idv f w f idv f w f hence f w dim v dim u dim w since dim v divisible j either dim u dim w divisible j hence either dim u j dim w j corresponding subspace proper subspace v dimension element j smaller dim v induction hypothesis f f common eigenvector subspace thus f f common eigenvector corresponding subspace equal v must subspace w since dim w v w every vector v eigenvector f assumption also f eigenvector exists least one common eigenvector f f follows assumption hold k r j mean hold well prove fundamental theorem algebra formulation theorem theorem v finite dimensional c-vector space every f l v v eigenvector fundamental theorem algebra proof prove assertion induction j dim v j start j thus showing assertion c-vector space odd dimension let v arbitrary c-vector space n dim v let f l v v consider arbitrary scalar product v scalar product always exists cp exercise well set self-adjoint map respect scalar product h g l v v g g ad lemma set h form r-vector space dimension n define h h l h h h g f g g f ad h g f g g f ad g h h h h h cp exercise since n odd also n odd lemma h h common eigenvector hence exists g h g g h g g h h ih g f g g h therefore particular g f g h ih since g exists v v g v g v f g v show g v v eigenvector f proof j complete assume j every c-vector space v dim v j every f l v v eigenvector lemma implies every two commuting f f l v v common eigenvector show every c-vector space v dim v every f l v v eigenvector since j j q q odd prove c-vector space v n dim v j q odd q let v vector space let f l v v given choose arbitrary basis v denote matrix representation f respect basis cn n let polynomial fundamental theorem algebra b cn n b b set complex symmetric n n matrix define h h l h b ab b h b ab b h h h h cp exercise lemma set form c-vector space dimension n n n j q odd natural number q thus n n j q j q q j q j induction hypothesis commuting endomorphisms h h common eigenvector hence exists b b b b h b particular b b multiplying equation left yield b b b b b b b b n factorize α β used every complex number square root αin β b bv β bv bv since b exists v bv eigenvector corresponding eigenvalue β β bv eigenvector corresponding eigenvalue α since β eigenvector also f eigenvector fundamental theorem algebra matlab-minute compute eigenvalue matrix using command eig definition real matrix real eigenvalue reason occurrence complex eigenvalue matlab interprets every matrix complex matrix mean within matlab every matrix unitarily triangulated since every complex polynomial degree least decomposes linear factor direct corollary fundamental theorem algebra lemma following result corollary v finite dimensional c-vector space two commuting f f l v v common eigenvector example two complex matrix b commute eigenvalue b hence b common eigenvalue common eigenvectors b using corollary schur theorem corollary generalized follows theorem v finite dimensional unitary vector space f f l v v commute f f simultaneously unitarily triangulated exists orthonormal basis b v f b b f b b upper triangular proof exercise polynomial fundamental theorem algebra exercise following exercise k arbitrary field prove lemma show following assertion k b c imply imply exists c k examine whether following polynomial irreducible q r c q r c determine decomposition irreducible factor decompose polynomial irreducible factor field k q k r k show following assertion p k deg p p irreducible b deg p p root p irreducible c deg p p irreducible p root let g l n c n let adj cn n adjunct show exist n matrix j cn n det j det j n aj adj hint use pa construct polynomial p c adj p express p product linear factor show two polynomial p q c common root exist polynomial c deg deg p deg deg q p q let v finite dimensional unitary vector space f l v v h g l v v g g ad let h h l v v g f g g f ad fundamental theorem algebra h h l v v g f g g f ad show h h l h h h h h h let cn n b cn n b b let h cn n b ab b h cn n b ab show h h l h h h h let v c-vector space f l v v let u finite dimensional f subspace show u contains least one eigenvector f let v k space let f l v v show following statement k c exists f subspace u v dim u b k r exists f subspace u v dim u prove theorem construct example showing condition f g g f theorem sufficient necessary simultaneous unitary triangulation f let k n n diagonal matrix pairwise distinct diagonal entry b k n n ab b show case b diagonal matrix say b diagonal entry pairwise distinct show matrix commute determine unitary matrix q q h aq q h b q upper triangular show following statement p k k n n g l n k p sp b b c k n n ab c ap b p c c k c cn n exists unitary matrix q q h aq q h p q upper triangular let v finite dimensional unitary vector space let f l v v normal f satisfies f f ad f ad f polynomial fundamental theorem algebra show λ c eigenvalue f v f λ f subspace b show using f diagonalizable hint show induction dim v v direct sum eigenspaces f c show using b f even unitarily diagonalizable exists orthonormal basis b v f b b diagonal matrix let g l v v unitarily diagonalizable show g normal show endomorphism finite dimensional unitary vector space normal unitarily diagonalizable give different proof result theorem let v finite dimensional k space f l v v v f subspace let furthermore f j f j l u j u j j every v v exist unique u u v u show also f v f u f u f u f u write f f f call f direct sum f f respect decomposition v b show rank f rank f rank f p f p p c show λ f λ f λ f λ k set λ h λ eigenvalue h l v v show g λ f g λ f g λ f λ k set g λ h dim ker λidv even λ eigenvalue h l v v e show p f p f p f p k  \n",
       "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              chapter cyclic subspace duality jordan canonical form chapter use duality theory analyze property endomorphism f finite dimensional vector space v detail particularly interested algebraic geometric multiplicity eigenvalue f characterization corresponding eigenspaces strategy analysis decompose vector space v direct sum f subspace appropriately chosen base essential property f obvious matrix representation matrix representation derive called jordan canonical form f great importance many different derivation form using different mathematical tool approach using duality theory based article vlastimil pták cyclic f subspace duality let v finite dimensional k space f l v v v exists uniquely defined smallest number n vector f f linearly independent vector f f f linearly dependent obviously dim v since dim v vector v linearly independent number called grade respect f denote grade f vector linearly dependent thus grade respect f springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi cyclic subspace duality jordan canonical form f vector f linearly dependent hold eigenvector f eigenvector f f every j n define subspace k j f span f f space k j f called jth krylov f lemma v finite dimensional k space f l v v v following assertion hold f km f f subspace v span f f km f j f j f u v f subspace contains vector km f u thus among f subspace v contain vector krylov subspace km f one smallest dimension f f n dim k j f j j proof exercise assertion trivial thus let f let u v f subspace contains u also contains vector f f km f u particular dim u dim km f let k f apply f side f thus since f apply inductively f k obtain thus vector f linearly independent implies dim k j f j j vector f f form construction basis krylov subspace km f application f vector f k basis yield next basis vector f k application f last vector f yield linear combination basis vector since f km f due special structure subspace km f called cyclic f subspace aleksey nikolaevich krylov cyclic f subspace duality definition let v k space endomorphism f l v v called nilpotent f hold time f f called nilpotent index zero map f nilpotent endomorphism index v zero map endomorphism map nilpotent index case omit requirement f f f nilpotent index v vector f v f f v f v f v hence f v eigenvector f corresponding eigenvalue construction sect show eigenvalue nilpotent endomorphism also cp exercise lemma v k space f l v v nilpotent index dim v proof f nilpotent index exists v f f lemma vector f linearly independent implies dim v example vector space k endomorphism f k k nilpotent index since f f f u f subspace v f l u u f u u u f u restriction f subspace u cp definition theorem let v finite dimensional k space f l v v exist f subspace v v v f l bijective f l nilpotent proof v ker f f v f f v f thus v ker f therefore ker f ker f proceeding inductively see ker f ker f ker f since v finite dimensional exists smallest number ker f ker f j j number let im f ker f cyclic subspace duality jordan canonical form f bijective v show space satisfy assertion first observe f v v f w w v therefore f v f f w f f w v f f v f f v f therefore f v application dimension formula linear map cp theorem f give dim v dim dim v v f w w v since v hence f v f f w f w first equation hold since v definition ker f ker f implies f w therefore v f w obtain v let v ker f given since v exists vector w v v f w implies f v f f w f w definition ker f ker f thus w ker f therefore v f w implies ker f f injective thus also bijective cp corollary hand v definition f v f v thus f zero map l f nilpotent development recall term result chap let v finite dimensional k space let v dual space u v w v two subspace bilinear form β u w k v h h v non-degenerate u w called dual pair respect β requires dim u dim w f l u u dual map f l u u defined f u u h h v u h u f h v h f v furthermore f k f k k set u h v h u u u called annihilator u set subspace v cp exercise analogously set w v v h v h w called annihilator set subspace v cyclic f subspace duality lemma let v finite dimensional k space f l v v v dual space v f l v v dual map f let u v w v two subspace following assertion hold dim v dim w dim w dim u dim u f nilpotent index f nilpotent index w v f subspace w v f subspace u w dual pair respect bilinear form defined v u w proof exercise v v f v hence h f v f h v f h v every h v v v f nilpotent index f f h h v therefore f h v h f v v implies f contradiction assumption f nilpotent index thus f nilpotent index let w w every h w f h w thus f h w h f w hence f w w u u w h u h w since u w since u w dual pair respect bilinear form defined u moreover dim u dim w using obtain dim v dim w dim w dim u dim w u w obtain v u w example consider vector space v canonical basis b subspace v u span w h v h b α α α r v u h v h b α α r v w span cyclic subspace duality jordan canonical form example easily see dim v dim w dim w dim u dim u u w form dual pair respect bilinear form defined k moreover v u w following theorem present given nilpotent f decomposition v f subspace idea decomposition construct dual pair subspace u v w v u f w f lemma w f lemma follows v u w theorem let v finite dimensional k space let f l v v nilpotent index let v satisfy f let h v satisfy h f f f h f f subspace km f v km f h v respectively dual pair respect bilinear form defined furthermore v km f km f h km f h f subspace proof let v vector f since f space km f m-dimensional f subspace v cp lemma let h v vector h f f h particular f h l v v since f nilpotent index also f nilpotent index cp lemma f h l v v therefore km f h m-dimensional f subspace v cp lemma remains show km f km f h dual pair let γ j f j km f vector h β h h km f h show inductively thus  \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            cyclic f subspace duality using f h km f h assumption vector yield f h h f γ j h f j h f last equation hold since f j j f h f obtain suppose k k using f h km f h assumption vector yield f h h f γ j h f γk h f last equation hold since γ j j k f j k asserted therefore bilinear form defined space km f km f h non-degenerate first variable analogously bilinear form non-degenerate second variable hence km f km f h dual pair using lemma v km f km f h space km f h lemma f subspace jordan canonical form let v finite dimensional k space f l v v exists basis b v consisting eigenvectors f f b b diagonal matrix f diagonalizable necessary sufficient condition characteristic polynomial p f decomposes linear factor k addition g f λ j f λ j every eigenvalue λ j cp theorem p f decomposes linear factor g f λ j f λ j hold least one eigenvalue λ j f diagonalizable still triangulated exists basis b v f b b upper triangular matrix cp theorem triangular matrix read algebraic usually geometric multiplicity eigenvalue goal following construction determine basis b v f b b upper triangular addition algebraic also reveals geometric multiplicity eigenvalue assumption p f decomposes linear factor k construct basis b v f b b block diagonal matrix form cyclic subspace duality jordan canonical form f b b jdm λm diagonal block form λj jd j λ j k j j λj λ j k j n j matrix form called jordan block size j corresponding eigenvalue λ j following construction first assume p f decomposes linear factor assume existence single eigenvalue k f using eigenvalue define endomorphism g f idv l v v theorem exist g-invariant subspace u v w v v u w nilpotent bijective u since otherwise w v g would bijective contradicts assumption eigenvalue f let nilpotent index construction dim u let u vector since vector eigenvector corresponding eigenvalue lemma vector linearly independent subspace u consider basis matrix representation respect basis given k jordan canonical form show particular characteristic polynomial given monomial hence eigenvalue moreover construction dim u construction complete moment hand dim u applying theorem l u u show u u consider exists subspace u map nilpotent index carry construction g g determine vector u eigenvector subspace u basis u k construction k dim u step procedure terminates found decomposition u form u kdk gk wk g kdk g wk second equation used kd j g j w j kd j g wk j combine constructed base bk basis b u b b bk bk jdk thus nilpotent endomorphism characteristic polynomial eigenvalue transfer result f g idv every g-invariant subspace f one observes easily kd j f w j kd j g w j j k cyclic subspace duality jordan canonical form cp exercise hence follows u f kdk f wk every j k j f g w j g g w j g w j g w j g w j g j w j matrix representation f respect basis b u therefore given f b b f f bk bk jdk map f idw bijective construction eigenvalue f therefore f dim u dk order determine g f let v u arbitrary vector exist scalar α j k j k α j g w j using obtain k j f v k j k j α j f g w j α j g w j α j g w j k j v α j g w j vector last sum linearly independent hence f v v α j j k j show every eigenvector f corresponding eigenvalue form k α j g j w j least one α j nonzero v f span g g dk wk jordan canonical form since g g dk wk linearly independent follows g f geometric multiplicity eigenvalue therefore equal number jordan block corresponding eigenvalue matrix representation furthermore observe every subspace kd j f w j endomorphism f exactly one linear independent eigenvector corresponding eigenvalue summarize result following theorem theorem let v finite dimensional k space let f l v v k eigenvalue f following assertion hold exist f subspace u v w v v u map f idu nilpotent map f idw bijective particular eigenvalue f subspace u written u f kdk f wk vector wk u kd j f w j j f invariant subspace v j called cyclic decomposition u exists basis b u f b b jdk f dk g f f eigenvalue eigenvalue restriction f l w w apply theorem f vector space w direct sum form w x f idx nilpotent f idy bijective space x cyclic decomposition analogous theorem exists matrix representation f analogous construction carried eigenvalue f characteristic polynomial p f decomposes linear factor k finally obtain cyclic decomposition entire space v give following theorem theorem let v finite dimensional k space let f l v v characteristic polynomial p f decomposes linear factor k exists basis b v f b b jdm λm λm k necessarily pairwise distinct eigenvalue f every eigenvalue λ j f f λ j equal sum size cyclic subspace duality jordan canonical form jordan block corresponding λ j g f λ j equal number jordan block corresponding λ j matrix representation form called jordan canonical f theorem know f l v v diagonalizable p f decomposes linear factor k g f λ j f λ j hold every eigenvalue λ j f p f decomposes linear factor jordan canonical form show g f λ j f λ j every jordan block corresponding λ j size fundamental theorem algebra yield following corollary theorem corollary v finite dimensional c-vector space every f l v v jordan canonical form following uniqueness result justifies name canonical form theorem let v finite dimensional k space f l v v jordan canonical form unique order jordan block diagonal proof let dim v n let two base v f well f k n n jdm λm k n n jck μk given eigenvalue λ j j define r λ j rank λ j λ j r λ j d λ j equal number jordan block λ j k diagonal number jordan block corresponding eigenvalue λ j exact size therefore given λ j λ j λ j λ j d λ j marie ennemond camille jordan derived form two year earlier karl weierstraß proved result implies jordan canonical form jordan canonical form cp example matrix similar therefore eigenvalue λm μk furthermore rank αin rank αin α k particular every λ j exists μi μk μi λ j μi matrix get r μi rank μi r λ j show matrix reordering jordan block diagonal matrix example example illustrates construction proof theorem get r d d cyclic subspace duality jordan canonical form consider power jordan block jd λ k since id jd commute k j jd j j k jd λ k λid jd k k p j λ jd j j every k p j jth derivative polynomial p k respect p k k p j k j k k k j j j easily show following result lemma p k polynomial degree k k p jd λ p j λ jd j j proof exercise considered linear map k k matrix jd represents upshift since jd αd clearly k αd jd jd hence linear map jd nilpotent index sum right hand side therefore term even deg p moreover right hand side show p jd λ upper triangular matrix constant entry diagonal matrix constant diagonal called toeplitz particular main diagonal entry p λ see p jd λ hold p λ p λ p λ thus shown following result otto toeplitz jordan canonical form lemma let p k polynomial jd λ k jordan block matrix p jd λ invertible λ root p jd λ k λ d-fold root p linear factor λ divisor let v finite dimensional k space let f l v v assume p f decomposes linear factor cayley-hamilton theorem theorem know p f f l v v exists monic polynomial degree dim v annihilates endomorphism f let k two monic polynomial smallest possible degree f f f since monic k polynomial deg deg deg minimality assumption deg deg implies thus every f l v v exists uniquely determined monic polynomial minimal degree annihilates f justifies following definition definition v finite dimensional k space f l v v uniquely determined monic polynomial minimal degree annihilates f called minimal polynomial f denote polynomial f construction always deg f deg p f dim v lemma v finite dimensional k space f l v v minimal polynomial f divide every polynomial annihilates f particular divisor characteristic polynomial p f proof p p f f divide p k polynomial p f deg f deg p using division remainder cp theorem exist uniquely determined polynomial q r k p q f r deg r deg f thus p f q f f f r f r f minimality deg f implies r hence f divide p f decomposes linear factor explicitly construct f using jordan canonical form f lemma let v finite dimensional k space f l v v jordan canonical form pairwise distinct eigenvalue λk respective maximal size corresponding jordan block mf k λ j j cyclic subspace duality jordan canonical form proof know lemma f divisor p f therefore mf k λ j j exponent jdm λm jordan canonical form f f f l v v equivalent f k n n n dim v f f jd j λ j j necessary sufficient f λ j j lemma hold every linear factor λ j j j k divisor f therefore f desired form example f endomorphism jordan canonical form p f f f show f f f l v v jordan canonical form great importance theoretical linear algebra practical application however usually matrix k r k c considered relevant since numerically stable method computing jordan canonical form general matrix finite precision arithmetic reason lack method entry jordan canonical form depend continuously entry given matrix example consider matrix ε ε  \n",
       "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           jordan canonical form every given ε matrix ε two distinct eigenvalue ε hence diagonal matrix j ε jordan canonical form ε however ε obtain ε j ε thus j ε converge jordan canonical form ε similar example given matrix exercise jordan block size n corresponding eigenvalue every ε obtain diagonalizable matrix ε cn n n pairwise distinct eigenvalue matlab-minute let random matrix constructed command rand construct several matrix always compute eigenvalue using command eig display eigenvalue format long one observes two eigenvalue real complex conjugate always error starting digit decimal point error order happen chance due behavior eigenvalue perturbation arise rounding error computer computation jordan canonical form derive method computation jordan canonical form endomorphism f finite dimensional k space assume p f decomposes linear factor k root p f eigenvalue f known construction follows important step existence proof jordan canonical form sect suppose λ eigenvalue f f corresponding jordan block size exist linearly independent vector t f b b j λ cyclic subspace duality jordan canonical form b t writing id instead idv simplicity notation f λid f λid f λid t hence j f λid j t j vector t form sequence one constructed context krylov subspace span t k f λid t reverse sequence called jordan chain f corresponding eigenvalue λ vector eigenvector f corresponding λ vector f f λid f λid hence ker f λid ker f λid general j ker f λid j ker f λid j motivates following definition definition let v finite dimensional k space let f l v v eigenvalue λ k let k vector v v v ker f λid k ker f λid called principal vector level k f corresponding eigenvalue λ principal vector level one eigenvectors principal vector higher level considered generalization eigenvectors therefore sometimes called generalized eigenvectors computation jordan canonical form f thus need know number length jordan chain corresponding different eigenvalue f correspond number size jordan block f f matrix representation f respect arbitrary basis cp proof theorem computation jordan canonical form d λ rank f λi rank f λi dim im f λid dim im f λid dim v dim ker f λid dim v dim ker f λid dim ker f λid dim ker f λid number jordan block corresponding λ size least implies particular d λ λ d λ λ number jordan block exact size corresponding λ exists smallest number n ker f λid ker f λid ker f λid ker f λid hence d λ jordan block corresponding λ size larger order compute jordan canonical form therefore proceed follows determine eigenvalue f every eigenvalue λ f carry following step determine smallest number n ker f ker f ker f ker f dim ker f λid λ f b determine d λ dim ker f λid dim ker f λid d λ λ dim ker f λid g λ f number jordan block corresponding λ c simplify notation write d d λ determine jordan chain follows since dm dm exist dm jordan block size block determine jordan chain dm principal vector level vector tdm ker f λid ker f λid cyclic subspace duality jordan canonical form following property dm αdm k αi ti ker f λid αdm first index ti j indicates number chain second indicates level principal vector ker f λid j ker f λid ii j proceed follows determined j principal vector level j say j j td j j apply f λid vector hence ti f λid ti j j order determine principal vector level j dj αi ti ker f λid αd j k dj f λid dj dj αi ti f λid thus αi ti j αi ti j ker f λid giving αd j j exist j jordan block size j need jordan chain length j thus extend already computed td j ker f λid ker f λid principal vector level j j via td ker f λid ker f λid following must hold αd k αi ti ker f λid αd completing step j obtained linearly independent vector ker f λid since dim ker f λid found basis ker f λid way determined different jordan chain combine follows tλ computation jordan canonical form chain begin eigenvector followed principal vector increasing level use convention chain ordered decreasingly according length jordan chain linearly independent first vector eigenvectors linearly independent show exercise thus pairwise distinct eigenvalue f basis f jordan canonical form example interpret matrix f endomorphism eigenvalue f root pf particular pf decomposes linear factor f jordan canonical form consider different eigenvalue f eigenvalue obtain ker f ker span dim ker f f eigenvalue obtain ker f ker span cyclic subspace duality jordan canonical form ker f ker span dim ker f f b dim ker f dim ker f dim ker f dim ker f c computation jordan chain principal vector level one choose form basis ker f r finished choose principal vector level two say vector r span compute f since add another principal vector level one choose since vector linearly independent ker f implies way get coordinate transformation matrix jordan canonical form f f computation jordan canonical form exercise following exercise k arbitrary field prove lemma prove lemma let v k space f l v v λ k prove disprove subspace u v f f λidv let v finite dimensional k space f l v v v v λ k show k j f v k j f λidv v j conclude grade v respect f equal grade v respect f λidv prove lemma let v finite dimensional euclidean unitary vector space let f l v v selfadjoint nilpotent show f let v finite dimensional k space let f l v v nilpotent index suppose p f decomposes linear factor show following assertion p f n n dim v b f c exists vector v v grade respect f every λ k f λ let v finite dimensional k space f l v v show following assertion ker f j ker f j exists ker f ker f ker f ker f j j b im f j im f j exists im f im f im f im f j j c minimal ker f ker f im f im f theorem implies v ker f im f decomposition v f subspace let v finite dimensional k space let f l v v projection cp exercise show following assertion v im f implies f v b v im f ker f c exists basis b v f b b ik k dim im f n dim v particular p f k λ every eigenvalue λ f cyclic subspace duality jordan canonical form map g idv f projection ker g im f im g ker f let v finite dimensional k space let u w v two subspace v u show exists uniquely determined projection f l v v im f u ker f determine jordan canonical form matrix r using method presented sect determine also minimal polynomial determine jordan canonical form minimal polynomial linear map f determine order block matrix j jordan canonical form pj j let v finite dimensional k space f l v v suppose p f decomposes linear factor show following assertion p f f hold g λ f eigenvalue λ f b f diagonalizable f simple root root multiplicity one c root λ k f simple ker f λidv ker f λidv let v k space dimension let f l v v p f decomposing linear factor show jordan canonical form f uniquely determined p f f hold longer dim v let k n n matrix characteristic polynomial decomposes linear factor show exists diagonalizable matrix nilpotent matrix n n n n let k n n matrix jordan canonical form define λ inr δi j jnr λ k n n λ computation jordan canonical form show following assertion b c inr jn λ inr jn λ similar jn λ inr jnr λ written product two symmetric matrix determine matrix two symmetric matrix  \n",
       "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      chapter matrix function system differential equation chapter give introduction area matrix function first define general matrix function derive important property using example network analysis chemical reaction illustrate matrix function arise naturally application network analysis example involves exponential function matrix study property important function detail analysis chemical reaction kinetics lead system ordinary differential equation whose solution based matrix exponential function matrix function matrix exponential function following study function yield given n n matrix n n matrix possible definition function given entrywise application one could define scalar function matrix instance ai j cn n function sin sin sin ai j however definition compatible matrix multiplication since general already following definition primary matrix function definition turn consistent matrix multiplication since definition based jordan canonical form assume simplicity cn n consideration also apply square matrix r long jordan canonical form definition let cn n jordan canonical form j diag jdm λm springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi matrix function system differential equation let c λm function f c said defined spectrum value f j λi j di exist f j λi j di jth derivative function f λ respect λ evaluated λi λi r real derivative λi c r complex derivative moreover assume equal eigenvalue occur different jordan block mapped value f defined spectrum primary matrix function f defined f f j f j diag f f jdm λm f jdi λi di λi f di f λi f λi f λi f λi f λi f λi f λi f λi note definition f existence value required example let let f z z square root function set f f definition choose branch square root function f f matrix primary square root taking different branch function different jordan block corresponding eigenvalue incompatible definition instance matrix x incompatible definition despite fact x x solution x cn n matrix equation x called square root matrix cn n example show may primary square root according definition following f always mean primary matrix function according definition usually omit term primary shown polynomial p c degree k matrix function matrix exponential function p jdi λi k p j λi jdi j j simple comparison show formula agrees f mean computation p jdi λi lead result definition p jdi λi generally following result hold lemma let cn n p αk k c f p yield matrix function f satisfies f αk ak proof exercise consider particular polynomial f resulting f equal product show definition primary matrix function f consistent matrix multiplication following theorem great practical theoretical importance show matrix f always written polynomial theorem let cn n minimal polynomial let f definition exists uniquely determined polynomial p c degree deg f p particular f f f f well f v av v f v v g l n c proof present proof since requires advanced result interpolation theory detail found chap using theorem show primary matrix function f definition independent choice jordan canonical form already know theorem jordan canonical form unique order jordan block j diag jdm λm λm diag two jordan canonical form p j p permutation matrix p rn n matrix j order diagonal block hence f j diag f f jdm λm p p diag f f jdm λm p p p diag f f λm p p f p matrix function system differential equation theorem applied matrix j yield existence polynomial p f j p j thus get f f j sp j p p p p j p p f j p f let u consider exponential function f z e z infinitely often complex differentiable throughout particular e z defined sense definition spectrum every given matrix sdiag jdm λm cn n c arbitrary fixed derivative function et z respect variable z given j tz e j et z dz j j use notation exp instead e exponential function matrix every jordan block jd λ f z e z exp jd λ etλ tλ jd k e k matrix exponential function exp given exp sdiag exp exp jdm λm parameter used next section context linear differential equation analysis shown every z c function e z represented absolutely convergent series ez zj j using series equation jd obtain matrix function matrix exponential function j tλ tλ exp jd λ e jd jd j j tλ jd j j tj j λi jd j tj λid jd j j jd λ j j derivation used absolute convergence exponential series finiteness series matrix jd allows application cauchy product absolutely convergent series also proven analysis lemma cn n c exp matrix exponential function j exp j proof shown already jordan block assertion follows j j j j j j representation matrix exponential function immediately see lemma matrix rn n every real matrix exponential function exp real matrix following result present important property matrix exponential function lemma two matrix b cn n commute exp b exp exp b every matrix cn n exp g l n c exp exp augustin louis cauchy matrix function system differential equation proof b commute cauchy product formula yield j j exp exp b b b j j j j b b j j j exp b used binomial formula commuting matrix cp exercise since commute exp exp exp exp j j hence exp g l n c exp exp non-commuting matrix statement lemma general hold cp exercise matlab-minute compute matrix exponential function exp matrix r using command look help expm also compute diagonalization using command form matrix exponential function exp compare matrix compute relative error norm look help norm example let ai j cn n symmetric matrix aii ai j j identify matrix graph g v e consisting set n vertex v n set edge e v v n row identified vertex e every entry ai j identified edge j e due symmetry ai j ji therefore consider following element matrix function matrix exponential function e unordered pair j j following example illustrates identification identified g v e e v graph g displayed follows path length vertex vertex ordered list vertex ki v closed path length example path given length respectively mathematical field graph theory one usually assumes vertex path pairwise distinct deviation convention motivated following interpretation matrix power entry ai j matrix mean exists path length vertex vertex j vertex j adjacent ai j path exists matrix therefore called adjacency matrix graph g square adjacency matrix entry j position given j n sum right hand side obtain given e j e sum right side therefore equal number vertex adjacent j hence j entry equal number pairwise distinct path j j pairwise distinct closed path length g generally one show following cp exercise let ai j cn n symmetric adjacency matrix aii ai j j n let g graph identified n j entry equal number pairwise distinct path j j pairwise distinct closed path length g matrix function system differential equation matrix obtain pairwise distinct closed path length pairwise distinct path length numerous real world application involve network modeled mathematically using graph example include social biological telecommunication airline network property network studied interdisciplinary area network science important task identify participant network central sense functionality significant impact entire network network modeled graph study centrality vertex example vertex considered central connected large part graph via many short closed path longer connection usually le important thus path scaled according length use scaling factor path length vertex graph g adjacency matrix obtain centrality measure form ii relative ordering vertex according formula changed add constant obtain centrality vertex exp ii ii another important quantity so-called communicability vertex j j given weighted sum pairwise distinct path j exp j ij matrix matlab function expm yield  \n",
       "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    matrix function matrix exponential function exp vertex largest centrality followed would define centrality vertex number adjacent vertex example could distinguish vertex largest communicability example exists vertex information concerning analysis network using adjacency matrix matrix function found article system linear ordinary differential equation differential equation describes relationship desired function derivative equation used area science engineering modeling physical phenomenon ordinary differential equation involve function one variable derivative partial differential equation involve function several variable partial derivative section focus ordinary differential equation first order function first derivative occur simple example modeling ordinary differential equation first order increase decrease biological population bacteria petri dish let size population time enough food external condition temperature pressure constant population grows real rate k proportional current number individual described equation dt clearly one also take k population shrink looking function r r satisfies general solution given exponential function cetk c r arbitrary constant unique solution need know size population given initial time way obtain initial value problem ky matrix function system differential equation show solved uniquely function e k example chemical reaction certain initial substance called educts reactant transformed substance called product reaction distinguished concerning order discus reaction first order reaction rate determined one educt reaction second higher order one typically obtains nonlinear differential equation beyond focus chapter example educt transformed product rate write reaction symbolically model mathematically ordinary differential equation value concentration substance time concentration product grows rate corresponding equation may happen reaction first order develops direction transforms rate transforms rate model reaction mathematically system linear ordinary differential equation combining function vector valued function write system ay system linear ordinary differential equation derivative function always considered entrywise reaction also several step example reaction form lead differential equation thus system ay sum entry column equal zero since every decrease substance certain rate substance increase rate summary chemical reaction first order lead system linear ordinary differential equation first order written ay real square matrix derive general theory system linear real complex ordinary differential equation first order form ay g k n n given matrix given positive real number g k given function k desired solution assume k r k g k system called homogeneous otherwise called non-homogeneous given system form system ay called associated homogeneous system matrix function system differential equation lemma solution homogeneous system form subspace infinite dimensional k space continuously differentiable function interval k proof show required property according lemma function w continuously differentiable solves homogeneous system thus solution set system empty k continuously differentiable solution k w continuously differentiable aw function w solution homogeneous system following characterization solution non-homogeneous system analogous characterization solution set non-homogeneous linear system equation lemma also cp lemma lemma k solution non-homogeneous system every solution written solution associated homogeneous system proof solution ay g g difference thus solution associated homogeneous system order describe solution system ordinary differential equation consider given matrix k n n matrix exponential function exp lemma consider real variable power series matrix exponential function lemma converges differentiated termwise respect variable derivative matrix respect variable considered entrywise yield exp dt dt exp result obtained entrywise differentiation matrix exp respect system linear ordinary differential equation obtain tλ exp jd λ e dt dt λetλ etλ λetλ etλ jd λid jd etλ jd λ exp jd λ also give dt exp exp theorem unique solution homogeneous differential equation system given initial condition k given function exp set solution homogeneous differential equation system form n-dimensional k space basis exp exp en proof exp exp exp exp dt dt exp ay exp hence solution satisfies initial condition w another solution u exp w exp w exp w exp dt exp aw k show function u constant entry particular u u w w exp used exp exp cp lemma matrix function system differential equation function exp e j exp en k j n solves homogeneous system ay since matrix exp k n n invertible every cp lemma function linearly independent arbitrary solution ay k unique solution initial value problem linear combination function exp consequence exp exp en describe solution non-homogeneous system need integral function form w k wn every fixed define w d d k wn d apply integral entrywise function definition dt w d w determine explicit solution formula system linear differential equation based so-called duhamel theorem unique solution non-homogeneous differential equation system initial condition k given exp exp exp g d proof derivative function defined exp exp g d exp g d exp exp g exp exp exp dt dt jean-marie constant duhamel system linear ordinary differential equation exp exp exp g d g ay furthermore exp exp exp g d also satisfies initial condition let another solution satisfies initial condition lemma w w solves homogeneous system therefore w exp c c k cp theorem obtain c c hence theorem shown explicit solution system linear ordinary differential equation first order compute matrix exponential function introduced function using jordan canonical form given matrix numerical computation based jordan canonical form advisable cp example significant practical relevance numerous different algorithm computing matrix exponential function proposed shown article existing algorithm completely satisfactory example example circuit simulation presented sect lead system ordinary differential equation r vc v dt l l l vc dt c using initial value vc obtain solution exp vc v exp d example let u also consider example mechanic weight mass attached spring spring constant μ let distance weight equilibrium position illustrated following figure matrix function system differential equation want determine position x weight time x extension spring described hooke corresponding ordinary differential equation second order μ x x dt initial condition x initial velocity weight write differential equation second order x system first order introducing velocity v new variable velocity given derivative position respect time v thus acceleration yield system ay mμ x v initial condition theorem unique solution homogeneous initial value problem given function exp consider element eigenvalue two complex non-real number iρ ρ mμ corresponding eigenvectors iρ thus exp sir itρ e iρ robert hooke system linear ordinary differential equation exercise construct matrix ai j determine solution x matrix equation x classify solution primary square root determine matrix x real entry x prove lemma prove following assertion cn n det exp exp trace b h exp unitary c exp e e let diag jdm λm cn n rank determine primary matrix function f f z z function also exist rank n let log z r eiϕ r ϕ π c r eiϕ ln r iϕ principle branch complex logarithm ln denotes real natural logarithm show function defined spectrum compute log well exp log compute exp exp sin π construct two matrix b exp b exp exp b prove assertion entry ad example let compute exp r solve homogeneous system differential equation ay initial condition compute matrix exp example explicitly thus show exp r despite fact eigenvalue eigenvectors real  \n",
       "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 chapter special class endomorphisms chapter discus class endomorphisms square matrix whose eigenvalue eigenvectors special property property exist assumption chapter assumption concern relationship given endomorphism adjoint endomorphism thus focus euclidean unitary vector space lead class normal orthogonal unitary selfadjoint endomorphisms class natural counterpart set square real complex matrix normal endomorphisms start definition endomorphism matrix definition let v finite dimensional euclidean unitary vector space endomorphism f l v v called normal f f ad f ad f matrix rn n cn n called normal h h respectively z c zz zz property normality therefore interpreted generalization property complex number first study property normal endomorphisms finite dimensional unitary vector space recall following result b orthonormal basis v f l v v f b b h f ad b b cp theorem every f l v v unitarily triangulated cp corollary schur theorem hold general euclidean case since every real polynomial decomposes linear factor term introduced otto toeplitz context bilinear form springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi special class endomorphisms using result obtain following characterization normal endomorphisms unitary vector space theorem v finite dimensional unitary vector space f l v v normal exists orthonormal basis b v f b b diagonal matrix f unitarily diagonalizable proof let f l v v normal let b orthonormal basis v r f b b upper triangular matrix r h f ad b b f f ad f ad f obtain r r h f f ad b b f ad f b b r h show induction n dim v r diagonal obvious n let assertion hold n let r upper triangular r r h r h write r r cn n upper triangular rrh rh r obtain hence induction hypothesis cn n diagonal therefore diagonal well conversely suppose exists orthonormal basis b v f b b diagonal f ad b b f b b h diagonal since diagonal matrix commute f f ad b b f b b f ad b b f ad b b f b b f ad f b b implies f f ad f ad f hence f normal application theorem unitary vector space v standard scalar product matrix cn n viewed element l v v yield following matrix version corollary matrix cn n normal exists orthonormal basis consisting eigenvectors unitarily diagonalizable normal endomorphisms following theorem present another characterization normal endomorphisms unitary vector space theorem v finite dimensional unitary vector space f l v v normal exists polynomial p c p f f ad proof p f f ad polynomial p c f f ad f p f p f f f ad f hence f normal conversely f normal exists orthonormal basis b v f b b diag λn furthermore f ad b b f b b h diag λn let p c polynomial p λ j λ j j polynomial explicitly constructed using lagrange basis c cp exercise f ad b b diag λn diag p p λn p diag λn p f b b p f b b hence also f ad p f several characterization normal endomorphisms finite dimensional unitary vector space normal matrix cn n found article see also exercise consider euclidean case focus real square matrix result formulated analogously normal endomorphisms finite dimensional euclidean vector space let rn n normal also satisfies h h considered element cn n unitarily diagonalizable d h hold unitary matrix cn n diagonal matrix cn n despite fact real entry neither real general since element rn n may diagonalizable instance normal matrix diagonalizable r considered element eigenvalue unitarily diagonalizable discus case real normal matrix detail first prove real version schur theorem special class endomorphisms theorem every matrix rn n exists orthogonal matrix u rn n n n u au r r rmm every j either r j j rjj j j j j j r second case r j j considered complex matrix pair complex conjugate eigenvalue form α j iβ j α j r β j r matrix r called real schur form proof proceed via induction n r u suppose assertion hold n let given consider element eigenvalue λ c α β r corresponding eigenvector v x iy x av λv dividing equation real imaginary part obtain two real equation ax αx β ay βx αy two case case β two equation ax αx ay αy thus least one real vector x eigenvector corresponding real eigenvalue α without loss generality assume vector x x extend x vector orthonormal basis respect standard scalar product matrix x orthogonal satisfies α matrix rn n induction hypothesis exists orthogonal matrix rn n desired form matrix u orthogonal satisfies normal endomorphisms u au α r r desired form case β first show x linearly independent x using β first equation implies also possible since eigenvector v x iy must nonzero thus x using β second equation implies also x linearly dependent exists μ r x μy two equation written ax α βμ x ax β αμ x μ implies β since μ r implies β contradicts assumption β consequently x linearly independent combine two equation system αβ x x α rank x applying gram-schmidt method respect standard scalar product matrix x yield x q q q g l r follows aq x x αβ αβ q α α real matrix αβ α considered element pair complex conjugate eigenvalue α iβ β particular nonzero since otherwise would two real eigenvalue extend vector orthonormal basis respect standard scalar product n list empty q orthogonal aq q  \n",
       "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    special class endomorphisms matrix analogously first case application induction hypothesis matrix yield desired matrix r u theorem implies following result real normal matrix corollary matrix rn n normal exists orthogonal matrix u rn n u au diag rm every j either r j rj αj βj j α j β j second case matrix r j considered complex matrix pair complex conjugate eigenvalue form α j iβ j proof exercise example matrix considered complex matrix eigenvalue therefore neither diagonalizable triangulated orthogonal matrix u transformed matrix u au real schur form orthogonal unitary endomorphisms section extend concept orthogonal unitary matrix endomorphisms orthogonal unitary endomorphisms definition let v finite dimensional euclidean unitary vector space endomorphism f l v v called orthogonal unitary respectively f ad f idv f ad f idv f ad f bijective hence f injective cp exercise corollary implies f bijective hence f ad unique inverse f also f f ad idv cp remark following definition note orthogonal unitary endomorphism f normal therefore result previous section also apply f lemma let v finite dimensional euclidean unitary vector space let f l v v orthogonal unitary respectively b orthonormal basis v f b b orthogonal unitary matrix respectively proof let dim v every orthonormal basis b v idv b b f ad f b b f ad b b f b b f b b h f b b thus f b b orthogonal unitary respectively euclidean case f b b h f b b following theorem show orthogonal unitary endomorphism characterized fact change scalar product arbitrary vector lemma let v finite dimensional euclidean unitary vector space scalar product f l v v orthogonal unitary respectively f v f w v w v w proof f orthogonal unitary v w v v w idv v w f ad f v w f v f w hand suppose v w f v f w v w v w f v f w v w v f ad f w v idv f ad f w since scalar product non-degenerate v chosen arbitrarily idv f ad f w w v hence idv f ad f following corollary cp lemma corollary v finite dimensional euclidean unitary vector space scalar product f l v v orthogonal unitary respectively norm induced scalar product f v v v v special class endomorphisms vector space v standard scalar product induced norm v v h v well unitary matrix cn n av v v thus av sup v cp example hold analogously orthogonal matrix rn n study eigenvalue eigenvectors orthogonal unitary endomorphisms lemma let v finite dimensional euclidean unitary vector space let f l v v orthogonal unitary respectively λ eigenvalue f proof let scalar product f v λv v v v idv v v f ad f v v f v f v λv λv v v v v implies statement lemma hold particular unitary orthogonal matrix however one keep mind orthogonal matrix orthogonal endomorphism may eigenvalue example orthogonal matrix characteristic polynomial pa real root considered element matrix eigenvalue theorem cn n unitary exists unitary matrix u cn n u h au diag λn j j rn n orthogonal exists orthogonal matrix u rn n u au diag rm every j either r j λ j λ j cj sj rj j c j j orthogonal unitary endomorphisms proof unitary matrix cn n normal hence unitarily diagonalizable cp corollary lemma eigenvalue absolute value orthogonal matrix normal hence corollary exists orthogonal matrix u rn n u au diag rm either r j αj βj rj j α j β j first case r j λ j j lemma since u orthogonal also u au orthogonal hence every diagonal block r j orthogonal well r tj r j obtain r j desired form study two important class orthogonal matrix example let j n n j n let α define ri j α j sin α co α sin α co α j matrix ri j α ri j rn n equal identity matrix except entry rii co α ri j sin α r ji sin α r j j co α n matrix α co α sin α sin α co α special class endomorphisms satisfies α α co α sin α sin α co α co α sin α sin α co α co α α α α α α one easily see matrix ri j α rn n orthogonal multiplication vector v matrix ri j α result counterclockwise rotation v angle α j plane numerical mathematics matrix ri j α called given illustrated figure vector v matrix represent rotation degree respectively example u define householder matrix h u ut u uu rn n u set h every u h u orthogonal matrix cp exercise multiplication vector v matrix h u describes reflection v hyperplane span u u hyperplane vector orthogonal u respect standard scalar product illustrated figure vector v householder matrix h u corresponds u wallace given pioneer numerical linear algebra  \n",
       "27  orthogonal unitary endomorphisms matlab-minute let u apply command norm u compute euclidean norm u form householder matrix u check orthogonality h via computation norm h form vector compare euclidean norm u selfadjoint endomorphisms already studied selfadjoint endomorphisms f finite dimensional euclidean unitary vector space defining property class endomorphisms f f ad cp definition obviously selfadjoint endomorphisms normal hence result sect hold strengthen result lemma finite dimensional euclidean unitary vector space v f l v v following statement equivalent f selfadjoint every orthonormal basis b v f b b f b b h exists orthonormal basis b v f b b f b b h euclidean case f b b h f b b proof corollary already shown implies obviously implies hold f b b f b b h f ad b b cp theorem hence f f ad hold following strong result diagonalizability selfadjoint endomorphisms euclidean unitary case theorem v finite dimensional euclidean unitary vector space f l v v selfadjoint exists orthonormal basis b v f b b real diagonal matrix special class endomorphisms proof consider first unitary case f selfadjoint f normal hence unitarily diagonalizable cp theorem let b orthonormal basis v f b b diagonal matrix f b b f ad b b f b b h implies diagonal entry f b b eigenvalue f real let v n-dimensional euclidean vector space b vn symmetric particular normal corolorthonormal basis v f b b lary exists orthogonal matrix u u j rn n u f b b u diag rm j either r j αj βj rj j α j β j since u f b b u symmetric block r j β j occur thus u real diagonal matrix u f b b define basis b wn v wn vn idv construction u idv b b hence u u b b therefore u f scalar product v vi v j δi j u f b b b b j u u get wi w j n u ki vk n n n n u u ki u vk u ki u k j δi j hence b orthonormal basis theorem following matrix version corollary rn n symmetric exist orthogonal matrix u rn n diagonal matrix rn n u du cn n hermitian exist unitary matrix u cn n diagonal matrix rn n u du h statement corollary known principal ax transformation briefly discus background name theory bilinear form application geometry symmetric matrix ai j rn n defines symmetric bilinear form via selfadjoint endomorphisms β r x ax n n ai j xi j map q r x β x x x ax called quadratic form associated symmetric bilinear form since symmetric exists orthogonal matrix u u u n u au real diagonal matrix en β set u u n form orthonormal basis respect standard scalar product u u n en u hence u change base obtain β β u au cp theorem thus real diagonal matrix represents bilinear form β defined respect basis quadratic form q associated β also transformed simpler form change base since analogously q x x ax x u du x dy n λi q u yn thus quadratic form q turned sum square defined quadratic form q principal ax transformation given change base canonical basis basis given pairwise orthonormal eigenvectors n pairwise orthogonal subspace span u j j n form n principal ax geometric interpretation term illustrated following example example symmetric matrix u au special class endomorphisms orthogonal matrix u u u c c number rounded fourth significant digit associated quadratic form q x define set e x q x described principal ax transformation consists transformation canonical coordinate system coordinate system given orthonormal basis eigenvectors carry transformation replace q quadratic form q get set ed r q set form ellipse centered origin two dimensional cartesian coordinate system spanned canonical basis vector ax length illustrated left part following figure element x e given x u e orthogonal matrix c c selfadjoint endomorphisms given rotation rotates ellipse e counterclockwise angle c approximately degree hence e rotated version e right part figure show ellipse e cartesian coordinate system dashed line indicate respective span vector u u eigenvectors principal ax ellipse e let rn n symmetric given vector v scalar α r q x x ax v x α x quadratic function n variable entry vector x set zero function set x q x called hypersurface degree quadric example already seen quadric case n v next give example example let n v α corresponding quadric surface ball radius around origin v α corresponding quadric let n parabola special class endomorphisms let n v α corresponding quadric parabolic cylinder corollary motivates following definition definition rn n symmetric cn n hermitian n positive n negative n zero eigenvalue counted corresponding multiplicity triple n n n called inertia let u first consider simplicity case real symmetric matrix lemma rn n symmetric inertia n n n diag congruent proof let rn n symmetric let u orthogonal matrix u rn n diag λn rn n inertia n n n assume without loss generality diag diagonal matrix contain positive negative eigenvalue respectively rn n diag rn n diag g l n r selfadjoint endomorphisms diag μm diag μm thus u u u u result used proof sylvester law theorem inertia symmetric matrix rn n invariant congruence every matrix g g l n r matrix g ag inertia proof assertion trivial let inertia n n n n n equal zero assume without loss generality n n following argument applied n lemma exist g g l n r diag g g let g g l n r arbitrary set b g ag b n n therefore b g b g b symmetric inertia n n diag matrix g g l n r show n n also n bg g b g g g g b g g g g g g l n r implies rank rank b rank b hence n set g u u n vn wn w w g w w since n let span u u n span dim x α j u j g αn αn r zero implies x ax james joseph sylvester proved result quadratic form also coined name law inertia according expressing fact existence invariable number inseparably attached bilinear form special class endomorphisms hand x analogous argument show x ax hence dimension formula subspace cp theorem yield dim v dim dim dim dim n n repeat construction interchanging role thus n n n n thus n n proof complete n following result transfer lemma theorem complex hermitian matrix theorem let cn n hermitian inertia n n n exists matrix g g l n c g h diag moreover every matrix g g l n c matrix g h ag inertia proof exercise finally discus special class symmetric hermitian matrix definition real symmetric complex hermitian n n matrix called positive semidefinite v h av v resp v positive definite v h av v resp v reverse inequality hold corresponding matrix called negative semidefinite negative definite respectively selfadjoint endomorphisms define analogously v finite dimensional euclidean unitary vector space scalar product f l v v selfadjoint f called positive semidefinite positive definite f v v v v resp f v v v v following theorem characterizes symmetric positive definite matrix see exercise exercise transfer result positive semidefinite matrix resp positive definite endomorphisms theorem rn n symmetric following statement equivalent positive definite eigenvalue real positive exists lower triangular matrix l g l n r l l selfadjoint endomorphisms proof symmetric matrix diagonalizable real eigenvalue cp corollary λ eigenvalue associated eigenvector v av λv λv v v av v v implies λ let u diag λn u diagonalization orthogonal matrix u rn n cp corollary λ j j let v arbitrary let w u w v u w v av u w u diag λn u u w w diag λn w n λ j w l l l g l n r every v v av v l l v l v since l invertible note need l lower triangular let u diag λn u diagonalization orthogonal matrix u rn n cp corollary since positive definite know λ j j set diag λn u u b b let b q r q rdecomposition invertible matrix b cp corollary q rn n orthogonal r rn n invertible upper triangular matrix b b q r q r l l l r one easily see analogous result hold complex hermitian matrix cn n case assertion lower triangular matrix l g l n c l l h factorization l l called cholesky special case lu theorem fact theorem show lu real symmetric positive definite matrix computed without row permutation order compute cholesky factorization symmetric positive definite matrix ai j rn n consider equation cholesky special class endomorphisms l l lnn lnn first row obtain j l j j analogously row n obtain aii li j li j lii aii ai j n lik l jk lik l jk lik l jk lii l ji ai j lik l jk j lii l ji symmetric hermitian positive definite matrix closely related positive definite bilinear form euclidian unitary vector space theorem v finite dimensional euclidian unitary vector space β symmetric hermitian bilinear form v respectively following statement equivalent β positive definite β v v v v every basis b v matrix representation β symmetric hermitian positive definite exists basis b v matrix representation β symmetric hermitian positive definite proof exercise exercise let rn n normal show α every α r ak every k p every p r normal let b rn n normal b ab normal well let normal symmetric show αβ α selfadjoint endomorphisms α r β r prove corollary using theorem show real skew-symmetric matrix matrix rn n complex skew-hermitian matrix matrix h cn n normal let v finite dimensional unitary vector space let f l v v normal show following assertion f f f selfadjoint b f f f f c f nilpotent f let v finite dimensional real complex vector space let f l v v diagonalizable show exists scalar product v f normal respect scalar product let cn n show following assertion normal exists normal matrix b n distinct eigenvalue commute b normal normal every c let h h hermitian h skew-hermitian part show h h h h h show furthermore normal h commute show cn n normal f z ad bc defined spectrum f bi c map f z called möbius transformation play important role function theory many area mathematics let v finite dimensional euclidian unitary vector space let f l v v orthogonal unitary respectively show f exists orthogonal unitary respectively let u let householder matrix h u defined show following assertion u matrix h u en orthogonally similar exists orthogonal matrix q rn n q h u q en implies h u eigenvalue algebraic multiplicity n respectively b every orthogonal matrix rn n written product n householder matrix exist u u n h u h u n august ferdinand möbius special class endomorphisms let v satisfy v v show exists orthogonal matrix u rn n u v transfer proof lemma theorem complex hermitian matrix thus show theorem determine symmetric matrix orthogonal matrix u u au diagonal positive definite let k r c let vn basis k prove disprove matrix h k n n positive definite v hj av j j use definition test whether symmetric matrix positive definite determine case inertia let rn n g l r rm called schur complement matrix r show positive definite positive definite schur complement see also exercise show cn n hermitian positive definite x h ax defines scalar product prove following version theorem positive semidefinite matrix rn n symmetric following statement equivalent positive semidefinite eigenvalue real nonnegative exists upper triangular matrix l rn n l l let v finite dimensional euclidian unitary vector space let f l v v selfadjoint show f positive definite eigenvalue f real positive let rn n matrix x rn n x called square root cp sect issai schur selfadjoint endomorphisms show symmetric positive definite matrix rn n symmetric positive definite square root b show matrix symmetric positive definite compute symmetric positive definite square root c show matrix jn n square root show matrix positive definite compute cholesky factorization using let b cn n hermitian let b furthermore positive definite show polynomial det b c exactly n real root prove theorem  \n",
       "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        chapter singular value decomposition matrix decomposition introduced chapter important many practical application since yield best possible approximation certain sense given matrix matrix low rank low rank approximation considered compression data represented given matrix illustrate example image processing first prove existence decomposition theorem let cn n given exist unitary matrix v cn n w cm h rn diag σr v r σr r rank proof set v cn w im finished let r rank since n r since h cm hermitian exists unitary matrix w wm cm w h h w diag λm rm cp corollary without loss generality assume λm every j h aw j λ j w j hence λ j w hj w j w hj h aw j j λ j j rank h rank r see modify proof lemma complex case therefore matrix h exactly r positive eigenvalue λr r time eigenvalue springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi singular value decomposition define σ j λ j j r σr let g l r x xm aw vr xr z xr xm vr vrh vr vrh z ir h h h v z x x w aw r zh z h vr z h z implies particular z vrh vr ir extend vector xr xn respect xr orthonormal basis xr standard scalar product matrix xr xn cn n v vr unitary x aw x vr z vr finally obtain vr dw h v h proof show theorem formulated analogously real matrix rn n case two matrix v w orthogonal n apply theorem h resp real case definition decomposition form called singular value decomposition short matrix diagonal entry matrix called singular value column v resp w called left resp right singular vector obtain unitary diagonalization matrix h ah h h h v w v singular value therefore uniquely determined positive square root positive eigenvalue h h unitary matrix v w singular value decomposition however eigenvectors general uniquely determined development decomposition special case middle century current general form many important player history linear algebra played role historical note concerning singular value decomposition one find contribution jordan sylvester schmidt current form shown carl henry eckart gale young singular value decomposition write svd form im wh w w h u p v h v u cn orthonormal column u h u im p p h cm positive semidefinite inertia r r factorization u p called polar decomposition viewed generalization polar representation complex number z eiϕ lemma suppose matrix cn rank r svd form v vn w wm considering element l im span vr ker span wr wm proof j r aw j v h w j v j σ j v j since σ j hence r linear independent vector satisfy vr im r rank dim im implies im span vr j r aw j hence linear independent vector satisfy wr wm ker dim ker dim im r implies ker span wr wm svd form written r σ j v j w hj thus written sum r matrix form σ j v j w hj rank σ j v j w hj let ak k σ j v j w hj k k rank ak k using matrix unitarily invariant cp exercise get ak σr hence approximated matrix ak rank approximating matrix approximation error matrix explicitly known singular value decomposition furthermore yield best possible approximation matrix rank k respect matrix theorem ak ak every matrix b cn rank b k singular value decomposition proof assertion clear k rank since ak ak let k rank let b cn rank b k given dim ker b k consider b element l wm right singular vector u span dimension k since ker b u subspace dim ker b dim u ker b u let v ker b u given exist c v α j w j j hence b v av α j aw j αjσjvj therefore α j σ j v j max b b j σ j since pairwise orthonormal j since ak completes proof matlab-minute command n generates n n n matrix entry n row column diagonal sum equal entry therefore magic square compute svd using command v w said singular value rank form ak k rank verify numerically equation svd one important practical mathematical tool almost area science engineering social science medicine even psychology great importance due fact svd allows distinguish important non-important information given data practice latter singular value decomposition corresponds measurement error noise transmission data fine detail signal image play important role often important information corresponds large singular value non-important information small one many application one see furthermore singular value given matrix decay rapidly exist large many small singular value case matrix approximated well matrix low rank since already small k approximation error ak small low rank approximation ak requires little storage capacity computer k scalar vector stored make svd powerful tool application data compression interest example illustrate use svd image compression picture obtained research center matheon mathematics key greyscale picture shown left figure consists pixel pixel given value value stored real matrix full rank compute svd v using command v w matlab diagonal entry matrix singular value ordered decreasingly matlab theorem k compute matrix ak rank k using command k k k matrix represent approximation original picture based k largest singular value corresponding singular vector three approximation shown next original picture quality approximation decrease decreasing k even approximation k show essential feature matheon bear another important application svd arises solution linear system equation cn svd form define matrix w v h n rm n thank falk ebert help original bear seen front mathematics building tu berlin information matheon found singular value decomposition one easily see ir w h rm r n invertible right hand side equation equal identity matrix case matrix therefore viewed generalized inverse case invertible matrix equal inverse definition matrix called moore-penrose pseudoinverse let cn b given linear system equation ax b x close possible solution try find x b using moore-penrose inverse obtain best possible approximation respect euclidean norm theorem let cn n b given v h x b satisfies svd x r h v b j x σj x proof let given let z ξm w h v v h b h b r n h h b σ j ξ j j j n h b j equality hold ξ j v hj b j j r satisfied z w h v h b last equation hold w v h b b eliakim hastings moore sir roger penrose singular value decomposition vector x therefore attains lower bound equation r h vj x σj easily checked every vector attains lower bound must form h vh b b r yr ym σr yr ym c implies x minimization problem vector x written x min τm pairwise distinct τm r minimization problem corresponds problem linear regression least square approximation example solved q r-decomposition q r decomposition h h cp exercise r h q h q r r h q h r r h r h q h r q h thus solution least-squares approximation example identical solution minimization problem using svd exercise show frobenius norm matrix unitarily invariant f f cn unitary matrix p cn n q cm hint frobenius norm one use trace h use result exercise show f σr singular value cn show h h cn show cn singular value decomposition let cn let moore-penrose inverse show following assertion rank h h b matrix x uniquely determined matrix satisfies following four matrix equation ax x ax x ax h ax x h x let b compute moore-penrose inverse vector x x x b x prove following theorem let cn b n h b h b b u matrix u n u h u b real u also chosen real hint one direction trivial direction consider unitary diagonalization h b h b yield matrix w svd b show assertion using two decomposition theorem application found article  \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           chapter basic mathematical concept chapter introduce mathematical concept form basis development following chapter begin set basic mathematical logic consider map set important property finally discus relation particular equivalence relation set set mathematical logic begin development concept set use following definition definition set collection well determined distinguishable object x perception thinking object called element object x definition well determined therefore uniquely decide whether x belongs set write x x element set otherwise write x furthermore element distinguishable mean element pairwise distinct two object x equal write x otherwise x mathematical object usually give formal definition equality example consider equality set see definition describe set curly bracket contain either list element example red yellow green georg cantor one founder set theory cantor published definition journal mathematische annalen springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi basic mathematical concept defining property example x x positive even number x x person owning bike well known set number denoted follows n natural number natural number including zero z integer q x x z b n rational number r x x real number real number construction characterization real number r usually done introductory course real analysis describe set via defining property formally write x p x p predicate may hold object x p x assertion p hold x general assertion statement classified either true false instance statement set n infinitely many element true sentence tomorrow weather good assertion since meaning term good weather unclear weather prediction general uncertain negation assertion assertion denote assertion true false false true instance negation true assertion set n infinitely many element given set n infinitely many element set n finitely many element false two assertion b combined via logical composition new assertion following list common logical composition together mathematical short hand notation composition conjunction disjunction implication notation equivalence wording b b implies b b sufficient condition b b necessary condition b equivalent true b true necessary sufficient b b necessary sufficient set mathematical logic example write assertion x real number x negative x r x whether assertion composed two assertion b true false depends logical value b following table logical value f denote true false respectively f f b f f f f f f f f f example assertion b true b true assertion b false true b false particular false b true independent logical value b thus true since true false since false hand assertion true since false following often prove certain implication b true table logical value show example illustrates prove assumption true assertion b true well instead assume true often write let hold easy see b exercise create table logical value compare table b truth b therefore proved showing truth implies truth b false implies false assertion called contraposition assertion b conclusion b called proof contraposition together assertion also often use so-called quantifier quantifier universal existential notation wording exists return set theory introduce subset equality set definition let n set called subset n denoted n every element also element n write n hold n called equal denoted n n n write n hold basic mathematical concept called proper subset n denoted n n n hold using notation mathematical logic write definition follows n n n x x x n n n n n assertion right side equivalence read follows object x truth x implies truth x n shorter x x hold x n hold special set set element define formally follows definition set ø x x x called empty set notation mean defined introduced empty set defining property every object x x x element ø hold object hence ø contain element set contains least one element called nonempty theorem every set following assertion hold ø ø ø proof show assertion x x ø x true since x ø assertion x ø false therefore x ø x true every x cp remark implication b let ø know ø hence ø follows definition theorem let n l set following assertion hold subset relation reflexivity n n l l transitivity proof show assertion x x x true x true x x implication two true assertion hence true show assertion x x x l true x true also x n true since n truth x n implies x l true since n hence assertion x x l true set mathematical logic definition let n set n n x x x n intersection n n x x x n difference n n x x x n n ø set n called disjoint set operation union intersection extended two set ø set set mi mi x x mi mi x x mi set called index set n n write union intersection set mn n mi n mi theorem let n two set n following equivalent n n ø proof show hold since n exists x n x thus x n n ø hold exists x n x hence n since n hold see n hold theorem let n l set following assertion hold n n commutativity n n n n associativity n l n l n l n distributivity n l n l n l n l n n l n l n l n l proof exercise notation n n union intersection set n introduced giuseppe peano one founder formal logic notation smallest common multiple n largest common divisor n set n suggested georg cantor catch  \n",
       "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          basic mathematical concept definition let set cardinality denoted number element power set denoted p set subset p n n empty set ø cardinality zero p ø ø thus ø cardinality p ø hence one show every set finitely many element finite cardinality hold map section discus map set definition let x nonempty set map f x rule assigns x x exactly one f x write f x x f x instead x f x also write f x set x called domain codomain f two map f x g x called equal f x g x hold x x write f definition assumed x nonempty since otherwise rule assigns element element x one set empty one define empty map however following always assume always explicitly state set given map act nonempty example two map x r r given f x f x x x g x x x analyze property map need terminology map definition let x nonempty set map id x x x x x called identity x let f x map let x n f f x x called image f f n x x f x n called pre-image n f f x x f x map ø x f x f x called restriction f one note definition f n set hence symbol f mean inverse map f map introduced definition example map domain x r following property f x x r x f f ø g x g g x r x definition let x nonempty set map f x called injective x equality f f implies surjective f x bijective f injective surjective every nonempty set x simplest example bijective map x x id x identity x example let x r x f r r f x x neither injective surjective f r f x x surjective injective f r f x x injective surjective f f x x bijective assertion used continuity map f x x discussed basic course analysis particular used fact continuous function map real interval real interval assertion also show important include domain codomain definition map theorem map f x bijective every exists exactly one x x f x proof let f bijective let since f surjective exists x f x also satisfies f basic mathematical concept follows injectivity f therefore exists unique x f since exists unique x x f x follows f x thus f surjective let x f f assumption implies f also injective one show two set x finite cardinality exists bijective map lemma set x n exist exactly pairwise distinct bijective map x proof exercise definition let f x x f x g z g map composition f g map g f x z x g f x expression g f read g f stress order composition first f applied x g f x one immediately see f id x f idy f every map f x theorem let f w x g x h z map h g f h g f composition map associative f g g f proof exercise theorem map f x bijective exists map g x g f id x f g idy proof f bijective theorem every exists x x x f x define map g g x g x let given hence f g idy f g f g f hand x x given f x theorem x exists unique x f x g f x g f g f g  \n",
       "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            map g f id x assumption g f id x thus g f injective thus also f injective see exercise moreover f g idy thus f g surjective hence also f surjective see exercise therefore f bijective map g x characterized theorem unique another map h x h f id x f h idy h id x h g f h g f h g idy lead following definition definition f x bijective map unique map g x theorem called inverse inverse map f denote inverse f f show given map g x unique inverse bijective map f x sufficient show one equation g f id x f g idy indeed f bijective g f id x g g idy g f f g f f id x f f way g f follows assumption f g idy theorem f x g z bijective map following assertion hold f bijective f f g f bijective g f f g proof exercise know theorem g f x z bijective therefore exists unique inverse g f map f g f g g f f g g f f g g f f idy f f f id x hence f g inverse g f relation first introduce cartesian two set named rené descartes founder analytic geometry georg cantor used name connection set n notation n basic mathematical concept definition n nonempty set set n x x n cartesian product n element x n called ordered pair easily generalize definition n n nonempty set mn mn xn xi mi n element xn mn called ordered n-tuple n-fold cartesian product single nonempty set n xn xi n n time definition least one set empty resulting cartesian product empty set well definition n nonempty set set r n called relation n n r called relation instead x r also write x r x clear relation considered definition least one set n empty every relation n also empty set since n ø instance n n q r x n x relation n expressed r n n n definition relation r set called reflexive x x hold x symmetric x x hold x transitive x z x z hold x z r reflexive transitive symmetric called equivalence relation example let r x x r reflexive since x hold x x also hence r symmetric finally r transitive example x r z r x z r relation relation r x x reflexive transitive symmetric f r r map r x f x f equivalence relation definition let r equivalence relation set x set x r x r x called equivalence class x respect set equivalence class x r x called quotient set respect equivalence class x r element x never empty set since always x x reflexivity therefore x x r clear equivalence relation r meant often write x instead oft x r also skip additional respect r theorem r equivalence relation set x following equivalent x x ø x proof since x x follows x x x follows x thus x x since x ø exists z x element z x z z thus x z z symmetry therefore x transitivity let x z x x z using symmetry transitivity obtain z hence z mean x analogous way one show x hence x hold theorem show two equivalence class x either x x ø thus every x contained exactly one equivalence class namely x equivalence relation r yield partitioning decomposition mutually disjoint subset every element x called representative equivalence class x useful general approach often use book partition set object set matrix equivalence class find class representative particularly simple structure representative called normal form respect given equivalence relation basic mathematical concept example given number n n set rn b b divisible n without remainder equivalence relation z since following property hold reflexivity divisible n without remainder symmetry b divisible n without remainder also b transitivity let b b c divisible n without remainder write c b b c summands right divisible n without remainder hence also hold z equivalence class called residue class modulo n nz nz z z equivalence relation rn yield partitioning z n mutually disjoint subset particular n set residue class modulo n quotient set respect rn often denoted thus n set play important role mathematical field number theory exercise let b c assertion show following assertion true associative law b c b c b c b c hold b commutative law b b b b hold c distributive law b c c b c b c c b c hold let b c assertion show following assertion true b b b b b relation c e f b b b b c c b c c assertion c called de morgan law prove theorem show two set n following hold n let x nonempty set u v nonempty subset let f x map show f u v f u f v let u v x nonempty check whether f u v f u f v hold following map injective surjective bijective f r r x b f r x x c f r x x n n even f n z n n odd show two map f x g z following assertion hold g f surjective g surjective b g f injective f injective let z given show map f z z f x x bijective prove lemma prove theorem prove theorem find two map f g n n simultaneously f surjective b g injective c g f bijective determine equivalence relation set determine symmetric transitive relation set b c reflexive  \n",
       "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  chapter kronecker product linear matrix equation many application particular stability analysis differential equation lead linear matrix equation ax x b matrix b c given goal determine matrix x solves equation give formal definition description solution equation kronecker another product matrix useful chapter develop important property product study application context linear matrix equation many result topic found book definition k field ai j k b k n n b b b ai j b b amm b called kronecker product b kronecker product sometimes called tensor product matrix product defines map k k n n k mn mn definition extended non-square matrix simplicity consider case square matrix following lemma present basic computational rule kronecker product lemma square matrix b c k following computational rule hold b c b leopold kronecker said used product lecture berlin defined formally first time johann georg zehfuss springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi kronecker product linear matrix equation μa b μb μ b μ k b c c b c whenever b defined b c b c whenever b c defined b b therefore kronecker product two symmetric matrix symmetric proof exercise particular contrast standard matrix multiplication order factor kronecker product change transposition following result describes matrix multiplication two kronecker product lemma c k b k n n b c ac b hence particular b im b im b b b b invertible proof since b ai j b c ci j block fi j k n n block matrix fi j b c given fi j aik b ck j aik ck j b aik ck j b block matrix g j ac b g j k n n obtain g j gi j b gi j aik ck j show b c ac b easily follow equation general kronecker product non-commutative cp exercise following relationship b b lemma k b k n n exists permutation matrix p k mn mn p b p b proof exercise computation determinant trace rank kronecker product exist simple formula kronecker product linear matrix equation theorem k b k n n following rule hold det b det n det b det b trace b trace trace b trace b rank b rank rank b rank b proof lemma multiplication theorem determinant cp theorem get det b det im b det det im b lemma exists permutation matrix p p p implies det det p p det det n since det im b det b follows det b det n det b therefore also det b det b b ai j b obtain trace b n aii b j j aii n b j j trace trace b trace b trace trace b exercise matrix k n column j k j n define vec k application vec turn matrix column vector thus vectorizes lemma map vec k n k isomorphism particular ak k n linearly independent vec vec ak k linearly independent proof exercise consider relationship kronecker product vec map kronecker product linear matrix equation theorem k b k n n c k n vec ac b b vec c hence particular vec ac vec c vec c b b im vec c vec ac c b b im vec c proof j n jth column ac b given ac b e j ac j n bk j ac ek n bk j cek j j bn j vec c implies vec ac b b vec c b resp im obtain linearity vec yield order study relationship eigenvalue matrix b kronecker product use bivariate polynomial polynomial two variable cp exercise p l j αi j k polynomial k b k n n define matrix p b l αi j ai b j careful order factor since general ai b j b j ai cp exercise example rm b rn n p r get matrix p b b following result known stephanos named cyparissos stephanos showed besides result also assertion lemma kronecker product linear matrix equation theorem let k b k n n two matrix jordan normal form eigenvalue λm k μn k respectively p b defined following assertion hold eigenvalue p b p λk k eigenvalue b λk k eigenvalue λk k proof let g l k g l n k j bt j b jordan canonical form matrix j j b upper triangular thus j j j matrix j ai j b j ai j b upper triangular eigenvalue j j j j j b λm μn respectively thus p λk k n diagonal entry matrix p j j b using lemma obtain p b l αi j j j b j l l αi j j ai j b j αi j j ai j b j l αi j j ai j b j l αi j j ai j b j p j j b implies assertion follow p p respectively following result matrix exponential function kronecker product helpful application involve system linear differential equation lemma cm b cn n c im b exp c exp exp b proof lemma know matrix im b commute using lemma obtain kronecker product linear matrix equation exp c exp im b exp exp im b j im b j j im b j j b j exp exp b used property matrix exponential series cp sect given matrix j k b j k n n j q c k n equation form x x aq x bq c called linear matrix equation unknown matrix x k n theorem matrix x k n solves x vec x solves linear system equation k gx vec c g q b tj j proof exercise consider two special case theorem cm b cn n c cm n sylvester ax x b c unique solution common eigenvalue eigenvalue b negative real part unique solution given x exp c exp b dt sect integral defined entrywise james joseph sylvester kronecker product linear matrix equation proof analogous representation theorem write sylvester equation b im x vec c b eigenvalue λm μn respectively g b im theorem eigenvalue λk k thus g invertible sylvester equation uniquely solvable λk k let b matrix eigenvalue negative real part common eigenvalue unique solution let j j b bt jordan canonical form b consider linear differential equation dz az z b z c dt solved function z cm n z exp c exp b cp exercise function satisfies lim z lim exp c exp b lim exp j exp j b constant integration equation yield z lim z z z dt z dt b use without proof existence infinite integral implies x z dt exp c exp b dt unique solution theorem also give solution another important matrix equation corollary c cn n lyapunov ax x h alexandr mikhailovich lyapunov also ljapunov liapunov kronecker product linear matrix equation unique solution x cn n eigenvalue negative real part furthermore c hermitian positive definite also x hermitian positive definite proof since assumption h common eigenvalue unique solvability follows theorem solution given matrix x exp exp h dt exp c exp h dt c hermitian positive definite x hermitian x h x xx x exp c exp dt x x h exp c exp h x dt last inequality follows monotonicity integral fact x also exp h x since exp h invertible every real exercise prove lemma construct two square matrix b b b prove lemma prove theorem prove lemma show b normal cm b cn n normal true b unitary b unitary use singular value decomposition v w ah cm b vb b w bh cn n derive singular value decomposition b show cm b cn n matrix equation b b hold prove theorem let cm b cn n c cm n show z exp c exp b solution matrix differential equation ddtz az z b initial condition z c  \n",
       "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             chapter algebraic structure algebraic structure set operation element follow certain rule example structure consider integer operation property addition already elementary school one learns sum b two integer b another integer moreover number every integer every integer exists integer analysis property concrete example lead definition abstract concept built simple axiom integer operation addition lead algebraic structure group principle abstraction concrete example one strength basic working principle mathematics extracting completely exposing mathematical kernel david hilbert also simplify work every proved assertion abstract concept automatically hold concrete example moreover combining defined concept move generalization way extend mathematical theory step step hermann günther graßmann described procedure mathematical method move forward simplest concept combination gain via combination new general group begin set operation specific property definition group set g map called operation g g g b b die mathematische methode hingegen schreitet von den einfachsten begriffen zu den zusammengesetzteren fort gewinnt durch verknüpfung de besonderen neue allgemeinere springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi algebraic structure satisfies following operation associative b c b c hold b c exists element e g called neutral element e g b every g exists g called inverse element b b hold b g group called commutative short hand notation group use g g clear operation used theorem every group g following assertion hold e g neutral element g e also e g neutral element g also e g contains exactly one neutral element every g exists unique inverse element proof let e g neutral element let g satisfy definition exists element g thus e e let e g neutral element let exists g also e follows e e let e g two neutral element e e since neutral element since e also neutral element follows e e second identity used assertion hence e let g two inverse element g let e g unique neutral element follows e e named niels henrik abel founder group theory group example z q r commutative group group neutral element number zero inverse number instead usually write b since operation addition group also called additive group natural number n addition form group since neutral element consider set includes also number zero inverse element hence also addition form group set q r usual multiplication form commutative group multiplicative group neutral element number one inverse element number instead also write ab integer z multiplication form group set z includes number z z inverse element z definition let g group h h group called subgroup g next theorem give alternative characterization subgroup theorem h subgroup group g following property hold ø h b h b h every h also inverse element satisfies proof exercise following definition characterizes map two group compatible respective group operation definition let g g group map ϕ g g g ϕ g called group homomorphism ϕ b ϕ ϕ b b g bijective group homomorphism called group isomorphism  \n",
       "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           algebraic structure ring field section extend concept group discus mathematical structure characterized two operation motivating example consider integer addition group z multiply element z multiplication associative b b c z furthermore addition multiplication satisfy distributive law b c b c b c c b c integer b property make z addition multiplication ring definition ring set r two operation r r r b b addition r r r b b multiplication satisfy following r commutative group call neutral element group zero write denote inverse element r write b instead multiplication associative b c b c b c distributive law hold b c r b c b c b c c b ring called commutative b b b element r called unit case r called ring unit right hand side two distributive law omitted parenthesis since multiplication supposed bind stronger addition b c b useful illustration purpose nevertheless use parenthesis sometimes write b c instead b c analogous notation group denote ring r r operation clear context ring unit unit element unique e r satisfy e e r particular e e r use following abbreviation sum product element n j n j ring field moreover n empty sum r n k define k j ring unit also define k empty product k j theorem every ring r following assertion hold b b b b proof every r adding left right hand side equality obtain way show since b b follows unique additive inverse b b way show b b furthermore b thus b immediately clear z commutative ring unit standard example concept ring modeled example let nonempty set let r set map f r operation r r r r r r f g f g f g f g f g x f x g x f g x f x g x commutative ring unit f x g x f x g x sum product two real number zero ring map r r x unit map r r x real number zero one definition ring additive inverse element occur formally define concept multiplicative inverse algebraic structure definition let r ring unit element b r called inverse r respect b b element r inverse called invertible clear definition b r inverse r r inverse b general however every element ring must invertible element invertible unique inverse shown following theorem theorem let r ring unit r invertible inverse unique denote b r invertible b r invertible b proof b b r inverse r b b b b b b b since b invertible r well defined b b b way show b thus b algebraic point view difference integer one hand rational real number set q r every element except number zero invertible additional structure make q r field definition commutative ring r unit called field every r invertible definition every field commutative ring unit converse hold one also introduce concept field based concept group cp exercise definition field set k two operation k k k k k k b b b b addition multiplication ring field satisfy following k commutative group call neutral element group zero write denote inverse element k write b instead k commutative group call neutral element group unit write denote inverse element k distributive law hold b c k b c b c b c c b show useful property field lemma every field k following assertion hold k least two element k b c imply b c b c k b imply b b k proof follows definition since k already shown ring cp theorem since know exists multiplying side b c left yield b suppose b finished exists multiplying side b left yield b ring r element r called zero b r exists b element called trivial zero divisor property lemma mean field contain trivial zero divisor also ring property hold instance ring integer z later chapter encounter ring matrix contain non-trivial zero divisor see proof theorem following definition analogous concept subgroup cp definition subring cp excercise definition let k field l k l field called subfield k two important example algebraic concept discussed discus field complex number ring polynomial concept zero divisor introduced karl theodor wilhelm weierstraß algebraic structure example set complex number defined c x x r r set define following operation addition multiplication c c c c c c right hand side use addition multiplication field c field neutral element respect addition multiplication given inverse element respect addition multiplication given x x c x x c x x x multiplicative inverse element written ab instead common notation considering subset l x x r c identify every x r element set l via bijective map x x particular thus interpret r subfield c although r really subset c distinguish zero unit element r special complex number imaginary unit satisfies identified real number complex number imaginary unit denoted hence write using identification x r x c write z x c x x x x iy z im z ring field last expression z x im z abbreviation real part imaginary part complex number z x since iy yi justified write complex number x iy x yi given complex number z x z x iy number z x respectively z x iy called associated complex conjugate number using real square root modulus absolute value complex number defined zz x iy x iy x ix iyx x simplification omitted multiplication sign two complex number equation show absolute value complex number nonnegative real number property complex number stated exercise end chapter example let r commutative ring unit polynomial r indeterminate variable expression form p αn n αn r coefficient polynomial instead α j j often write α j j set polynomial r denoted r let p αn n q βm two polynomial r n n set β j j n call p q equal written p q α j β j j particular αn n αn n n degree polynomial p αn n denoted deg p defined largest index j α j index exists polynomial zero polynomial p set deg p let p q r degree n respectively n n set β j j define following operation r algebraic structure p q αn βn n αi β j p q γk operation r commutative ring unit zero given polynomial p unit p r field since every polynomial p r invertible even r field example p polynomial q βm r p q βm hence p invertible polynomial substitute variable object resulting expression evaluated algebraically example may substitute λ r interpret addition multiplication corresponding operation ring defines map r r λ p λ αn λn λk λ λ k n k time r empty product one confuse ring element p λ polynomial p rather think p λ evaluation p λ study property polynomial detail later also evaluate polynomial object matrix endomorphisms exercise determine following whether form group x r x b b b r b ab let b r map f b r r r r x ax ay set g f b b r given show g commutative group operation g g g defined composition two map cp definition let x ø set let x f x x f bijective show x group let g group g denote g unique inverse element show following rule element g b c b b b ring field prove theorem let g group fixed g let z g g g g g show z g subgroup subgroup element g commute called centralizer let ϕ g h group homomorphism show following assertion u g subgroup also ϕ u h subgroup furthermore g commutative also ϕ u commutative even h commutative b v h subgroup also v g subgroup let ϕ g h group homomorphism let eg e h neutral element group g h respectively show ϕ eg e h b let ker ϕ g g ϕ g e h show ϕ injective ker ϕ eg show property definition r example order show r commutative ring unit suppose example replace codomain r map commutative ring unit r still commutative ring unit let r ring n show following assertion n even r n n n odd b exists unit r n r invertible element r n n n called nilpotent let r ring unit show r let r ring unit let r denote set invertible element show r group called group unit r b determine set k k k field fixed n n let nz nk k z n example show nz subgroup z b define b b b b b b addition multiplication addition multiplication z show following assertion algebraic structure well defined ii commutative ring unit iii field n prime number let r ring subset r called subring r ring show subring r following property hold r r also r r r also show definition field describe mathematical structure let k field show l subfield k cp definition following property hold l k k b l b l b l l l show field hold let r commutative ring contain non-trivial zero divisor ring called integral domain define r r relation x x x x show equivalence relation b denote equivalence class x xy show following map well defined x x x x x x x x denotes quotient set respect cp definition c show field field called quotient field associated field r z exercise consider r k ring polynomial field k construct way field rational function ring field let c b determine b b b ab ba show following rule complex number z z z z z z z z z z b z z z z z c show absolute value complex number satisfies following property z z z b z c equality z c z z z c  \n",
       "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 chapter matrix chapter define matrix important operation study several group ring matrix james joseph sylvester coined term matrix described matrix oblong arrangement term matrix operation defined chapter introduced arthur cayley article memoir theory matrix first consider matrix independent algebraic object book matrix form central approach theory linear algebra basic definition operation begin formal definition matrix definition let r commutative ring unit let n array form ai j anm latin word matrix mean womb sylvester considered matrix object may form various system determinant cp chap interestingly english writer charles lutwidge dodgson better known pen name lewis carroll objected sylvester term wrote aware word matrix already use express meaning use word block surely former word mean rather mould form algebraic quantity may introduced actual assemblage quantity dodgson also objected notation ai j matrix entry space occupied number wholly superfluous important part notation reduced minute subscript alike difficult writer springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi matrix ai j r n j called matrix size n ai j called entry coefficient matrix set matrix denoted r n following usually assume without explicitly mentioning excludes trivial case ring contains zero element cp exercise formally definition n obtain empty matrix size n denote matrix used technical reason proof analyze algebraic property matrix however always consider n zero matrix r n denoted matrix entry equal matrix size n n called square matrix square entry aii n called diagonal entry identity matrix r n n matrix δi j j δi j kronecker delta-function.2 clear n considered write instead n set ith row r n aim r n use comma optical separation entry jth column j j r j j thus row column matrix matrix matrix ai aim r n given combine matrix n r leopold kronecker anm basic definition operation write square bracket around row way combine n matrix j j j r j j matrix n r anm n n ai j r ni j j combine four matrix matrix r n matrix ai j called block block matrix introduce four operation matrix begin addition r n r n r n b b ai j bi j addition r n operates entrywise based addition note addition defined matrix equal size multiplication two matrix defined follows r n r r n b b ci j ci j aik bk j thus entry ci j product b constructed successive multiplication summing entry ith row jth column b clearly order define product b number column must equal number row b definition entry ci j matrix b written multiplication symbol element follows usual convention omitting multiplication sign clear multiplication considered eventually also omit multiplication sign matrix matrix illustrate multiplication rule cij equal ith row time jth column b follows j b b b mj m aim ci j anm important note matrix multiplication general commutative example matrix b hand b although b b defined obviously b b case one recognizes non-commutativity matrix multiplication fact b b different size even b b defined size general b b example yield two product b matrix multiplication however associative distributive respect matrix addition basic definition operation lemma r n b b r c r k following assertion hold b c b b b b b b b b im proof show property others exercise let r n b r c r k well b c di j b c di j definition matrix multiplication using associative distributive law r get di j ait bts c j ait bts c j c j ait bts ait bts c j di j n j k implies b c b c right hand side lemma written parenthesis since use common convention multiplication matrix bind stronger addition r n n define k n ak k time another multiplicative operation matrix multiplication defined follows r r n r n λ λ λai j easily see r n addition scalar multiplication following property lemma b r n c r λ μ r following assertion hold λμ λ μ λ μ λ μ term scalar introduced sir william rowan hamilton originates latin word scale mean ladder matrix λ b λ λ b λ c λ c λ c proof exercise fourth matrix operation introduce transposition r n r n ai j bi j bi j ji example matrix called transpose definition r n n satisfies called symmetric called skew-symmetric transposition following property lemma r n b r λ r following assertion hold λ λ b b proof property exercise proof let b ci j ai j b bi j b ci j ci j aik bk j ci j c ji jk bki ak j bik ak j see b b matlab-minute carry following command order get used matrix operation chapter matlab notation order see matlab output put semicolon end command basic definition operation example consider example car insurance premium chap recall pi j denotes probability customer class ci year move class c j example consists four class probability associated row-stochastic matrix cp denote suppose insurance company following distribution customer four class class class class class matrix describes initial customer distribution using matrix multiplication compute p contains distribution customer next year example consider entry p position computed customer class year move class thus respective initial percentage multiplied probability customer class class probability respectively yield two product continuing way obtain k year distribution pk k k formula also hold k since p insurance company use formula compute revenue payment premium rate coming year assume full premium rate class euro per year rate class euro discount customer initially revenue first year euro  \n",
       "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               matrix customer cancel contract model yield revenue year k pk p k example revenue next year rounded full euro number decrease annually rate decrease seems slow exists stationary state state revenue changing significantly property model guarantee existence state important practical question insurance company existence stationary state guarantee significant revenue long-time future since formula depends essentially entry matrix p k reached interesting problem linear algebra analysis property row-stochastic matrix analyze property sect matrix group ring section study algebraic structure formed certain set matrix matrix operation introduced begin addition r n theorem r n commutative group neutral element r n zero matrix ai j r n inverse element j r n write b instead proof using associativity addition r arbitrary b c r n obtain b c ai j bi j ci j ai j bi j ci j ai j bi j ci j ai j bi j ci j b c thus addition r n associative zero matrix r n satisfies ai j ai j ai j given ai j r n j r n j ai j j ai j finally commutativity addition r implies b ai j bi j ai j bi j bi j ai j b note lemma implies transposition homomorphism even isomorphism group r n r n cp definition matrix group ring theorem r n n ring unit given identity matrix ring commutative n proof already shown r n n commutative group cp theorem property ring associativity distributivity existence unit element follow lemma commutativity n hold commutativity multiplication ring example show ring r n n commutative n example proof theorem show n ring r n n non-trivial zero-divisors exist matrix b r n n b exist even r field let u consider invertibility matrix ring r n n respect r n n must satisfy matrix multiplication given matrix r n n inverse cp definition inverse two equation r n n exists invertible inverse unique denoted cp theorem invertible matrix sometimes called non-singular non-invertible matrix called singular show corollary existence inverse already implied one two equation one hold invertible correct check validity equation matrix r n n invertible simple example non-invertible matrix r r another non-invertible matrix however considered element unique inverse given lemma b r n n invertible following assertion hold invertible also write matrix b invertible b b matrix proof using lemma int int thus inverse already shown theorem general ring unit thus hold particular ring r n n next result show invertible matrix form multiplicative group theorem set invertible matrix r form group respect matrix multiplication denote group g l n r gl abbreviates general linear group proof associativity multiplication g l n r clear shown lemma product two invertible matrix invertible matrix neutral element g l n r identity matrix since every g l n r assumed invertible exists g l n r introduce important class matrix definition let ai j r n n called upper triangular ai j j called lower triangular ai j j upper triangular called diagonal ai j j upper lower triangular write diagonal matrix diag ann next investigate set matrix respect group property beginning invertible upper lower triangular matrix theorem set invertible upper triangular n n matrix invertible lower triangular n n matrix r form subgroup g l n r proof show result upper triangular matrix proof lower triangular matrix analogous order establish subgroup property prove three property theorem since invertible upper triangular matrix set invertible upper triangular matrix nonempty subset g l n r next show two invertible upper triangular matrix b r n n product c b invertible upper triangular matrix invertibility c ci j follows lemma j matrix group ring n ci j aik bk j bk j k j aik bk j aik k j since j j therefore c upper triangular remains prove inverse invertible upper triangular matrix upper triangular matrix n assertion hold trivially assume n let ci j equation written system n equation j j ann cn j δn j j δi j kronecker delta-function defined prove inductively n n diagonal entry aii invertible cii n ci j δi j j formula implies particular ci j j n last row given ann cn j δn j j j n ann cnn cnn ann second equation use cnn commutativity multiplication therefore ann invertible ann thus δn j j cn j ann equivalent note n sum empty thus equal zero particular cn j j n assume assertion hold n k k n particular ci j k n j word row n k upper triangular order prove assertion k consider kth row given matrix akk ck j ak j akn cn j δk j j j k n obtain akk ckk ak k akn cnk induction hypothesis k cn k implies akk ckk ckk akk used commutativity multiplication hence ckk get akk invertible akk δk j ak j akn cn j ck j akk j n hence hold k j δk j j cn j give ck j point represents recursive formula computing entry inverse invertible upper triangular matrix using formula entry computed bottom top right left process sometimes called backward substitution following frequently partition matrix block make use block multiplication every k n write r n n r k k r b r n n partitioned like product b evaluated blockwise particular g l k r g l r g l n r direct computation show matrix group ring matlab-minute create block matrix matlab carrying following command tridiag zero k investigate meaning command full compute product well inverse inv inv b compute inverse b matlab formula corollary set invertible diagonal n n matrix r form commutative subgroup respect matrix multiplication invertible upper lower triangular n n matrix proof since invertible diagonal matrix invertible diagonal n matrix form nonempty subset invertible upper lower triangular n n matrix diag ann b diag bnn invertible b invertible cp lemma diagonal since b diag ann diag bnn diag ann bnn moreover diag ann invertible aii r invertible n cp proof theorem inverse given ann finally commutativity property invertible diagonal matrix diag b b follows directly commutativity definition matrix p r n n called permutation matrix every row every column p exactly one unit entry zero term permutation mean exchange matrix r n n multiplied permutation matrix left right row column respectively exchanged permuted example p matrix p p theorem set n n permutation matrix r form subgroup g l n r particular p r n n permutation matrix p invertible p p proof exercise omit multiplication sign matrix multiplication write ab instead b exercise following exercise r commutative ring unit consider following matrix z b c determine possible matrix c bc b c c b ac c b consider matrix ai j r n x r ym r xn following expression well defined n n x b x c yx yx e x ay f x ay g x ay h x ay x j x k ax l x show following computational rule r n r r matrix group ring prove lemma prove lemma prove lemma let determine n n let p αn n r polynomial cp example r define p r p αn im b fixed matrix r consider map f r r p p show f p q f p f q f pq f p f q p q r map f ring homomorphism ring r r c show f r p p r commutative subring r f r subring r cp exercise multiplication subring commutative map f surjective determine p p z let k field show every matrix k n n written symmetric matrix k n n skew-symmetric matrix k n n also hold field give proof counterexample show binomial formula commuting matrix b r n n k j j k k ab b b j b kj j j n n invertible show let r matrix j hold every let r n n matrix n exists let smallest natural number property investigate whether invertible give particularly simple representation inverse b determine cardinality set ak k n let ai j r n n j j n show subring r n n b show r n n subring property called left ideal r n n c determine analogous subring b r n n b b r n n b b subring property called left ideal r n n examine whether g matrix co α sin α α r sin α co α subgroup g l r generalize block multiplication matrix r n b r determine invertible upper triangular matrix r n n let r n n r n n r n n r n n let b let r n n g l n r show invertible invertible derive case formula g l n r show invertible invertible derive case formula let g l n r u r n v r n show following assertion u v g l n r hold im v u g l r b im v u g l r u v u im v u v last equation called sherman-morrison-woodbury formula named jack sherman winifred morrison max woodbury show set block upper triangular matrix invertible diagonal block set matrix amm aii g l r group respect matrix multiplication prove theorem group permutation matrix commutative show following equivalence relation r n n exists permutation matrix p p b company produce four raw material five intermediate product z z z z z three final product e e e following table show many unit ri z j required producing one unit z k e respectively matrix group ring instance five unit one unit required producing one unit z determine help matrix operation corresponding table show many unit ri required producing one unit e b determine many unit four raw material required producing unit e unit e unit e  \n",
       "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    chapter echelon form rank matrix chapter develop systematic method transforming matrix entry field special form called echelon form transformation consists sequence multiplication left certain elementary matrix invertible echelon form identity matrix inverse product inverse elementary matrix non-invertible matrix echelon form sense closest possible matrix identity matrix form motivates concept rank matrix introduce chapter use frequently later elementary matrix let r commutative ring unit n n j n let r n n identity matrix let ei ith column en define e j ei e tj ei r n n column j entry j e j entry n j define pi j e j e ei e en r n n thus pi j permutation matrix cp definition obtained exchanging column j multiplication r n left pi j mean exchange ofthe row j example springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi echelon form rank matrix λ r define mi λ λei en r n n thus mi λ diagonal matrix obtained replacing ith column λei multiplication r n left mi λ mean multiplication ith row λ example n j λ r define g j λ λe ji ei λe j en r n n thus lower triangular matrix g j λ obtained replacing ith column ei λe j multiplication r n left g j λ mean λ time ith row added jth row similarly multiplication r n left upper triangular matrix g j λ mean λ time jth row added ith row example g g g lemma elementary matrix pi j mi λ invertible λ r g j λ defined respectively invertible following inverse j pi j pi j mi λ mi g j λ g j proof invertibility pi j j pi j already shown theorem symmetry pi j easily seen  \n",
       "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              elementary matrix since λ r invertible matrix mi well defined straightforward computation show mi mi λ mi λ mi since e tj ei j e ei e tj ei e tj therefore g j λ g j λe ji e ji λe ji e ji e similar computation show g j g j λ echelon form gaussian elimination constructive proof following theorem relies gaussian elimination given matrix k n k field algorithm construct matrix g l n k c quasi-upper triangular obtain special form left-multiplication elementary matrix pi j mi j λ g j λ left-multiplications corresponds application one so-called elementary row operation matrix pi j exchange two row mi λ multiply row invertible scalar g j λ add multiple one row another row assume entry field rather ring proof theorem require nonzero entry invertible generalization result hold certain ring integer z given hermite normal play important role number theory theorem let k field let k n exist invertible matrix st k n n product elementary matrix c st echelon form either c named carl friedrich gauß similar method already described chap rectangular array nine chapter mathematical art text developed ancient china several decade bc stated problem every day life gave practical mathematical solution method detailed commentary analysis written liu hui approx ad around ad charles hermite echelon form rank matrix c denotes arbitrary zero nonzero entry precisely c ci j either zero matrix exists sequence natural number jr called step echelon form jr r min n ci j r j ji ci j r n j ci ji r entry column ji zero n k n n invertible c case st proof set c done let let index first column ai j consist zero let ai first entry column nonzero form proceed follows first permute row finally eliminate normalize new first row multiply ai j nonzero element first entry column permuting normalizing lead echelon form gaussian elimination j set order eliminate column left matrix multiply g g n g g n ai j n j keep index larger matrix smaller matrix finished since c echelon form case r least one entry nonzero apply step described matrix k define matrix sk recursively sk k k sk jk matrix constructed analogous first identify first column jk k completely zero well first nonzero entry ai k k jk column permuting normalizing yield matrix k ai k j mk k aik jk pk ik k echelon form rank matrix k k set pk k k k k pk ik jk g k jk mk aik jk sk g k n sk indeed product elementary matrix form elementary matrix size n k n k continue procedure inductively end r min n step either r r r step sr construction entry position r jr r echelon form see discussion beginning proof r still eliminate nonzero entry column jr denote matrix r ri j form k r recursively r k ri k j sr r g k sr g k jk jk c st echelon form suppose n c st echelon form invertible c product invertible matrix thus invertible invertible matrix row containing zero r n hence c hand c invertibility elementary matrix echelon form gaussian elimination implies product invertible matrix invertible st literature echelon form sometimes called reduced row echelon form example transformation matrix echelon form via left multiplication elementary matrix g g g g matlab-minute echelon form computed matlab command rref reduced row echelon form apply rref eye order compute inverse matrix gallery tridiag cp exercise formulate conjecture general form prove conjecture proof theorem lead so-called lu square matrix theorem every matrix k n n exists permutation matrix p k n n lower triangular matrix l gln k one diagonal upper triangular matrix u k n n plu matrix u invertible invertible echelon form rank matrix u upper proof k n n eq form sn u triangular r n set sn sr since matrix invertible invertible sn invertible follows u n every matrix si form pi j si si sn ji n pi ji permutation necessary therefore sn sn n sn j j sn j j form permutation matrix k n k implies pk j pk jk k sn sn echelon form gaussian elimination hold certain j k j hence sn sn n sn sn n sn invertible lower triangular matrix permutation matrix form group respect matrix multiplication cp theorem thus permutasn l p l invertible lower triangular p lnn invertible tion matrix since l li j invertible also diag l p l u u obtain p lu p p construction diagonal entry l equal one example computation lu matrix g g u hence p l g g thus p p diag echelon form rank matrix l u u gln k lu yield u l p hence computing lu one obtains inverse essentially inverting two triangular matrix since achieved efficient recursive formula lu popular method scientific computing application require inversion matrix solution linear system equation cp chap context however alternative strategy choice permutation matrix used example instead first nonzero entry column one chooses entry large largest absolute value row exchange subsequent elimination strategy influence rounding error computation reduced matlab-minute hilbert matrix ai j qn n entry ai j j j generated matlab command hilb n carry command l u p hilb order compute lu matrix hilb matrix p l u look like compute also lu matrix full gallery tridiag study corresponding matrix p l u show given matrix matrix c theorem uniquely determined certain sense need following definition definition c k n echelon form theorem position r jr called pivot position also need following result lemma z gln k x k z x x proof exercise theorem let b k n echelon form z b matrix z gln k b david hilbert echelon form gaussian elimination proof b zero matrix zb hence b let b let b respective column ai bi furthermore let r jr r pivot position b show every matrix z gln k z b form ir z z k since b echelon form entry b row r zero follows b z b since first pivot position b bi k b first column z b implies ai k z b z since z invertible lemma implies k since echelon form b furthermore z z n z z k cp exercise r done r proceed pivot position analogous way since b echelon form kth pivot position give b jk ek jk z b jk invertibility z obtain jk b jk z z z k result yield uniqueness echelon form matrix invariance left-multiplication invertible matrix corollary k n following assertion hold unique matrix c k n echelon form transformed elementary row operation left-multiplication elementary matrix matrix c called echelon form gln k matrix c also echelon form echelon form matrix invariant left-multiplication invertible matrix proof echelon form invertible theorem give gln k echelon form get theorem give  \n",
       "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   echelon form rank matrix rank equivalence matrix seen corollary echelon form k n unique particular every matrix k n exists unique number pivot position cp definition echelon form justifies following definition definition number r pivot position echelon form k n called denoted rank see immediately k n always rank min n rank moreover theorem show k n n invertible rank property rank summarized following theorem theorem k n following assertion hold exist matrix q gln k z glm k q az ir r rank r q gln k z glm k rank rank q az bc b k n c k rank rank b b rank rank c rank rank exist matrix b k n c k bc rank proof let q g l n k q b echelon form q q bc matrix q bc first rank b row contain nonzero entry corollary echelon form q equal echelon form thus normal echelon form also first rank b row nonzero implies rank rank b rank r ir assertion hold arbitrary matrix q g l n k z g l k r exists matrix q g l n k q echelon form r pivot position exists permutation matrix p k product elementary permutation matrix pi j concept rank introduced context bilinear form first ferdinand georg frobenius rank equivalence matrix p q ir v matrix v k r r v following simplicity omit size zero matrix matrix invertible thus ir ir v k k ir ypa q z p g l k obtain ir q az suppose hold k n matrix q g l n k z g l k obtain rank rank az z rank az rank thus particular rank rank az due invariance echelon form hence rank left-multiplication invertible matrix cp corollary get rank rank az rank q az rank ir k q gln k z glm k invariance rank left-multiplication invertible matrix used showing rank rank q az z rank q az rank az rank hence particular rank rank q az rank r exist matrix q g l n k z g l k q az r therefore echelon form rank matrix rank rank q az rank ir rank ir rank q az rank z q rank using obtain rank rank rank c b rank c rank c let bc b k n c k rank rank bc rank b let hand rank r exist matrix q ir thus obtain g l n k z g l k q az q ir r ir r z bc b k n c k example matrix example echelon form since two pivot position rank multiplying right q yield ab hence rank ab rank assertion theorem motivates following definition rank equivalence matrix definition two matrix b k n called equivalent exist matrix q g l n k z g l k q b z name suggests defines equivalence relation set k n since following property hold reflexivity q az q z im symmetry q b z b q az transitivity q b z b q c z q q c z z equivalence class k n given q az q g l n k z g l k rank r theorem ir r therefore ir ir consequently rank fully determines equivalence class matrix ir k n called equivalence normal form obtain ir k r ir ø r n min n hence min n pairwise distinct equivalence class ir n r min n complete set representative proof theorem know k n n n noncommutative ring unit contains non-trivial zero divisor using equivalence normal form characterized follows k n n invertible zero divisor since ab implies b echelon form rank matrix k n n zero divisor invertible hence rank r n equivalence normal form identity matrix let q z g l n k given q az r every matrix v r r r b z v ab q ir k n n r v b since z invertible exercise following exercise k arbitrary field compute echelon form matrix b c e simplicity element denoted k instead k state elementary matrix carry transformation one matrix invertible compute inverse product elementary matrix αβ let k αδ βγ determine echelon form γ δ k n n k b k show let b g l n k b g l k consider matrix k rank equivalence matrix k field rational function cp exercise examine whether invertible determine possible verify result computing show g l n k echelon form k given inverse invertible matrix thus computed via transformation echelon form two matrix b k n called left equivalent exists matrix q g l n k q b show defines equivalence relation k n determine simple representative equivalence class prove lemma determine lu cp theorem matrix r one matrix invertible determine inverse using lu decomposition let hilbert matrix cp matlab-minute definition determine rank lu theorem p determine rank matrix αβ γ dependence α β γ let b k n n given show rank rank b rank ac b c k n n examine inequality strict let b c determine rank ba b let b ba ab show following assertion b b b c b c c b ii λa μb c λm c μm b c λ μ r iii rank b exist λ μ r λ μ λa μb iv rank b  \n",
       "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 chapter linear system equation solving linear system equation central problem linear algebra discus introductory way chapter system arise numerous application engineering natural social science major source linear system equation discretization differential equation linearization nonlinear equation chapter analyze solution set linear system equation characterize number solution using echelon form chap also develop algorithm computation solution definition linear system equation field k n equation unknown xm form xm xm anm xm bn ax b coefficient matrix ai j k n right hand side b bi k given b linear system called homogeneous otherwise x b called solution linear non-homogeneous every x k system x form solution set linear system denote l b next result characterizes solution set l b linear system ax b using solution set l associated homogeneous linear system ax springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi linear system equation lemma let k n b k l b ø given x l b l b x l x z z l proof z l thus x z x l x z x z b b hence x z l b show x l l b z x let l b let x b b z x z x l show l b z l hence x l closer look set l clearly l ø z l λ k z λ z λ hence z l furthermore z z l z z z z z l thus l nonempty subset k hence z closed scalar multiplication addition lemma k n b k k n n l b l sb moreover invertible l b l sb proof x l b also x sb thus x l sb show l b l sb invertible l sb sb multiplying left yield b since l b l sb l b consider linear system equation ax b theorem find matrix g l n k echelon form let b bi sb l b l b lemma linear system ax b take form x bn linear system equation suppose rank r let jr pivot column using rightmultiplication permutation matrix move r pivot column first r column achieved p e e jr e e e e e jr e jr em k yield p ir r k r r permutation lead simplification following presentation usually omitted practical computation b p p x b ay b since p p im write ax form ir yr br yr b r r bn ym apt x b left-multiplication x p mean different ordering unknown xm thus solution ax b easily recovered solution ay b vice versa l b x p l b l b solution determined using extended coefficient matrix b k n note rank obtained attaching b extra column rank b equality br bn rank bn nonzero rank b least one br solution since entry row r n zero rank bn hand rank b br written yr yr ym br linear system equation representation implies particular br l b b order determine lemma know l b b l set br yield l ym yr ym arbitrary yr yr ym l thus r b solution ay b uniquely determined example extended coefficient matrix b rank rank b l b ø b written ay b hence b l b l arbitrary summarizing consideration following algorithm solving linear system equation algorithm let k n b k given determine g l n k echelon form define b sb rank rank b l b l b ø p r rank rank b define l b l b l b b l l b determined well l b p rank b rank since rank rank rank b rank b discussion also yield following result different case solvability linear system equation linear system equation corollary k n b k following assertion hold rank rank b l b ø rank rank b b exists unique solution rank rank b exist many solution field k infinitely many element k q k r k c exist infinitely many pairwise distinct solution different case corollary studied example example let k q consider linear system equation ax b b form b apply gaussian elimination algorithm order transform echelon form b rank rank b hence exist solution pivot column ji p p ax b written linear system equation consequently b l b l b b l l x arbitrary exercise find field k matrix k n k n n b k l b l sb determine l b following b b b r b let α q bα α determine l l bα dependence α let k n b k n denote bi ith column b show linear system equation ax b least one solution x k rank rank rank rank b find condition solution unique linear system equation let n n βn bn αn given βi αi determine recursive formula entry solution linear system ax b  \n",
       "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       chapter determinant matrix determinant map assigns every square matrix r n n r commutative ring unit element map interesting important property instance yield necessary sufficient condition invertibility r n n moreover form basis definition characteristic polynomial matrix chap definition determinant several different approach define determinant matrix use constructive approach via permutation definition let n n given bijective map σ n n j σ j called permutation number n denote set map sn permutation σ sn written form σ σ σ n example lemma know n springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi determinant matrix set sn composition map form group cp exercise sometimes called symmetric group neutral element group permutation n commutative group group sn n noncommutative example consider n permutation definition let n σ sn pair σ σ j j n σ σ j called inversion σ k number inversion σ sgn σ k called sign σ n define sgn short inversion permutation σ pair order term inversion confused inverse map σ exists since σ bijective sign permutation sometimes also called signature example permutation inversion sgn permutation inversion sgn define determinant map definition let r commutative ring unit let n map det r n n r ai j det sgn σ n ai σ called determinant ring element det called determinant formula det called signature formula term sgn σ definition interpreted element ring r either sgn σ r sgn σ r r unique additive inverse unit example n thus det sgn n get det det sgn sgn gottfried wilhelm leibniz definition determinant n sarrus det order compute det using signature formula leibniz form n product n factor large n costly even modern computer see corollary efficient way computing det signature formula mostly theoretical relevance since represents determinant explicitly term entry considering n entry variable interpret det polynomial variable r r r c standard technique analysis show det continuous function entry study group permutation detail permutation σ inversion sgn σ moreover σ σ σ σ σ σ σ j σ j sgn σ observation generalized follows lemma σ sn sgn σ σ j σ j proof n left hand side empty product defined cp sect hold n let n σ sn sgn σ k k number pair σ σ j j σ σ j σ j σ k j σ k j last equation used fact two product factor except possibly order pierre frédéric sarrus determinant matrix theorem sn sgn sgn sgn particular sgn σ sgn σ σ sn proof lemma sgn j j j j σ j σ j j sgn j j j sgn j sgn sgn σ sn sgn n sgn σ σ sgn σ sgn σ sgn σ sgn σ theorem show map sgn homomorphism group sn operation second group standard multiplication integer definition transposition permutation τ sn n exchange exactly two distinct element k n τ k τ k τ j j j n k obviously τ τ every transposition τ sn lemma let τ sn transposition exchange k k τ exactly inversion hence sgn τ proof k j j thus τ given τ k k j k k j k n point denote value τ increasing thus correct order simple counting argument show τ exactly j k inversion  \n",
       "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           property determinant property determinant section prove important property determinant map lemma r n n following assertion hold λ r det λ det λ n λ det ai j upper lower triangular det aii zero row column det n two equal row two equal column det det det proof exercise follows application upper lower triangular matrix zero row column every σ sn least one factor n ai σ equal zero thus det product let row k k ai j equal ak j j let τ sn transposition exchange element k let tn σ sn σ k σ since set tn contains permutation σ sn σ k σ sn tn σ τ σ tn moreover ai σ k ak σ k σ k ak σ σ σ k ak σ k thus using theorem lemma obtain sgn σ n ai σ sgn σ τ ai σ n n ai sgn σ n ai σ determinant matrix implies det sgn σ n ai σ sgn σ n ai σ sgn σ n ai σ proof case two equal column analogous observe first σ n σ n every σ sn see let n fixed σ j σ j thus σ j element first set j σ j j element second set since σ bijective two set equal let ai j bi j bi j ji det sgn σ bi σ sgn σ n aσ sgn σ n sgn σ n n aσ sgn σ n ai ai σ det used sgn σ cp theorem fact sgn σ n ai factor two product aσ example matrix b c obtain det lemma det b det c lemma may also compute determinant using sarrus rule example item lemma show particular det identity matrix en r n n reason determinant map called normalized property determinant σ sn matrix pσ eσ eσ eσ n called permutation matrix associated σ map group sn group permutation matrix r n n bijective inverse permutation matrix transpose cp theorem easily check pσt r n n j r jth column pσ aσ aσ aσ n right-multiplication pσ exchange column according permutation σ hand ai r n ith row aσ pσt aσ n left-multiplication pσt exchange row according permutation σ next study determinant elementary matrix lemma σ sn associated permutation matrix pσ r n n sgn σ det pσ n pi j defined det pi j mi λ g j λ defined respectively det mi λ λ det g j λ proof σ sn ai j r n n j j j n entry zero hence det σ det σ sgn σ n aσ j j sgn σ n σ σ j j sgn σ σ permutation matrix pi j associated transposition exchange j hence det pi j follows lemma since mi λ g j λ lower triangular matrix assertion follows lemma determinant matrix result lead important computational rule determinant lemma r n n n λ r following assertion hold multiplication row λ lead multiplication det λ det mi λ λ det det mi λ det addition row another row change det det g j λ det det g j λ det det g j λ det det g j λ det exchanging two row change sign det det pi j det det pi j det proof mi λ amk amk amk λamk amk hence det n sgn σ σ sgn σ ai σ n σ σ σ λ det g j λ amk amk amk amk j jk λaik j hence det sgn σ j σ j λai σ j n σ j sgn σ n σ λ sgn σ ai σ j n σ j first term equal det second equal determinant matrix two equal column thus equal zero proof matrix g j λ analogous property determinant permutation matrix pi j exchange row j j exchange expressed following four elementary row operation multiply row j add row row j add row j row add row row j therefore pi j g j g j g j j one may verify also carrying matrix multiplication using obtain det pi j det g j g j g j j det g j det g j det g j det j det det since det det cp lemma result lemma row formulated analogously column example consider matrix b simple calculation show det since b obtained exchanging first two column det b det determinant map interpreted map r n r map n column matrix r n n ring ai j r two column ai j det det j ai lemma due property determinant map called alternating map column analogously determinant map alternating map row j form λa μa λ μ r kth row j j n akn r j determinant matrix n det det μa sgn σ λak σ k μak σ k ai σ sgn σ ak σ k n ak σ k μ sgn σ ak σ k λ det μ det n ai σ property called linearity determinant map respect row analogously linearity respect column linear map studied detail later chapter next result called multiplication theorem determinant theorem k field b k n n det ab det det b moreover invertible det det proof theorem know k n n exist invertible elementary st echelon form lemma matrix st det det det det well det ab det ab det det det ab thus also ab zero two case invertible det ab row det implies det hence det ab det det b hand invertible echelon form det give det ab det det b since finally invertible det det det det hence det det since proof relies theorem valid matrix field k formulated theorem b k n n however multiplication theorem determinant also hold matrix commutative ring r unit direct proof based signature formula leibniz found example book advanced linear algebra loehr sect book also contains proof cauchy-binet formula det ab r n b r n n sometimes use det ab det det b  \n",
       "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            property determinant hold b r n n although shown result theorem b k n n proof theorem suggests det easily computed transforming k n n echelon form using elementary row operation corollary k n n let st k n n elementary matrix zero row hence st echelon form either det hence det det det st shown theorem every matrix k n n factorized p lu hence det det p det l det u determinant matrix right hand side easily computed since permutation triangular matrix lu matrix therefore yield efficient way compute det matlab-minute look matrix wilkinson n matlab find general formula entry compute n l u p lu cp matlab-minute definition det l det u det p det p l u det permutation associated computed matrix p det integer odd n minor laplace expansion show determinant used deriving formula inverse invertible matrix solution linear system equation formula however theoretical practical relevance definition let r commutative ring unit let r n n n matrix j r obtained deleting jth row ith column called minor matrix adj bi j r n n bi j j det j called adjunct adjunct also called adjungate classical adjoint term introduced james joseph sylvester determinant matrix theorem r n n n adj adj det particular invertible det r invertible case det det det adj proof let b bi j entry bi j j det j c ci j adj satisfies ci j n bik ak j n det k ak j let column let ek r n n k ek kth column identity matrix exist permutation matrix p q perform k row column exchange respectively q p k k using lemma obtain q det p k k det q det p det k det k det det k det k linearity determinant respect column give ci j n ak j det k det j j det j δi j det thus adj det analogously show adj det minor laplace expansion det r invertible det adj det adj invertible det adj hand invertible det det det det det det used multiplication theorem determinant r cp comment following proof theorem thus det invertible det det det adj example det thus invertible invertible considered element since case det det z det matrix invertible since z invertible note r n n invertible theorem show obtained inverting one ring element det use theorem multiplication theorem matrix commutative ring unit prove result already announced sect order r n n unique inverse r n n one two show need checked equation r n n exists corollary let r n n matrix invertible multiplication theorem determinant yield proof det det det det det det thus also invertible det r invertible det det unique inverse n obvious n shown right theorem multiply equation get analogous proof starting determinant matrix let u summarize invertibility criterion square matrix field shown far g l n k theorem echelon form identity matrix definition clear rank n rank rank b n b k algorithm b b k theorem det alternatively obtain g l n k theorem echelon form least one zero row definition clear rank n rank n algorithm l theorem det field q r c usual absolute value number formulate following useful invertibility criterion matrix theorem k n n k q r c diagonally dominant n j n j det proof prove assertion contraposition showing det implies diagonally dominant det l homogeneous linear system xn let xm equation ax least one solution x entry x maximal absolute value xm x j j x given particular xm mth row amn xn amm xm n j xj j take absolute value side use triangle inequality yield minor laplace expansion xm n j j n j xm hence n j j j diagonally dominant converse theorem hold example matrix det diagonally dominant theorem obtain laplace determinant particularly useful contains many zero entry cp example corollary r n n n following assertion hold n det n j ai j det j laplace expansion det respect ith row j n det n j ai j det j laplace expansion det respect jth column proof two expansion det follow immediately comparison diagonal entry matrix equation det adj det adj laplace expansion allows recursive definition determinant r n n n let det defined corollary choose arbitrary row column formula det contains matrix size use laplace expansion expressing determinant term determinant n n matrix recursively matrix remain r define det finally state cramer give explicit formula solution linear system form determinant rule theoretical value order compute n component solution requires evaluation n determinant n n matrix pierre-simon gabriel laplace published expansion cramer determinant matrix corollary let k field g l n k b k unique solution linear system equation ax b given xn b det adj b x xi det b det example consider q b laplace expansion respect last column yield det det det thus invertible ax b unique solution x b cramer rule following entry det det det det det det det det minor laplace expansion exercise permutation σ sn called r exists subset ir n r element σ k k r σ ir σ ir write r σ ir particular transposition τ sn let n given compute b let n σ determine σ j j c show inverse cycle ir given ir show two cycle disjoint element ir j ir j ø commute e show every permutation σ sn written product disjoint cycle except order uniquely determined σ prove lemma using show group homomorphism sgn sn satisfies following assertion set σ sn sgn σ subgroup sn cp exercise b σ π sn π σ π compute determinant following matrix en zn n ei ith column identity matrix b b bi j zn n bi j c c eπ π e π π πe e determinant matrix wilkinson matrix cp matlab-minute end sect construct matrix b rn n n det b det det b let r commutative ring unit n r n n show following assertion hold b c e f g adj adj ab adj b adj b r n n invertible adj λa adj λ adj adj det adj det invertible adj adj det adj adj invertible one drop requirement invertibility b e let n ai j rn n ai j xi xn j yn hence particular xi j j matrix called cauchy show det x j x j x j yi b use derive formula determinant n n hilbert matrix cp matlab-minute definition let r commutative ring unit αn r n n n r αn vn called vandermonde show det vn james hardy wilkinson louis cauchy alexandre-théophile vandermonde augustin α j αi minor laplace expansion b let k field let k set polynomial variable degree n show two polynomial p q k equal exist pairwise distinct βn k p β j q β j show following assertion let k field let k n n n odd det b g l n r det let k field k n n k n n k n n k n n show following assertion g l n k det det det b g l n k det det det c det det det show also matrix defined commutative ring unit construct matrix rn n n det det det det det let ai j g l n r ai j z j show following assertion hold qn n b zn n det c linear system equation ax b unique solution x every b det show g zn n det subgroup g l n q  \n",
       "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                chapter characteristic polynomial eigenvalue matrix already characterized matrix using rank determinant chapter use determinant map order assign every square matrix unique polynomial called characteristic polynomial matrix polynomial contains important information matrix example one read determinant thus see whether matrix invertible even important root characteristic polynomial called eigenvalue matrix characteristic polynomial cayley-hamilton theorem let r commutative ring unit let r corresponding ring polynomial cp example ai j r n n set r n n n ann entry matrix element commutative ring unit r diagonal entry polynomial degree entry constant polynomial using definition form determinant matrix element r springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi characteristic polynomial eigenvalue matrix definition let r commutative ring unit r n n pa det r called characteristic polynomial example n pa det det n obtain pa det using definition see general form pa matrix r n n given n sgn σ δi σ ai σ pa following lemma present basic property characteristic polynomial lemma r n n pa pat pa n n aii det proof using lemma obtain pa det det det pat using pa see n pa n aii sgn σ n δi σ ai σ characteristic polynomial cayley-hamilton theorem first term right hand side form n aii polynomial degree n n second term polynomial degree n thus claimed moreover definition yield aii pa det n det det lemma show characteristic polynomial r n n always degree coefficient n polynomial called monic coefficient given sum diagonal entry quantity called trace n trace aii following lemma show every monic polynomial p r degree n exists matrix r n n pa lemma n n p n r p characteristic polynomial matrix r n n n matrix called companion matrix proof prove assertion induction n p pa det let assertion hold n consider p βn n r characteristic polynomial eigenvalue matrix using laplace expansion respect first row cp corollary induction hypothesis get pa det det βn det det βn n βn βn n example polynomial p z companion matrix identity matrix characteristic polynomial det pa thus different matrix may characteristic polynomial example seen evaluate polynomial p r scalar λ analogously evaluate p matrix r cp exercise p βn n r define p βn n im r multiplication right hand side scalar multiplication β j r j r j recall im evaluating given polynomial matrix r therefore defines map r r characteristic polynomial cayley-hamilton theorem particular using characteristic polynomial pa r n n satisfies n pa sgn σ δi σ ai σ im r note r n n pa det obvious equation pa det wrong definition pa r n n det r two expression even n following result called cayley-hamilton theorem every matrix r n n characteristic polynomial pa r pa r n n proof n pa pa let n let ei ith column identity matrix r n n aei ani en n equivalent n aii ei ji e j last n equation written bε ann en hence b r n n r p p r r n n set r form commutative ring unit given identity matrix cp exercise using theorem obtain adj b b det b claimed verified n feel necessary give proof general sir william rowan hamilton proved theorem case n context investigation quaternion one first proof general n given ferdinand georg frobenius james joseph sylvester coined name theorem calling no-little-marvelous hamilton-cayley theorem arthur cayley showed theorem n  \n",
       "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   characteristic polynomial eigenvalue matrix det b r identity matrix r n n matrix n time identity matrix diagonal multiplying equation right ε yield adj b bε det b ε implies det b r n n finally using lemma give n det b δi σ aσ sgn σ n δσ aσ sgn σ pat pa completes proof eigenvalue eigenvectors section present introduction topic eigenvalue eigenvectors square matrix field k concept studied detail later chapter definition let k n n λ k v k satisfy av λv λ called eigenvalue v called eigenvector corresponding λ definition v never eigenvector matrix λ may eigenvalue example v eigenvector corresponding eigenvalue λ α k αv αv α av α λv λ αv thus also αv eigenvector corresponding λ eigenvalue eigenvectors theorem k n n following assertion hold λ eigenvalue λ root characteristic polynomial pa λ k λ eigenvalue det λ eigenvalue λ eigenvalue proof equation pa λ det λin hold matrix λin invertible cp equivalent l λin x however mean exists vector x λin x x λ eigenvalue pa assertion follows pa n det cp lemma follows pa pat cp lemma whether matrix k n n eigenvalue may depend field k considered example matrix characteristic polynomial pa r polynomial root since equation real solution consider element pa c root two complex number eigenvalue item theorem show eigenvalue eigenvector however may eigenvector example matrix characteristic polynomial pa hence eigenvalue λ characteristic polynomial eigenvalue matrix λ thus eigenvector corresponding eigenvalue eigenvector hand λ λ thus eigenvector corresponding eigenvalue eigenvector theorem implies criterion invertibility k n n cp g l n k eigenvalue root pa definition two matrix b k n n called similar exists matrix z g l n k z b z one easily show defines equivalence relation set k n n cp proof following definition theorem two matrix b k n n similar pa pb proof z b z multiplication theorem determinant yield pa det det z b z det z b z det z det b det z det b det z z pb cp remark theorem theorem theorem show two similar matrix eigenvalue condition b similar sufficient necessary pa pb example let pa pb every matrix z g l n k z b z thus pa pb although b similar cp also example  \n",
       "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  eigenvalue eigenvectors matlab-minute root polynomial p αn n computed approximated matlab using command root p p n matrix entry p n compute root p monic polynomial p r display output using format long exact root p large numerical error computation root using root p form matrix p compare structure one companion matrix lemma transfer proof lemma structure matrix compute eigenvalue command eig compare output one root p observe eigenvectors stochastic matrix consider eigenvalue problem presented sect context pagerank algorithm mathematical modeling lead equation written form ax x ai j rn n n number document satisfies n ai j ai j j matrix called column-stochastic note column-stochastic row-stochastic matrix also occurred car insurance application considered sect example want determine x xn ax x entry xi describes importance document importance value nonnegative xi thus want determine entrywise nonnegative eigenvector corresponding eigenvalue λ first check whether problem solution study whether solution unique presentation based article lemma column-stochastic matrix rn n eigenvector corresponding eigenvalue proof since column-stochastic eigenvalue theorem show also eigenvalue hence exists corresponding eigenvector characteristic polynomial eigenvalue matrix matrix real entry called positive entry positive lemma rn n positive column-stochastic x eigenvector corresponding eigenvalue either x positive proof x xn eigenvector ai j corresponding eigenvalue n xi ai j x j suppose entry x positive entry x negative exists least one index k n ak j x j n ak j j implies n n n n n ai j j n n n j ai j j ai j j impossible indeed x must positive prove following uniqueness result theorem rn n positive column-stochastic exists xi ax unique positive x xn proof lemma least one positive eigenvector corresponding eigenvalue suppose x xn x xn j two eigenvectors suppose normalized xi j assumption made without loss generality since every nonzero multiple eigenvector still eigenvector show x x α r define x α x αx ax α ax α ax x αx x α α equal zero thus α first entry x lemma x α eigenvector corresponding eigenvalue ax α x α implies x α hence α xi xi eigenvectors stochastic matrix summing n equation yield n xi α n xi α get xi xi n therefore x x unique positive eigenvector x theorem called perron eigenvector positive matrix theory eigenvalue eigenvectors positive general nonnegative matrix important area matrix theory since matrix arise many application construction matrix rn n pagerank algorithm columnstochastic positive since usually many entry ai j order obtain uniquely solvable problem one use following trick let si j rn n si j obviously positive columnstochastic real number α define matrix α α αs matrix positive column-stochastic hence unique positive eigenvector u corresponding eigenvalue thus α u α u α u u α u n large number document entire internet number small α u u therefore solution eigenvalue problem u α u small α potentially give good approximation u satisfies au u practical solution eigenvalue problem matrix α topic field numerical linear algebra matrix represents link structure document mutually linked thus document equally important matrix α α αs therefore model following internet surfing behavior user follows proposed link probability arbitrary link probability α originally google used value α oskar perron characteristic polynomial eigenvalue matrix exercise following exercise k arbitrary field determine characteristic polynomial following matrix q verify cayley-hamilton theorem case direct computation two matrix b c similar let r commutative ring unit n show every g l n r exists polynomial p r degree n adj p conclude q hold polynomial q r degree n b let r n n apply theorem matrix r n n derive alternative proof cayley-hamilton theorem formula det adj let k n n matrix ak k matrix called nilpotent show λ eigenvalue b determine pa show n hint may assume pa form λn k c show μin invertible μ k show determine eigenvalue corresponding eigenvectors following matrix r b c difference consider b c matrix c let n ε consider matrix ε ε eigenvectors stochastic matrix element cn n determine eigenvalue dependence ε many pairwise distinct eigenvalue ε determine eigenvalue corresponding eigenvectors b simplicity element denoted k instead k let k n n b k n c k n rank c ac c b show every eigenvalue b eigenvalue show following assertion trace λ μb λ trace μ trace b hold λ μ k b k n n b trace ab trace b hold b k n n c b k n n similar trace trace b prove disprove following statement exist matrix b k n n trace ab trace trace b b exist matrix b k n n ab b suppose matrix ai j cn n real entry ai j show λ eigenvalue corresponding eigenvector v νn also λ eigenvalue corresponding eigenvector v ν ν n  \n",
       "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    chapter vector space previous chapter focussed matrix property defined algebraic operation matrix derived important concept associated including rank determinant characteristic polynomial eigenvalue chapter place concept abstract framework introducing idea vector space matrix form one important example vector space property certain namely finite dimensional vector space studied transparent way using matrix next chapter study linear map vector space connection matrix play central role well basic definition property vector space begin definition vector space field k definition let k field vector space k shortly k space set v two operation v v v v w v w addition k v v λ v λ v scalar multiplication satisfy following v commutative group v w v λ μ k following assertion hold b c λ μ v λμ v λ v w λ v λ λ μ v λ v μ springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi vector space element v v called element λ k called scalar usually omit sign scalar multiplication usually write λv instead λ clear context important field using often omit explicit reference k simply write vector space instead k space example set k n matrix addition scalar multiplication form k space obvious reason element k k sometimes called column row vector respectively set k form k space addition defined example usual addition polynomial scalar multiplication p αn n k defined λ p λ λ λ αn n continuous real valued function defined real interval α β pointwise addition scalar multiplication f g x f x g x λ f x λ f x form r-vector space shown using addition two continuous function well multiplication continuous function real number yield continuous function since definition v commutative group already know vector space property theory group cp chap particular every vector space contains unique neutral element respect addition called null vector every vector v v unique additive inverse v v v v usual write v w instead v lemma let v k space k neutral null element k v respectively following assertion hold k v v λ λ k λ v v λ v v λ k term introduced sir william rowan hamilton context quaternion motivated latin verb vehi vehor vectus sum mean ride drive also term scalar introduced hamilton see footnote scalar multiplication basic definition property vector space proof v v k v k k v k v k adding k v side identity give k λ k λ λ λ λ adding λ side identity give λ λ k v v λ v v λ λ v k v well λ v λ λ v v λ following write instead k clear null element meant group ring field identify substructure vector space vector space definition let v k space let u u k space called subspace v substructure must closed respect given operation addition scalar multiplication lemma u subspace k space v ø u v following assertion hold v w u v w u λv u λ k v u proof exercise example every vector space v trivial subspace u v u let k n u l k u solution set homogeneous linear system ax u u empty v w u v w av aw v w u furthermore λ k λ v λ av λ λv u hence u subspace k every n set k p k deg p n subspace k definition let v k space n n vn vector form n λi vi v λn vn  \n",
       "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              vector space called linear combination vn coefficient λn k linear span vn set span vn n λi vi λn k let set suppose every vector vm let set vector called system vector denoted vm linear span system vm denoted span vm defined set vector v v linear combination finitely many vector system definition consistently extended case n case vn list length zero empty list define empty sum vector v obtain span vn span ø following consider list vector vn set vector vn usually mean n case empty list associated zero vector space v sometimes discussed separately example vector space k k spanned vector set k form subspace k spanned vector lemma v vector space vn v span vn subspace proof clear ø span vn furthermore span vn definition closed respect addition scalar multiplication lemma satisfied base dimension vector space discus central theory base dimension vector space start concept linear independence definition let v k space vector vn v called linearly independent equation n λi vi λn k λi vi always implies λn otherwise hold scalar λn k equal zero vector vn called linearly dependent base dimension vector space empty list linear independent set every vector vm v corresponding system vm called linearly independent finitely many vector system always linearly independent sense otherwise system called linearly dependent vector vn linearly independent zero vector linearly combined trivial way vn consequently one vector zero vector vn linearly dependent single vector v linearly independent v following result give useful characterization linear independence finitely many least two given vector lemma vector vn n linearly independent vector vi n written linear combination others proof prove assertion contraposition vector vn linearly dependent n λi vi least one scalar λ j equivalently vj n j λi vi j v j linear combination vector using concept linear independence define concept basis vector space definition let v vector space set vn v called basis v vn linearly independent span vn set ø basis zero vector space v let set suppose every vector vm set vm called basis v corresponding system vm linearly independent span vm short basis linearly independent spanning set vector space example let e j k n matrix entry position j entry cp sect set vector space e j n j basis vector space k n cp example matrix e j k n n j linearly independent since n λi j e j λi j implies λi j n j ai j k n n ai j e j hence span e j n j k n basis called canonical standard basis vector space k n denote canonical basis vector k en vector also called unit vector n column identity matrix basis vector space k cp example given set since corresponding system linearly independent every polynomial p k linear combination finitely many vector system next result called basis extension theorem theorem let v vector space let vr v r vr linearly independent span vr v set vr extended basis v using vector set proof note r list vr empty hence linearly independent due definition prove assertion induction span vr v linear independence vr show set basis v base dimension vector space let assertion hold suppose vr v given vr linearly independent span vr vr already basis v done suppose therefore span vr exists least one j j span vr particular w j w j λw j r λi vi implies λ otherwise would w j span vr therefore λr due linear independence vr thus vr w j linearly independent induction hypothesis extend set vr w j basis v using vector set w j contains element example consider vector space v k cp example vector vector linearly independent basis v since span example vector element v span span extend get linearly independent vector indeed span thus basis basis extension theorem every vector space spanned finitely many vector basis consisting finitely many element central result theory vector space every basis number element order show result first prove following exchange lemma λi vi lemma let v vector space let vm v let w v span w vm span vm proof assumption λi vi span vm say γi vi λi vi w γi vi γi λi vi span w vm hand w αi vi span w vm vector space λi vi αi vi λi αi vi span vm thus span w vm span vm using lemma prove exchange theorem theorem let w wn u u u finite subset vector space let wn linearly independent w span u u n n element u numbered appropriately element u u n exchanged n element w way span wn u u span u u n u u λi u scalar λm proof assumption zero otherwise contradicts linear independence wn appropriate renumbering lemma yield span u u span u u u suppose r r n exchanged vector u u r wr span wr u r u span u u r u r u clear r assumption wr span u u thus wr r λi wi λi u scalar λm one scalar λr λm must nonzero otherwise wr span wr contradicts linear independence wm appropriate renumbering λr lemma yield span wr u r u span wr u r u continue construction r n obtain literature theorem sometimes called steinitz exchange theorem ernst steinitz result first proved hermann günther graßmann base dimension vector space span wn u u span u u n u u particular n using fundamental theorem following result unique number basis element simple corollary corollary vector space v spanned finitely many vector v basis consisting finitely many element two base v number element proof assertion clear v cp definition let v span vm theorem extend span using element vm basis thus v basis finitely many element let u u u w wk two base w v span u u theorem k u v span wk theorem k thus define dimension vector space definition exists basis k space v consists finitely many element v called finite dimensional unique number basis element called dimension denote dimension dim k v dim v clear field meant v spanned finitely many vector v called infinite dimensional write dim k v note zero vector space v basis ø thus dimension zero cp definition v finite dimensional vector space vm v dim v vector vm must linearly dependent vector linearly independent could extend via theorem basis v would contain dim v element example set form basis vector space k n basis n element hence dim k n n hand vector space k spanned finitely many vector cp example hence infinite dimensional example let v vector space continuous real valued function real interval cp example define n function f n v  \n",
       "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            vector space f n x j x n n x n x every linear combination k x n n x λ j f j continuous function value λ j thus equation k λ j f j v implies λ j must zero f f k v linearly independent k consequently dim v coordinate change basis study linear combination basis vector finite dimensional vector space particular study happens linear combination change another basis vector space lemma vn basis k space v every v v exist uniquely determined scalar λn k v λn vn scalar called coordinate v respect basis vn proof let v λi vi μi vi scalar λi μi k n n λi μi vi coordinate change basis linear independence vn implies λi μi definition coordinate vector depend given basis particular depend ordering numbering basis vector author distinguish basis set collection element without particular ordering ordered basis book keep set notation basis vn index indicate ordering basis vector let v k space vn v need linearly independent v λn vn coefficient λn k let u write vn λn vn λn vn n-tuple v vn v n v n time n v skip parenthesis write v instead v notation formally defines multiplication map v n k α k α v α α λn vn vn αλn μn k u μn vn vn μn v u λn μn vn vn λn μn vector space show vector given linear combination operation scalar multiplication addition correspond operation coefficient vector respect linear combination extend notation let ai j k n let j u j vn j j write linear combination u u system u u vn side equation element v right-multiplication arbitrary n-tuple vn v n matrix k n thus corresponds forming linear combination vector vn corresponding coefficient given entry formally defines multiplication map v n k n v lemma let v k space let vn v linearly independent let k n let u u vn vector u u linearly independent rank proof exercise consider also matrix b bi j k using obtain u u b vn lemma previous notation vn b vn ab proof exercise let vn wn base v let v lemma exist unique coordinate λn μn respectively v vn wn λn μn describe method transforming coordinate λn respect basis vn coordinate μn respect basis wn vice versa coordinate change basis every basis vector v j j n exist unique coordinate pi j n j v j wn j pn j defining p pi j k n n write n equation vector v j analogous vn wn way every basis vector w j j n exist unique coordinate qi j n j w j vn j qn j set q qi j k n n analogously get wn vn q thus wn vn q wn p q wn p q implies wn p q mean n linear combination basis vector wn corresponding coordinate given entry n column p q equal zero vector since basis vector linearly independent coordinate must zero hence p q k n n p q analogously obtain equation q p therefore matrix p k n n invertible p q furthermore v vn wn p wn p λn λn λn  \n",
       "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 vector space due uniqueness coordinate v respect basis wn obtain p p μn λn λn μn hence multiplication matrix p transforms coordinate v respect basis vn respect basis wn multiplication p yield inverse transformation therefore p p called coordinate transformation matrix summarize result obtained follows theorem let vn wn base k space uniquely determined matrix p k n n invertible yield coordinate transformation vn wn v vn vn λn μn p μn λn example consider vector space v r entrywise addition scalar multiplication basis v given set another basis v set corresponding coordinate transformation matrix obtained defining equation p q relation vector space dimension first result describes relation vector space subspace lemma v finite dimensional vector space u v subspace dim u dim v equality u v relation vector space dimension proof let u v let u u basis u u u ø u using theorem extend set basis u proper subset v least one basis vector need added hence dim u dim v u v every basis v also basis u thus dim u dim v subspace vector space v intersection given u v u u cp definition sum two subspace defined u u v u u lemma subspace vector space v following assertion hold subspace equality proof exercise important result following dimension formula subspace theorem finite dimensional subspace vector space v dim dim dim dim proof let vr basis extend set basis vr basis vr xk assume r k one list empty following argument easily modified suffices show vr xk basis obviously span vr xk hence suffices show vr xk linearly independent let r k λi vi μi wi γi xi vector space k γi xi r λi vi μi wi left hand side equation definition vector γi xi construction right hand side vector therefore vector vr however vr basis μi wi implies linearly independent therefore also r λi vi k γi xi hence λr γk due linear independence vr xk least one subspace theorem infinite dimensional assertion still formally correct since case dim dim dim example subspace k k k dim dim k dim k dim definition sum extended arbitrary finite number subspace uk k subspace vector space v define uk k u j k u j u j u j j k sum called direct ui k u j k case write direct sum relation vector space dimension uk k uj particular sum two subspace v direct following theorem present two equivalent characterization direct sum subspace theorem u uk sum k subspace vector space v following assertion equivalent sum u direct ui u j every vector u u representation form u u j uniquely determined u j u j j u j u j u j j k implies u j j proof let u u j u j u j u j u j j every k ui ui u j u j ui uj u hence u u ui u j implies u obvious u j u given let u u j u particular implies u j u j j hence j u thus ui u j exercise following exercise k arbitrary field following set usual addition scalar multiplication r-vector space determine possible basis dimension determine basis r-vector space c dimr c determine basis c-vector space c dimc c show k linearly independent det vector space let v k space nonempty set map v set map show map v operation map v map v map v f g f g f g x f x g x x k map v map v λ f λ f λ f x λ f x x k space show function sin co map r r linearly independent let v vector space n dim v n let vn show following statement equivalent vn linearly independent span vn vn basis show k n k space cp example find subspace k space show k k space cp example show k subspace k cp example determine dim k show polynomial linearly independent q extend basis q let n n n j αi j αi j k k element k called bivariate polynomial k unknown define scalar multiplication addition k becomes vector space determine basis k show lemma let k n b k solution set l b ax b subspace k let k n n let λ k eigenvalue show set v k av λv subspace k let k n n let two eigenvalue show two associated eigenvectors linearly independent show b c relation vector space dimension base vector space k determine corresponding coordinate transformation matrix examine element following set linear independence vector space k determine dimension subspace spanned element one set basis k show set sequence αi q n entrywise addition scalar multiplication form infinite dimensional vector space determine basis system prove lemma prove lemma prove lemma let finite dimensional subspace vector space show sum direct dim dim dim let uk k finite dimensional subspace vector space suppose ui u j j sum uk direct let u subspace finite dimensional vector space show u u subspace u called exists another subspace u complement u determine three subspace v v subspace v uniquely determined complement  \n",
       "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      appendix short introduction matlab interactive software system numerical computation simulation visualization contains large number predefined function allows user implement program so-called m-files name matlab originates matrix laboratory indicates matrix orientation software indeed matrix major object matlab due simple intuitive use matrix consider matlab well suited teaching field linear algebra short introduction explain important way enter operate matrix matlab one learn essential matrix operation well important algorithm concept context matrix linear algebra general actively using matlab-minutes book use predefined function matrix matlab entered form list entry enclosed square bracket entry list ordered row natural order index top bottom left right new row start every semicolon example matrix entered matlab typing semicolon matrix suppresses output matlab omitted matlab writes entered computed quantity example entering registered trademark mathworks springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi appendix short introduction matlab matlab give output one access part matrix corresponding index list index k abbreviated k colon mean row given column index column given row index example matrix matrix matrix several predefined function produce matrix particular given positive integer n eye n identity matrix zero n one n n matrix zero n matrix one rand n n random matrix several matrix appropriate size combined new matrix example command b c lead e help function matlab started command help order get information specific function one add name function example appendix short introduction matlab input help ops information operation operator matlab particular addition multiplication transposition help matfun matlab function operate matrix help gallery collection example matrix help det determinant help expm matrix exponential function  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data['by_page_body'].groupby(\n",
    "        ['section_level_1', 'section_level_2', 'section_level_3']\n",
    "    )['clean_content'].apply(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>page_number</th>\n",
       "      <th>real_page_num</th>\n",
       "      <th>section_level_1</th>\n",
       "      <th>section_level_2</th>\n",
       "      <th>section_level_3</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chapter 1\\n\\nLinear Algebra in Every Day Life\\n\\nOne has to familiarize the student with actual questions from applications, so that he learns to deal with real world problems.1\\nLothar Collatz (1910–1990)\\n\\n1.1 The PageRank Algorithm\\nThe PageRank algorithm is a method to assess the “importance” of documents with mutual links, such as web pages, on the basis of the link structure.\\nIt was developed by Sergei Brin and Larry Page, the founders of Google Inc., at Stanford University in the late 1990s.\\nThe basic idea of the algorithm is the following:\\nInstead of counting links, PageRank essentially interprets a link of page A to page\\nB as a vote of page A for page B. PageRank then assesses the importance of a page by the number of received votes.\\nPageRank also considers the importance of the page that casts the vote, since votes of some pages have a higher value, and thus also assign a higher value to the page they point to.\\nImportant pages will be rated higher and thus lead to a higher position in the search results.2\\nLet us describe (model) this idea mathematically.\\nOur presentation uses ideas from the article [BryL06].\\nFor a given set of web pages, every page k will be assigned an importance value xk ≥ 0.\\nA page k is more important than a page j if xk &gt; x j .\\nIf a page k has a link to a page j, we say that page j has a backlink from page k.\\nIn the above description these backlinks are the votes.\\nAs an example, consider the following link structure:\\n\\n1 “Man muss den Lernenden mit konkreten Fragestellungen aus den Anwendungen vertraut machen, dass er lernt, konkrete Fragen zu behandeln.” of a text found in 2010 on http://www.google.de/corporate/tech.html.\\n\\n2 Translation\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_1\\n\\n1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>&lt;FEFF&gt;</td>\n",
       "      <td>1 Linear Algebra in Every Day Life</td>\n",
       "      <td>1.1 The PageRank Algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter linear algebra every day life one familiarize student actual question application learns deal real world lothar collatz pagerank algorithm pagerank algorithm method ass importance document mutual link web page basis link structure developed sergei brin larry page founder google stanford university late basic idea algorithm following instead counting link pagerank essentially interprets link page page b vote page page pagerank ass importance page number received vote pagerank also considers importance page cast vote since vote page higher value thus also assign higher value page point important page rated higher thus lead higher position search let u describe model idea mathematically presentation us idea article given set web page every page k assigned importance value xk page k important page j xk x j page k link page j say page j backlink page description backlinks vote example consider following link structure man mus den lernenden mit konkreten fragestellungen au den anwendungen vertraut machen das er lernt konkrete fragen zu text found http translation springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2\\n\\n1 Linear Algebra in Every Day Life\\n\\nHere the page 1 has links to the pages 2, 3 and 4, and a backlink from page 3.\\nThe easiest approach to define importance of web pages is to count its backlinks; the more votes are cast for a page, the more important the page is.\\nIn our example this gives the importance values x1 = 1, x2 = 3, x3 = 2, x4 = 3.\\nThe pages 2 and 4 are thus the most important pages, and they are equally important.\\nHowever, the intuition and also the above description from Google suggests that backlinks from important pages are more important for the value of a page than those from less important pages.\\nThis idea can be modeled by defining xk as the sum of all importance values of the backlinks of the page k.\\nIn our example this results in four equations that have to be satisfied simultaneously, x1 = x3 , x2 = x1 + x3 + x4 , x3 = x1 + x4 , x4 = x1 + x2 + x3 .\\nA disadvantage of this approach is that it does not consider the number of links of the pages.\\nThus, it would be possible to (significantly) increase the importance of a page just by adding links to that page.\\nIn order to avoid this, the importance values of the backlinks in the PageRank algorithm are divided by the number of links of the corresponding page.\\nThis creates a kind of “internet democracy”: Every page can vote for other pages, where in total it can cast one vote.\\nIn our example this gives the equations x1 = x3 x4 x4 x3 x1 x1 x1 x3\\n, x2 =\\n+\\n+ , x3 =\\n+ , x4 =\\n+ x2 + .\\n3\\n3\\n3\\n2\\n3\\n2\\n3\\n3\\n\\n(1.1)\\n\\nThese are four equations for the four unknowns, and all equations are linear,3 i.e., the unknowns occur only in first power.\\nIn Chap.\\n6 we will see how to write the equations in (1.1) in form of a linear system of equations.\\nAnalyzing and solving such systems is one of the most important tasks of Linear Algebra.\\nThe example of the PageRank algorithm shows that Linear Algebra presents a powerful modeling\\n3 The term “linear” originates from the Latin word “linea”, which means “(straight) line”, and\\n“linearis” means “consisting of (straight) lines”.</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1 Linear Algebra in Every Day Life</td>\n",
       "      <td>1.1 The PageRank Algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear algebra every day life page link page backlink page easiest approach define importance web page count backlinks vote cast page important page example give importance value page thus important page equally important however intuition also description google suggests backlinks important page important value page le important page idea modeled defining xk sum importance value backlinks page example result four equation satisfied simultaneously disadvantage approach consider number link page thus would possible significantly increase importance page adding link page order avoid importance value backlinks pagerank algorithm divided number link corresponding page creates kind internet democracy every page vote page total cast one vote example give equation four equation four unknown equation unknown occur first power chap see write equation form linear system equation analyzing solving system one important task linear algebra example pagerank algorithm show linear algebra present powerful modeling term linear originates latin word linea mean straight line linearis mean consisting straight line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.1 The PageRank Algorithm\\n\\n3 tool: We have turned the real world problem of assessing the importance of web pages into a problem of Linear Algebra.\\nThis problem will be examined further in\\nSect.\\n8.3.\\nFor completeness, we mention that a solution for the four unknowns (computed with MATLAB and rounded to the second significant digit) is given by x1 = 0.14, x2 = 0.54, x3 = 0.41, x4 = 0.72.\\nThus, page 4 is the most important one.\\nIt is possible to multiply the solution, i.e., the importance values xk , by a positive constant.\\nSuch a multiplication or scaling is often advantageous for computational methods or for the visual display of the results.\\nFor example, the scaling could be used to give the most important page the value 1.00.\\nA scaling is allowed, since it does not change the ranking of the pages, which is the essential information provided by the PageRank algorithm.\\n\\n1.2 No Claim Discounting in Car Insurances\\nInsurance companies compute the premiums for their customers on the basis of the insured risk: the higher the risk, the higher the premium.\\nIt is therefore important to identify the factors that lead to higher risk.\\nIn the case of a car insurance these factors include the number of miles driven per year, the distance between home and work, the marital status, the engine power, or the age of the driver.\\nUsing such information, the company calculates the initial premium.\\nUsually the best indicator for future accidents, and hence future insurance claims, is the number of accidents of the individual customer in the past, i.e., the claims history.\\nIn order to incorporate this information into the premium rates, insurers establish a system of risk classes, which divide the customers into homogeneous risk groups with respect to their previous claims history.\\nCustomers with fewer accidents in the past get a discount on their premium.\\nThis approach is called a no claims discounting scheme.\\nFor a mathematical model of this scheme we need a set of risk classes and a transition rule for moving between the classes.\\nAt the end of a policy year, the customer may move to a different class depending on the claims made during the year.\\nThe discount is given in percent of the premium in the initial class.\\nAs a simple example we consider four risk classes,\\nC1 C2 C3 C4\\n% discount 0 10 20 40 and the following transition rules:\\n• No accident: Step up one class (or stay in C4 ).</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1 Linear Algebra in Every Day Life</td>\n",
       "      <td>1.2 No Claim Discounting in Car Insurances</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pagerank algorithm tool turned real world problem assessing importance web page problem linear algebra problem examined sect completeness mention solution four unknown computed matlab rounded second significant digit given thus page important one possible multiply solution importance value xk positive constant multiplication scaling often advantageous computational method visual display result example scaling could used give important page value scaling allowed since change ranking page essential information provided pagerank algorithm claim discounting car insurance insurance company compute premium customer basis insured risk higher risk higher premium therefore important identify factor lead higher risk case car insurance factor include number mile driven per year distance home work marital status engine power age driver using information company calculates initial premium usually best indicator future accident hence future insurance claim number accident individual customer past claim history order incorporate information premium rate insurer establish system risk class divide customer homogeneous risk group respect previous claim history customer fewer accident past get discount premium approach called claim discounting scheme mathematical model scheme need set risk class transition rule moving class end policy year customer may move different class depending claim made year discount given percent premium initial class simple example consider four risk class discount following transition rule accident step one class stay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4\\n\\n1 Linear Algebra in Every Day Life\\n\\n• One accident: Step back one class (or stay in C1 ).\\n• More than one accident: Step back to class C1 (or stay in C1 ).\\nNext, the insurance company has to estimate the probability that a customer who is in the class Ci in this year will move to the class C j .\\nThis probability is denoted by pi j .\\nLet us assume, for simplicity, that the probability of exactly one accident for every customer is 0.1, i.e., 10 %, and the probability of two or more accidents for every customer is 0.05, i.e., 5 %.\\n(Of course, in practice the insurance companies determine these probabilities in dependence of the classes.)\\nFor example, a customer in the class C1 will stay in C1 in case of at least one accident.\\nThis happens with the probability 0.15, so that p11 = 0.15.\\nA customer in\\nC1 has no accident with the probability 0.85, so that p12 = 0.85.\\nThere is no chance to move from C1 to C3 or C4 in the next year, so that p13 = p14 = 0.00.\\nIn this way we obtain 16 values pi j , i, j = 1, 2, 3, 4, which we can arrange in a 4 × 4 matrix as follows:\\n⎡\\n⎤\\n⎡\\n⎤ p11 p12 p13 p14\\n0.15 0.85 0.00 0.00\\n⎢ p21 p22 p23 p24 ⎥\\n⎢0.15 0.00 0.85 0.00⎥\\n⎢\\n⎥\\n⎢\\n⎥\\n(1.2)\\n⎣ p31 p32 p33 p34 ⎦ = ⎣0.05 0.10 0.00 0.85⎦.\\np41 p42 p43 p44\\n0.05 0.00 0.10 0.85\\nAll entries of this matrix are nonnegative real numbers, and the sum of all entries in each row is equal to 1.00, i.e., pi1 + pi2 + pi3 + pi4 = 1.00 for each i = 1, 2, 3, 4.\\nSuch a matrix is called row-stochastic.\\nThe analysis of matrix properties is a central topic of Linear Algebra that is developed throughout this book.\\nAs in the example with the PageRank algorithm, we have translated a practical problem into the language of Linear Algebra, and we can now study it using Linear Algebra techniques.\\nThis example of premium rates will be discussed further in Example 4.7.\\n\\n1.3 Production Planning in a Plant\\nThe production planning in a plant has to consider many different factors, in particular commodity prices, labor costs, and available capital, in order to determine a production plan.\\nWe consider a simple example:\\nA company produces the products P1 and P2 .\\nIf xi units of the product Pi are produced, where i = 1, 2, then the pair (x1 , x2 ) is called a production plan.\\nSuppose that the raw materials and labor for the production of one unit of the product Pi cost a1i and a2i Euros, respectively.\\nIf b1 Euros are available for the purchase of raw materials and b2 Euros for the payment of labor costs, then a production plan must</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1 Linear Algebra in Every Day Life</td>\n",
       "      <td>1.3 Production Planning in a Plant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear algebra every day life one accident step back one class stay one accident step back class stay next insurance company estimate probability customer class ci year move class c j probability denoted pi j let u assume simplicity probability exactly one accident every customer probability two accident every customer course practice insurance company determine probability dependence class example customer class stay case least one accident happens probability customer accident probability chance move next year way obtain value pi j j arrange matrix follows entry matrix nonnegative real number sum entry row equal matrix called row-stochastic analysis matrix property central topic linear algebra developed throughout book example pagerank algorithm translated practical problem language linear algebra study using linear algebra technique example premium rate discussed example production planning plant production planning plant consider many different factor particular commodity price labor cost available capital order determine production plan consider simple example company produce product xi unit product pi produced pair called production plan suppose raw material labor production one unit product pi cost euro respectively euro available purchase raw material euro payment labor cost production plan must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.3 Production Planning in a Plant\\n\\n5 satisfy the constraint inequalities a11 x1 + a12 x2 ≤ b1 , a21 x1 + a22 x2 ≤ b2 .\\nIf a production plan satisfies these constraints, it is called feasible.\\nLet pi be the profit from selling one unit of product Pi .\\nThen the goal is to determine a production plan that maximizes the profit function\\n\u0002(x1 , x2 ) = p1 x1 + p2 x2 .\\nHow can we find this maximum?\\nThe two equations a11 x1 + a12 x2 = b1 and a21 x1 + a22 x2 = b2 describe straight lines in the coordinate system that has the variables x1 and x2 on its axes.\\nThese two lines form boundary lines of the feasible production plans, which are\\n“below” the lines; see the figure below.\\nNote that we also must have xi ≥ 0, since we cannot produce negative units of a product.\\nFor planned profits yi , i = 1, 2, 3, . . . , the equations p1 x1 + p2 x2 = yi describe parallel straight lines in the coordinate system; see the dashed lines in the figure.\\nIf x1 and x2 satisfy p1 x1 + p2 x2 = yi , then\\n\u0002(x1 , x2 ) = yi .\\nThe profit maximization problem can now be solved by moving the dashed lines until one of them reaches the corner with the maximal y:\\n\\nIn case of more variables we cannot draw such a simple figure and obtain the solution “graphically”.\\nBut the general idea of finding a corner with the maximum profit is still the same.\\nThis is an example of a linear optimization problem.\\nAs before, we have formulated a real world problem in the language of Linear Algebra, and we can use mathematical methods for its solution.\\n\\n1.4 Predicting Future Profits\\nThe prediction of profits or losses of a company is a central planning instrument of economics.\\nAnalogous problems arise in many areas of political decision making,</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16</td>\n",
       "      <td>1 Linear Algebra in Every Day Life</td>\n",
       "      <td>1.4 Predicting Future Profits</td>\n",
       "      <td>NaN</td>\n",
       "      <td>production planning plant satisfy constraint inequality production plan satisfies constraint called feasible let pi profit selling one unit product pi goal determine production plan maximizes profit function find maximum two equation describe straight line coordinate system variable ax two line form boundary line feasible production plan line see figure note also must xi since produce negative unit product planned profit yi equation yi describe parallel straight line coordinate system see dashed line figure satisfy yi yi profit maximization problem solved moving dashed line one reach corner maximal case variable draw simple figure obtain solution graphically general idea finding corner maximum profit still example linear optimization problem formulated real world problem language linear algebra use mathematical method solution predicting future profit prediction profit loss company central planning instrument economics analogous problem arise many area political decision making</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6\\n\\n1 Linear Algebra in Every Day Life for example in budget planning, tax estimates or the planning of new infrastructures.\\nWe consider a specific example:\\nIn the four quarters of a year a company has profits of 10, 8, 9, 11 million Euros.\\nThe board now wants to predict the future profits development on the basis of these values.\\nEvidence suggests, that the profits behave linearly.\\nIf this was true, then the profits would form a straight line y(t) = αt + β that connects the points\\n(1, 10), (2, 8), (3, 9), (4, 11) in the coordinate system having “time” and “profit” as its axes.\\nThis, however, does neither hold in this example nor in practice.\\nTherefore one tries to find a straight line that deviates “as little as possible” from the given points.\\nOne possible approach is to choose the parameters α and β in order to minimize the sum of the squared distances between the given points and the straight line.\\nOnce the parameters α and β have been determined, the resulting line y(t) can be used for estimating or predicting the future profits, as illustrated in the following figure:\\n\\nThe determination of the parameters α and β that minimize a sum of squares is called a least squares problem.\\nWe will solve least squares problems using methods of Linear Algebra in Example 12.16.\\nThe approach itself is sometimes called a parameter identification.\\nIn Statistics, the modeling of given data (here the company profits) using a linear predictor function (here y(t) = αt + β) is known as linear regression.\\n\\n1.5 Circuit Simulation\\nThe current development of electronic devices is very rapid.\\nIn short intervals, nowadays often less than a year, new models of laptops or mobile phones have to be issued to the market.\\nTo achieve this, continuously new generations of computer chips have to be developed.\\nThese typically become smaller and more powerful, and naturally should use as little energy as possible.\\nAn important factor in this development is to plan and simulate the chips virtually, i.e., in the computer and without producing a physical prototype.\\nThis model-based planning and optimization of products is a central method in many high technology areas, and it is based on modern mathematics.</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17</td>\n",
       "      <td>1 Linear Algebra in Every Day Life</td>\n",
       "      <td>1.5 Circuit Simulation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear algebra every day life example budget planning tax estimate planning new infrastructure consider specific example four quarter year company profit million euro board want predict future profit development basis value evidence suggests profit behave linearly true profit would form straight line αt β connects point coordinate system time profit ax however neither hold example practice therefore one try find straight line deviate little possible given point one possible approach choose parameter α β order minimize sum squared distance given point straight line parameter α β determined resulting line used estimating predicting future profit illustrated following figure determination parameter α β minimize sum square called least square problem solve least square problem using method linear algebra example approach sometimes called parameter identification statistic modeling given data company profit using linear predictor function αt β known linear regression circuit simulation current development electronic device rapid short interval nowadays often le year new model laptop mobile phone issued market achieve continuously new generation computer chip developed typically become smaller powerful naturally use little energy possible important factor development plan simulate chip virtually computer without producing physical prototype model-based planning optimization product central method many high technology area based modern mathematics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.5 Circuit Simulation\\n\\n7\\n\\nUsually, the switching behavior of a chip is modeled by a mathematical system consisting of differential and algebraic equations that describe the relation between currents and voltages.\\nWithout going into details, consider the following circuit:\\n\\nIn this circuit description, VS (t) is the given input current at time t, and the characteristic values of the components are R for the resistor, L for the inductor, and\\nC for the capacitor.\\nThe functions for the potential differences at the three components are denoted by VR (t), VL (t), and VC (t); I (t) is the current.\\nApplying the Kirchhoff laws4 of electrical engineering leads to the following system of linear equations and differential equations that model the dynamic behavior of the circuit: d\\nI = VL , dt d\\nC VC = I, dt\\nR I = VR ,\\nL\\n\\nVL + VC + VR = VS .\\nIn this example it is easy to solve the last two equations for VL and VR , and hence to obtain a system of differential equations\\nR\\n1\\n1 d\\nI = − I − VC + VS , dt\\nL\\nL\\nL d\\n1\\nVC = − I, dt\\nC for the functions I und VC .\\nWe will discuss and solve this system in Example 17.13.\\nThis simple example demonstrates that for the simulation of a circuit a system of linear differential equations and algebraic equations has to be solved.\\nModern computer chips in industrial practice require solving such systems with millions of differential-algebraic equations.\\nLinear Algebra is one of central tools for the theoretical analysis of such systems as well as the development of efficient solution methods.\\n\\n4 Gustav\\n\\nRobert Kirchhoff (1824–1887).</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1 Linear Algebra in Every Day Life</td>\n",
       "      <td>1.5 Circuit Simulation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>circuit simulation usually switching behavior chip modeled mathematical system consisting differential algebraic equation describe relation current voltage without going detail consider following circuit circuit description v given input current time characteristic value component r resistor l inductor c capacitor function potential difference three component denoted vr vl vc current applying kirchhoff electrical engineering lead following system linear equation differential equation model dynamic behavior circuit vl dt c vc dt r vr l vl vc vr v example easy solve last two equation vl vr hence obtain system differential equation r vc v dt l l l vc dt c function und vc discus solve system example simple example demonstrates simulation circuit system linear differential equation algebraic equation solved modern computer chip industrial practice require solving system million differential-algebraic equation linear algebra one central tool theoretical analysis system well development efficient solution method gustav robert kirchhoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chapter 2\\n\\nBasic Mathematical Concepts\\n\\nIn this chapter we introduce the mathematical concepts that form the basis for the developments in the following chapters.\\nWe begin with sets and basic mathematical logic.\\nThen we consider maps between sets and their most important properties.\\nFinally we discuss relations and in particular equivalence relations on a set.\\n\\n2.1 Sets and Mathematical Logic\\nWe begin our development with the concept of a set and use the following definition of Cantor.1\\nDefinition 2.1 A set is a collection M of well determined and distinguishable objects x of our perception or our thinking.\\nThe objects are called the elements of M.\\nThe objects x in this definition are well determined, and therefore we can uniquely decide whether x belongs to a set M or not.\\nWe write x ∈ M if x is an element of the set M, otherwise we write x ∈\\n/ M. Furthermore, the elements are distinguishable, which means that all elements of M are (pairwise) distinct.\\nIf two objects x and y are equal, then we write x = y, otherwise x \u0003= y.\\nFor mathematical objects we usually have to give a formal definition of equality.\\nAs an example consider the equality of sets; see Definition 2.2 below.\\nWe describe sets with curly brackets { } that contain either a list of the elements, for example\\n{red, yellow, green}, {1, 2, 3, 4}, {2, 4, 6, . . . },\\n\\n1 Georg\\n\\nCantor (1845–1918), one of the founders of set theory.\\nCantor published this definition in the journal “Mathematische Annalen” in 1895.\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_2\\n\\n9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>&lt;FEFF&gt;</td>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.1 Sets and Mathematical Logic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter basic mathematical concept chapter introduce mathematical concept form basis development following chapter begin set basic mathematical logic consider map set important property finally discus relation particular equivalence relation set set mathematical logic begin development concept set use following definition definition set collection well determined distinguishable object x perception thinking object called element object x definition well determined therefore uniquely decide whether x belongs set write x x element set otherwise write x furthermore element distinguishable mean element pairwise distinct two object x equal write x otherwise x mathematical object usually give formal definition equality example consider equality set see definition describe set curly bracket contain either list element example red yellow green georg cantor one founder set theory cantor published definition journal mathematische annalen springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10\\n\\n2 Basic Mathematical Concepts or a defining property, for example\\n{x | x is a positive even number},\\n{x | x is a person owning a bike}.\\nSome of the well known sets of numbers are denoted as follows:\\nN = {1, 2, 3, . . . }\\nN0 = {0, 1, 2, . . . }\\n\\n(the natural numbers),\\n(the natural numbers including zero),\\n\\nZ = {. . . , −2, −1, 0, 1, 2, . . . }\\n(the integers),\\nQ = {x | x = a/b with a ∈ Z and b ∈ N} (the rational numbers),\\nR = {x | x is a real number}\\n(the real numbers).\\nThe construction and characterization of the real numbers R is usually done in an introductory course in Real Analysis.\\nTo describe a set via its defining property we formally write {x | P(x)}.\\nHere\\nP is a predicate which may hold for an object x or not, and P(x) is the assertion\\n“P holds for x”.\\nIn general, an assertion is a statement that can be classified as either “true” or\\n“false”.\\nFor instance the statement “The set N has infinitely many elements” is true.\\nThe sentence “Tomorrow the weather will be good” is not an assertion, since the meaning of the term “good weather” is unclear and the weather prediction in general is uncertain.\\nThe negation of an assertion A is the assertion “not A”, which we denote by ¬A.\\nThis assertion is true if and only if A is false, and false if and only if A is true.\\nFor instance, the negation of the true assertion “The set N has infinitely many elements” is given by “The set N does not have infinitely many elements” (or “The set N has finitely many elements”), which is false.\\nTwo assertions A and B can be combined via logical compositions to a new assertion.\\nThe following is a list of the most common logical compositions, together with their mathematical short hand notation:\\nComposition conjunction disjunction implication\\n\\nNotation\\n∧\\n∨\\n⇒ equivalence\\n\\n⇔\\n\\nWording\\nA and B\\nA or B\\nA implies B\\nIf A then B\\nA is a sufficient condition for B\\nB is a necessary condition for A\\nA and B are equivalent\\nA is true if and only if B is true\\nA is necessary and sufficient for B\\nB is necessary and sufficient for A</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.1 Sets and Mathematical Logic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic mathematical concept defining property example x x positive even number x x person owning bike well known set number denoted follows n natural number natural number including zero z integer q x x z b n rational number r x x real number real number construction characterization real number r usually done introductory course real analysis describe set via defining property formally write x p x p predicate may hold object x p x assertion p hold x general assertion statement classified either true false instance statement set n infinitely many element true sentence tomorrow weather good assertion since meaning term good weather unclear weather prediction general uncertain negation assertion assertion denote assertion true false false true instance negation true assertion set n infinitely many element given set n infinitely many element set n finitely many element false two assertion b combined via logical composition new assertion following list common logical composition together mathematical short hand notation composition conjunction disjunction implication notation equivalence wording b b implies b b sufficient condition b b necessary condition b equivalent true b true necessary sufficient b b necessary sufficient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.1 Sets and Mathematical Logic\\n\\n11\\n\\nFor example, we can write the assertion “x is a real number and x is negative” as x ∈ R ∧ x &lt; 0.\\nWhether an assertion that is composed of two assertions A and B is true or false, depends on the logical values of A and B. We have the following table of logical values (“t” and “f” denote true and false, respectively):\\nA t t f f\\n\\nB t f t f\\n\\nA∧B t f f f\\n\\nA∨B t t t f\\n\\nA⇒B t f t t\\n\\nA⇔B t f f t\\n\\nFor example, the assertion A ∧ B is true only when A and B are both true.\\nThe assertion A ⇒ B is false only when A is true and B is false.\\nIn particular, if A is false, then A ⇒ B is true, independent of the logical value of B.\\nThus, 3 &lt; 5 ⇒ 2 &lt; 4 is true, since 3 &lt; 5 and 2 &lt; 4 are both true.\\nBut\\n3 &lt; 5 ⇒ 2 &gt; 4 is false, since 2 &gt; 4 is false.\\nOn the other hand, the assertions\\n4 &lt; 2 ⇒ 3 &gt; 5 and 4 &lt; 2 ⇒ 3 &lt; 5 are both true, since 4 &lt; 2 is false.\\nIn the following we often have to prove that certain implications A ⇒ B are true.\\nAs the table of logical values shows and the example illustrates, we then only have to prove that under the assumption that A is true the assertion B is true as well.\\nInstead of “Assume that A is true” we will often write “Let A hold”.\\nIt is easy to see that\\n(A ⇒ B)\\n\\n⇔\\n\\n(¬B ⇒ ¬A).\\n\\n(As an exercise create the table of logical values for ¬B ⇒ ¬A and compare it with the table for A ⇒ B.)\\nThe truth of A ⇒ B can therefore be proved by showing that the truth of ¬B implies the truth of ¬A, i.e., that “B is false” implies “A is false”.\\nThe assertion ¬B ⇒ ¬A is called the contraposition of the assertion A ⇒ B and the conclusion from A ⇒ B to ¬B ⇒ ¬A is called proof by contraposition.\\nTogether with assertions we also often use so-called quantifiers:\\nQuantifier universal existential\\n\\nNotation\\n∀\\n∃\\n\\nWording\\nFor all\\nThere exists\\n\\nNow we return to set theory and introduce subsets and the equality of sets.\\nDefinition 2.2 Let M, N be sets.\\n(1) M is called a subset of N , denoted by M ⊆ N , if every element of M is also an element of N .\\nWe write M \u0002 N , if this does not hold.\\n(2) M and N are called equal, denoted by M = N , if M ⊆ N and N ⊆ M. We write M \u0003= N is this does not hold.</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21</td>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.1 Sets and Mathematical Logic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>set mathematical logic example write assertion x real number x negative x r x whether assertion composed two assertion b true false depends logical value b following table logical value f denote true false respectively f f b f f f f f f f f f example assertion b true b true assertion b false true b false particular false b true independent logical value b thus true since true false since false hand assertion true since false following often prove certain implication b true table logical value show example illustrates prove assumption true assertion b true well instead assume true often write let hold easy see b exercise create table logical value compare table b truth b therefore proved showing truth implies truth b false implies false assertion called contraposition assertion b conclusion b called proof contraposition together assertion also often use so-called quantifier quantifier universal existential notation wording exists return set theory introduce subset equality set definition let n set called subset n denoted n every element also element n write n hold n called equal denoted n n n write n hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12\\n\\n2 Basic Mathematical Concepts\\n\\n(3) M is called a proper subset of N , denoted by M ⊂ N , if both M ⊆ N and\\nM \u0003= N hold.\\nUsing the notation of mathematical logic we can write this definition as follows:\\n(1) M ⊆ N\\n(2) M = N\\n(3) M ⊂ N\\n\\n⇔\\n⇔\\n⇔\\n\\n(∀ x : x ∈ M ⇒ x ∈ N ).\\n(M ⊆ N ∧ N ⊆ M).\\n(M ⊆ N ∧ M \u0003= N ).\\n\\nThe assertion on the right side of the equivalence in (1) reads as follows: For all objects x the truth of x ∈ M implies the truth of x ∈ N .\\nOr shorter: For all x, if x ∈ M holds, then x ∈ N holds.\\nA very special set is the set with no elements, which we define formally as follows.\\nDefinition 2.3 The set Ø := {x | x \u0003= x} is called the empty set.\\nThe notation “:=” means is defined as.\\nWe have introduced the empty set by a defining property: Every object x with x \u0003= x is any element of Ø.\\nThis cannot hold for any object, and hence Ø does not contain any element.\\nA set that contains at least one element is called nonempty.\\nTheorem 2.4 For every set M the following assertions hold:\\n(1) Ø ⊆ M.\\n(2) M ⊆ Ø ⇒ M = Ø.\\nProof\\n(1) We have to show that the assertion “∀ x : x ∈ Ø ⇒ x ∈ M” is true.\\nSince there is no x ∈ Ø, the assertion “x ∈ Ø” is false, and therefore “x ∈ Ø ⇒ x ∈ M” is true for every x (cp. the remarks on the implication A ⇒ B).\\n(2) Let M ⊆ Ø.\\nFrom (1) we know that Ø ⊆ M and hence M = Ø follows by (2) in Definition 2.2.\\nTheorem 2.5 Let M, N , L be sets.\\nThen the following assertions hold for the subset relation “⊆”:\\n(1) M ⊆ M (reflexivity).\\n(2) If M ⊆ N and N ⊆ L, then M ⊆ L (transitivity).\\nProof\\n(1) We have to show that the assertion “∀ x : x ∈ M ⇒ x ∈ M” is true.\\nIf “x ∈ M” is true, then “x ∈ M ⇒ x ∈ M” is an implication with two true assertions, and hence it is true.\\n(2) We have to show that the assertion “∀ x : x ∈ M ⇒ x ∈ L” is true.\\nIf “x ∈ M” is true, then also “x ∈ N ” is true, since M ⊆ N .\\nThe truth of “x ∈ N ” implies that “x ∈ L” is true, since N ⊆ L. Hence the assertion “x ∈ M ⇒ x ∈ L” is true.</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22</td>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.1 Sets and Mathematical Logic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic mathematical concept called proper subset n denoted n n n hold using notation mathematical logic write definition follows n n n x x x n n n n n assertion right side equivalence read follows object x truth x implies truth x n shorter x x hold x n hold special set set element define formally follows definition set ø x x x called empty set notation mean defined introduced empty set defining property every object x x x element ø hold object hence ø contain element set contains least one element called nonempty theorem every set following assertion hold ø ø ø proof show assertion x x ø x true since x ø assertion x ø false therefore x ø x true every x cp remark implication b let ø know ø hence ø follows definition theorem let n l set following assertion hold subset relation reflexivity n n l l transitivity proof show assertion x x x true x true x x implication two true assertion hence true show assertion x x x l true x true also x n true since n truth x n implies x l true since n hence assertion x x l true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.1 Sets and Mathematical Logic\\n\\n13\\n\\nDefinition 2.6 Let M, N be sets.\\n(1) The union2 of M and N is M ∪ N := {x | x ∈ M ∨ x ∈ N }.\\n(2) The intersection of M and N is M ∩ N := {x | x ∈ M ∧ x ∈ N }.\\n(3) The difference of M and N is M \\ N := {x | x ∈ M ∧ x ∈\\n/ N }.\\nIf M ∩ N = Ø, then the sets M and N are called disjoint.\\nThe set operations union and intersection can be extended to more than two sets: If I \u0003= Ø is a set and if for all i ∈ I there is a set Mi , then\\n\u0002\\n\\nMi := {x | ∃ i ∈ I with x ∈ Mi } and i∈I\\n\\n\u0003\\n\\nMi := {x | ∀ i ∈ I we have x ∈ Mi }.\\ni∈I\\n\\nThe set I is called an index set.\\nFor I = {1, 2, . . . , n} ⊂ N we write the union and intersection of the sets M1 , M2 , . . . , Mn as n\\n\u0002 i=1\\n\\nMi and n\\n\u0003\\n\\nMi .\\ni=1\\n\\nTheorem 2.7 Let M ⊆ N for two sets M, N .\\nThen the following are equivalent:\\n(1) M ⊂ N .\\n(2) N \\ M \u0003= Ø.\\nProof We show that (1) ⇒ (2) and (2) ⇒ (1) hold.\\n(1) ⇒ (2): Since M \u0003= N , there exists an x ∈ N with x ∈\\n/ M. Thus x ∈ N \\ M, so that N \\ M \u0003= Ø holds.\\n(2) ⇒ (1): There exists an x ∈ N with x ∈\\n/ M, and hence N \u0003= M. Since M ⊆ N holds, we see that M ⊂ N holds.\\nTheorem 2.8 Let M, N , L be sets.\\nThen the following assertions hold:\\nM ∩ N ⊆ M and M ⊆ M ∪ N .\\nCommutativity: M ∩ N = N ∩ M and M ∪ N = N ∪ M.\\nAssociativity: M ∩ (N ∩ L) = (M ∩ N ) ∩ L and M ∪ (N ∪ L) = (M ∪ N ) ∪ L.\\nDistributivity: M ∪ (N ∩ L) = (M ∪ N ) ∩ (M ∪ L) and M ∩ (N ∪ L) =\\n(M ∩ N ) ∪ (M ∩ L).\\n(5) M \\ N ⊆ M.\\n(6) M \\ (N ∩ L) = (M \\ N ) ∪ (M \\ L) and M \\ (N ∪ L) = (M \\ N ) ∩ (M \\ L).\\n(1)\\n(2)\\n(3)\\n(4)\\n\\nProof Exercise.\\nnotations M ∪ N and M ∩ N for union and intersection of sets M and N were introduced in 1888 by Giuseppe Peano (1858–1932), one of the founders of formal logic.\\nThe notation of the\\n“smallest common multiple M(M, N )” and “largest common divisor D(M, N )” of the sets M and\\nN suggested by Georg Cantor (1845–1918) did not catch on.\\n\\n2 The</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23</td>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.1 Sets and Mathematical Logic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>set mathematical logic definition let n set n n x x x n intersection n n x x x n difference n n x x x n n ø set n called disjoint set operation union intersection extended two set ø set set mi mi x x mi mi x x mi set called index set n n write union intersection set mn n mi n mi theorem let n two set n following equivalent n n ø proof show hold since n exists x n x thus x n n ø hold exists x n x hence n since n hold see n hold theorem let n l set following assertion hold n n commutativity n n n n associativity n l n l n l n distributivity n l n l n l n l n n l n l n l n l proof exercise notation n n union intersection set n introduced giuseppe peano one founder formal logic notation smallest common multiple n largest common divisor n set n suggested georg cantor catch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14\\n\\n2 Basic Mathematical Concepts\\n\\nDefinition 2.9 Let M be a set.\\n(1) The cardinality of M, denoted by |M|, is the number of elements of M.\\n(1) The power set of M, denoted by P(M), is the set of all subsets of M, i.e.,\\nP(M) := {N | N ⊆ M}.\\nThe empty set Ø has cardinality zero and P(Ø) = {Ø}, thus |P(Ø)| = 1.\\nFor\\nM = {1, 3} the cardinality is |M| = 2 and\\nP(M) = { Ø, {1}, {3}, M }, and hence |P(M)| = 4 = 2|M| .\\nOne can show that for every set M with finitely many elements, i.e., finite cardinality, |P(M)| = 2|M| holds.\\n\\n2.2 Maps\\nIn this section we discuss maps between sets.\\nDefinition 2.10 Let X, Y be nonempty sets.\\n(1) A map f from X to Y is a rule that assigns to each x ∈ X exactly one y = f (x) ∈ Y .\\nWe write this as f : X → Y, x \u0011→ y = f (x).\\n\\nInstead of x \u0011→ y = f (x) we also write f (x) = y.\\nThe sets X and Y are called domain and codomain of f .\\n(2) Two maps f : X → Y and g : X → Y are called equal when f (x) = g(x) holds for all x ∈ X .\\nWe then write f = g.\\nIn Definition 2.10 we have assumed that X and Y are nonempty, since otherwise there can be no rule that assigns an element of Y to each element of X .\\nIf one of these sets is empty, one can define an empty map.\\nHowever, in the following we will always assume (but not always explicitly state) that the sets between which a given map acts are nonempty.\\nExample 2.11 Two maps from X = R to Y = R are given by f : X → Y, f (x) = x 2 ,\\n\u0004\\n0, x ≤ 0, g : X → Y, x \u0011→\\n1, x &gt; 0.\\nTo analyze the properties of maps we need some further terminology.\\n\\n(2.1)\\n(2.2)</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24</td>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.2 Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic mathematical concept definition let set cardinality denoted number element power set denoted p set subset p n n empty set ø cardinality zero p ø ø thus ø cardinality p ø hence one show every set finitely many element finite cardinality hold map section discus map set definition let x nonempty set map f x rule assigns x x exactly one f x write f x x f x instead x f x also write f x set x called domain codomain f two map f x g x called equal f x g x hold x x write f definition assumed x nonempty since otherwise rule assigns element element x one set empty one define empty map however following always assume always explicitly state set given map act nonempty example two map x r r given f x f x x x g x x x analyze property map need terminology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.2 Maps\\n\\n15\\n\\nDefinition 2.12 Let X, Y be nonempty sets.\\n(1) The map Id X : X → X , x \u0011→ x, is called the identity on X .\\n(2) Let f : X → Y be a map and let M ⊆ X and N ⊆ Y .\\nThen f (M) := { f (x) | x ∈ M } ⊆ Y is called the image of M under f , f\\n\\n−1\\n\\n(N ) := { x ∈ X | f (x) ∈ N } is called the pre-image of N under f .\\n\\n(3) If f : X → Y , x \u0011→ f (x) is a map and Ø \u0003= M ⊆ X , then f | M : M → Y , x \u0011→ f (x), is called the restriction of f to M.\\nOne should note that in this definition f −1 (N ) is a set, and hence the symbol f −1 here does not mean the inverse map of f .\\n(This map will be introduced below in\\nDefinition 2.21.)\\nExample 2.13 For the maps with domain X = R in (2.1) and (2.2) we have the following properties: f (X ) = {x ∈ R | x ≥ 0}, f −1 (R− ) = {0}, f −1 ({−1}) = Ø, g(X ) = {0, 1}, g −1 (R− ) = g −1 ({0}) = R− , where R− := {x ∈ R | x ≤ 0}.\\nDefinition 2.14 Let X, Y be nonempty sets.\\nA map f : X → Y is called\\n(1) injective, if for all x1 , x2 ∈ X the equality f (x1 ) = f (x2 ) implies that x1 = x2 ,\\n(2) surjective, if f (X ) = Y ,\\n(3) bijective, if f is injective and surjective.\\nFor every nonempty set X the simplest example of a bijective map from X to X is Id X , the identity on X .\\nExample 2.15 Let R+ := {x ∈ R | x ≥ 0}, then f : R → R, f (x) = x 2 , is neither injective nor surjective.\\nf : R → R+ , f (x) = x 2 , is surjective but not injective.\\nf : R+ → R, f (x) = x 2 , is injective but not surjective.\\nf : R+ → R+ , f (x) = x 2 , is bijective.\\nIn these assertions we have used the continuity of the map f (x) = x 2 that is discussed in the basic courses on analysis.\\nIn particular, we have used the fact that continuous functions map real intervals to real intervals.\\nThe assertions also show why it is important to include the domain and codomain in the definition of a map.\\nTheorem 2.16 A map f : X → Y is bijective if and only if for every y ∈ Y there exists exactly one x ∈ X with f (x) = y.\\nProof ⇒: Let f be bijective and let y1 ∈ Y .\\nSince f is surjective, there exists an x1 ∈ X with f (x1 ) = y1 .\\nIf some x2 ∈ X also satisfies f (x2 ) = y1 , then x1 = x2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.2 Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>map definition let x nonempty set map id x x x x x called identity x let f x map let x n f f x x called image f f n x x f x n called pre-image n f f x x f x map ø x f x f x called restriction f one note definition f n set hence symbol f mean inverse map f map introduced definition example map domain x r following property f x x r x f f ø g x g g x r x definition let x nonempty set map f x called injective x equality f f implies surjective f x bijective f injective surjective every nonempty set x simplest example bijective map x x id x identity x example let x r x f r r f x x neither injective surjective f r f x x surjective injective f r f x x injective surjective f f x x bijective assertion used continuity map f x x discussed basic course analysis particular used fact continuous function map real interval real interval assertion also show important include domain codomain definition map theorem map f x bijective every exists exactly one x x f x proof let f bijective let since f surjective exists x f x also satisfies f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16\\n\\n2 Basic Mathematical Concepts follows from the injectivity of f .\\nTherefore, there exists a unique x1 ∈ X with f (x1 ) = y1 .\\n⇐: Since for all y ∈ Y there exists a unique x ∈ X with f (x) = y, it follows that f (X ) = Y .\\nThus, f surjective.\\nLet now x1 , x2 ∈ X with f (x1 ) = f (x2 ) = y ∈ Y .\\nThen the assumption implies x1 = x2 , so that f is also injective.\\nOne can show that between two sets X and Y of finite cardinality there exists a bijective map if and only if |X | = |Y |.\\nLemma 2.17 For sets X, Y with |X | = |Y | = m ∈ N, there exist exactly m! :=\\n1 · 2 · . . . · m pairwise distinct bijective maps between X and Y .\\nProof Exercise.\\nDefinition 2.18 Let f : X → Y , x \u0011→ f (x), and g : Y → Z , y \u0011→ g(y) be maps.\\nThen the composition of f and g is the map g ◦ f : X → Z, x \u0011→ g( f (x)).\\n\\nThe expression g ◦ f should be read “g after f ”, which stresses the order of the composition: First f is applied to x and then g to f (x).\\nOne immediately sees that f ◦ Id X = f = IdY ◦ f for every map f : X → Y .\\nTheorem 2.19 Let f : W → X , g : X → Y , h : Y → Z be maps.\\nThen\\n(1) h ◦ (g ◦ f ) = (h ◦ g) ◦ f , i.e., the composition of maps is associative.\\n(2) If f and g are injective/surjective/bijective, then g ◦ f is injective/ surjective/bijective.\\nProof Exercise.\\nTheorem 2.20 A map f : X → Y is bijective if and only if there exists a map g : Y → X with g ◦ f = Id X and f ◦ g = IdY .\\nProof ⇒: If f is bijective, then by Theorem 2.16 for every y ∈ Y there exists an x = x y ∈ X with f (x y ) = y.\\nWe define the map g by g : Y → X, g(y) = x y .\\nLet \u0005 y ∈ Y be given, then y, hence f ◦ g = IdY .\\n( f ◦ g)(\u0005 y) = f (g(\u0005 y)) = f (x\u0005y ) = \u0005\\nIf, on the other hand, \u0005 x ∈ X is given, then \u0005 y = f (\u0005 x ) ∈ Y .\\nBy Theorem 2.16, there y such that \u0005 x = x\u0005y .\\nSo with exists a unique x\u0005y ∈ X with f (x\u0005y ) = \u0005 y) = x\u0005y = \u0005 x,\\n(g ◦ f )(\u0005 x ) = (g ◦ f )(x\u0005y ) = g( f (x\u0005y )) = g(\u0005</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26</td>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.2 Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic mathematical concept follows injectivity f therefore exists unique x f since exists unique x x f x follows f x thus f surjective let x f f assumption implies f also injective one show two set x finite cardinality exists bijective map lemma set x n exist exactly pairwise distinct bijective map x proof exercise definition let f x x f x g z g map composition f g map g f x z x g f x expression g f read g f stress order composition first f applied x g f x one immediately see f id x f idy f every map f x theorem let f w x g x h z map h g f h g f composition map associative f g g f proof exercise theorem map f x bijective exists map g x g f id x f g idy proof f bijective theorem every exists x x x f x define map g g x g x let given hence f g idy f g f g f hand x x given f x theorem x exists unique x f x g f x g f g f g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.2 Maps\\n\\n17 we have g ◦ f = Id X .\\n⇐: By assumption g ◦ f = Id X , thus g ◦ f is injective and thus also f is injective\\n(see Exercise 2.7).\\nMoreover, f ◦ g = IdY , thus f ◦ g is surjective and hence also f is surjective (see Exercise 2.7).\\nTherefore, f is bijective.\\nThe map g : Y → X that was characterized in Theorem 2.20 is unique: If there were another map h : Y → X with h ◦ f = Id X and f ◦ h = IdY , then h = Id X ◦ h = (g ◦ f ) ◦ h = g ◦ ( f ◦ h) = g ◦ IdY = g.\\nThis leads to the following definition.\\nDefinition 2.21 If f : X → Y is a bijective map, then the unique map g : Y → X from Theorem 2.20 is called the inverse (or inverse map) of f .\\nWe denote the inverse of f by f −1 .\\nTo show that a given map g : Y → X is the unique inverse of the bijective map f : X → Y , it is sufficient to show one of the equations g ◦ f = Id X or f ◦ g = IdY .\\nIndeed, if f is bijective and g ◦ f = Id X , then g = g ◦ IdY = g ◦ ( f ◦ f −1 ) = (g ◦ f ) ◦ f −1 = Id X ◦ f −1 = f −1 .\\nIn the same way g = f −1 follows from the assumption f ◦ g = IdY .\\nTheorem 2.22 If f : X → Y and g : Y → Z are bijective maps, then the following assertions hold:\\n(1) f −1 is bijective with ( f −1 )−1 = f .\\n(2) g ◦ f is bijective with (g ◦ f )−1 = f −1 ◦ g −1 .\\nProof\\n(1) Exercise.\\n(2) We know from Theorem 2.19 that g ◦ f : X → Z is bijective.\\nTherefore, there exists a (unique) inverse of g ◦ f .\\nFor the map f −1 ◦ g −1 we have\\n\u0006\\n\u0007\\n\u0006\\n\u0007\\n( f −1 ◦ g −1 ) ◦ (g ◦ f ) = f −1 ◦ g −1 ◦ (g ◦ f ) = f −1 ◦ (g −1 ◦ g) ◦ f\\n= f −1 ◦ (IdY ◦ f ) = f −1 ◦ f = Id X .\\nHence, f −1 ◦ g −1 is the inverse of g ◦ f .\\n\\n2.3 Relations\\nWe first introduce the cartesian product3 of two sets.\\n3 Named after René Descartes (1596–1650), the founder of Analytic Geometry.\\nGeorg Cantor (1845–\\n\\n1918) used in 1895 the name “connection set of M and N ” and the notation (M.N ) = {(m, n)}.</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27</td>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.3 Relations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>map g f id x assumption g f id x thus g f injective thus also f injective see exercise moreover f g idy thus f g surjective hence also f surjective see exercise therefore f bijective map g x characterized theorem unique another map h x h f id x f h idy h id x h g f h g f h g idy lead following definition definition f x bijective map unique map g x theorem called inverse inverse map f denote inverse f f show given map g x unique inverse bijective map f x sufficient show one equation g f id x f g idy indeed f bijective g f id x g g idy g f f g f f id x f f way g f follows assumption f g idy theorem f x g z bijective map following assertion hold f bijective f f g f bijective g f f g proof exercise know theorem g f x z bijective therefore exists unique inverse g f map f g f g g f f g g f f g g f f idy f f f id x hence f g inverse g f relation first introduce cartesian two set named rené descartes founder analytic geometry georg cantor used name connection set n notation n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18\\n\\n2 Basic Mathematical Concepts\\n\\nDefinition 2.23 If M, N are nonempty sets, then the set\\nM × N := {(x, y) | x ∈ M ∧ y ∈ N } is the cartesian product of M and N .\\nAn element (x, y) ∈ M × N is called an\\n(ordered) pair.\\nWe can easily generalize this definition to n ∈ N nonempty sets M1 , . . . , Mn :\\nM1 × . . . × Mn := {(x1 , . . . , xn ) | xi ∈ Mi for i = 1, . . . , n}, where an element (x1 , . . . , xn ) ∈ M1 × · · · × Mn is called an (ordered) n-tuple.\\nThe n-fold cartesian product of a single nonempty set M is\\nM n := \bM × . . . × M = {(x1 , . . . , xn ) | xi ∈ M for i = 1, . . . , n}.\\nn times\\n\\nIf in these definitions at least one of the sets is empty, then the resulting cartesian product is the empty set as well.\\nDefinition 2.24 If M, N are nonempty sets then a set R ⊆ M × N is called a relation between M and N .\\nIf M = N , then R is called a relation on M. Instead of (x, y) ∈ R we also write x ∼ R y or x ∼ y, if it is clear which relation is considered.\\nIf in this definition at least one of the sets M and N is empty, then every relation between M and N is also the empty set, since then M × N = Ø.\\nIf, for instance M = N and N = Q, then\\nR = {(x, y) ∈ M × N | x y = 1} is a relation between M and N that can be expressed as\\nR = {(1, 1), (2, 1/2), (3, 1/3), . . . } = {(n, 1/n) | n ∈ N}.\\nDefinition 2.25 A relation R on a set M is called\\n(1) reflexive, if x ∼ x holds for all x ∈ M,\\n(2) symmetric, if (x ∼ y) ⇒ (y ∼ x) holds for all x, y ∈ M,\\n(3) transitive, if (x ∼ y ∧ y ∼ z) ⇒ (x ∼ z) holds for all x, y, z ∈ M.\\nIf R is reflexive, transitive and symmetric, then it is called an equivalence relation on M.\\nExample 2.26\\n(1) Let R = {(x, y) ∈ Q2 | x = −y}.\\nThen R is not reflexive, since x = −x holds only for x = 0.\\nIf x = −y, then also y = −x, and hence R is symmetric.\\nFinally, R is not transitive.\\nFor example, (x, y) = (1, −1) ∈ R and (y, z) =\\n(−1, 1) ∈ R, but (x, z) = (1, 1) ∈\\n/ R.</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28</td>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.3 Relations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic mathematical concept definition n nonempty set set n x x n cartesian product n element x n called ordered pair easily generalize definition n n nonempty set mn mn xn xi mi n element xn mn called ordered n-tuple n-fold cartesian product single nonempty set n xn xi n n time definition least one set empty resulting cartesian product empty set well definition n nonempty set set r n called relation n n r called relation instead x r also write x r x clear relation considered definition least one set n empty every relation n also empty set since n ø instance n n q r x n x relation n expressed r n n n definition relation r set called reflexive x x hold x symmetric x x hold x transitive x z x z hold x z r reflexive transitive symmetric called equivalence relation example let r x x r reflexive since x hold x x also hence r symmetric finally r transitive example x r z r x z r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.3 Relations\\n\\n19\\n\\n(2) The relation R = {(x, y) ∈ Z2 | x ≤ y} is reflexive and transitive, but not symmetric.\\n(3) If f : R → R is a map, then R = {(x, y) ∈ R2 | f (x) = f (y)} is an equivalence relation on R.\\nDefinition 2.27 let R be an equivalence relation on the set M. Then, for x ∈ M the set\\n[x] R := {y ∈ M | (x, y) ∈ R} = {y ∈ M | x ∼ y} is called the equivalence class of x with respect to R. The set of equivalence classes\\nM/R := {[x] R | x ∈ M} is called the quotient set of M with respect to R.\\nThe equivalence class [x] R of elements x ∈ M is never the empty set, since always x ∼ x (reflexivity) and therefore x ∈ [x] R .\\nIf it is clear which equivalence relation\\nR is meant, we often write [x] instead oft [x] R and also skip the additional “with respect to R”.\\nTheorem 2.28 If R is an equivalence relation on the set M and if x, y ∈ M, then the following are equivalent:\\n(1) [x] = [y].\\n(2) [x] ∩ [y] \u0003= Ø.\\n(3) x ∼ y.\\nProof\\n(1) ⇒ (2) : Since x ∼ x, it follows that x ∈ [x].\\nFrom [x] = [y] it follows that x ∈ [y] and thus x ∈ [x] ∩ [y].\\n(2) ⇒ (3) : Since [x] ∩ [y] \u0003= Ø, there exists a z ∈ [x] ∩ [y].\\nFor this element z we have x ∼ z and y ∼ z, and thus x ∼ z and z ∼ y (symmetry) and, therefore, x ∼ y (transitivity).\\n(3) ⇒ (1) : Let x ∼ y and z ∈ [x], i.e., x ∼ z.\\nUsing symmetry and transitivity, we obtain y ∼ z, and hence z ∈ [y].\\nThis means that [x] ⊆ [y].\\nIn an analogous way one shows that [y] ⊆ [x], and hence [x] = [y] holds.\\nTheorem 2.28 shows that for two equivalence classes [x] and [y] we have either\\n[x] = [y] or [x]∩[y] = Ø.\\nThus every x ∈ M is contained in exactly one equivalence class (namely in [x]), so that an equivalence relation R yields a partitioning or decomposition of M into mutually disjoint subsets.\\nEvery element of [x] is called a representative of the equivalence class [x].\\nA very useful and general approach that we will often use in this book is to partition a set of objects (e.g. sets of matrices) into equivalence classes, and to find in each such class a representative with a particularly simple structure.\\nSuch a representative is called a normal form with respect to the given equivalence relation.</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29</td>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.3 Relations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relation relation r x x reflexive transitive symmetric f r r map r x f x f equivalence relation definition let r equivalence relation set x set x r x r x called equivalence class x respect set equivalence class x r x called quotient set respect equivalence class x r element x never empty set since always x x reflexivity therefore x x r clear equivalence relation r meant often write x instead oft x r also skip additional respect r theorem r equivalence relation set x following equivalent x x ø x proof since x x follows x x x follows x thus x x since x ø exists z x element z x z z thus x z z symmetry therefore x transitivity let x z x x z using symmetry transitivity obtain z hence z mean x analogous way one show x hence x hold theorem show two equivalence class x either x x ø thus every x contained exactly one equivalence class namely x equivalence relation r yield partitioning decomposition mutually disjoint subset every element x called representative equivalence class x useful general approach often use book partition set object set matrix equivalence class find class representative particularly simple structure representative called normal form respect given equivalence relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20\\n\\n2 Basic Mathematical Concepts\\n\\nExample 2.29 For a given number n ∈ N the set\\nRn := {(a, b) ∈ Z2 | a − b is divisible by n without remainder} is an equivalence relation on Z, since the following properties hold:\\n• Reflexivity: a − a = 0 is divisible by n without remainder.\\n• Symmetry: If a − b is divisible by n without remainder, then also b − a.\\n• Transitivity: Let a − b and b − c be divisible by n without remainder and write a − c = (a − b) + (b − c).\\nBoth summands on the right are divisible by n without remainder and hence this also holds for a − c.\\nFor a ∈ Z the equivalence class [a] is called residue class of a modulo n, and\\n[a] = a + nZ := {a + nz | z ∈ Z}.\\nThe equivalence relation Rn yields a partitioning of Z into n mutually disjoint subsets.\\nIn particular, we have n−1\\n\u0002\\n\\n[0] ∪ [1] ∪ · · · ∪ [n − 1] =\\n\\n[a] = Z.\\na=0\\n\\nThe set of all residue classes modulo n, i.e., the quotient set with respect to Rn , is often denoted by Z/nZ.\\nThus, Z/nZ := {[0], [1], . . . , [n − 1]}.\\nThis set plays an important role in the mathematical field of Number Theory.\\nExercises\\n2.1 Let A, B, C be assertions.\\nShow that the following assertions are true:\\n(a) For ∧ and ∨ the associative laws\\n[(A ∧ B) ∧ C] ⇔ [A ∧ (B ∧ C)],\\n\\n[(A ∨ B) ∨ C] ⇔ [A ∨ (B ∨ C)] hold.\\n(b) For ∧ and ∨ the commutative laws\\n(A ∧ B) ⇔ (B ∧ A),\\n\\n(A ∨ B) ⇔ (B ∨ A) hold.\\n(c) For ∧ and ∨ the distributive laws\\n[(A ∧ B) ∨ C] ⇔ [(A ∨ C) ∧ (B ∨ C)],\\n\\n[(A ∨ B) ∧ C] ⇔ [(A ∧ C) ∨ (B ∧ C)] hold.\\n2.2 Let A, B, C be assertions.\\nShow that the following assertions are true:\\n(a) A ∧ B ⇒ A.\\n(b) [A ⇔ B] ⇔ [(A ⇒ B) ∧ (B ⇒ A)].</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30</td>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.3 Relations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic mathematical concept example given number n n set rn b b divisible n without remainder equivalence relation z since following property hold reflexivity divisible n without remainder symmetry b divisible n without remainder also b transitivity let b b c divisible n without remainder write c b b c summands right divisible n without remainder hence also hold z equivalence class called residue class modulo n nz nz z z equivalence relation rn yield partitioning z n mutually disjoint subset particular n set residue class modulo n quotient set respect rn often denoted thus n set play important role mathematical field number theory exercise let b c assertion show following assertion true associative law b c b c b c b c hold b commutative law b b b b hold c distributive law b c c b c b c c b c hold let b c assertion show following assertion true b b b b b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.3 Relations\\n\\n(c)\\n(d)\\n(e)\\n(f)\\n\\n21\\n\\n¬(A ∨ B) ⇔ [(¬A) ∧ (¬B)].\\n¬(A ∧ B) ⇔ [(¬A) ∨ (¬B)].\\n[(A ⇒ B) ∧ (B ⇒ C)] ⇒ [A ⇒ C].\\n[A ⇒ (B ∨ C)] ⇔ [(A ∧ ¬B) ⇒ C].\\n\\n(The assertions (c) and (d) are called the De Morgan laws for ∧ and ∨.)\\n2.3 Prove Theorem 2.8.\\n2.4 Show that for two sets M, N the following holds:\\nN⊆M\\n\\n⇔\\n\\nM∩N =N\\n\\n⇔\\n\\nM ∪ N = M.\\n\\n2.5 Let X, Y be nonempty sets, U, V ⊆ Y nonempty subsets and let f : X → Y be a map.\\nShow that f −1 (U ∩ V ) = f −1 (U ) ∩ f −1 (V ).\\nLet U, V ⊆ X be nonempty.\\nCheck whether f (U ∪ V ) = f (U ) ∪ f (V ) holds.\\n2.6 Are the following maps injective, surjective, bijective?\\n(a) f 1 : R \\ {0} → R, x \u0011→ x1 .\\n(b) f 2 : R2 → R, (x, y) \u0011→ x + y.\\n2\\n2\\n(c) f 3 : R2 → R, (x, y)\\n\u0004 \u0011→ x + y − 1.\\nn\\n, n even,\\n(d) f 4 : N → Z, n \u0011→ 2 n−1\\n− 2 , n odd.\\n2.7 Show that for two maps f : X → Y and g : Y → Z the following assertions hold:\\n(a) g ◦ f is surjective ⇒ g is surjective.\\n(b) g ◦ f is injective ⇒ f is injective.\\n2.8 Let a ∈ Z be given.\\nShow that the map f a : Z → Z, f a (x) = x + a is bijective.\\n2.9 Prove Lemma 2.17.\\n2.10 Prove Theorem 2.19.\\n2.11 Prove Theorem 2.22 (1).\\n2.12 Find two maps f, g : N → N, so that simultaneously\\n(a) f is not surjective,\\n(b) g is not injective, and\\n(c) g ◦ f is bijective.\\n2.13 Determine all equivalence relations on the set {1, 2}.\\n2.14 Determine a symmetric and transitive relation on the set {a, b, c} that is not reflexive.</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31</td>\n",
       "      <td>2 Basic Mathematical Concepts</td>\n",
       "      <td>2.3 Relations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relation c e f b b b b c c b c c assertion c called de morgan law prove theorem show two set n following hold n let x nonempty set u v nonempty subset let f x map show f u v f u f v let u v x nonempty check whether f u v f u f v hold following map injective surjective bijective f r r x b f r x x c f r x x n n even f n z n n odd show two map f x g z following assertion hold g f surjective g surjective b g f injective f injective let z given show map f z z f x x bijective prove lemma prove theorem prove theorem find two map f g n n simultaneously f surjective b g injective c g f bijective determine equivalence relation set determine symmetric transitive relation set b c reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chapter 3\\n\\nAlgebraic Structures\\n\\nAn algebraic structure is a set with operations between its elements that follow certain rules.\\nAs an example of such a structure consider the integers and the operation ‘+.’\\nWhat are the properties of this addition?\\nAlready in elementary school one learns that the sum a + b of two integers a and b is another integer.\\nMoreover, there is a number 0 such that 0 + a = a for every integer a, and for every integer a there exists an integer −a such that (−a) + a = 0.\\nThe analysis of the properties of such concrete examples leads to definitions of abstract concepts that are built on a few simple axioms.\\nFor the integers and the operation addition, this leads to the algebraic structure of a group.\\nThis principle of abstraction from concrete examples is one of the strengths and basic working principles of Mathematics.\\nBy “extracting and completely exposing the mathematical kernel” (David Hilbert) we also simplify our further work:\\nEvery proved assertion about an abstract concept automatically holds for all concrete examples.\\nMoreover, by combining defined concepts we can move to further generalizations and in this way extend the mathematical theory step by step.\\nHermann Günther Graßmann (1809–1877) described this procedure as follows1 : “... the mathematical method moves forward from the simplest concepts to combinations of them and gains via such combinations new and more general concepts.”\\n\\n3.1 Groups\\nWe begin with a set and an operation with specific properties.\\nDefinition 3.1 A group is a set G with a map, called operation,\\n⊕ : G × G → G, (a, b) \u0004→ a ⊕ b,\\n1 “...\\ndie mathematische Methode hingegen schreitet von den einfachsten Begriffen zu den zusammengesetzteren fort, and gewinnt so durch Verknüpfung des Besonderen neue and allgemeinere\\nBegriffe.”\\n© Springer International Publishing Switzerland 2015\\n23\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>&lt;FEFF&gt;</td>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.1 Groups</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter algebraic structure algebraic structure set operation element follow certain rule example structure consider integer operation property addition already elementary school one learns sum b two integer b another integer moreover number every integer every integer exists integer analysis property concrete example lead definition abstract concept built simple axiom integer operation addition lead algebraic structure group principle abstraction concrete example one strength basic working principle mathematics extracting completely exposing mathematical kernel david hilbert also simplify work every proved assertion abstract concept automatically hold concrete example moreover combining defined concept move generalization way extend mathematical theory step step hermann günther graßmann described procedure mathematical method move forward simplest concept combination gain via combination new general group begin set operation specific property definition group set g map called operation g g g b b die mathematische methode hingegen schreitet von den einfachsten begriffen zu den zusammengesetzteren fort gewinnt durch verknüpfung de besonderen neue allgemeinere springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>24\\n\\n3 Algebraic Structures that satisfies the following:\\n(1) The operation ⊕ is associative, i.e., (a ⊕ b) ⊕ c = a ⊕ (b ⊕ c) holds for all a, b, c ∈ G.\\n(2) There exists an element e ∈ G, called a neutral element, for which\\n(a) e ⊕ a = a for all a ∈ G, and\\n(b) for every a ∈ G there exists an \u0002 a ∈ G, called an inverse element of a, with\\n\u0002 a ⊕ a = e.\\nIf a ⊕ b = b ⊕ a holds for all a, b ∈ G, then the group is called commutative or\\nAbelian.2\\nAs short hand notation for a group we use (G, ⊕) or just G, if is clear which operation is used.\\nTheorem 3.2 For every group (G, ⊕) the following assertions hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n\\nIf e ∈ G is a neutral element and if a,\u0002 a ∈ G with\u0002 a ⊕a = e, then also a ⊕\u0002 a = e.\\nIf e ∈ G is a neutral element and if a ∈ G, then also a ⊕ e = a.\\nG contains exactly one neutral element.\\nFor every a ∈ G there exists a unique inverse element.\\n\\nProof\\n(1) Let e ∈ G be a neutral element and let a,\u0002 a ∈ G satisfy \u0002 a ⊕ a = e.\\nThen by\\nDefinition 3.1 there exists an element a1 ∈ G with a1 ⊕ \u0002 a = e.\\nThus, a ) ⊕ (a ⊕ \u0002 a )a1 ⊕ ((\u0002 a ⊕ a) ⊕ \u0002 a) a ⊕\u0002 a = e ⊕ (a ⊕ \u0002 a ) = (a1 ⊕ \u0002\\n= a1 ⊕ (e ⊕ \u0002 a ) = a1 ⊕ \u0002 a = e.\\n(2) Let e ∈ G be a neutral element and let a ∈ G. Then there exists \u0002 a ∈ G with\\n\u0002 a ⊕ a = e.\\nBy (1) then also a ⊕ \u0002 a = e and it follows that a ⊕ e = a ⊕ (\u0002 a ⊕ a) = (a ⊕ \u0002 a ) ⊕ a = e ⊕ a = a.\\n(3) Let e, e1 ∈ G be two neutral elements.\\nThen e1 ⊕ e = e, since e1 is a neutral element.\\nSince e is also a neutral element, it follows that e1 = e ⊕ e1 = e1 ⊕ e, where for the second identity we have used assertion (2).\\nHence, e = e1 .\\n(4) Let \u0002 a , a1 ∈ G be two inverse elements of a ∈ G and let e ∈ G be the (unique) neutral element.\\nThen with (1) and (2) it follows that a = a1 ⊕ (a ⊕ \u0002 a ) = a1 ⊕ e = a1 .\\n\u0002 a = e ⊕\u0002 a = (a1 ⊕ a) ⊕ \u0002\\n\\n2 Named after Niels Henrik Abel (1802–1829), the founder of group theory.\\n\\n\u0007\\n\u0006</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33</td>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.1 Groups</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algebraic structure satisfies following operation associative b c b c hold b c exists element e g called neutral element e g b every g exists g called inverse element b b hold b g group called commutative short hand notation group use g g clear operation used theorem every group g following assertion hold e g neutral element g e also e g neutral element g also e g contains exactly one neutral element every g exists unique inverse element proof let e g neutral element let g satisfy definition exists element g thus e e let e g neutral element let exists g also e follows e e let e g two neutral element e e since neutral element since e also neutral element follows e e second identity used assertion hence e let g two inverse element g let e g unique neutral element follows e e named niels henrik abel founder group theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.1 Groups\\n\\n25\\n\\nExample 3.3\\n(1) (Z, +), (Q, +) and (R, +) are commutative groups.\\nIn all these groups the neutral element is the number 0 (zero) and the inverse of a is the number −a.\\nInstead of a + (−b) we usually write a − b.\\nSince the operation is the addition, these groups are also called additive groups.\\nThe natural numbers N with the addition do not form a group, since there is no neutral element in N. If we consider the set N0 , which includes also the number\\n0 (zero), then 0 + a = a + 0 = a for all a ∈ N0 , but only a = 0 has an inverse element in N. Hence also N0 with the addition does not form a group.\\n(2) The sets Q \\ {0} and R \\ {0} with the usual multiplication form commutative groups.\\nIn these multiplicative groups, the neutral element is the number 1 (one) and the inverse element of a is the number a1 (or a −1 ).\\nInstead of a · b−1 we also write ab or a/b.\\nThe integers Z with the multiplication do not form a group.\\nThe set Z includes the number 1, for which 1 · a = a · 1 = a for all a ∈ Z, but no a ∈ Z \\ {−1, 1} has an inverse element in Z.\\nDefinition 3.4 Let (G, ⊕) be a group and H ⊆ G. If (H, ⊕) is a group, then it is called a subgroup of (G, ⊕).\\nThe next theorem gives an alternative characterization of a subgroup.\\nTheorem 3.5 (H, ⊕) is a subgroup of the group (G, ⊕) if and only if the following properties hold:\\n(1) Ø = H ⊆ G.\\n(2) a ⊕ b ∈ H for all a, b ∈ H .\\n(3) For every a ∈ H also the inverse element satisfies \u0002 a ∈ H.\\n\u0007\\n\u0006\\n\\nProof Exercise.\\n\\nThe following definition characterizes maps between two groups which are compatible with the respective group operations.\\nDefinition 3.6 Let (G 1 , ⊕) and (G 2 , \u0002) be groups.\\nA map\\nϕ : G 1 → G 2 , g \u0004→ ϕ(g), is called a group homomorphism, if\\nϕ(a ⊕ b) = ϕ(a) \u0002 ϕ(b) for all a, b ∈ G 1 .\\nA bijective group homomorphism is called a group isomorphism.</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34</td>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.1 Groups</td>\n",
       "      <td>NaN</td>\n",
       "      <td>group example z q r commutative group group neutral element number zero inverse number instead usually write b since operation addition group also called additive group natural number n addition form group since neutral element consider set includes also number zero inverse element hence also addition form group set q r usual multiplication form commutative group multiplicative group neutral element number one inverse element number instead also write ab integer z multiplication form group set z includes number z z inverse element z definition let g group h h group called subgroup g next theorem give alternative characterization subgroup theorem h subgroup group g following property hold ø h b h b h every h also inverse element satisfies proof exercise following definition characterizes map two group compatible respective group operation definition let g g group map ϕ g g g ϕ g called group homomorphism ϕ b ϕ ϕ b b g bijective group homomorphism called group isomorphism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26\\n\\n3 Algebraic Structures\\n\\n3.2 Rings and Fields\\nIn this section we extend the concept of a group and discuss mathematical structures that are characterized by two operations.\\nAs motivating example consider the integers with the addition, i.e., the group (Z, +).\\nWe can multiply the elements of Z and this multiplication is associative, i.e., (a ·b)·c = a ·(b ·c) for all a, b, c ∈ Z. Furthermore the addition and multiplication satisfy the distributive laws a · (b + c) = a · b + a · c and (a + b) · c = a · c + b · c for all integers a, b, c.\\nThese properties make Z with addition and multiplication into a ring.\\nDefinition 3.7 A ring is a set R with two operations\\n+ : R × R → R,\\n\\n(a, b) \u0004→ a + b,\\n\\n(addition)\\n\\n∗ : R × R → R,\\n\\n(a, b) \u0004→ a ∗ b,\\n\\n(multiplication) that satisfy the following:\\n(1) (R, +) is a commutative group.\\nWe call the neutral element in this group zero, and write 0.\\nWe denote the inverse element of a ∈ R by −a, and write a − b instead of a + (−b).\\n(2) The multiplication is associative, i.e., (a ∗ b) ∗ c = a ∗ (b ∗ c) for all a, b, c ∈ R.\\n(3) The distributive laws hold, i.e., for all a, b, c ∈ R we have a ∗ (b + c) = a ∗ b + a ∗ c,\\n(a + b) ∗ c = a ∗ c + b ∗ c.\\nA ring is called commutative if a ∗ b = b ∗ a for all a, b ∈ R.\\nAn element 1 ∈ R is called unit if 1 ∗ a = a ∗ 1 = a for all a ∈ R. In this case R is called a ring with unit.\\nOn the right hand side of the two distributive laws we have omitted the parentheses, since multiplication is supposed to bind stronger than addition, i.e., a + (b ∗ c) = a + b ∗ c.\\nIf it is useful for illustration purposes we nevertheless use parentheses, e.g., we sometimes write (a ∗ b) + (c ∗ d) instead of a ∗ b + c ∗ d.\\nAnalogous to the notation for groups we denote a ring with (R, +, ∗) or just with\\nR, if the operations are clear from the context.\\nIn a ring with unit, the unit element is unique: If 1, e ∈ R satisfy 1 ∗ a = a ∗ 1 = a and e ∗ a = a ∗ e = a for all a ∈ R, then in particular e = e ∗ 1 = 1.\\nFor a1 , a2 , . . . , an ∈ R we use the following abbreviations for the sum and product of these elements: n\\n\u0003 j=1 a j := a1 + a2 + . . . + an and n\\n\u0004 j=1 a j := a1 ∗ a2 ∗ . . . ∗ an .</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35</td>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.2 Rings and Fields</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algebraic structure ring field section extend concept group discus mathematical structure characterized two operation motivating example consider integer addition group z multiply element z multiplication associative b b c z furthermore addition multiplication satisfy distributive law b c b c b c c b c integer b property make z addition multiplication ring definition ring set r two operation r r r b b addition r r r b b multiplication satisfy following r commutative group call neutral element group zero write denote inverse element r write b instead multiplication associative b c b c b c distributive law hold b c r b c b c b c c b ring called commutative b b b element r called unit case r called ring unit right hand side two distributive law omitted parenthesis since multiplication supposed bind stronger addition b c b useful illustration purpose nevertheless use parenthesis sometimes write b c instead b c analogous notation group denote ring r r operation clear context ring unit unit element unique e r satisfy e e r particular e e r use following abbreviation sum product element n j n j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.2 Rings and Fields\\n\\nMoreover, a n := empty sum as\\n\\n27\\n\\n\u0005n j=1 a for all a ∈ R and n ∈ N. If \u0002 &gt; k, then we define the k\\n\u0003 a j := 0.\\nj=\u0002\\n\\nIn a ring with unit we also define for \u0002 &gt; k the empty product as k\\n\u0004 a j := 1.\\nj=\u0002\\n\\nTheorem 3.8 For every ring R the following assertions hold:\\n(1) 0 ∗ a = a ∗ 0 = 0 for all a ∈ R.\\n(2) a ∗ (−b) = −(a ∗ b) = (−a) ∗ b and (−a) ∗ (−b) = a ∗ b for all a, b ∈ R.\\nProof\\n(1) For every a ∈ R we have 0 ∗ a = (0 + 0) ∗ a = (0 ∗ a) + (0 ∗ a).\\nAdding\\n−(0 ∗ a) on the left and right hand sides of this equality we obtain 0 = 0 ∗ a.\\nIn the same way we can show that a ∗ 0 = 0 for all a ∈ R.\\n(2) Since (a ∗ b) + (a ∗ (−b)) = a ∗ (b + (−b)) = a ∗ 0 = 0, it follows that a ∗ (−b) is the (unique) additive inverse of a ∗ b, i.e., a ∗ (−b) = −(a ∗ b).\\nIn the same way we can show that (−a) ∗ b = −(a ∗ b).\\nFurthermore, we have\\n0 = 0 ∗ (−b) = (a + (−a)) ∗ (−b) = a ∗ (−b) + (−a) ∗ (−b)\\n= −(a ∗ b) + (−a) ∗ (−b), and thus (−a) ∗ (−b) = a ∗ b.\\n\\n\u0007\\n\u0006\\n\\nIt is immediately clear that (Z, +, ∗) is a commutative ring with unit.\\nThis is the standard example, by which the concept of a ring was modeled.\\nExample 3.9 Let M be a nonempty set and let R be the set of maps f : M → R.\\nThen (R, +, ∗) with the operations\\n+ : R × R → R,\\n∗ : R × R → R,\\n\\n( f, g) \u0004→ f + g,\\n( f, g) \u0004→ f ∗ g,\\n\\n( f + g)(x) := f (x) + g(x),\\n( f ∗ g)(x) := f (x) · g(x), is a commutative ring with unit.\\nHere f (x) + g(x) and f (x) · g(x) are the sum and product of two real numbers.\\nThe zero in this ring is the map 0 R : M → R, x \u0004→ 0, and the unit is the map 1 R : M → R, x \u0004→ 1, where 0 and 1 are the real numbers zero and one.\\nIn the definition of a ring only additive inverse elements occur.\\nWe will now formally define the concept of a multiplicative inverse.</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36</td>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.2 Rings and Fields</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ring field moreover n empty sum r n k define k j ring unit also define k empty product k j theorem every ring r following assertion hold b b b b proof every r adding left right hand side equality obtain way show since b b follows unique additive inverse b b way show b b furthermore b thus b immediately clear z commutative ring unit standard example concept ring modeled example let nonempty set let r set map f r operation r r r r r r f g f g f g f g f g x f x g x f g x f x g x commutative ring unit f x g x f x g x sum product two real number zero ring map r r x unit map r r x real number zero one definition ring additive inverse element occur formally define concept multiplicative inverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>28\\n\\n3 Algebraic Structures\\n\\nDefinition 3.10 Let (R, +, ∗) be a ring with unit.\\nAn element b ∈ R is called an inverse of a ∈ R (with respect to ∗), if a ∗ b = b ∗ a = 1.\\nAn element of R that has an inverse is called invertible.\\nIt is clear from the definition that b ∈ R is an inverse of a ∈ R if and only if a ∈ R is an inverse of b ∈ R. In general, however, not every element in a ring must be (or is) invertible.\\nBut if an element is invertible, then it has a unique inverse, as shown in the following theorem.\\nTheorem 3.11 Let (R, +, ∗) be a ring with unit.\\n(1) If a ∈ R is invertible, then the inverse is unique and we denote it by a −1 .\\n(2) If a, b ∈ R are invertible then a ∗ b ∈ R is invertible and (a ∗ b)−1 = b−1 ∗ a −1 .\\nProof\\n(1) If b, \u0002 b ∈ R are inverses of a ∈ R, then b = b ∗ 1 = b ∗ (a ∗ \u0002 b) = (b ∗ a) ∗ \u0002 b=\\n\u0002\\n\u0002\\n1 ∗ b = b.\\n(2) Since a and b are invertible, b−1 ∗ a −1 ∈ R is well defined and\\n(b−1 ∗ a −1 )∗(a ∗ b) = ((b−1 ∗ a −1 )∗ a)∗b = (b−1 ∗ (a −1 ∗ a))∗ b = b−1 ∗ b = 1.\\n\\nIn the same way we can show that (a ∗ b) ∗ (b−1 ∗ a −1 ) = 1, and thus\\n\u0007\\n\u0006\\n(a ∗ b)−1 = b−1 ∗ a −1 .\\nFrom an algebraic point of view the difference between the integers on the one hand, and the rational or real numbers on the other, is that in the sets Q and R every element (except for the number zero) is invertible.\\nThis “additional structure” makes\\nQ and R into fields.\\nDefinition 3.12 A commutative ring R with unit is called a field, if 0 = 1 and every a ∈ R \\ {0} is invertible.\\nBy definition, every field is a commutative ring with unit, but the converse does not hold.\\nOne can also introduce the concept of a field based on the concept of a group (cp.\\nExercise 3.15).\\nDefinition 3.13 A field is a set K with two operations\\n+ : K × K → K,\\n∗ : K × K → K,\\n\\n(a, b) \u0004→ a + b,\\n(a, b) \u0004→ a ∗ b,\\n\\n(addition)\\n(multiplication)</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37</td>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.2 Rings and Fields</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algebraic structure definition let r ring unit element b r called inverse r respect b b element r inverse called invertible clear definition b r inverse r r inverse b general however every element ring must invertible element invertible unique inverse shown following theorem theorem let r ring unit r invertible inverse unique denote b r invertible b r invertible b proof b b r inverse r b b b b b b b since b invertible r well defined b b b way show b thus b algebraic point view difference integer one hand rational real number set q r every element except number zero invertible additional structure make q r field definition commutative ring r unit called field every r invertible definition every field commutative ring unit converse hold one also introduce concept field based concept group cp exercise definition field set k two operation k k k k k k b b b b addition multiplication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.2 Rings and Fields\\n\\n29 that satisfy the following:\\n(1) (K , +) is a commutative group.\\nWe call the neutral element in this group zero, and write 0.\\nWe denote the inverse element of a ∈ K by −a, and write a − b instead of a + (−b).\\n(2) (K \\ {0}, ∗) is a commutative group.\\nWe call the neutral element in this group unit, and write 1.\\nWe denote the inverse element of a ∈ K \\ {0} by a −1 .\\n(3) The distributive laws hold, i.e., for all a, b, c ∈ K we have a ∗ (b + c) = a ∗ b + a ∗ c,\\n(a + b) ∗ c = a ∗ c + b ∗ c.\\nWe now show a few useful properties of fields.\\nLemma 3.14 For every field K the following assertions hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n\\nK has at least two elements.\\n0 ∗ a = a ∗ 0 = 0 for all a ∈ K .\\na ∗ b = a ∗ c and a = 0 imply that b = c for all a, b, c ∈ K .\\na ∗ b = 0 imply that a = 0 or b = 0 for all a, b ∈ K .\\n\\nProof\\n(1) This follows from the definition, since 0, 1 ∈ K with 0 = 1.\\n(2) This has already been shown for rings (cp.\\nTheorem 3.8).\\n(3) Since a = 0, we know that a −1 exists.\\nMultiplying both sides of a ∗ b = a ∗ c from the left with a −1 yields b = c.\\n(4) Suppose that a ∗ b = 0.\\nIf a = 0, then we are finished.\\nIf a = 0, then a −1 exists\\n\u0007 and multiplying both sides of a ∗ b = 0 from the left with a −1 yields b = 0. \u0006\\nFor a ring R an element a ∈ R is called a zero divisor,3 if a b ∈ R \\ {0} exists with a ∗ b = 0.\\nThe element a = 0 is called the trivial zero divisor.\\nProperty (4) in\\nLemma 3.14 means that fields contain only the trivial zero divisor.\\nThere are also rings in which property (4) holds, for instance the ring of integers Z. In later chapters we will encounter rings of matrices that contain non-trivial zero divisors (see e.g. the proof of Theorem 4.9 below).\\nThe following definition is analogous to the concepts of a subgroup (cp.\\nDefinition 3.4) and a subring (cp.\\nExcercise 3.14).\\nDefinition 3.15 Let (K , +, ∗) be a field and L ⊆ K .\\nIf (L , +, ∗) is a field, then it is called a subfield of (K , +, ∗).\\nAs two very important examples for algebraic concepts discussed above we now discuss the field of complex numbers and the ring of polynomials.\\n3 The concept of zero divisors was introduced in 1883 by Karl Theodor Wilhelm Weierstraß (1815–\\n\\n1897).</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38</td>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.2 Rings and Fields</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ring field satisfy following k commutative group call neutral element group zero write denote inverse element k write b instead k commutative group call neutral element group unit write denote inverse element k distributive law hold b c k b c b c b c c b show useful property field lemma every field k following assertion hold k least two element k b c imply b c b c k b imply b b k proof follows definition since k already shown ring cp theorem since know exists multiplying side b c left yield b suppose b finished exists multiplying side b left yield b ring r element r called zero b r exists b element called trivial zero divisor property lemma mean field contain trivial zero divisor also ring property hold instance ring integer z later chapter encounter ring matrix contain non-trivial zero divisor see proof theorem following definition analogous concept subgroup cp definition subring cp excercise definition let k field l k l field called subfield k two important example algebraic concept discussed discus field complex number ring polynomial concept zero divisor introduced karl theodor wilhelm weierstraß</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>30\\n\\n3 Algebraic Structures\\n\\nExample 3.16 The set of complex numbers is defined as\\nC := { (x, y) | x, y ∈ R } = R × R.\\nOn this set we define the following operations as addition and multiplication:\\n+ : C × C → C, (x1 , y1 ) + (x2 , y2 ) := (x1 + x2 , y1 + y2 ),\\n· : C × C → C, (x1 , y1 ) · (x2 , y2 ) := (x1 · x2 − y1 · y2 , x1 · y2 + x2 · y1 ).\\nOn the right hand sides we here use the addition and the multiplication in the field\\nR. Then (C, +, ·) is a field with the neutral elements with respect to addition and multiplication given by\\n0C = (0, 0),\\n1C = (1, 0), and the inverse elements with respect to addition and multiplication given by\\n−(x, y) = (−x, −y) for all (x, y) ∈ C,\\n\u0007\\n\u0006 x y\\n−1 for all (x, y) ∈ C \\ {(0, 0)}.\\n,− 2\\n(x, y) = x 2 + y2 x + y2\\nIn the multiplicative inverse element we have written ab instead of a · b−1 , which is the common notation in R.\\nConsidering the subset L := {(x, 0) | x ∈ R} ⊂ C, we can identify every x ∈ R with an element of the set L via the (bijective) map x \u0004→ (x, 0).\\nIn particular,\\n0R \u0004→ (0, 0) = 0C and 1R \u0004→ (1, 0) = 1C .\\nThus, we can interpret R as subfield of C\\n(although R is not really a subset of C), and we do not have to distinguish between the zero and unit elements in R and C.\\nA special complex number is the imaginary unit (0, 1), which satisfies\\n(0, 1) · (0, 1) = (0 · 0 − 1 · 1, 0 · 1 + 0 · 1) = (−1, 0) = −1.\\nHere again we have identified the real number −1 with the complex number (−1, 0).\\nThe imaginary unit is denoted by i, i.e., i := (0, 1), and hence we can write i2 = −1.\\nUsing the identification of x ∈ R with (x, 0) ∈ C we can write z = (x, y) ∈ C as\\n(x, y) = (x, 0) + (0, y) = (x, 0) + (0, 1) · (y, 0) = x + iy = Re(z) + i Im(z).</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39</td>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.2 Rings and Fields</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algebraic structure example set complex number defined c x x r r set define following operation addition multiplication c c c c c c right hand side use addition multiplication field c field neutral element respect addition multiplication given inverse element respect addition multiplication given x x c x x c x x x multiplicative inverse element written ab instead common notation considering subset l x x r c identify every x r element set l via bijective map x x particular thus interpret r subfield c although r really subset c distinguish zero unit element r special complex number imaginary unit satisfies identified real number complex number imaginary unit denoted hence write using identification x r x c write z x c x x x x iy z im z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.2 Rings and Fields\\n\\n31\\n\\nIn the last expression Re(z) = x and Im(z) = y are the abbreviations for real part and imaginary part of the complex number z = (x, y).\\nSince (0, 1) · (y, 0) =\\n(y, 0) · (0, 1), i.e., iy = yi, it is justified to write the complex number x + iy as x + yi.\\nFor a given complex number z = (x, y) or z = x + iy the number z := (x, −y), respectively z := x − iy, is called the associated complex conjugate number.\\nUsing the (real) square root, the modulus or absolute value of a complex number is defined as\\n\b\\n|z| := (zz)1/2 = (x + iy) (x − iy)\\n\\n1/2\\n\\n\b\\n= x 2 − ix y + iyx − i2 y 2\\n\\n1/2\\n\\n= (x 2 + y 2 )1/2 .\\n\\n(Again, for simplification we have omitted the multiplication sign between two complex numbers.)\\nThis equation shows that the absolute value of a complex number is a nonnegative real number.\\nFurther properties of complex numbers are stated in the exercises at the end of this chapter.\\nExample 3.17 Let (R, +, ·) be a commutative ring with unit.\\nA polynomial over R and in the indeterminate or variable t is an expression of the form p = α0 · t 0 + α1 · t 1 + . . . + αn · t n , where α0 , α1 , . . . , αn ∈ R are the coefficients of the polynomial.\\nInstead of α0 · t 0 , t 1 and α j · t j we often just write α0 , t and α j t j .\\nThe set of all polynomials over R is denoted by R[t].\\nLet p = α0 + α1 · t + . . . + αn · t n , q = β0 + β1 · t + . . . + βm · t m be two polynomials in R[t] with n ≥ m.\\nIf n &gt; m, then we set β j = 0 for j = m + 1, . . . , n and call p and q equal, written p = q, if α j = β j for j = 0, 1, . . . , n.\\nIn particular, we have\\nα0 + α1 · t + . . . + αn · t n = αn · t n + . . . + α1 · t + α0 ,\\n0 + 0 · t + . . . + 0 · t n = 0.\\nThe degree of the polynomial p = α0 + α1 · t + . . . + αn · t n , denoted by deg( p), is defined as the largest index j, for which α j = 0.\\nIf no such index exists, then the polynomial is the zero polynomial p = 0 and we set deg( p) := −∞.\\nLet p, q ∈ R[t] as above have degrees n, m, respectively, with n ≥ m.\\nIf n &gt; m, then we again set β j = 0, j = m + 1, . . . , n.\\nWe define the following operations on\\nR[t]:</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40</td>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.2 Rings and Fields</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ring field last expression z x im z abbreviation real part imaginary part complex number z x since iy yi justified write complex number x iy x yi given complex number z x z x iy number z x respectively z x iy called associated complex conjugate number using real square root modulus absolute value complex number defined zz x iy x iy x ix iyx x simplification omitted multiplication sign two complex number equation show absolute value complex number nonnegative real number property complex number stated exercise end chapter example let r commutative ring unit polynomial r indeterminate variable expression form p αn n αn r coefficient polynomial instead α j j often write α j j set polynomial r denoted r let p αn n q βm two polynomial r n n set β j j n call p q equal written p q α j β j j particular αn n αn n n degree polynomial p αn n denoted deg p defined largest index j α j index exists polynomial zero polynomial p set deg p let p q r degree n respectively n n set β j j define following operation r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32\\n\\n3 Algebraic Structures p + q := (α0 + β0 ) + (α1 + β1 ) · t + . . . + (αn + βn ) · t n ,\\n\u0003\\nαi β j .\\np ∗ q := γ0 + γ1 · t + . . . + γn+m · t n+m , γk := i+ j=k\\n\\nWith these operations (R[t], +, ∗) is a commutative ring with unit.\\nThe zero is given by the polynomial p = 0 and the unit is p = 1 · t 0 = 1.\\nBut R[t] it is not a field, since not every polynomial p ∈ R[t] \\ {0} is invertible, not even if R is a field.\\nFor example, for p = t and any other polynomial q = β0 + β1 t + . . . + βm t m ∈ R[t] we have p ∗ q = β0 t + β1 t 2 + . . . + βm t m+1 = 1, and hence p is not invertible.\\nIn a polynomial we can “substitute” the variable t by some other object when the resulting expression can be evaluated algebraically.\\nFor example, we may substitute t by any λ ∈ R and interpret the addition and multiplication as the corresponding operations in the ring R. This defines a map from R to R by\\nλ \u0004→ p(λ) = α0 · λ0 + α1 · λ1 + . . . + αn · λn , λk := λ · . . . · λ, k = 0, 1, . . . , n, k times where λ0 = 1 ∈ R (this is an empty product).\\nHere one should not confuse the ring element p(λ) with the polynomial p itself, but rather think of p(λ) as an evaluation of p at λ.\\nWe will study the properties of polynomials in more detail later on, and we will also evaluate polynomials at other objects such as matrices or endomorphisms.\\nExercises\\n3.1 Determine for the following (M, ⊕) whether they form a group:\\n(a) M = {x ∈ R | x &gt; 0} and ⊕ : M × M → M, (a, b) \u0004→ a b .\\n(b) M = R \\ {0} and ⊕ : M × M → M, (a, b) \u0004→ ab .\\n3.2 Let a, b ∈ R, the map f a,b : R × R → R × R, (x, y) \u0004→ (ax − by, ay), and the set G = { f a,b | a, b ∈ R, a = 0} be given.\\nShow that (G, ◦) is a commutative group, when the operation ◦ : G × G → G is defined as the composition of two maps (cp.\\nDefinition 2.18).\\n3.3 Let X = Ø be a set and let S(X ) = { f : X → X | f is bijective}.\\nShow that\\n(S(X ), ◦) is a group.\\n3.4 Let (G, ⊕) be a group.\\nFor a ∈ G denote by −a ∈ G the (unique) inverse element.\\nShow the following rules for elements of G:\\n(a)\\n(b)\\n(c)\\n(d)\\n\\n−(−a) = a.\\n−(a ⊕ b) = (−b) ⊕ (−a).\\na ⊕ b1 = a ⊕ b2 ⇒ b1 = b2 .\\na1 ⊕ b = a2 ⊕ b ⇒ a1 = a2 .</td>\n",
       "      <td>41.0</td>\n",
       "      <td>41</td>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.2 Rings and Fields</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algebraic structure p q αn βn n αi β j p q γk operation r commutative ring unit zero given polynomial p unit p r field since every polynomial p r invertible even r field example p polynomial q βm r p q βm hence p invertible polynomial substitute variable object resulting expression evaluated algebraically example may substitute λ r interpret addition multiplication corresponding operation ring defines map r r λ p λ αn λn λk λ λ k n k time r empty product one confuse ring element p λ polynomial p rather think p λ evaluation p λ study property polynomial detail later also evaluate polynomial object matrix endomorphisms exercise determine following whether form group x r x b b b r b ab let b r map f b r r r r x ax ay set g f b b r given show g commutative group operation g g g defined composition two map cp definition let x ø set let x f x x f bijective show x group let g group g denote g unique inverse element show following rule element g b c b b b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.2 Rings and Fields\\n\\n33\\n\\n3.5 Prove Theorem 3.5.\\n3.6 Let (G, ⊕) be a group and for a fixed a ∈ G let Z G (a) = {g ∈ G | a ⊕ g = g ⊕ a}.\\nShow that Z G (a) is a subgroup of G.\\n(This subgroup of all elements of G that commute with a is called centralizer of a.)\\n3.7 Let ϕ : G → H be a group homomorphism.\\nShow the following assertions:\\n(a) If U ⊆ G is a subgroup, then also ϕ(U ) ⊆ H is a subgroup.\\nIf, furthermore, G is commutative, then also ϕ(U ) is commutative (even if H is not commutative).\\n(b) If V ⊆ H is a subgroup, then also ϕ−1 (V ) ⊆ G is a subgroup.\\n3.8 Let ϕ : G → H be a group homomorphism and let eG and e H be the neutral elements of the groups G and H , respectively.\\n(a) Show that ϕ(eG ) = e H .\\n(b) Let ker(ϕ) := {g ∈ G | ϕ(g) = e H }.\\nShow that ϕ is injective if and only if ker(ϕ) = {eG }.\\n3.9 Show the properties in Definition 3.7 for (R, +, ∗) from Example 3.9 in order to show that (R, +, ∗) is a commutative ring with unit.\\nSuppose that in Example\\n3.9 we replace the codomain R of the maps by a commutative ring with unit.\\nIs (R, +, ∗) then still a commutative ring with unit?\\n3.10 Let R be a ring and n ∈ N. Show the following assertions:\\n\u000e an , if n is even,\\n(a) For all a ∈ R we have (−a)n =\\n−a n , if n is odd.\\n(b) If there exists a unit in R and if a n = 0 for a ∈ R, then 1 − a is invertible.\\n(An element a ∈ R with a n = 0 for some n ∈ N is called nilpotent.)\\n3.11 Let R be a ring with unit.\\nShow that 1 = 0 if and only if R = {0}.\\n3.12 Let (R, +, ∗) be a ring with unit and let R × denote the set of all invertible elements of R.\\n(a) Show that (R × , ∗) is a group (called the group of units of R).\\n(b) Determine the sets Z× , K × , and K [t]× , when K is a field.\\n3.13 For fixed n ∈ N let nZ = {nk | k ∈ Z} and Z/nZ = {[0], [1], . . . , [n − 1]} be as in Example 2.29.\\n(a) Show that nZ is a subgroup of Z.\\n(b) Define by\\n⊕ : Z/nZ × Z/nZ → Z/nZ, ([a], [b]) \u0004→ [a] ⊕ [b] = [a + b],\\n\u0010 : Z/nZ × Z/nZ → Z/nZ, ([a], [b]) \u0004→ [a] \u0010 [b] = [a · b], an addition and multiplication in Z/nZ, (with + and · being the addition and multiplication in Z).\\nShow the following assertions:</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42</td>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.2 Rings and Fields</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ring field prove theorem let g group fixed g let z g g g g g show z g subgroup subgroup element g commute called centralizer let ϕ g h group homomorphism show following assertion u g subgroup also ϕ u h subgroup furthermore g commutative also ϕ u commutative even h commutative b v h subgroup also v g subgroup let ϕ g h group homomorphism let eg e h neutral element group g h respectively show ϕ eg e h b let ker ϕ g g ϕ g e h show ϕ injective ker ϕ eg show property definition r example order show r commutative ring unit suppose example replace codomain r map commutative ring unit r still commutative ring unit let r ring n show following assertion n even r n n n odd b exists unit r n r invertible element r n n n called nilpotent let r ring unit show r let r ring unit let r denote set invertible element show r group called group unit r b determine set k k k field fixed n n let nz nk k z n example show nz subgroup z b define b b b b b b addition multiplication addition multiplication z show following assertion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>34\\n\\n3 Algebraic Structures\\n\\n(i) ⊕ and \u0010 are well defined.\\n(ii) (Z/nZ, ⊕, \u0010) is a commutative ring with unit.\\n(iii) (Z/nZ, ⊕, \u0010) is a field if and only if n is a prime number.\\n3.14 Let (R, +, ∗) be a ring.\\nA subset S ⊆ R is called a subring of R, if (S, +, ∗) is a ring.\\nShow that S is a subring of R if and only if the following properties hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n\\nS ⊆ R.\\n0 R ∈ S.\\nFor all r, s ∈ S also r + s ∈ S and r ∗ s ∈ S.\\nFor all r ∈ S also −r ∈ S.\\n\\n3.15 Show that the Definitions 3.12 and 3.13 of a field describe the same mathematical structure.\\n3.16 Let (K , +, ∗) be a field.\\nShow that (L , +, ∗) is a subfield of (K , +, ∗) (cp.\\nDefinition 3.15), if and only if the following properties hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\n\\nL ⊆ K.\\n0 K , 1 K ∈ L.\\na + b ∈ L and a ∗ b ∈ L for all a, b ∈ L.\\n−a ∈ L for all a ∈ L.\\na −1 ∈ L for all a ∈ L \\ {0}.\\n\\n3.17 Show that in a field 1 + 1 = 0 holds if and only if 1 + 1 + 1 + 1 = 0.\\n3.18 Let (R, +, ∗) be a commutative ring with 1 = 0 that does not contain non-trivial zero divisors.\\n(Such a ring is called an integral domain.)\\n(a) Define on M = R × R \\ {0} a relation by\\n(x, y) ∼ (\u000f x,\u000f y)\\n\\n⇔ x ∗\u000f y = y ∗\u000f x.\\n\\nShow that this is an equivalence relation.\\n(b) Denote the equivalence class [(x, y)] by xy .\\nShow that the following maps are well defined:\\n\u000f x x ∗\u000f y + y ∗\u000f x x\\n⊕ :=\\n, y \u000f y y ∗\u000f y x\\n\u000f x x ∗\u000f x\\n\u0010 : (M/ ∼) × (M/ ∼) → (M/ ∼) with\\n\u0010 :=\\n, y \u000f y y ∗\u000f y\\n\\n⊕ : (M/ ∼) × (M/ ∼) → (M/ ∼) with where M/ ∼ denotes the quotient set with respect to ∼ (cp.\\nDefinition 2.27).\\n(c) Show that (M/ ∼, ⊕, \u0010) is a field.\\n(This field is called the quotient field associated with R.)\\n(d) Which field is (M/ ∼, ⊕, \u0010) for R = Z?\\n3.19 In Exercise 3.18 consider R = K [t], the ring of polynomials over the field K , and construct in this way the field of rational functions.</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43</td>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.2 Rings and Fields</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algebraic structure well defined ii commutative ring unit iii field n prime number let r ring subset r called subring r ring show subring r following property hold r r also r r r also show definition field describe mathematical structure let k field show l subfield k cp definition following property hold l k k b l b l b l l l show field hold let r commutative ring contain non-trivial zero divisor ring called integral domain define r r relation x x x x show equivalence relation b denote equivalence class x xy show following map well defined x x x x x x x x denotes quotient set respect cp definition c show field field called quotient field associated field r z exercise consider r k ring polynomial field k construct way field rational function</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.2 Rings and Fields\\n\\n35\\n\\n3.20 Let a = 2 + i ∈ C and b = 1 − 3i ∈ C. Determine −a, −b, a + b, a − b, a −1 , b−1 , a −1 a, b−1 b, ab, ba.\\n3.21 Show the following rules for the complex numbers:\\n(a) z 1 + z 2 = z 1 + z 2 and z 1 z 2 = z 1 z 2 for all z 1 , z 2 ∈ C.\\n(b) z −1 = (z)−1 and Re(z −1 ) = |z|1 2 Re(z) for all z ∈ C \\ {0}.\\n3.22 Show that the absolute value of complex numbers satisfies the following properties:\\n(a) |z 1 z 2 | = |z 1 | |z 2 | for all z 1 , z 2 ∈ C.\\n(b) |z| ≥ 0 for all z ∈ C with equality if and only if z = 0.\\n(c) |z 1 + z 2 | ≤ |z 1 | + |z 2 | for all z 1 , z 2 ∈ C.</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44</td>\n",
       "      <td>3 Algebraic Structures</td>\n",
       "      <td>3.2 Rings and Fields</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ring field let c b determine b b b ab ba show following rule complex number z z z z z z z z z z b z z z z z c show absolute value complex number satisfies following property z z z b z c equality z c z z z c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Chapter 4\\n\\nMatrices\\n\\nIn this chapter we define matrices with their most important operations and we study several groups and rings of matrices.\\nJames Joseph Sylvester (1814–1897) coined the term matrix 1 in 1850 and described matrices as “an oblong arrangement of terms”.\\nThe matrix operations defined in this chapter were introduced by Arthur Cayley\\n(1821–1895) in 1858.\\nHis article “A memoir on the theory of matrices” was the first to consider matrices as independent algebraic objects.\\nIn our book matrices form the central approach to the theory of Linear Algebra.\\n\\n4.1 Basic Definitions and Operations\\nWe begin with a formal definition of matrices.\\nDefinition 4.1 Let R be a commutative ring with unit and let n, m ∈ N0 .\\nAn array of the form\\n⎤\\n⎡ a11 a12 · · · a1m\\n⎢a21 a22 · · · a2m ⎥\\n⎥\\n⎢\\nA = [ai j ] = ⎢ . .\\n.. ⎥\\n⎣ .. ..\\n. ⎦ an1 an2 · · · anm\\n\\n1 The\\n\\nLatin word “matrix” means “womb”.\\nSylvester considered matrices as objects “out of which we may form various systems of determinants” (cp.\\nChap.\\n5).\\nInterestingly, the English writer\\nCharles Lutwidge Dodgson (1832–1898), better known by his pen name Lewis Carroll, objected to\\nSylvester’s term and wrote in 1867: “I am aware that the word ‘Matrix’ is already in use to express the very meaning for which I use the word ‘Block’; but surely the former word means rather the mould, or form, into which algebraic quantities may be introduced, than an actual assemblage of such quantities”.\\nDodgson also objected to the notation ai j for the matrix entries: “…most of the space is occupied by a number of a’s, which are wholly superfluous, while the only important part of the notation is reduced to minute subscripts, alike difficult to the writer and the reader.”\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_4\\n\\n37</td>\n",
       "      <td>45.0</td>\n",
       "      <td>&lt;FEFF&gt;</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.1 Basic Definitions and Operations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter matrix chapter define matrix important operation study several group ring matrix james joseph sylvester coined term matrix described matrix oblong arrangement term matrix operation defined chapter introduced arthur cayley article memoir theory matrix first consider matrix independent algebraic object book matrix form central approach theory linear algebra basic definition operation begin formal definition matrix definition let r commutative ring unit let n array form ai j anm latin word matrix mean womb sylvester considered matrix object may form various system determinant cp chap interestingly english writer charles lutwidge dodgson better known pen name lewis carroll objected sylvester term wrote aware word matrix already use express meaning use word block surely former word mean rather mould form algebraic quantity may introduced actual assemblage quantity dodgson also objected notation ai j matrix entry space occupied number wholly superfluous important part notation reduced minute subscript alike difficult writer springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>38\\n\\n4 Matrices with ai j ∈ R, i = 1, . . . , n, j = 1, . . . , m, is called a matrix of size n × m over R.\\nThe ai j are called the entries or coefficients of the matrix.\\nThe set of all such matrices is denoted by R n,m .\\nIn the following we usually assume (without explicitly mentioning it) that 1 \u0003= 0 in R. This excludes the trivial case of the ring that contains only the zero element\\n(cp.\\nExercise 3.11).\\nFormally, in Definition 4.1 for n = 0 or m = 0 we obtain “empty matrices” of the size 0 × m, n × 0 or 0 × 0.\\nWe denote such matrices by [ ].\\nThey will be used for technical reasons in some of the proofs below.\\nWhen we analyze algebraic properties of matrices, however, we always consider n, m ≥ 1.\\nThe zero matrix in R n,m , denoted by 0n,m or just 0, is the matrix that has all its entries equal to 0 ∈ R.\\nA matrix of size n × n is called a square matrix or just square.\\nThe entries aii for i = 1, . . . , n are called the diagonal entries of A. The identity matrix in R n,n is the matrix In := [δi j ], where\\n\b\\n1, if i = j,\\nδi j :=\\n(4.1)\\n0, if i \u0003= j.\\nis the Kronecker delta-function.2 If it is clear which n is considered, then we just write I instead of In .\\nFor n = 0 we set I0 := [ ].\\nThe ith row of A ∈ R n,m is [ai1 , ai2 , . . . , aim ] ∈ R 1,m , i = 1, . . . , n, where we use commas for the optical separation of the entries.\\nThe jth column of A is\\n⎡ ⎤ a1 j\\n⎢a2 j ⎥\\n⎢ ⎥\\n⎢ .. ⎥ ∈ R n,1 ,\\n⎣ . ⎦ j = 1, . . . , m.\\nan j\\nThus, the rows and columns of a matrix are again matrices.\\nIf 1 × m matrices ai := [ai1 , ai2 , . . . , aim ] ∈ R 1,m , i = 1, . . . , n, are given, then we can combine them to the matrix\\n⎡ ⎤ ⎡\\n⎤ a11 a12 · · · a1m a1\\n⎢a2 ⎥ ⎢a21 a22 · · · a2m ⎥\\n⎢ ⎥ ⎢\\n⎥ n,m\\nA=⎢.⎥=⎢ . .\\n.. ⎥ ∈ R .\\n⎣ .. ⎦ ⎣ .. ..\\n. ⎦ an\\n\\n2 Leopold\\n\\nKronecker (1823–1891).\\nan1 an2 · · · anm</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.1 Basic Definitions and Operations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix ai j r n j called matrix size n ai j called entry coefficient matrix set matrix denoted r n following usually assume without explicitly mentioning excludes trivial case ring contains zero element cp exercise formally definition n obtain empty matrix size n denote matrix used technical reason proof analyze algebraic property matrix however always consider n zero matrix r n denoted matrix entry equal matrix size n n called square matrix square entry aii n called diagonal entry identity matrix r n n matrix δi j j δi j kronecker delta-function.2 clear n considered write instead n set ith row r n aim r n use comma optical separation entry jth column j j r j j thus row column matrix matrix matrix ai aim r n given combine matrix n r leopold kronecker anm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.1 Basic Definitions and Operations\\n\\n39\\n\\nWe then do not write square brackets around the rows of A. In the same way we can combine the n × 1 matrices\\n⎡ ⎤ a1 j\\n⎢a2 j ⎥\\n⎢ ⎥ a j := ⎢ . ⎥ ∈ R n,1 , j = 1, . . . , m,\\n⎣ .. ⎦ an j to the matrix\\n⎡ a11\\n⎢a21\\n⎢\\nA = [a1 , a2 , . . . , am ] = ⎢ .\\n⎣ ..\\na12 · · · a22 · · ·\\n..\\n.\\n\\n⎤ a1m a2m ⎥\\n⎥ n,m\\n.. ⎥ ∈ R .\\n. ⎦ an1 an2 · · · anm\\n\\nIf n 1 , n 2 , m 1 , m 2 ∈ N0 and Ai j ∈ R ni ,m j , i, j = 1, 2, then we can combine these four matrices to the matrix\\nA=\\n\\nA11 A12\\nA21 A22\\n\\n∈ R n 1 +n 2 ,m 1 +m 2 .\\n\\nThe matrices Ai j are then called blocks of the block matrix A.\\nWe now introduce four operations for matrices and begin with the addition:\\n+ : R n,m × R n,m → R n,m ,\\n\\n(A, B) \u0006→ A + B := [ai j + bi j ].\\n\\nThe addition in R n,m operates entrywise, based on the addition in R. Note that the addition is only defined for matrices of equal size.\\nThe multiplication of two matrices is defined as follows: m\\n\\n∗ : R n,m × R m,s → R n,s , (A, B) \u0006→ A ∗ B = [ci j ], ci j := aik bk j .\\nk=1\\n\\nThus, the entry ci j of the product A ∗ B is constructed by successive multiplication and summing up the entries in the ith row of A and the jth column of B. Clearly, in order to define the product A ∗ B, the number of columns of A must be equal to the number of rows in B.\\nIn the definition of the entries ci j of the matrix A ∗ B we have not written the multiplication symbol for the elements in R. This follows the usual convention of omitting the multiplication sign when it is clear which multiplication is considered.\\nEventually we will also omit the multiplication sign between matrices.</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.1 Basic Definitions and Operations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic definition operation write square bracket around row way combine n matrix j j j r j j matrix n r anm n n ai j r ni j j combine four matrix matrix r n matrix ai j called block block matrix introduce four operation matrix begin addition r n r n r n b b ai j bi j addition r n operates entrywise based addition note addition defined matrix equal size multiplication two matrix defined follows r n r r n b b ci j ci j aik bk j thus entry ci j product b constructed successive multiplication summing entry ith row jth column b clearly order define product b number column must equal number row b definition entry ci j matrix b written multiplication symbol element follows usual convention omitting multiplication sign clear multiplication considered eventually also omit multiplication sign matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>40\\n\\n4 Matrices\\n\\nWe can illustrate the multiplication rule “cij equals ith row of A times jth column of B” as follows:\\n⎡\\n⎤\\n⎡ ⎤ b11 · · · b1 j\\n· · · b1s\\n⎢ ..\\n⎢ .. ⎥\\n.. ⎥\\n⎣ .\\n⎣ . ⎦\\n. ⎦\\n·\\n·\\n·\\n·\\n·\\n· b b b mj ms\\n⎡\\n⎤ ⎡m1\\n⎤ a11 · · · a1m\\n⎢ ..\\n.. ⎥ ⎢\\n⎥\\n⎢ .\\n. ⎥\\n⎥\\n↓\\n⎢\\n⎥ ⎢\\n⎢\\n⎥\\n⎢[ ai1 · · · aim ]⎥ ⎢\\n⎥ ci j\\n⎢\\n⎥ ⎢ −→\\n⎥\\n⎢ .\\n.. ⎥ ⎣\\n⎦\\n.\\n⎣ .\\n. ⎦ an1 · · · anm\\nIt is important to note that the matrix multiplication in general is not commutative.\\nExample 4.2 For the matrices\\n⎡\\n\\nA=\\n\\n⎤\\n−1 1\\nB = ⎣ 0 0 ⎦ ∈ Z3,2\\n1 −1\\n\\n123\\n∈ Z2,3 ,\\n456 we have\\nA∗B =\\n\\n2 −2\\n∈ Z2,2 .\\n2 −2\\n\\nOn the other hand, B ∗ A ∈ Z3,3 .\\nAlthough A ∗ B and B ∗ A are both defined, we obviously have A ∗ B \u0003= B ∗ A. In this case one recognizes the non-commutativity of the matrix multiplication from the fact that A ∗ B and B ∗ A have different sizes.\\nBut even if A ∗ B and B ∗ A are both defined and have the same size, in general\\nA ∗ B \u0003= B ∗ A. For example,\\nA=\\n\\n12\\n∈ Z2,2 ,\\n03\\n\\nB=\\n\\n40\\n∈ Z2,2\\n56 yield the two products\\nA∗B =\\n\\n14 12\\n15 18 and B ∗ A =\\n\\n4 8\\n.\\n5 28\\n\\nThe matrix multiplication is, however, associative and distributive with respect to the matrix addition.</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.1 Basic Definitions and Operations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix illustrate multiplication rule cij equal ith row time jth column b follows j b b b mj m aim ci j anm important note matrix multiplication general commutative example matrix b hand b although b b defined obviously b b case one recognizes non-commutativity matrix multiplication fact b b different size even b b defined size general b b example yield two product b matrix multiplication however associative distributive respect matrix addition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.1 Basic Definitions and Operations\\n\\n41\\n\\nLemma 4.3 For A, A ∈ R n,m , B, B ∈ R m,\u0002 and C ∈ R \u0002,k the following assertions hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n\\nA ∗ (B ∗ C) = (A ∗ B) ∗ C.\\n(A + A) ∗ B = A ∗ B + A ∗ B.\\nA ∗ (B + B) = A ∗ B + A ∗ B.\\nIn ∗ A = A ∗ Im = A.\\n\\nProof We only show property (1); the others are exercises.\\nLet A ∈ R n,m , B ∈ R m,\u0002 ,\\nC ∈ R \u0002,k as well as (A ∗ B) ∗ C = [di j ] and A ∗ (B ∗ C) = [di j ].\\nBy the definition of the matrix multiplication and using the associative and distributive law in R, we get\\n\u0002\\n\\n\u000e m di j = s=1 t=1 ait t=1 bts cs j\\n\\n\u0002 m m\\n\\n(ait bts ) cs j = s=1 t=1\\n\\n\u000e\\n\\n\u0002 m\\n\\n=\\n\\n\u0002 cs j = ait bts\\n\\n\u000f\\n\u0010 ait bts cs j s=1 t=1\\n\\n= di j , s=1 for 1 ≤ i ≤ n and 1 ≤ j ≤ k, which implies that (A ∗ B) ∗ C = A ∗ (B ∗ C).\\nOn the right hand sides of (2) and (3) in Lemma 4.3 we have not written parentheses, since we will use the common convention that the multiplication of matrices binds stronger than the addition.\\nFor A ∈ R n,n we define\\n. . ∗ A\u0014 for k ∈ N,\\nAk := \u0011A ∗ .\u0012\u0013 k times\\n\\nA := In .\\n0\\n\\nAnother multiplicative operation for matrices is the multiplication with a scalar,3 which is defined as follows:\\n· : R × R n,m → R n,m , (λ, A) \u0006→ λ · A := [λai j ].\\n\\n(4.2)\\n\\nWe easily see that 0 · A = 0n,m and 1 · A = A for all A ∈ R n,m .\\nIn addition, the scalar multiplication has the following properties.\\nLemma 4.4 For A, B ∈ R n,m , C ∈ R m,\u0002 and λ, μ ∈ R the following assertions hold:\\n(1) (λμ) · A = λ · (μ · A).\\n(2) (λ + μ) · A = λ · A + μ · A.\\n3 The term “scalar” was introduced in 1845 by Sir William Rowan Hamilton (1805–1865).\\nIt originates from the Latin word “scale” which means “ladder”.</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.1 Basic Definitions and Operations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic definition operation lemma r n b b r c r k following assertion hold b c b b b b b b b b im proof show property others exercise let r n b r c r k well b c di j b c di j definition matrix multiplication using associative distributive law r get di j ait bts c j ait bts c j c j ait bts ait bts c j di j n j k implies b c b c right hand side lemma written parenthesis since use common convention multiplication matrix bind stronger addition r n n define k n ak k time another multiplicative operation matrix multiplication defined follows r r n r n λ λ λai j easily see r n addition scalar multiplication following property lemma b r n c r λ μ r following assertion hold λμ λ μ λ μ λ μ term scalar introduced sir william rowan hamilton originates latin word scale mean ladder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>42\\n\\n4 Matrices\\n\\n(3) λ · (A + B) = λ · A + λ · B.\\n(4) (λ · A) ∗ C = λ · (A ∗ C) = A ∗ (λ · C).\\nProof Exercise.\\nThe fourth matrix operation that we introduce is the transposition:\\nT : R n,m → R m,n ,\\n\\nA = [ai j ] \u0006→ A T = [bi j ], bi j := a ji .\\n\\nFor example,\\n\\n⎡\\n\\nA=\\n\\n⎤\\n14\\nA T = ⎣2 5⎦ ∈ Z3,2 .\\n36\\n\\n123\\n∈ Z2,3 ,\\n456\\n\\nThe matrix A T is called the transpose of A.\\nDefinition 4.5 If A ∈ R n,n satisfies A = A T , then A is called symmetric.\\nIf A =\\n−A T , then A is called skew-symmetric.\\nFor the transposition we have the following properties.\\nLemma 4.6 For A, A ∈ R n,m , B ∈ R m,\u0002 and λ ∈ R the following assertions hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n\\n(A T )T = A.\\n(A + A)T = A T + A T .\\n(λ · A)T = λ · A T .\\n(A ∗ B)T = B T ∗ A T .\\n\\nProof \u0015\\nProperties (1)–(3) are exercises.\\nFor the proof of (4) let A ∗ B = [ci j ] with\\nT ai j ], B T = [\u0016 bi j ] and (A ∗ B)T = [\u0016 ci j ].\\nThen ci j = m k=1 aik bk j , A = [\u0016 m\\n\\n\u0016 ci j = c ji = m a jk bki = k=1\\n\\n\u0016 ak j \u0016 bik = k=1 m\\n\\n\u0016 ak j , bik\u0016 k=1 from which we see that (A ∗ B)T = B T ∗ A T .\\n\\nMATLAB-Minute.\\nCarry out the following commands in order to get used to the matrix operations of this chapter in MATLAB notation: A=ones(5,2), A+A, A-3∗A, A’, A’∗A,\\nA∗A’.\\n(In order to see MATLAB’s output, do not put a semicolon at the end of the command.)</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.1 Basic Definitions and Operations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix λ b λ λ b λ c λ c λ c proof exercise fourth matrix operation introduce transposition r n r n ai j bi j bi j ji example matrix called transpose definition r n n satisfies called symmetric called skew-symmetric transposition following property lemma r n b r λ r following assertion hold λ λ b b proof property exercise proof let b ci j ai j b bi j b ci j ci j aik bk j ci j c ji jk bki ak j bik ak j see b b matlab-minute carry following command order get used matrix operation chapter matlab notation order see matlab output put semicolon end command</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4.1 Basic Definitions and Operations\\n\\n43\\n\\nExample 4.7 Consider again the example of car insurance premiums from Chap.\\n1.\\nRecall that pi j denotes the probability that a customer in class Ci in this year will move to the class C j .\\nOur example consists of four such classes, and the 16 probabilities can be associated with a row-stochastic 4 × 4 matrix (cp.\\n(1.2)), which we denote by\\nP. Suppose that the insurance company has the following distribution of customers in the four classes: 40 % in class C1 , 30 % in class C2 , 20 % in class C3 , and 10 % in class C4 .\\nThen the 1 × 4 matrix p0 := [0.4, 0.3, 0.2, 0.1] describes the initial customer distribution.\\nUsing the matrix multiplication we now compute\\n⎡\\n0.15\\n⎢0.15 p1 := p0 ∗ P = [0.4, 0.3, 0.2, 0.1] ∗ ⎢\\n⎣0.05\\n0.05\\n\\n0.85\\n0.00\\n0.10\\n0.00\\n\\n0.00\\n0.85\\n0.00\\n0.10\\n\\n⎤\\n0.00\\n0.00⎥\\n⎥\\n0.85⎦\\n0.85\\n\\n= [0.12, 0.36, 0.265, 0.255].\\nThen p1 contains the distribution of the customers in the next year.\\nAs an example, consider the entry of p0 ∗ P in position (1, 4), which is computed by\\n0.4 · 0.00 + 0.3 · 0.00 + 0.2 · 0.85 + 0.1 · 0.85 = 0.255.\\nA customer in the classes C1 or C2 in this year cannot move to the class C4 .\\nThus, the respective initial percentages are multiplied by the probabilities p14 = 0.00 and p24 = 0.00.\\nA customer in the class C3 or C4 will be in the class C4 with the probabilities p34 = 0.85 or p44 = 0.85, respectively.\\nThis yields the two products\\n0.2 · 0.85 and 0.1 · 0.85.\\nContinuing in the same way we obtain after k years the distribution pk := p0 ∗P k , k = 0, 1, 2, . . . .\\n(This formula also holds for k = 0, since P 0 = I4 .)\\nThe insurance company can use this formula to compute the revenue from the payments of premium rates in the coming years.\\nAssume that the full premium rate (class C1 ) is 500 Euros per year.\\nThen the rates in classes C2 , C3 , and C4 are 450, 400 and 300 Euros (10, 20 and\\n40 % discount).\\nIf there are 1000 customers initially, then the revenue in the first year\\n(in Euros) is\\n\u0010\\n\u000f\\n1000 · p0 ∗ [500, 450, 400, 300]T = 445000.</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.1 Basic Definitions and Operations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic definition operation example consider example car insurance premium chap recall pi j denotes probability customer class ci year move class c j example consists four class probability associated row-stochastic matrix cp denote suppose insurance company following distribution customer four class class class class class matrix describes initial customer distribution using matrix multiplication compute p contains distribution customer next year example consider entry p position computed customer class year move class thus respective initial percentage multiplied probability customer class class probability respectively yield two product continuing way obtain k year distribution pk k k formula also hold k since p insurance company use formula compute revenue payment premium rate coming year assume full premium rate class euro per year rate class euro discount customer initially revenue first year euro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>44\\n\\n4 Matrices\\n\\nIf no customer cancels the contract, then this model yields the revenue in year k ≥ 0 as\\n1000 ·\\n\\n\u0017 pk ∗ [500, 450, 400, 300]T\\n\\n\u0018\\n\\n\u0017\\n\u0018\\n= 1000 · p0 ∗ (P k ∗ [500, 450, 400, 300]T ) .\\n\\nFor example, the revenue in the next 4 years is 404500, 372025, 347340 and 341819\\n(rounded to full Euros).\\nThese numbers decrease annually, but the rate of the decrease seems to slow down.\\nDoes there exists a “stationary state”, i.e., a state when the revenue is not changing (significantly) any more?\\nWhich properties of the model guarantee the existence of such a state?\\nThese are important practical questions for the insurance company.\\nOnly the existence of a stationary state guarantees significant revenues in the long-time future.\\nSince the formula depends essentially on the entries of the matrix P k , we have reached an interesting problem of Linear Algebra: the analysis of the properties of row-stochastic matrices.\\nWe will analyze these properties in Sect.\\n8.3.\\n\\n4.2 Matrix Groups and Rings\\nIn this section we study algebraic structures that are formed by certain sets of matrices and the matrix operations introduced above.\\nWe begin with the addition in R n,m .\\nTheorem 4.8 (R n,m , +) is a commutative group.\\nThe neutral element is 0 ∈ R n,m\\n(the zero matrix) and for A = [ai j ] ∈ R n,m the inverse element is −A := [−ai j ] ∈\\nR n,m .\\n(We write A − B instead of A + (−B).)\\nProof Using the associativity of the addition in R, for arbitrary A, B, C ∈ R n,m , we obtain\\n(A + B) + C = [ai j + bi j ] + [ci j ] = [(ai j + bi j ) + ci j ] = [ai j + (bi j + ci j )]\\n= [ai j ] + [bi j + ci j ] = A + (B + C).\\nThus, the addition in R n,m is associative.\\nThe zero matrix 0 ∈ R n,m satisfies 0 + A = [0] + [ai j ] = [0 + ai j ] = [ai j ] = A.\\nFor a given A = [ai j ] ∈ R n,m and −A := [−ai j ] ∈ R n,m we have −A + A =\\n[−ai j ] + [ai j ] = [−ai j + ai j ] = [0] = 0.\\nFinally, the commutativity of the addition in R implies that A+ B = [ai j ]+[bi j ] =\\n[ai j + bi j ] = [bi j + ai j ] = B + A.\\nNote that (2) in Lemma 4.6 implies that the transposition is a homomorphism (even an isomorphism) between the groups (R n,m , +) and (R m,n , +) (cp.\\nDefinition 3.6).</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.2 Matrix Groups and Rings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix customer cancel contract model yield revenue year k pk p k example revenue next year rounded full euro number decrease annually rate decrease seems slow exists stationary state state revenue changing significantly property model guarantee existence state important practical question insurance company existence stationary state guarantee significant revenue long-time future since formula depends essentially entry matrix p k reached interesting problem linear algebra analysis property row-stochastic matrix analyze property sect matrix group ring section study algebraic structure formed certain set matrix matrix operation introduced begin addition r n theorem r n commutative group neutral element r n zero matrix ai j r n inverse element j r n write b instead proof using associativity addition r arbitrary b c r n obtain b c ai j bi j ci j ai j bi j ci j ai j bi j ci j ai j bi j ci j b c thus addition r n associative zero matrix r n satisfies ai j ai j ai j given ai j r n j r n j ai j j ai j finally commutativity addition r implies b ai j bi j ai j bi j bi j ai j b note lemma implies transposition homomorphism even isomorphism group r n r n cp definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.2 Matrix Groups and Rings\\n\\n45\\n\\nTheorem 4.9 (R n,n , +, ∗) is a ring with unit given by the identity matrix In .\\nThis ring is commutative only for n = 1.\\nProof We have already shown that (R n,n , +) is a commutative group (cp.\\nTheorem 4.8).\\nThe other properties of a ring (associativity, distributivity and the existence of a unit element) follow from Lemma 4.3.\\nThe commutativity for n = 1 holds because of the commutativity of the multiplication in the ring R. The example\\n01\\n10\\n00\\n01\\n10\\n01\\n∗\\n=\\n\u0003=\\n=\\n∗\\n00\\n00\\n00\\n00\\n00\\n00 shows that the ring R n,n is not commutative for n ≥ 2.\\nThe example in the proof of Theorem 4.9 shows that for n ≥ 2 the ring R n,n has non-trivial zero-divisors, i.e., there exist matrices A, B ∈ R n,n \\ {0} with A ∗ B = 0.\\nThese exist even when R is a field.\\nLet us now consider the invertibility of matrices in the ring R n,n (with respect to the\\n\u0016 ∈ R n,n must satisfy matrix multiplication).\\nFor a given matrix A ∈ R n,n , an inverse A\\n\u0016 = In (cp.\\nDefinition 3.10).\\nIf an inverse of\\n\u0016 ∗ A = In and A ∗ A the two equations A\\nA ∈ R n,n exists, i.e., if A is invertible, then the inverse is unique and denoted by A−1\\n(cp.\\nTheorem 3.11).\\nAn invertible matrix is sometimes called non-singular, while a non-invertible matrix is called singular.\\nWe will show in Corollary 7.20 that the\\n\u0016 ∗ A = In existence of the inverse already is implied by one of the two equations A\\n\u0016 = In , i.e., if one of them holds, then A is invertible and A−1 = A.\\n\u0016 Until and A ∗ A then, to be correct, we will have to check the validity of both equations.\\nNot all matrices A ∈ R n,n are invertible.\\nSimple examples are the non-invertible matrices\\n10\\nA = [0] ∈ R 1,1 and A =\\n∈ R 2,2 .\\n00\\nAnother non-invertible matrix is\\n11\\n∈ Z2,2 .\\n02\\n\\nA=\\n\\nHowever, considered as an element of Q2,2 , the (unique) inverse of A is given by\\n\u0019\\nA\\n\\n−1\\n\\n=\\n\\n1 − 21\\n0\\n\\n1\\n2\\n\\n\u001a\\n∈ Q2,2 .\\n\\nLemma 4.10 If A, B ∈ R n,n are invertible, then the following assertions hold:\\n(1) A T is invertible with (A T )−1 = (A−1 )T .\\n(We also write this matrix as A−T .)\\n(2) A ∗ B is invertible with (A ∗ B)−1 = B −1 ∗ A−1 .</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.2 Matrix Groups and Rings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix group ring theorem r n n ring unit given identity matrix ring commutative n proof already shown r n n commutative group cp theorem property ring associativity distributivity existence unit element follow lemma commutativity n hold commutativity multiplication ring example show ring r n n commutative n example proof theorem show n ring r n n non-trivial zero-divisors exist matrix b r n n b exist even r field let u consider invertibility matrix ring r n n respect r n n must satisfy matrix multiplication given matrix r n n inverse cp definition inverse two equation r n n exists invertible inverse unique denoted cp theorem invertible matrix sometimes called non-singular non-invertible matrix called singular show corollary existence inverse already implied one two equation one hold invertible correct check validity equation matrix r n n invertible simple example non-invertible matrix r r another non-invertible matrix however considered element unique inverse given lemma b r n n invertible following assertion hold invertible also write matrix b invertible b b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>46\\n\\n4 Matrices\\n\\nProof\\n(1) Using (4) in Lemma 4.6 we have\\n(A−1 )T ∗ A T = (A ∗ A−1 )T = InT = In = InT = (A−1 ∗ A)T = A T ∗ (A−1 )T , and thus (A−1 )T is the inverse of A T .\\n(2) This was already shown in Theorem 3.11 for general rings with unit and thus it holds, in particular, for the ring (R n,n , +, ∗).\\nOur next result shows that the invertible matrices form a multiplicative group.\\nTheorem 4.11 The set of invertible n×n matrices over R forms a group with respect to the matrix multiplication.\\nWe denote this group by G L n (R) (“GL” abbreviates\\n“general linear (group)”).\\nProof The associativity of the multiplication in G L n (R) is clear.\\nAs shown in (2) in Lemma 4.10, the product of two invertible matrices is an invertible matrix.\\nThe neutral element in G L n (R) is the identity matrix In , and since every A ∈ G L n (R) is assumed to be invertible, A−1 exists with (A−1 )−1 = A ∈ G L n (R).\\nWe now introduce some important classes of matrices.\\nDefinition 4.12 Let A = [ai j ] ∈ R n,n .\\n(1) A is called upper triangular, if ai j = 0 for all i &gt; j.\\nA is called lower triangular, if ai j = 0 for all j &gt; i (i.e., A T is upper triangular).\\n(2) A is called diagonal, if ai j = 0 for all i \u0003= j (i.e., A is upper and lower triangular).\\nWe write a diagonal matrix as A = diag(a11 , . . . , ann ).\\nWe next investigate these sets of matrices with respect to their group properties, beginning with the invertible upper and lower triangular matrices.\\nTheorem 4.13 The sets of the invertible upper triangular n × n matrices and of the invertible lower triangular n × n matrices over R form subgroups of G L n (R).\\nProof We will only show the result for the upper triangular matrices; the proof for the lower triangular matrices is analogous.\\nIn order to establish the subgroup property we will prove the three properties from Theorem 3.5.\\nSince In is an invertible upper triangular matrix, the set of the invertible upper triangular matrices is a nonempty subset of G L n (R).\\nNext we show that for two invertible upper triangular matrices A, B ∈ R n,n the product C = A ∗ B is again an invertible upper triangular matrix.\\nThe invertibility of C = [ci j ] follows from (2) in Lemma 4.10.\\nFor i &gt; j we have</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.2 Matrix Groups and Rings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix proof using lemma int int thus inverse already shown theorem general ring unit thus hold particular ring r n n next result show invertible matrix form multiplicative group theorem set invertible matrix r form group respect matrix multiplication denote group g l n r gl abbreviates general linear group proof associativity multiplication g l n r clear shown lemma product two invertible matrix invertible matrix neutral element g l n r identity matrix since every g l n r assumed invertible exists g l n r introduce important class matrix definition let ai j r n n called upper triangular ai j j called lower triangular ai j j upper triangular called diagonal ai j j upper lower triangular write diagonal matrix diag ann next investigate set matrix respect group property beginning invertible upper lower triangular matrix theorem set invertible upper triangular n n matrix invertible lower triangular n n matrix r form subgroup g l n r proof show result upper triangular matrix proof lower triangular matrix analogous order establish subgroup property prove three property theorem since invertible upper triangular matrix set invertible upper triangular matrix nonempty subset g l n r next show two invertible upper triangular matrix b r n n product c b invertible upper triangular matrix invertibility c ci j follows lemma j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4.2 Matrix Groups and Rings\\n\\n47 n ci j = aik bk j\\n\\n(here bk j = 0 for k &gt; j) aik bk j\\n\\n(here aik = 0 for k = 1, . . . , j, since i &gt; j) k=1 j\\n\\n= k=1\\n\\n= 0.\\nTherefore, C is upper triangular.\\nIt remains to prove that the inverse A−1 of an invertible upper triangular matrix A is an upper triangular matrix.\\nFor n = 1 the assertion holds trivially, so we assume that n ≥ 2.\\nLet A−1 = [ci j ], then the equation A ∗ A−1 = In can be written as a system of n equations\\n⎡ a11\\n⎢\\n⎢0\\n⎢\\n⎢ ..\\n⎣ .\\n0\\n\\n⎡ ⎤\\n⎤ ⎡ ⎤ c1 j\\nδ1 j\\n· · · · · · a1n\\n⎢ .. ⎥\\n.. ⎥ ⎢ .. ⎥\\n..\\n⎢ ⎥\\n⎢ ⎥\\n.\\n. ⎥\\n⎥ ∗ ⎢ . ⎥ = ⎢ . ⎥,\\n⎢\\n⎢ . ⎥\\n⎥\\n⎥\\n.\\n.\\n.. .. . ⎦ ⎣ . ⎦\\n⎣ .. ⎦\\n. . .\\n.\\n· · · 0 ann cn j\\nδn j j = 1, . . . , n.\\n\\n(4.3)\\n\\nHere, δi j is the Kronecker delta-function defined in (4.1).\\nWe will now prove inductively for i = n, n − 1, . . . , 1 that the diagonal entry aii of A is invertible with aii−1 = cii , and that n ci j = aii−1\\n\\nδi j −\\n\\n\u000e ai\u0002 c\u0002j , j = 1, . . . , n.\\n\\n(4.4)\\n\\n\u0002=i+1\\n\\nThis formula implies, in particular, that ci j = 0 for i &gt; j.\\nFor i = n the last row of (4.3) is given by ann cn j = δn j , j = 1, . . . , n.\\n\\nFor j = n we have ann cnn = 1 = cnn ann , where in the second equation we use the\\n−1\\n= cnn , commutativity of the multiplication in R. Therefore, ann is invertible with ann and thus\\n−1\\nδn j , j = 1, . . . , n.\\ncn j = ann\\nThis is equivalent to (4.4) for i = n.\\n(Note that for i = n in (4.4) the sum is empty and thus equal to zero.)\\nIn particular, cn j = 0 for j = 1, . . . , n − 1.\\nNow assume that our assertion holds for i = n, . . . , k + 1, where 1 ≤ k ≤ n − 1.\\nThen, in particular, ci j = 0 for k + 1 ≤ i ≤ n and i &gt; j.\\nIn words, the rows i = n, . . . , k + 1 of A−1 are in “upper triangular from”.\\nIn order to prove the assertion for i = k, we consider the kth row in (4.3), which is given by</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.2 Matrix Groups and Rings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix group ring n ci j aik bk j bk j k j aik bk j aik k j since j j therefore c upper triangular remains prove inverse invertible upper triangular matrix upper triangular matrix n assertion hold trivially assume n let ci j equation written system n equation j j ann cn j δn j j δi j kronecker delta-function defined prove inductively n n diagonal entry aii invertible cii n ci j δi j j formula implies particular ci j j n last row given ann cn j δn j j j n ann cnn cnn ann second equation use cnn commutativity multiplication therefore ann invertible ann thus δn j j cn j ann equivalent note n sum empty thus equal zero particular cn j j n assume assertion hold n k k n particular ci j k n j word row n k upper triangular order prove assertion k consider kth row given</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>48\\n\\n4 Matrices akk ck j + ak,k+1 ck+1, j + . . . + akn cn j = δk j , j = 1, . . . , n.\\n\\n(4.5)\\n\\nFor j = k (&lt; n) we obtain akk ckk + ak,k+1 ck+1,k + . . . + akn cnk = 1.\\nBy the induction hypothesis, we have ck+1,k = · · · = cn,k = 0.\\nThis implies akk ckk =\\n1 = ckk akk , where we have used the commutativity of the multiplication in R. Hence\\n−1\\n= ckk .\\nFrom (4.5) we get akk is invertible with akk\\n\u000f\\n\u0010\\n−1\\nδk j − ak,k+1 ck+1, j − . . . − akn cn j , ck j = akk j = 1, . . . , n, and hence (4.4) holds for i = k.\\nIf k &gt; j, then δk j = 0 and ck+1, j = · · · = cn j = 0, which gives ck j = 0.\\nWe point out that (4.4) represents a recursive formula for computing the entries of the inverse of an invertible upper triangular matrix.\\nUsing this formula the entries are computed “from bottom to top” and “from right to left”.\\nThis process is sometimes called backward substitution.\\nIn the following we will frequently partition matrices into blocks and make use of the block multiplication: For every k ∈ {1, . . . , n − 1}, we can write A ∈ R n,n as\\nA=\\n\\nA11 A12\\nA21 A22 with A11 ∈ R k,k and A22 ∈ R n−k,n−k .\\n\\nIf A, B ∈ R n,n are both partitioned like this, then the product A ∗ B can be evaluated blockwise, i.e.,\\nB11 B12\\nA11 A12\\n∗\\nA21 A22\\nB21 B22\\n\\n=\\n\\nA11 ∗ B11 + A12 ∗ B21 A11 ∗ B12 + A12 ∗ B22\\n.\\nA21 ∗ B11 + A22 ∗ B21 A21 ∗ B12 + A22 ∗ B22\\n(4.6)\\n\\nIn particular, if\\nA=\\n\\nA11 A12\\n0 A22 with A11 ∈ G L k (R) and A22 ∈ G L n−k (R), then A ∈ G L n (R) and a direct computation shows that\\n−1\\n−1\\nA−1\\n11 −A11 ∗ A12 ∗ A22\\n.\\n(4.7)\\nA−1 =\\n−1\\n0\\nA22</td>\n",
       "      <td>56.0</td>\n",
       "      <td>56</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.2 Matrix Groups and Rings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix akk ck j ak j akn cn j δk j j j k n obtain akk ckk ak k akn cnk induction hypothesis k cn k implies akk ckk ckk akk used commutativity multiplication hence ckk get akk invertible akk δk j ak j akn cn j ck j akk j n hence hold k j δk j j cn j give ck j point represents recursive formula computing entry inverse invertible upper triangular matrix using formula entry computed bottom top right left process sometimes called backward substitution following frequently partition matrix block make use block multiplication every k n write r n n r k k r b r n n partitioned like product b evaluated blockwise particular g l k r g l r g l n r direct computation show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.2 Matrix Groups and Rings\\n\\n49\\n\\nMATLAB-Minute.\\nCreate block matrices in MATLAB by carrying out the following commands: k=5;\\nA11=gallery(’tridiag’,-ones(k-1,1),2∗ones(k,1),-ones(k-1,1));\\nA12=zeros(k,2); A12(1,1)=1; A12(2,2)=1;\\nA22=-eye(2);\\nA=full([A11 A12; A12’ A22])\\nB=full([A11 A12; zeros(2,k) -A22])\\nInvestigate the meaning of the command full.\\nCompute the products A∗B and B∗A as well as the inverses inv(A) and inv(B).\\nCompute the inverse of\\nB in MATLAB with the formula (4.7).\\nCorollary 4.14 The set of the invertible diagonal n × n matrices over R forms a commutative subgroup (with respect to the matrix multiplication) of the invertible upper (or lower) triangular n × n matrices over R.\\nProof Since In is an invertible diagonal matrix, the invertible diagonal n ×n matrices form a nonempty subset of the invertible upper (or lower) triangular n × n matrices.\\nIf A = diag(a11 , . . . , ann ) and B = diag(b11 , . . . , bnn ) are invertible, then A ∗ B is invertible (cp.\\n(2) in Lemma 4.10) and diagonal, since\\nA ∗ B = diag(a11 , . . . , ann ) ∗ diag(b11 , . . . , bnn ) = diag(a11 b11 , . . . , ann bnn ).\\nMoreover, if A = diag(a11 , . . . , ann ) is invertible, then aii ∈ R is invertible for all i = 1, . . . , n (cp. the proof of Theorem 4.13).\\nThe inverse A−1 is given by the\\n−1\\n−1\\n, . . . , ann\\n).\\nFinally, the commutativity property invertible diagonal matrix diag(a11\\nA ∗ B = B ∗ A follows directly from the commutativity in R.\\nDefinition 4.15 A matrix P ∈ R n,n is called a permutation matrix, if in every row and every column of P there is exactly one unit and all other entries are zero.\\nThe term “permutation” means “exchange”.\\nIf a matrix A ∈ R n,n is multiplied with a permutation matrix from the left or from the right, then its rows or columns, respectively, are exchanged (or permuted).\\nFor example, if\\n⎡\\n⎤\\n001\\nP = ⎣0 1 0⎦,\\n100\\n\\n⎡\\n\\n⎤\\n123\\nA = ⎣4 5 6⎦ ∈ Z3,3 ,\\n789</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.2 Matrix Groups and Rings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix group ring matlab-minute create block matrix matlab carrying following command tridiag zero k investigate meaning command full compute product well inverse inv inv b compute inverse b matlab formula corollary set invertible diagonal n n matrix r form commutative subgroup respect matrix multiplication invertible upper lower triangular n n matrix proof since invertible diagonal matrix invertible diagonal n matrix form nonempty subset invertible upper lower triangular n n matrix diag ann b diag bnn invertible b invertible cp lemma diagonal since b diag ann diag bnn diag ann bnn moreover diag ann invertible aii r invertible n cp proof theorem inverse given ann finally commutativity property invertible diagonal matrix diag b b follows directly commutativity definition matrix p r n n called permutation matrix every row every column p exactly one unit entry zero term permutation mean exchange matrix r n n multiplied permutation matrix left right row column respectively exchanged permuted example p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>50\\n\\n4 Matrices\\n\\n⎡\\n\\n⎡\\n⎤\\n⎤\\n789\\n321\\nP ∗ A = ⎣4 5 6⎦ and A ∗ P = ⎣6 5 4⎦ .\\n123\\n987 then\\n\\nTheorem 4.16 The set of the n × n permutation matrices over R forms a subgroup of G L n (R).\\nIn particular, if P ∈ R n,n is a permutation matrix, then P is invertible with P −1 = P T .\\nProof Exercise.\\nFrom now on we will omit the multiplication sign in the matrix multiplication and write AB instead of A ∗ B.\\nExercises\\n(In the following exercises R is a commutative ring with unit.)\\n4.1 Consider the following matrices over Z:\\n⎡\\n\\nA=\\n\\n1 −2 4\\n,\\n−2 3 −5\\n\\n⎤\\n2 4\\n−1 0\\nB = ⎣3 6⎦, C =\\n.\\n11\\n1 −2\\n\\nDetermine, if possible, the matrices C A, BC, B T A, A T C, (−A)T C, B T A T ,\\nAC and C B.\\n4.2 Consider the matrices\\n⎡ ⎤ x1\\n\u001b \u001c",
       "\\n⎢ ⎥\\nA = ai j ∈ R n,m , x = ⎣ ... ⎦ ∈ R n,1 , y = [y1 , . . . , ym ] ∈ R 1,m .\\nxn\\nWhich of the following expressions are well defined for m \u0003= n or m = n?\\n\\n(a) x y,\\n(b) x T y,\\n(c) yx, (d) yx T , (e) x Ay, (f) x T Ay,\\nT\\nT\\nT\\n(g) x Ay , (h) x Ay , (i) x y A, (j) x y A T , (k) Ax y, (l) A T x y.\\n4.3 Show the following computational rules:\\nμ1 x1 + μ2 x2 = [x1 , x2 ]\\n\\nμ1\\nμ2 and A[x1 , x2 ] = [Ax1 , Ax2 ] for A ∈ R n,m , x1 , x2 ∈ R m,1 and μ1 , μ2 ∈ R.</td>\n",
       "      <td>58.0</td>\n",
       "      <td>58</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.2 Matrix Groups and Rings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix p p theorem set n n permutation matrix r form subgroup g l n r particular p r n n permutation matrix p invertible p p proof exercise omit multiplication sign matrix multiplication write ab instead b exercise following exercise r commutative ring unit consider following matrix z b c determine possible matrix c bc b c c b ac c b consider matrix ai j r n x r ym r xn following expression well defined n n x b x c yx yx e x ay f x ay g x ay h x ay x j x k ax l x show following computational rule r n r r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.2 Matrix Groups and Rings\\n\\n51\\n\\n4.4 Prove Lemma 4.3 (2)–(4).\\n4.5 Prove Lemma 4.4.\\n4.6 Prove Lemma\\n⎡ 4.6⎤(1)–(3).\\n011\\n4.7 Let A = ⎣0 0 1⎦ ∈ Z3,3 .\\nDetermine An for all n ∈ N ∪ {0}.\\n000\\n4.8 Let p = αn t n + . . . + α1 t + α0 t 0 ∈ R[t] be a polynomial (cp.\\nExample 3.17) and A ∈ R m,m .\\nWe define p(A) ∈ R m,m as p(A) := αn An +. . . +α1 A +α0 Im .\\n10\\n∈ Z2,2 .\\n31\\n(b) For a fixed matrix A ∈ R m,m consider the map f A : R[t] → R m,m , p \u0006→ p(A).\\nShow that f A ( p + q) = f A ( p) + f A (q) and f A ( pq) = f A ( p) f A (q) for all p, q ∈ R[t].\\n(The map f A is a ring homomorphism between the rings R[t] and R m,m .)\\n(c) Show that f A (R[t]) = { p(A) | p ∈ R[t]} is a commutative subring of R m,m , i.e., that f A (R[t]) is a subring of R m,m (cp.\\nExercise 3.14) and that the multiplication in this subring is commutative.\\n(d) Is the map f A surjective?\\n(a) Determine p(A) for p = t 2 − 2t + 1 ∈ Z[t] and A =\\n\\n4.9 Let K be a field with 1 + 1 \u0003= 0.\\nShow that every matrix A ∈ K n,n can be written as A = M + S with a symmetric matrix M ∈ K n,n (i.e., M T = M) and a skew-symmetric matrix S ∈ K n,n (i.e., S T = −S).\\nDoes this also hold in a field with 1+1 = 0?\\nGive a proof or a counterexample.\\n4.10 Show the binomial formula for commuting matrices: If\u0017 A,\u0018 B ∈ R n,n with\\n\u0015k \u0017 k \u0018 j k− j k!\\nk\\n.\\nAB = B A, then (A + B) = j=0 j A B , where kj := j! (k− j)!\\nn,n for which In − A is invertible.\\nShow that (In −\\n4.11 Let A ∈ R be a matrix\\n\u0015\\nA)−1 (In − Am+1 ) = mj=0 A j holds for every m ∈ N.\\n4.12 Let A ∈ R n,n be a matrix for which an m ∈ N with Am = In exists and let m be smallest natural number with this property.\\n(a) Investigate whether A is invertible, and if so, give a particularly simple representation of the inverse.\\n(b) Determine the cardinality of the set {Ak | k ∈ N}.\\n\u001e",
       "\\n\u001d",
       "\\n\u001f\\n4.13 Let A = [ai j ] ∈ R n,n \u001e",
       " an j = 0 for j = 1, . . . , n .\\n(a) Show that A is a subring of R n,n .\\n(b) Show that AM ∈ A for all M ∈ R n,n and A ∈ A.\\n(A subring with this property is called a left ideal of R n,n .)\\n(c) Determine an analogous subring B of R n,n , such that M B ∈ B for all\\nM ∈ R n,n and B ∈ B.\\n(A subring with this property is called a left ideal of R n,n .)\\n4.14 Examine whether (G, ∗) with</td>\n",
       "      <td>59.0</td>\n",
       "      <td>59</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.2 Matrix Groups and Rings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix group ring prove lemma prove lemma prove lemma let determine n n let p αn n r polynomial cp example r define p r p αn im b fixed matrix r consider map f r r p p show f p q f p f q f pq f p f q p q r map f ring homomorphism ring r r c show f r p p r commutative subring r f r subring r cp exercise multiplication subring commutative map f surjective determine p p z let k field show every matrix k n n written symmetric matrix k n n skew-symmetric matrix k n n also hold field give proof counterexample show binomial formula commuting matrix b r n n k j j k k ab b b j b kj j j n n invertible show let r matrix j hold every let r n n matrix n exists let smallest natural number property investigate whether invertible give particularly simple representation inverse b determine cardinality set ak k n let ai j r n n j j n show subring r n n b show r n n subring property called left ideal r n n c determine analogous subring b r n n b b r n n b b subring property called left ideal r n n examine whether g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>52\\n\\n4 Matrices\\n\\nG=\\n\\n\u001e",
       "\\n!\\ncos(α) − sin(α) \u001e",
       "\u001e",
       "\\nα\\n∈\\nR sin(α) cos(α) is a subgroup of G L 2 (R).\\n4.15 Generalize the block multiplication (4.6) to matrices A ∈ R n,m and B ∈ R m,\u0002 .\\n4.16 Determine all invertible upper triangular matrices A ∈ R n,n with A−1 = A T .\\n4.17 Let A11 ∈ R n 1 ,n 1 , A12 ∈ R n 1 ,n 2 , A21 ∈ R n 2 ,n 1 , A22 ∈ R n 2 ,n 2 and\\nA=\\n(a) Let A11 ∈\\nA21 A−1\\n11 A12\\n(b) Let A22 ∈\\nA12 A−1\\n22 A21\\n\\nA11 A12\\n∈ R n 1 +n 2 ,n 1 +n 2 .\\nA21 A22\\n\\nG L n 1 (R).\\nShow that A is invertible if and only if A22 − is invertible and derive in this case a formula for A−1 .\\nG L n 2 (R).\\nShow that A is invertible if and only if A11 − is invertible and derive in this case a formula for A−1 .\\n\\n4.18 Let A ∈ G L n (R), U ∈ R n,m and V ∈ R m,n .\\nShow the following assertions:\\n(a) A + U V ∈ G L n (R) holds if and only if Im + V A−1 U ∈ G L m (R).\\n(b) If Im + V A−1 U ∈ G L m (R), then\\n(A + U V )−1 = A−1 − A−1 U (Im + V A−1 U )−1 V A−1 .\\n(This last equation is called the Sherman-Morrison-Woodbury formula; named after Jack Sherman, Winifred J. Morrison and Max A. Woodbury.)\\n4.19 Show that the set of block upper triangular matrices with invertible 2 × 2 diagonal blocks, i.e., the set of matrices\\n⎡\\n\\n⎤\\n· · · A1m\\n· · · A2m ⎥\\n⎥\\n.. ⎥,\\n..\\n. . ⎦\\n0 · · · 0 Amm\\n\\nA11\\n⎢ 0\\n⎢\\n⎢ ..\\n⎣ .\\n\\nA12\\nA22\\n..\\n.\\n\\nAii ∈ G L 2 (R), i = 1, . . . , m, is a group with respect to the matrix multiplication.\\n4.20 Prove Theorem 4.16.\\nIs the group of permutation matrices commutative?\\n4.21 Show that the following is an equivalence relation on R n,n :\\nA∼B\\n\\n⇔\\n\\nThere exists a permutation matrix P with A = P T B P.\\n\\n4.22 A company produces from four raw materials R1 , R2 , R3 , R4 five intermediate products Z 1 , Z 2 , Z 3 , Z 4 , Z 5 , and from these three final products E 1 , E 2 , E 3 .\\nThe following tables show how many units of Ri and Z j are required for producing one unit of Z k and E \u0002 , respectively:</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.2 Matrix Groups and Rings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix co α sin α α r sin α co α subgroup g l r generalize block multiplication matrix r n b r determine invertible upper triangular matrix r n n let r n n r n n r n n r n n let b let r n n g l n r show invertible invertible derive case formula g l n r show invertible invertible derive case formula let g l n r u r n v r n show following assertion u v g l n r hold im v u g l r b im v u g l r u v u im v u v last equation called sherman-morrison-woodbury formula named jack sherman winifred morrison max woodbury show set block upper triangular matrix invertible diagonal block set matrix amm aii g l r group respect matrix multiplication prove theorem group permutation matrix commutative show following equivalence relation r n n exists permutation matrix p p b company produce four raw material five intermediate product z z z z z three final product e e e following table show many unit ri z j required producing one unit z k e respectively</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4.2 Matrix Groups and Rings\\n\\nR1\\nR2\\nR3\\nR4\\n\\n53\\n\\nZ1\\n0\\n5\\n1\\n0\\n\\nZ2\\n1\\n0\\n1\\n2\\n\\nZ3\\n1\\n1\\n1\\n0\\n\\nZ4\\n1\\n2\\n1\\n1\\n\\nZ5\\n2\\n1\\n0\\n0\\n\\nZ1\\nZ2\\nZ3\\nZ4\\nZ5\\n\\nE1\\n1\\n1\\n0\\n4\\n3\\n\\nE2\\n1\\n2\\n1\\n1\\n1\\n\\nE3\\n1\\n0\\n1\\n1\\n1\\n\\nFor instance, five units of R2 and one unit of R3 are required for producing one unit of Z 1 .\\n(a) Determine, with the help of matrix operations, a corresponding table which shows how many units of Ri are required for producing one unit of E \u0002 .\\n(b) Determine how many units of the four raw materials are required for producing 100 units of E 1 , 200 units of E 2 and 300 units of E 3 .</td>\n",
       "      <td>61.0</td>\n",
       "      <td>61</td>\n",
       "      <td>4 Matrices</td>\n",
       "      <td>4.2 Matrix Groups and Rings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix group ring instance five unit one unit required producing one unit z determine help matrix operation corresponding table show many unit ri required producing one unit e b determine many unit four raw material required producing unit e unit e unit e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Chapter 5\\n\\nThe Echelon Form and the Rank of Matrices\\n\\nIn this chapter we develop a systematic method for transforming a matrix A with entries from a field into a special form which is called the echelon form of A. The transformation consists of a sequence of multiplications of A from the left by certain\\n“elementary matrices”.\\nIf A is invertible, then its echelon form is the identity matrix, and the inverse A−1 is the product of the inverses of the elementary matrices.\\nFor a non-invertible matrix its echelon form is, in some sense, the “closest possible” matrix to the identity matrix.\\nThis form motivates the concept of the rank of a matrix, which we introduce in this chapter and will use frequently later on.\\n\\n5.1 Elementary Matrices\\nLet R be a commutative ring with unit, n ∈ N and i, j ∈ {1, . . . , n}.\\nLet In ∈ R n,n be the identity matrix and let ei be its ith column, i.e., In = [e1 , . . . , en ].\\nWe define\\nE i j := ei e Tj = [0, . . . , 0, ei , 0, . . . , 0] ∈ R n,n ,\\n\u0002\u0003\u0004\u0005 column j i.e., the entry (i, j) of E i j is 1, all other entries are 0.\\nFor n ≥ 2 and i &lt; j we define\\nPi j := [e1 , . . . , ei−1 , e j , ei+1 , . . . , e j−1 , ei , e j+1 , . . . , en ] ∈ R n,n .\\n\\n(5.1)\\n\\nThus, Pi j is a permutation matrix (cp.\\nDefinition 4.12) obtained by exchanging the columns i and j of In .\\nA multiplication of A ∈ R n,m from the left with Pi j means an exchange ofthe rows i and j of A. For example,\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_5\\n\\n55</td>\n",
       "      <td>62.0</td>\n",
       "      <td>&lt;FEFF&gt;</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.1 Elementary Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter echelon form rank matrix chapter develop systematic method transforming matrix entry field special form called echelon form transformation consists sequence multiplication left certain elementary matrix invertible echelon form identity matrix inverse product inverse elementary matrix non-invertible matrix echelon form sense closest possible matrix identity matrix form motivates concept rank matrix introduce chapter use frequently later elementary matrix let r commutative ring unit n n j n let r n n identity matrix let ei ith column en define e j ei e tj ei r n n column j entry j e j entry n j define pi j e j e ei e en r n n thus pi j permutation matrix cp definition obtained exchanging column j multiplication r n left pi j mean exchange ofthe row j example springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>56\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\n⎡\\n\\n⎤\\n123\\nA = ⎣4 5 6⎦ ,\\n789\\n\\n⎡\\n⎤\\n001\\nP13 = [e3 , e2 , e1 ] = ⎣0 1 0⎦ ,\\n100\\n\\n⎡\\n\\n⎤\\n789\\nP13 A = ⎣4 5 6⎦ .\\n123\\n\\nFor λ ∈ R we define\\nMi (λ) := [e1 , . . . , ei−1 , λei , ei+1 , . . . , en ] ∈ R n,n .\\n\\n(5.2)\\n\\nThus, Mi (λ) is a diagonal matrix obtained by replacing the ith column of In by λei .\\nA multiplication of A ∈ R n,m from the left with Mi (λ) means a multiplication of the ith row of A by λ.\\nFor example,\\n⎡\\n\\n⎤\\n123\\nA = ⎣4 5 6⎦ ,\\n789\\n\\n⎡\\n\\n⎤\\n1 00\\nM2 (−1) = [e1 , −e2 , e3 ] = ⎣ 0 −1 0 ⎦ ,\\n0 01\\n\\n⎡\\n\\n⎤\\n1 2 3\\nM2 (−1)A = ⎣ −4 −5 −6 ⎦ .\\n7 8 9\\n\\nFor n ≥ 2, i &lt; j and λ ∈ R we define\\nG i j (λ) := In + λE ji = [e1 , . . . , ei−1 , ei + λe j , ei+1 , . . . , en ] ∈ R n,n .\\n\\n(5.3)\\n\\nThus, the lower triangular matrix G i j (λ) is obtained by replacing the ith column of\\nIn by ei + λe j .\\nA multiplication of A ∈ R n,m from the left with G i j (λ) means that\\nλ times the ith row of A is added to the jth row of A. Similarly, a multiplication of\\nA ∈ R n,m from the left by the upper triangular matrix G i j (λ)T means that λ times the jth row of A is added to the ith row of A. For example,\\n⎡\\n\\n1\\nA = ⎣4\\n7\\n⎡\\n1\\nG 23 (−1)A = ⎣4\\n3\\n\\n⎤\\n⎡\\n⎤\\n3\\n1 00\\n6⎦ , G 23 (−1) = [e1 , e2 − e3 , e3 ] = ⎣ 0 1 0 ⎦ ,\\n9\\n0 −1 1\\n⎡\\n⎤\\n⎤\\n23\\n1 2 3\\n5 6⎦ , G 23 (−1)T A = ⎣ −3 −3 −3 ⎦ .\\n33\\n7 8 9\\n\\n2\\n5\\n8\\n\\nLemma 5.1 The elementary matrices Pi j , Mi (λ) for invertible λ ∈ R, and G i j (λ) defined in (5.1), (5.2), and (5.3), respectively, are invertible and have the following inverses:\\nT\\n(1) Pi−1 j = Pi j = Pi j .\\n(2) Mi (λ)−1 = Mi (λ−1 ).\\n(3) G i j (λ)−1 = G i j (−λ).\\n\\nProof\\nT\\n(1) The invertibility of Pi j with Pi−1 j = Pi j was already shown in Theorem 4.16; the symmetry of Pi j is easily seen.</td>\n",
       "      <td>63.0</td>\n",
       "      <td>63</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.1 Elementary Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>echelon form rank matrix λ r define mi λ λei en r n n thus mi λ diagonal matrix obtained replacing ith column λei multiplication r n left mi λ mean multiplication ith row λ example n j λ r define g j λ λe ji ei λe j en r n n thus lower triangular matrix g j λ obtained replacing ith column ei λe j multiplication r n left g j λ mean λ time ith row added jth row similarly multiplication r n left upper triangular matrix g j λ mean λ time jth row added ith row example g g g lemma elementary matrix pi j mi λ invertible λ r g j λ defined respectively invertible following inverse j pi j pi j mi λ mi g j λ g j proof invertibility pi j j pi j already shown theorem symmetry pi j easily seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>5.1 Elementary Matrices\\n\\n57\\n\\n(2) Since λ ∈ R is invertible, the matrix Mi (λ−1 ) is well defined.\\nA straightforward computation now shows that Mi (λ−1 )Mi (λ) = Mi (λ)Mi (λ−1 ) = In .\\n(3) Since e Tj ei = 0 for i &lt; j, we have E 2ji = (ei e Tj )(ei e Tj ) = 0, and therefore\\nG i j (λ)G i j (−λ) = (In + λE ji )(In + (−λ)E ji )\\n= In + λE ji + (−λ)E ji + (−λ2 )E 2ji = In .\\nA similar computation shows that G i j (−λ)G i j (λ) = In .\\n\\n\u0005\\n\u0004\\n\\n5.2 The Echelon Form and Gaussian Elimination\\nThe constructive proof of the following theorem relies on the Gaussian elimination algorithm.1 For a given matrix A ∈ K n,m , where K is a field, this algorithm constructs a matrix S ∈ G L n (K ) such that S A = C is quasi-upper triangular.\\nWe obtain this special form by left-multiplication of A with elementary matrices Pi j , Mi j (λ) and\\nG i j (λ).\\nEach of these left-multiplications corresponds to the application of one of the so-called “elementary row operations” to the matrix A:\\n• Pi j : exchange two rows of A.\\n• Mi (λ): multiply a row of A with an invertible scalar.\\n• G i j (λ): add a multiple of one row of A to another row of A.\\nWe assume that the entries of A are in a field (rather than a ring) because in the proof of the theorem we require that nonzero entries of A are invertible.\\nA generalization of the result which holds over certain rings (e.g. the integers Z) is given by the Hermite normal form,2 which plays an important role in Number Theory.\\nTheorem 5.2 Let K be a field and let A ∈ K n,m .\\nThen there exist invertible matrices\\nS1 , . . . , St ∈ K n,n (these are products of elementary matrices) such that C :=\\nSt · · · S1 A is in echelon form, i.e., either C = 0 or\\n\\n1 Named after Carl Friedrich Gauß (1777–1855).\\nA similar method was already described in Chap.\\n8,\\n\\n“Rectangular Arrays”, of the “Nine Chapters on the Mathematical Art”.\\nThis text developed in ancient China over several decades BC stated problems of every day life and gave practical mathematical solution methods.\\nA detailed commentary and analysis was written by Liu Hui (approx.\\n220–280 AD) around 260 AD.\\n2 Charles Hermite (1822–1901).</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.2 The Echelon Form and Gaussian Elimination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>elementary matrix since λ r invertible matrix mi well defined straightforward computation show mi mi λ mi λ mi since e tj ei j e ei e tj ei e tj therefore g j λ g j λe ji e ji λe ji e ji e similar computation show g j g j λ echelon form gaussian elimination constructive proof following theorem relies gaussian elimination given matrix k n k field algorithm construct matrix g l n k c quasi-upper triangular obtain special form left-multiplication elementary matrix pi j mi j λ g j λ left-multiplications corresponds application one so-called elementary row operation matrix pi j exchange two row mi λ multiply row invertible scalar g j λ add multiple one row another row assume entry field rather ring proof theorem require nonzero entry invertible generalization result hold certain ring integer z given hermite normal play important role number theory theorem let k field let k n exist invertible matrix st k n n product elementary matrix c st echelon form either c named carl friedrich gauß similar method already described chap rectangular array nine chapter mathematical art text developed ancient china several decade bc stated problem every day life gave practical mathematical solution method detailed commentary analysis written liu hui approx ad around ad charles hermite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>58\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\n⎡\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\nC = ⎢\\n⎢ 0\\n⎢\\n⎢\\n⎢\\n⎣\\n\\n1\\n\\n\u0002\\n\\n0\\n1\\n\\n\u0002\\n\\n0\\n0\\n\\n\u0002\\n\\n..\\n.\\n\\n1\\n0\\n\\n0\\n\\n⎤\\n\\n0\\n\\n..\\n0\\n\\n.\\n0\\n1\\n0\\n\\n⎥\\n⎥\\n⎥\\n\u0002⎥\\n⎥\\n⎥.\\n⎥\\n⎥\\n⎥\\n⎥\\n⎦\\n\\nHere \u0002 denotes an arbitrary (zero or nonzero) entry of C.\\nMore precisely, C = [ci j ] is either the zero matrix, or there exists a sequence of natural numbers j1 , . . . , jr (these are called the “steps” of the echelon form), where\\n1 ≤ j1 &lt; · · · &lt; jr ≤ m and 1 ≤ r ≤ min{n, m}, such that\\n(1) ci j = 0 for 1 ≤ i ≤ r and 1 ≤ j &lt; ji ,\\n(2) ci j = 0 for r &lt; i ≤ n and 1 ≤ j ≤ m,\\n(3) ci, ji = 1 for 1 ≤ i ≤ r and all other entries in column ji are zero.\\nIf n = m, then A ∈ K n,n is invertible if and only if C = In .\\nIn this case A−1 =\\nSt · · · S1 .\\nProof If A = 0, then we set t = 1, S1 = In , C = 0 and we are done.\\nNow let A \u0007= 0 and let j1 be the index of the first column of\\n:= A\\nA(1) = ai(1) j that does not consist of all zeros.\\nLet ai(1) be the first entry in this column that is\\n1 , j1 nonzero, i.e., A(1) has the form\\n\u000e\\n\u000e\\n\u000e 0 \u000e\\n\u000e\\n\u000e\\n\u000e .. \u000e\\n⎢\\n\u000e . \u000e\\n⎢\\n\u000e\\n\u000e\\n⎢\\n\u000e 0 \u000e\\n⎢\\n\u000e (1) \u000e\\n⎢\\n\u000e\\n\u000e\\n= ⎢\\n⎢ 0 \u000e ai1 , j1 \u000e\\n\u000e \u0002 \u000e\\n⎢\\n\u000e\\n\u000e\\n⎢\\n\u000e . \u000e\\n⎢\\n\u000e .. \u000e\\n⎣\\n\u000e\\n\u000e\\n\u000e \u0002 \u000e j1\\n\\n⎤\\n\\n⎡\\n\\nA(1)\\n\\n\u0002\\n\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥.\\n⎥\\n⎥\\n⎥\\n⎥\\n⎦\\n\\nWe then proceed as follows: First we permute the rows i 1 and 1 (if i 1 &gt; 1).\\nThen we\\n\u0010−1\\n\u000f\\n.\\nFinally we eliminate normalize the new first row, i.e., we multiply it with ai(1)\\n, j\\n1 1 the nonzero elements below the first entry in column j1 .\\nPermuting and normalizing leads to</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.2 The Echelon Form and Gaussian Elimination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>echelon form rank matrix c denotes arbitrary zero nonzero entry precisely c ci j either zero matrix exists sequence natural number jr called step echelon form jr r min n ci j r j ji ci j r n j ci ji r entry column ji zero n k n n invertible c case st proof set c done let let index first column ai j consist zero let ai first entry column nonzero form proceed follows first permute row finally eliminate normalize new first row multiply ai j nonzero element first entry column permuting normalizing lead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>5.2 The Echelon Form and Gaussian Elimination\\n\\n59\\n\\n\u000e\\n\u000e\\n\u000e 1 \u000e\\n\u000e (1) \u000e\\n\u000e\u0011\\n\u000e\\n⎢\\n\u000e a2, j1 \u000e\\n⎢\\n= ⎢ 0 \u000e . \u000e\\n\u000e .. \u000e\\n⎣\\n\u000e\\n\u000e\\n(1) \u000e\\n\u000e\u0011 an, j1 j1\\n\\n⎤\\n\\n⎡\\n\\n\u0011(1) = ãi,(1)j := M1\\nA\\n\\n\u0012\u000f\\n\u0010−1 \u0013\\n(1) ai1 , j1\\nP1,i1 A(1)\\n\\n\u0002\\n\\n⎥\\n⎥\\n⎥.\\n⎦\\n\\nIf i 1 = 1, then we set P1,1 := In .\\nIn order to eliminate below the 1 in column j1 , we\\n\u0011(1) from the left with the matrices multiply A\\n\u000f\\n\u0010\\n\u000f\\n\u0010\\n(1)\\n(1) an,\\n,\\n.\\n.\\n.\\n,\\nG\\n−\u0011 a\\nG 1,n −\u0011\\n1,2 j1\\n2, j1 .\\n⎡\\n\\nThen we have\\n\\n0\\n\\n⎢\\n⎢\\nS1 A(1) = ⎢\\n⎣0\\n\\n1\\n0\\n..\\n.\\n\\n\u0002\\nA(2)\\n\\n⎤\\n⎥\\n⎥\\n⎥,\\n⎦\\n\\n0 j1 where\\n\\n\u0012\u000f\\n\u000f\\n\u0010\\n\u000f\\n\u0010\\n\u0010−1 \u0013\\n(1)\\n(1)\\n(1) a\\nP1,i1\\n·\\n·\\n·\\nG\\n− ã\\nM\\nS1 := G 1,n −ãn,\\n1,2\\n1 j1\\n2, j1 i 1 , j1 and A(2) = [ai(2) j ] with i = 2, . . . , n, j = j1 + 1, . . . , m, i.e., we keep the indices of the larger matrix A(1) in the smaller matrix A(2) .\\nIf A(2) = [ ] or A(2) = 0, then we are finished, since then C := S1 A(1) is in echelon form.\\nIn this case r = 1.\\nIf at least one of the entries of A(2) is nonzero, then we apply the steps described above to the matrix A(2) .\\nFor k = 2, 3, . . . we define the matrices Sk recursively as\\n⎡\\n\u0014\\nSk =\\n\\n0\\n\\n\u0015\\n⎢\\nIk−1 0\\n⎢\\n(k)\\n\u0011\\n, where\\nS\\nA\\n=\\n⎢ k\\n0 \u0011\\nSk\\n⎣0\\n\\n1\\n0\\n..\\n.\\n\\n\u0002\\nA\\n\\n(k+1)\\n\\n⎤\\n⎥\\n⎥\\n⎥.\\n⎦\\n\\n0 jk\\nEach matrix S̃k is constructed analogous to S1 : First we identify the first column jk of A(k) that is not completely zero, as well as the first nonzero entry ai(k) in that k , jk column.\\nThen permuting and normalizing yields the matrix\\n\u0011(k) = [\u0011 ai(k)\\nA j ] := Mk\\n\\n\u0012\u000f\\n\u0010−1 \u0013\\n(k) aik , jk\\nPk,ik A(k) .</td>\n",
       "      <td>66.0</td>\n",
       "      <td>66</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.2 The Echelon Form and Gaussian Elimination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>echelon form gaussian elimination j set order eliminate column left matrix multiply g g n g g n ai j n j keep index larger matrix smaller matrix finished since c echelon form case r least one entry nonzero apply step described matrix k define matrix sk recursively sk k k sk jk matrix constructed analogous first identify first column jk k completely zero well first nonzero entry ai k k jk column permuting normalizing yield matrix k ai k j mk k aik jk pk ik k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>60\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\nIf k = i k , then we set Pk,k := In−k+1 .\\nNow\\n\u0012\u000f\\n\u000f\\n\u0010\\n\u000f\\n\u0010\\n\u0010−1 \u0013\\n(k)\\n(k)\\n(k)\\n\u0011\\nPk,ik , an, jk · · · G k,k+1 −\u0011 ak+1, jk Mk aik , jk\\nSk = G k,n −\u0011 so that Sk is indeed a product of elementary matrices of the form\\n\u0014\\n\\n\u0015\\nIk−1 0\\n,\\n0 T where T is an elementary matrix of size (n − k + 1) × (n − k + 1).\\nIf we continue this procedure inductively, it will end after r ≤ min{n, m} steps with either A(r +1) = 0 or A(r +1) = [ ].\\nAfter r steps we have\\nSr · · · S1 A(1) =\\n⎡\\n1\\n\u0002 \u0002\\n\u0002\\n⎢\\n1\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢ 0\\n⎢\\n0\\n⎢\\n0\\n⎢\\n⎢\\n⎣\\n\\n\u0002\\n\\n⎤\\n\\n\u0002\\n\\n\u0002\\n\\n\u0002\\n\\n1\\n\\n..\\n.\\n..\\n\\n0\\n\\n. \u0002\\n1\\n0\\n\\n⎥\\n⎥\\n⎥\\n\u0002 ⎥\\n⎥\\n⎥.\\n⎥\\n⎥\\n⎥\\n⎥\\n⎦\\n\\n(5.4)\\n\\nBy construction, the entries 1 in (5.4) are in the positions\\n(1, j1 ), (2, j2 ), . . . , (r, jr ).\\nIf r = 1, then S1 A(1) is in echelon form (see the discussion at the beginning of the proof).\\nIf r &gt; 1, then we still have to eliminate the nonzero entries above the 1 in columns j2 , . . . , jr .\\nTo do this, we denote the matrix in (5.4) by R (1) = [ri(1) j ] and form for k = 2, . . . , r recursively\\n(k−1)\\n,\\nR (k) = [ri(k) j ] := Sr +k−1 R where\\n\\n\u000f\\n\u0010T\\n\u000f\\n\u0010T\\n(k−1)\\n(k−1)\\n· · · G k−1,k −rk−1,\\n.\\nSr +k−1 := G 1,k −r1, jk jk\\n\\nFor t := 2r − 1 we have C := St St−1 · · · S1 A in echelon form.\\nSuppose now that n = m and that C = St St−1 · · · S1 A is in echelon form.\\nIf A is invertible, then C is a product of invertible matrices and thus invertible.\\nAn invertible matrix cannot have a row containing only zeros, so that r = n and hence C = In .\\nIf, on the other hand, C = In , then the invertibility of the elementary matrices</td>\n",
       "      <td>67.0</td>\n",
       "      <td>67</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.2 The Echelon Form and Gaussian Elimination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>echelon form rank matrix k k set pk k k k k pk ik jk g k jk mk aik jk sk g k n sk indeed product elementary matrix form elementary matrix size n k n k continue procedure inductively end r min n step either r r r step sr construction entry position r jr r echelon form see discussion beginning proof r still eliminate nonzero entry column jr denote matrix r ri j form k r recursively r k ri k j sr r g k sr g k jk jk c st echelon form suppose n c st echelon form invertible c product invertible matrix thus invertible invertible matrix row containing zero r n hence c hand c invertibility elementary matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>5.2 The Echelon Form and Gaussian Elimination\\n\\n61 implies that S1−1 · · · St−1 = A. As a product of invertible matrices, A is invertible and\\n\u0005\\n\u0004\\nA−1 = St · · · S1 .\\nIn the literature, the echelon form is sometimes called reduced row echelon form.\\nExample 5.3 Transformation of a matrix from Q3,5 to echelon form via left multiplication with elementary matrices:\\n⎡\\n⎤\\n02133\\n⎣0 2 0 1 1⎦\\n02011\\n⎤\\n⎡\\n0 1 21 23 23 j1 = 2, i 1 = 1\\n⎣0 2 0 1 1 ⎦\\n−→\\n\u0016 \u0017\\nM1 21\\n02011\\n⎡\\n−→\\nG 12 (−2)\\n\\n1\\n2\\n\\n3\\n2\\n\\n3\\n2\\n\\n−→\\nG 13 (−2)\\n\\n⎤\\n\\n⎤\\n⎡\\n0 1 21 23 23 j2 = 3, i 2 = 2\\n⎣0 0 1 2 2⎦\\n−→\\nM2 (−1)\\n0 0 −1 −2 −2\\n\\n⎣ 0 0 −1 −2 −2 ⎦\\n0 0 −1 −2 −2\\n⎡\\n\\n−→\\nG 23 (1)\\n\\n01\\n\\n3\\n2\\n\\n⎤\\n\\n⎢\\n⎣ 0\\n\\n1\\n\\n1\\n2\\n\\n0\\n\\n1\\n\\n⎥\\n2 2 ⎦\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n⎤\\n0 1 21 23 23\\n⎣0 2 0 1 1⎦\\n0 0 −1 −2 −2\\n⎡\\n\\n3\\n2\\n\\n0\\n\\n⎡\\n\\n0\\n\\n−→\\n\u0016 \u0017T ⎢\\n⎣ 0\\nG 12 − 21\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n2\\n\\n1\\n2\\n\\n⎤\\n\\n⎥\\n2 2 ⎦.\\n0 0\\n\\nMATLAB-Minute.\\nThe echelon form is computed in MATLAB with the command rref (“reduced row echelon form”).\\nApply rref to [A eye(n+1)] in order to compute the inverse of the matrix A=full(gallery(’tridiag’,\\n-ones(n,1),2∗ones(n+1,1),-ones(n,1))) for n=1,2,3,4,5 (cp.\\nExercise 5.5).\\nFormulate a conjecture about the general form of A−1 .\\n(Can you prove your conjecture?)\\nThe proof of Theorem 5.2 leads to the so-called LU -decomposition of a square matrix.\\nTheorem 5.4 For every matrix A ∈ K n,n , there exists a permutation matrix P ∈\\nK n,n , a lower triangular matrix L ∈ GLn (K ) with ones on the diagonal and an upper triangular matrix U ∈ K n,n , such that A = PLU.\\nThe matrix U is invertible if and only if A is invertible.</td>\n",
       "      <td>68.0</td>\n",
       "      <td>68</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.2 The Echelon Form and Gaussian Elimination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>echelon form gaussian elimination implies product invertible matrix invertible st literature echelon form sometimes called reduced row echelon form example transformation matrix echelon form via left multiplication elementary matrix g g g g matlab-minute echelon form computed matlab command rref reduced row echelon form apply rref eye order compute inverse matrix gallery tridiag cp exercise formulate conjecture general form prove conjecture proof theorem lead so-called lu square matrix theorem every matrix k n n exists permutation matrix p k n n lower triangular matrix l gln k one diagonal upper triangular matrix u k n n plu matrix u invertible invertible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>62\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\n\u0011, where U\\n\u0011 is upper\\nProof For A ∈ K n,n the Eq.\\n(5.4) has the form Sn · · · S1 A = U triangular.\\nIf r &lt; n, then we set Sn = Sn−1 = · · · = Sr +1 = In .\\nSince the matrices\\n\u0011 is invertible if and only if A is invertible.\\nS1 , . . . , Sn are invertible, it follows that U\\nFor i = 1, . . . , n every matrix Si has the form\\n⎤\\n⎡\\n1\\n⎥\\n⎢ ..\\n⎥\\n⎢\\n.\\n⎥\\n⎢\\n⎥\\n⎢\\n1\\n⎥\\n⎢\\n⎥ Pi, j ,\\n⎢ si,i\\nSi = ⎢ i\\n⎥\\n⎥\\n⎢ s\\n1 i+1,i\\n⎥\\n⎢\\n⎢\\n..\\n.. ⎥\\n⎣\\n. ⎦\\n.\\n1 sn,i where ji ≥ i for i = 1, . . . , n and Pi,i := In (if ji = i, then no permutation was necessary).\\nTherefore,\\n⎡\\n1\\n⎢ .\\n⎢ .\\n.\\n⎢\\nSn · · · S1 = ⎢\\n⎢\\n1\\n⎢\\n⎣\\n1\\n\\n⎤⎡\\n1\\n⎥⎢ .\\n⎥⎢ .\\n.\\n⎥⎢\\n⎥⎢\\n⎥⎢\\n1\\n⎥⎢\\n⎦⎣ sn−1,n−1 sn,n sn,n−1\\n⎤\\n\\n⎤\\n⎥\\n⎥\\n⎥\\n⎥ Pn−1, j\\n⎥ n−1\\n⎥\\n⎦\\n1\\n\\n⎡\\n⎡\\n1\\n1\\n⎢ .\\n⎥\\n⎢ .\\n⎥\\n⎢\\n.\\n⎢\\n⎥\\n⎢\\n⎢\\n⎥\\n⎢\\n⎢\\n⎥ Pn−2, j\\n1\\n···⎢\\n⎢\\n⎥\\n⎢ n−2\\n⎢\\n⎥\\n⎢ sn−2,n−2\\n⎢\\n⎥\\n⎣\\n⎣ sn−1,n−2 1 ⎦ sn,n−2 0 1\\n\\n⎤\\n\\n⎤ s11\\n⎥\\n⎥\\n⎢s 1 s22\\n⎥\\n⎥\\n⎢ 21\\n⎥\\n⎥\\n⎢ s32 1\\n⎥ P2, j ⎢ s31 1\\n⎥ P1, j .\\n⎥\\n⎥\\n⎢\\n2\\n1\\n.\\n.\\n.\\n.\\n⎥\\n⎢ .\\n.. ⎥\\n.\\n.\\n⎦\\n⎣ .\\n. ⎦\\n.\\nsn,2 sn,1\\n1\\n1\\n⎡\\n\\nThe form of the permutation matrices for k = 2, . . . , n − 1 and \u0003 = 1, . . . , k − 1 implies that\\n⎡\\n⎤\\n⎤\\n⎡\\n1\\n1\\n⎢ ..\\n⎥\\n⎥\\n⎢ ..\\n⎢\\n⎥\\n⎥\\n⎢\\n.\\n.\\n⎢\\n⎥\\n⎥\\n⎢\\n⎢\\n⎥\\n⎥\\n⎢\\n1\\n1\\n⎢\\n⎥\\n⎥\\n⎢\\n⎢\\n⎥\\n⎥ Pk, j s s\\n=\\nPk, jk ⎢\\n\u0003,\u0003\\n\u0003,\u0003 k\\n⎢\\n⎥\\n⎥\\n⎢\\n⎢\\n⎥\\n⎥\\n⎢ s\\n1\\n\u0011 s\\n1\\n\u0003+1,\u0003\\n\u0003+1,\u0003\\n⎢\\n⎥\\n⎥\\n⎢\\n⎢\\n⎢\\n..\\n..\\n.. ⎥\\n.. ⎥\\n⎣\\n⎣\\n. ⎦\\n. ⎦\\n.\\n.\\n1\\n1 sn,\u0003\\n\u0011 sn,\u0003</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.2 The Echelon Form and Gaussian Elimination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>echelon form rank matrix u upper proof k n n eq form sn u triangular r n set sn sr since matrix invertible invertible sn invertible follows u n every matrix si form pi j si si sn ji n pi ji permutation necessary therefore sn sn n sn j j sn j j form permutation matrix k n k implies pk j pk jk k sn sn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5.2 The Echelon Form and Gaussian Elimination\\n\\n63 holds for certain \u0011 s j,\u0003 ∈ K , j = \u0003 + 1, . . . , n.\\nHence,\\nSn · · · S1 =\\n⎡\\n⎤\\n⎤ 1\\n⎡\\n1\\n⎢\\n⎥\\n⎥ ⎢ ...\\n⎢ ..\\n⎥\\n⎥⎢\\n⎢\\n⎥\\n.\\n⎥⎢\\n⎢\\n⎥\\n1\\n⎥⎢\\n⎢\\n⎥···\\n1\\n⎥⎢\\n⎢\\n⎥ s n−2,n−2\\n⎥\\n⎦⎢\\n⎣ sn−1,n−1\\n⎣\\n\u0011 sn−1,n−2 1 ⎦ sn,n sn,n−1 sn,n\\n1\\n\u0011 sn,n−2\\n⎡\\n⎤⎡\\n⎤ s11\\n1\\n⎢ s22\\n⎥ ⎢\u0011\\n⎥\\n⎢\\n⎥ ⎢ s21 1\\n⎥\\n⎢\\n⎢ \u0011\\n⎥\\n⎥ s31 1\\n⎢ s32 1\\n⎥ ⎢\u0011\\n⎥ Pn−1, jn−1 · · · P1, j1 .\\n⎢\\n⎢ ..\\n⎥\\n.\\n.. ⎦ ⎣ .\\n.. ⎥\\n⎣ .\\n.\\n. ⎦\\n.\\n1 \u0011\\n1\\n\u0011 sn2 sn,1\\nThe invertible lower triangular matrices and the permutation matrices form groups with respect to the matrix multiplication (cp.\\nTheorems 4.13 and 4.16).\\nThus,\\n\u0011 where \u0011\\n\u0011 is a permutaSn · · · S1 = \u0011\\nL P,\\nL is invertible and lower triangular, and P l11 , . . . ,\u0011 lnn ) is invertible, tion matrix.\\nSince \u0011\\nL = [\u0011 li j ] is invertible, also D := diag(\u0011\\n\u0011T , L := \u0011\\n\u0011−1 = P\\n\u0011.\\nBy\\nL −1 D and U := D −1 U and we obtain A = P LU with P := P construction, all diagonal entries of L are equal to one.\\n\u0005\\n\u0004\\nExample 5.5 Computation of an LU -decomposition of a matrix from Q3,3 :\\n⎡\\n⎤\\n224\\n⎣2 2 1⎦\\n201\\n⎡\\n⎤ j1 = 2, i 1 = 1\\n112\\n−→\\n⎣2 2 1⎦\\n−→\\n\u00161\u0017\\nG 13 (−2)\\nM1 2\\n⎡2 0 1\\n⎤\\n1 1 2\\n−→\\n⎣ 0 0 −3 ⎦ −→\\nG 12 (−2)\\nP23\\n0 −2 −3\\n\\n⎡\\n\\n⎤\\n1 1 2\\n⎣2 2 1⎦\\n⎡ 0 −2 −3 ⎤\\n1 1 2\\n\u0011.\\n⎣ 0 −2 −3 ⎦ = U\\n0 0 −3\\n\\n\u0011 = P23 ,\\nHence, P\\n⎡ 1\\n⎤\\n\u0012 \u0013\\n00\\n2\\n1\\n\u0011\\n= ⎣ −2 1 0 ⎦ ,\\nL = G 12 (−2)G 13 (−2)M1\\n2\\n−2 1 1\\nT\\n\u0011T = P23 and thus, P = P\\n= P23 ,\\n\\n\u0013\\n1\\n, 1, 1 ,\\nD = diag\\n2\\n\u0012</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.2 The Echelon Form and Gaussian Elimination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>echelon form gaussian elimination hold certain j k j hence sn sn n sn sn n sn invertible lower triangular matrix permutation matrix form group respect matrix multiplication cp theorem thus permutasn l p l invertible lower triangular p lnn invertible tion matrix since l li j invertible also diag l p l u u obtain p lu p p construction diagonal entry l equal one example computation lu matrix g g u hence p l g g thus p p diag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>64\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\n⎡\\n⎤\\n⎡\\n⎤\\n100\\n2 2 4\\n\u0011 = ⎣ 0 −2 −3 ⎦.\\nL=\u0011\\nL −1 D = ⎣1 1 0⎦ , U = D −1 U\\n101\\n0 0 −3\\nIf A ∈ GLn (K ), then the LU -decomposition yields A−1 = U −1 L −1 P T .\\nHence after computing the LU -decomposition, one obtains the inverse of A essentially by inverting the two triangular matrices.\\nSince this can be achieved by the efficient recursive formula (4.4), the LU -decomposition is a popular method in scientific computing applications that require the inversion of matrices or the solution of linear systems of equations (cp.\\nChap.\\n6).\\nIn this context, however, alternative strategies for the choice of the permutation matrices are used.\\nFor example, instead of the first nonzero entry in a column one chooses an entry with large (or largest) absolute value for the row exchange and the subsequent elimination.\\nBy this strategy the influence of rounding errors in the computation is reduced.\\n\\nMATLAB-Minute.\\nThe Hilbert matrix 3 A = [ai j ] ∈ Qn,n has the entries ai j = 1/(i + j − 1) for i, j = 1, . . . , n.\\nIt can be generated in MATLAB with the command hilb(n).\\nCarry out the command [L,U,P]=lu(hilb(4)) in order to compute an LU -decomposition of the matrix hilb(4).\\nHow do the matrices P, L and U look like?\\nCompute also the LU -decomposition of the matrix full(gallery(’tridiag’,-ones(3,1),2∗ones(4,1),-ones(3,1))) and study the corresponding matrices P, L and U.\\nWe will now show that, for a given matrix A, the matrix C in Theorem 5.2 is uniquely determined in a certain sense.\\nFor this we need the following definition.\\nDefinition 5.6 If C ∈ K n,m is in echelon form (as in Theorem 5.2), then the positions of (1, j1 ), . . . , (r, jr ) are called the pivot positions of C.\\nWe also need the following results.\\nLemma 5.7 If Z ∈ GLn (K ) and x ∈ K n,1 , then Z x = 0 if and only if x = 0.\\nProof Exercise.\\n\\n\u0005\\n\u0004\\n\\nTheorem 5.8 Let A, B ∈ K n,m be in echelon form.\\nIf A = Z B for a matrix Z ∈\\nGLn (K ), then A = B.\\n\\n3 David\\n\\nHilbert (1862–1943).</td>\n",
       "      <td>71.0</td>\n",
       "      <td>71</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.2 The Echelon Form and Gaussian Elimination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>echelon form rank matrix l u u gln k lu yield u l p hence computing lu one obtains inverse essentially inverting two triangular matrix since achieved efficient recursive formula lu popular method scientific computing application require inversion matrix solution linear system equation cp chap context however alternative strategy choice permutation matrix used example instead first nonzero entry column one chooses entry large largest absolute value row exchange subsequent elimination strategy influence rounding error computation reduced matlab-minute hilbert matrix ai j qn n entry ai j j j generated matlab command hilb n carry command l u p hilb order compute lu matrix hilb matrix p l u look like compute also lu matrix full gallery tridiag study corresponding matrix p l u show given matrix matrix c theorem uniquely determined certain sense need following definition definition c k n echelon form theorem position r jr called pivot position also need following result lemma z gln k x k z x x proof exercise theorem let b k n echelon form z b matrix z gln k b david hilbert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5.2 The Echelon Form and Gaussian Elimination\\n\\n65\\n\\nProof If B is the zero matrix, then A = ZB = 0, and hence A = B.\\nLet now B \u0007= 0 and let A, B have the respective columns ai , bi , 1 ≤ i ≤ m.\\nFurthermore, let (1, j1 ), . . . , (r, jr ) be the r ≥ 1 pivot positions of B. We will show that every matrix Z ∈ GLn (K ) with A = Z B has the form\\n\u0014\\nZ=\\n\\n\u0015\\nIr \u0002\\n,\\n0 Z n−r where Z n−r ∈ GLn−r (K ).\\nSince B is in echelon form and all entries of B below its row r are zero, it then follows that B = Z B = A.\\nSince (1, j1 ) is the first pivot position of B, we have bi = 0 ∈ K n,1 for 1 ≤ i ≤ j1 − 1 and b j1 = e1 (the first column of In ).\\nThen A = Z B implies ai = 0 ∈ K n,1 for 1 ≤ i ≤ j1 − 1 and a j1 = Z b j1 = Z e1 .\\nSince Z is invertible, Lemma 5.7 implies that a j1 \u0007= 0 ∈ K n,1 .\\nSince A is in echelon form, a j1 = e1 = b j1 .\\nFurthermore,\\n\u0014\\n\\n1\\nZ = Z n :=\\n0\\n\\n\u0015\\n\u0002\\n,\\nZ n−1 where Z n−1 ∈ GLn−1 (K ) (cp.\\nExercise 5.3).\\nIf r = 1, then we are done.\\nIf r &gt; 1, then we proceed with the other pivot positions in an analogous way:\\nSince B is in echelon form, the kth pivot position gives b jk = ek .\\nFrom a jk = Z b jk and the invertibility of Z n−k+1 we obtain a jk = b jk and\\n⎡\\n\\n⎤\\nIk−1 0 \u0002\\nZ = ⎣ 0 1 \u0002 ⎦,\\n0 0 Z n−k where Z n−k ∈ GLn−k (K ).\\n\\n\u0005\\n\u0004\\n\\nThis result yields the uniqueness of the echelon form of a matrix and its invariance under left-multiplication with invertible matrices.\\nCorollary 5.9 For A ∈ K n,m the following assertions hold:\\n(1) There is a unique matrix C ∈ K n,m in echelon form to which A can be transformed by elementary row operations, i.e., by left-multiplication with elementary matrices.\\nThis matrix C is called the echelon form of A.\\n(2) If M ∈ GLn (K ), then the matrix C in (1) is also the echelon form of M A, i.e., the echelon form of a matrix is invariant under left-multiplication with invertible matrices.\\nProof\\n(1) If S1 A = C1 and S2 A\u0016 = C2 ,\u0017 where C1 , C2 are in echelon form and S1 , S2 are invertible, then C1 = S1 S2−1 C2 .\\nTheorem 5.8 now gives C1 = C2 .\\n(2) If M ∈ GLn (K ) and S\u00163 (M A) =\\n\u0017 C3 is in echelon form, then with S1 A = C1 from (1) we get C3 = S3 M S1−1 C1 .\\nTheorem 5.8 now gives C3 = C1 .\\n\u0005\\n\u0004</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.2 The Echelon Form and Gaussian Elimination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>echelon form gaussian elimination proof b zero matrix zb hence b let b let b respective column ai bi furthermore let r jr r pivot position b show every matrix z gln k z b form ir z z k since b echelon form entry b row r zero follows b z b since first pivot position b bi k b first column z b implies ai k z b z since z invertible lemma implies k since echelon form b furthermore z z n z z k cp exercise r done r proceed pivot position analogous way since b echelon form kth pivot position give b jk ek jk z b jk invertibility z obtain jk b jk z z z k result yield uniqueness echelon form matrix invariance left-multiplication invertible matrix corollary k n following assertion hold unique matrix c k n echelon form transformed elementary row operation left-multiplication elementary matrix matrix c called echelon form gln k matrix c also echelon form echelon form matrix invariant left-multiplication invertible matrix proof echelon form invertible theorem give gln k echelon form get theorem give</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>66\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\n5.3 Rank and Equivalence of Matrices\\nAs we have seen in Corollary 5.9, the echelon form of A ∈ K n,m is unique.\\nIn particular, for every matrix A ∈ K n,m , there exists a unique number of pivot positions\\n(cp.\\nDefinition 5.6) in its echelon form.\\nThis justifies the following definition.\\nDefinition 5.10 The number r of pivot positions in the echelon form of A ∈ K n,m is called the rank4 of A and denoted by rank(A).\\nWe see immediately that for A ∈ K n,m always 0 ≤ rank(A) ≤ min{n, m}, where rank(A) = 0 if and only if A = 0.\\nMoreover, Theorem 5.2 shows that A ∈ K n,n is invertible if and only if rank(A) = n.\\nFurther properties of the rank are summarized in the following theorem.\\nTheorem 5.11 For A ∈ K n,m the following assertions hold:\\n(1) There exist matrices Q ∈ GLn (K ) and Z ∈ GLm (K ) with\\n\u0014\\nQ AZ =\\n\\nIr\\n0n−r,r\\n\\n0r,m−r\\n0n−r,m−r\\n\\n\u0015 if and only if rank(A) = r .\\n(2) If Q ∈ GLn (K ) and Z ∈ GLm (K ), then rank(A) = rank(Q AZ ).\\n(3) If A = BC with B ∈ K n,\u0003 and C ∈ K \u0003,m , then\\n(a) rank(A) ≤ rank(B),\\n(b) rank(A) ≤ rank(C).\\n(4) rank(A) = rank(A T ).\\n(5) There exist matrices B ∈ K n,\u0003 and C ∈ K \u0003,m with A = BC if and only if rank(A) ≤ \u0003.\\nProof\\n(3a) Let Q ∈ G L n (K ) be such that Q B is in echelon form.\\nThen Q A = Q BC.\\nIn the matrix Q BC at most the first rank(B) rows contain nonzero entries.\\nBy\\nCorollary 5.9, the echelon form of Q A is equal to the echelon form of A. Thus, in the normal echelon form of A also at most the first rank(B) rows will be nonzero, which implies rank(A) ≤ rank(B).\\n(1) ⇐: If rank(A) = r = 0, i.e., A = 0, then Ir = [ ] and the assertion holds for arbitrary matrices Q ∈ G L n (K ) and Z ∈ G L m (K ).\\nIf r ≥ 1, then there exists a matrix Q ∈ G L n (K ) such that Q A is in echelon form with r pivot positions.\\nThen there exists a permutation matrix P ∈ K m,m , that is a product of elementary permutation matrices Pi j , with\\n4 The concept of the rank was introduced (in the context of bilinear forms) first in 1879 by Ferdinand\\n\\nGeorg Frobenius (1849–1917).</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.3 Rank and Equivalence of Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>echelon form rank matrix rank equivalence matrix seen corollary echelon form k n unique particular every matrix k n exists unique number pivot position cp definition echelon form justifies following definition definition number r pivot position echelon form k n called denoted rank see immediately k n always rank min n rank moreover theorem show k n n invertible rank property rank summarized following theorem theorem k n following assertion hold exist matrix q gln k z glm k q az ir r rank r q gln k z glm k rank rank q az bc b k n c k rank rank b b rank rank c rank rank exist matrix b k n c k bc rank proof let q g l n k q b echelon form q q bc matrix q bc first rank b row contain nonzero entry corollary echelon form q equal echelon form thus normal echelon form also first rank b row nonzero implies rank rank b rank r ir assertion hold arbitrary matrix q g l n k z g l k r exists matrix q g l n k q echelon form r pivot position exists permutation matrix p k product elementary permutation matrix pi j concept rank introduced context bilinear form first ferdinand georg frobenius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>5.3 Rank and Equivalence of Matrices\\n\\n67\\n\\n\u0014\\nP AT Q T =\\n\\nIr 0r,n−r\\nV 0m−r,n−r\\n\\n\u0015 for some matrix V ∈ K m−r,r .\\nIf r = m, then V = [ ].\\nIn the following, for simplicity, we omit the sizes of the zero matrices.\\nThe matrix\\n\u0014\\nY := is invertible with\\nY −1 =\\nThus,\\n\\nIr 0\\n−V Im−r\\n\u0014\\n\\nIr 0\\nV Im−r\\n\\n\u0015\\n∈ K m,m\\n\u0015\\n∈ K m,m .\\n\\n\u0015\\nIr 0\\n,\\nYPA Q =\\n0 0\\n\u0014\\n\\nT\\n\\nT and with Z := P T Y T ∈ G L m (K ) we obtain\\n\u0015\\nIr 0\\n.\\nQ AZ =\\n0 0\\n\u0014\\n\\n(5.5)\\n\\n⇒: Suppose that (5.5) holds for A ∈ K n,m and matrices Q ∈ G L n (K ) and\\nZ ∈ G L m (K ).\\nThen with (3a) we obtain rank(A) = rank(AZ Z −1 ) ≤ rank(AZ) ≤ rank(A), and thus, in particular, rank(A) = rank(AZ ).\\nDue to the invariance of the echelon form (and hence the rank) under left-multiplication with invertible matrices (cp.\\nCorollary 5.9), we get\\n\u0012\u0014 rank(A) = rank(AZ ) = rank(Q AZ ) = rank\\n\\nIr 0\\n0 0\\n\\n\u0015\u0013\\n= r.\\n\\n(2) If A ∈ K n×n , Q ∈ GLn (K ) and Z ∈ GLm (K ), then the invariance of the rank under left-multiplication with invertible matrices and (3a) can again be used for showing that rank(A) = rank(Q AZ Z −1 ) ≤ rank(Q AZ ) = rank(AZ ) ≤ rank(A), and hence, in particular, rank(A) = rank(Q AZ ).\\n(4) If rank(A) = r , then by\\n\u0014 (1)\u0015 there exist matrices Q ∈ G L n (K ) and Z ∈\\nI 0\\nG L m (K ) with Q AZ = r .\\nTherefore,\\n0 0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.3 Rank and Equivalence of Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rank equivalence matrix p q ir v matrix v k r r v following simplicity omit size zero matrix matrix invertible thus ir ir v k k ir ypa q z p g l k obtain ir q az suppose hold k n matrix q g l n k z g l k obtain rank rank az z rank az rank thus particular rank rank az due invariance echelon form hence rank left-multiplication invertible matrix cp corollary get rank rank az rank q az rank ir k q gln k z glm k invariance rank left-multiplication invertible matrix used showing rank rank q az z rank q az rank az rank hence particular rank rank q az rank r exist matrix q g l n k z g l k q az r therefore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>68\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\n\u0012\u0014 rank(A) = rank(Q AZ ) = rank\\n\\nIr 0\\n0 0\\n\\n\u0018\u0014\\n\\n\u0015\u0013\\n= rank\\n\\nIr 0\\n0 0\\n\\n\u0015T \u0019\\n\\n= rank((Q AZ )T )\\n\\n= rank(Z T A T Q T ) = rank(A T ).\\n\\n(3b) Using (3a) and (4), we obtain rank(A) = rank(A T ) = rank(C T B T ) ≤ rank(C T ) = rank(C).\\n(5) Let A = BC with B ∈ K n,\u0003 , C ∈ K \u0003,m .\\nThen by (3a), rank(A) = rank(BC) ≤ rank(B) ≤ \u0003.\\nLet, on the other hand, rank(A) = r ≤ \u0014\u0003.\\nThen\\n\u0015 there exist matrices Q ∈\\nIr 0\\n.\\nThus, we obtain\\nG L n (K ) and Z ∈ G L m (K ) with Q AZ =\\n0 0\\n\u0012\\nA=\\n\\nQ −1\\n\\n\u0014\\n\\nIr\\n\\n0n−r,r\\n\\n0r,\u0003−r\\n0n−r,\u0003−r\\n\\n\u0015\u0013 \u0012\u0014\\n\\nIr\\n\\n0\u0003−r,r\\n\\n\u0015\\n\u0013\\n0r,m−r\\nZ −1 =: BC,\\n0\u0003−r,m−r where B ∈ K n,\u0003 and C ∈ K \u0003,m .\\n\\n\u0005\\n\u0004\\n\\nExample 5.12 The matrix\\n⎡\\n⎤\\n02133\\nA = ⎣0 2 0 1 1⎦ ∈ Q3,5\\n02011 from Example 5.3 has the echelon form\\n⎡\\n\\n010\\n\\n1 1\\n2 2\\n\\n⎤\\n\\n⎢\\n⎥\\n⎣0 0 1 2 2 ⎦.\\n00000\\nSince there are two pivot positions, we have rank(A) = 2.\\nMultiplying A from the right by\\n⎡\\n⎤\\n100 0 0\\n⎢0 0 0 0 0⎥\\n⎢\\n⎥\\n5,5\\n⎥\\nB=⎢\\n⎢0 0 0 0 0⎥ ∈ Q ,\\n⎣ 0 0 0 −1 −1 ⎦\\n0 0 0 −1 −1 yields AB = 0 ∈ Q3,5 , and hence rank(AB) = 0 &lt; rank(A).\\nAssertion (1) in Theorem 5.11 motivates the following definition.</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.3 Rank and Equivalence of Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>echelon form rank matrix rank rank q az rank ir rank ir rank q az rank z q rank using obtain rank rank rank c b rank c rank c let bc b k n c k rank rank bc rank b let hand rank r exist matrix q ir thus obtain g l n k z g l k q az q ir r ir r z bc b k n c k example matrix example echelon form since two pivot position rank multiplying right q yield ab hence rank ab rank assertion theorem motivates following definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5.3 Rank and Equivalence of Matrices\\n\\n69\\n\\nDefinition 5.13 Two matrices A, B ∈ K n,m are called equivalent, if there exist matrices Q ∈ G L n (K ) and Z ∈ G L m (K ) with A = Q B Z .\\nAs the name suggests, this defines an equivalence relation on the set K n,m , since the following properties hold:\\n• Reflexivity: A = Q AZ with Q = In and Z = Im .\\n• Symmetry: If A = Q B Z , then B = Q −1 AZ −1 .\\n• Transitivity: If A = Q 1 B Z 1 and B = Q 2 C Z 2 , then A = (Q 1 Q 2 )C(Z 2 Z 1 ).\\nThe equivalence class of A ∈ K n,m is given by\\n\u001b\\n\u001a\\n[A] = Q AZ | Q ∈ G L n (K ) and Z ∈ G L m (K ) .\\nIf rank(A) = r , then by (1) in Theorem 5.11 we have\\n\u0014\\n\\nIr\\n0n−r,r and, therefore,\\n\\n0r,m−r\\n0n−r,m−r\\n\u0014\u0014\\n\\nIr 0\\n0 0\\n\\n\u0015\\n\\n\u0015\\nIr 0\\n∈ [A]\\n=\\n0 0\\n\u0014\\n\\n\u0015\u0015\\n= [A].\\n\\nConsequently, the rank of A fully determines the equivalence class [A].\\nThe matrix\\n\u0014\\n\\n\u0015\\nIr 0\\n∈ K n,m\\n0 0 is called the equivalence normal form of A. We obtain\\n\u0015\u0015\\nIr 0\\n, where\\nK\\n=\\n0 0 r =0\\n\u0014\u0014\\n\u0015\u0015\\n\u0014\u0014\\n\u0015\u0015\\nIr 0 \u001d",
       " I\u0003 0\\n= Ø, if r \u0007= \u0003.\\n0 0\\n0 0 n,m min{n,m}\\n\u001c",
       " \u0014\u0014\\n\\nHence there are 1 + min{n, m} pairwise distinct equivalence classes, and\\n\u000e\\n\u001f\\n\u001e",
       "\u0014\\n\u0015\\n\u000e\\nIr 0 n,m \u000e\\n∈K\\n\u000e r = 0, 1, . . . , min{n, m}\\n0 0 is a complete set of representatives.\\nFrom the proof of Theorem 4.9 we know that (K n,n , +, ∗) for n ≥ 2 is a noncommutative ring with unit that contains non-trivial zero divisors.\\nUsing the equivalence normal form these can be characterized as follows:\\n• If A ∈ K n,n is invertible, then A cannot be a zero divisor, since then AB = 0 implies that B = 0.</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.3 Rank and Equivalence of Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rank equivalence matrix definition two matrix b k n called equivalent exist matrix q g l n k z g l k q b z name suggests defines equivalence relation set k n since following property hold reflexivity q az q z im symmetry q b z b q az transitivity q b z b q c z q q c z z equivalence class k n given q az q g l n k z g l k rank r theorem ir r therefore ir ir consequently rank fully determines equivalence class matrix ir k n called equivalence normal form obtain ir k r ir ø r n min n hence min n pairwise distinct equivalence class ir n r min n complete set representative proof theorem know k n n n noncommutative ring unit contains non-trivial zero divisor using equivalence normal form characterized follows k n n invertible zero divisor since ab implies b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>70\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\n• If A ∈ K n,n \\ {0} is a zero divisor, then A cannot be invertible, and hence 1 ≤ rank(A) = r &lt; n, so that the equivalence normal form of A is not the identity matrix In .\\nLet Q, Z ∈ G L n (K ) be given with\\n\u0015\\n\u0014\\nI 0\\n.\\nQ AZ = r\\n0 0\\nThen for every matrix\\n\\n\u0014\\n\\n0 0\\nV := r,r r,n−r\\nV21 V22 and B := Z V we have\\nAB = Q\\n\\n−1\\n\\n\u0014\\n\\nIr 0\\n0 0\\n\\n\u0015\\n∈ K n,n\\n\\n\u0015\u0014\\n\u0015\\n0r,r 0r,n−r\\n= 0.\\nV21 V22\\n\\nIf V \u0007= 0, then B \u0007= 0, since Z is invertible.\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n5.1 Compute the echelon forms of the matrices\\n\\n⎡\\n1 i −i\\n\u0014 \u0015\\n⎢\\n12 3\\n1 i\\n0\\n0 0\\nA=\\n∈ Q2,3 , B =\\n∈ C2,2 , C = ⎢\\n⎣ 5 0 −6i\\n2 4 48 i 1\\n01 0\\n⎡ ⎤\\n⎡\\n⎤\\n10\\n1020\\nD = ⎣1 1⎦ ∈ (Z/2Z)3,2 , E = ⎣2 0 1 1⎦ ∈ (Z/3Z)3,4 .\\n01\\n1202\\n\u0014\\n\\n\u0015\\n\\n⎤\\n0\\n1⎥\\n⎥ ∈ C4,4 ,\\n0⎦\\n0\\n\\n(Here for simplicity the elements of Z/nZ are denoted by k instead of [k].)\\nState the elementary matrices that carry out the transformations.\\nIf one of the matrices is invertible, then compute its inverse as a product of the elementary matrices. \u0014\\n\u0015\\nαβ\\n5.2 Let A =\\n∈ K 2,2 with αδ \u0007= βγ.\\nDetermine the echelon form of A and\\nγ δ a formula\u0014for A−1\u0015.\\n1 A12\\n∈ K n,n with A12 ∈ K 1,n−1 and B ∈ K n−1,n−1 .\\nShow that\\n5.3 Let A =\\n0 B\\nA ∈ G L n (K ) if and only if B ∈ G L n−1 (K ).\\n5.4 Consider the matrix\\n!\\nA= t+1 t−1 t−1 t 2 t 2 t−1 t+1 t+1\\n\\n∈ (K (t))2,2 ,</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.3 Rank and Equivalence of Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>echelon form rank matrix k n n zero divisor invertible hence rank r n equivalence normal form identity matrix let q z g l n k given q az r every matrix v r r r b z v ab q ir k n n r v b since z invertible exercise following exercise k arbitrary field compute echelon form matrix b c e simplicity element denoted k instead k state elementary matrix carry transformation one matrix invertible compute inverse product elementary matrix αβ let k αδ βγ determine echelon form γ δ k n n k b k show let b g l n k b g l k consider matrix k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5.3 Rank and Equivalence of Matrices\\n\\n5.5\\n\\n5.6\\n\\n5.7\\n5.8\\n\\n71 where K (t) is the field of rational functions (cp.\\nExercise 3.19).\\nExamine whether A is invertible and determine, if possible, A−1 .\\nVerify your result by computing A−1 A and A A−1 .\\nShow that if A ∈ G L n (K ), then the echelon form of [A, In ] ∈ K n,2n is given by [In , A−1 ].\\n(The inverse of an invertible matrix A can thus be computed via the transformation of [A, In ] to its echelon form.)\\nTwo matrices A, B ∈ K n,m are called left equivalent, if there exists a matrix\\nQ ∈ G L n (K ) with A = Q B. Show that this defines an equivalence relation on\\nK n,m and determine a most simple representative for each equivalence class.\\nProve Lemma 5.7.\\nDetermine LU -decompositions (cp.\\nTheorem 5.4) of the matrices\\n⎡\\n\\n1\\n⎢4\\nA=⎢\\n⎣5\\n0\\n\\n2\\n0\\n0\\n1\\n\\n3\\n0\\n6\\n0\\n\\n⎤\\n0\\n1⎥\\n⎥,\\n0⎦\\n0\\n\\n⎡\\n\\n⎤\\n2 0 −2 0\\n⎢ −4 0 4 −1 ⎥\\n4,4\\n⎥\\nB=⎢\\n⎣ 0 −1 −1 −2 ⎦ ∈ R .\\n0 0 1 1\\n\\nIf one of these matrices is invertible, then determine its inverse using its LU decomposition.\\n5.9 Let A be the 4 × 4 Hilbert matrix (cp. the MATLAB-Minute above Definition 5.6).\\nDetermine rank(A).\\nDoes A have an LU -decomposition as in Theorem 5.4 with P = I4 ?\\n5.10 Determine the rank of the matrix\\n⎡\\n⎤\\n0 αβ\\nA = ⎣ −α 0 γ ⎦ ∈ R3,3\\n−β −γ 0 in dependence of α, β, γ ∈ R.\\n5.11 Let A, B ∈ K n,n be given.\\nShow that\\n\u0012\u0014 rank(A) + rank(B) ≤ rank\\n\\nAC\\n0 B\\n\\n\u0015\u0013 for all C ∈ K n,n .\\nExamine when this inequality is strict.\\n5.12 Let a, b, c ∈ Rn,1 .\\n(a) Determine rank(ba T ).\\n(b) Let M(a, b) := ba T − ab T .\\nShow the following assertions:\\n(i) M(a, b) = −M(b, a) and M(a, b)c + M(b, c)a + M(c, a)b = 0,\\n(ii) M(λa + μb, c) = λM(a, c) + μM(b, c) for λ, μ ∈ R,\\n(iii) rank(M(a, b)) = 0 if and only if there exist λ, μ ∈ R with λ \u0007= 0 or\\nμ \u0007= 0 and λa + μb = 0,\\n(iv) rank(M(a, b)) ∈ {0, 2}.</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78</td>\n",
       "      <td>5 The Echelon Form and the Rank of Matrices</td>\n",
       "      <td>5.3 Rank and Equivalence of Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rank equivalence matrix k field rational function cp exercise examine whether invertible determine possible verify result computing show g l n k echelon form k given inverse invertible matrix thus computed via transformation echelon form two matrix b k n called left equivalent exists matrix q g l n k q b show defines equivalence relation k n determine simple representative equivalence class prove lemma determine lu cp theorem matrix r one matrix invertible determine inverse using lu decomposition let hilbert matrix cp matlab-minute definition determine rank lu theorem p determine rank matrix αβ γ dependence α β γ let b k n n given show rank rank b rank ac b c k n n examine inequality strict let b c determine rank ba b let b ba ab show following assertion b b b c b c c b ii λa μb c λm c μm b c λ μ r iii rank b exist λ μ r λ μ λa μb iv rank b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Chapter 6\\n\\nLinear Systems of Equations\\n\\nSolving linear systems of equations is a central problem of Linear Algebra that we discuss in an introductory way in this chapter.\\nSuch systems arise in numerous applications from engineering to the natural and social sciences.\\nMajor sources of linear systems of equations are the discretization of differential equations and the linearization of nonlinear equations.\\nIn this chapter we analyze the solution sets of linear systems of equations and we characterize the number of solutions using the echelon form from Chap.\\n5.\\nWe also develop an algorithm for the computation of the solutions.\\nDefinition 6.1 A linear system (of equations) over a field K with n equations in m unknowns x1 , . . . , xm has the form a11 x1 + . . . + a1m xm = b1 , a21 x1 + . . . + a2m xm = b2 ,\\n..\\n.\\nan1 x1 + . . . + anm xm = bn or\\nAx = b, where the coefficient matrix A = [ai j ] ∈ K n,m and the right hand side b = [bi ] ∈\\nK n,1 are given.\\nIf b = 0, then the linear system is called homogeneous, otherwise x = b is called a solution of the linear non-homogeneous.\\nEvery \u0002 x ∈ K m,1 with A\u0002 system.\\nAll these \u0002 x form the solution set of the linear system, which we denote by\\nL (A, b).\\nThe next result characterizes the solution set L (A, b) of the linear system Ax = b using the solution set L (A, 0) of the associated homogeneous linear system Ax = 0.\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_6\\n\\n73</td>\n",
       "      <td>79.0</td>\n",
       "      <td>&lt;FEFF&gt;</td>\n",
       "      <td>6 Linear Systems of Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter linear system equation solving linear system equation central problem linear algebra discus introductory way chapter system arise numerous application engineering natural social science major source linear system equation discretization differential equation linearization nonlinear equation chapter analyze solution set linear system equation characterize number solution using echelon form chap also develop algorithm computation solution definition linear system equation field k n equation unknown xm form xm xm anm xm bn ax b coefficient matrix ai j k n right hand side b bi k given b linear system called homogeneous otherwise x b called solution linear non-homogeneous every x k system x form solution set linear system denote l b next result characterizes solution set l b linear system ax b using solution set l associated homogeneous linear system ax springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>74\\n\\n6 Linear Systems of Equations\\n\\nLemma 6.2 Let A ∈ K n,m and b ∈ K n,1 with L (A, b) \u0003= Ø be given.\\nIf \u0002 x ∈\\nL (A, b), then\\nL (A, b) = \u0002 x + L (A, 0) := {\u0002 x +\u0002 z |\u0002 z ∈ L (A, 0)}.\\nProof If \u0002 z ∈ L (A, 0), and thus \u0002 x +\u0002 z ∈\u0002 x + L (A, 0), then\\nA(\u0002 x +\u0002 z) = A\u0002 x + A\u0002 z = b + 0 = b.\\nHence \u0002 x +\u0002 z ∈ L (A, b), which shows that \u0002 x + L (A, 0) ⊆ L (A, b).\\nz := \u0002 x1 − \u0002 x .\\nThen\\nLet now \u0002 x1 ∈ L (A, b) and let \u0002 x = b − b = 0,\\nA\u0002 z = A\u0002 x1 − A\u0002 x +\u0002 z ∈\u0002 x + L (A, 0), which shows that L (A, b) ⊆ i.e.,\u0002 z ∈ L (A, 0).\\nHence \u0002 x1 = \u0002\\n\u0002 x + L (A, 0).\\n\u0002\\nWe will have a closer look at the set L (A, 0): Clearly, 0 ∈ L (A, 0) \u0003= Ø.\\nIf\\n\u0002 z ∈ L (A, 0), then for all λ ∈ K we have A(λ\u0002 z) = λ(A\u0002 z) = λ · 0 = 0, and hence\\nλ\u0002 z ∈ L (A, 0).\\nFurthermore, for \u0002 z 1 ,\u0002 z 2 ∈ L (A, 0) we have z 2 ) = A\u0002 z 1 + A\u0002 z 2 = 0 + 0 = 0,\\nA(\u0002 z 1 +\u0002 z 2 ∈ L (A, 0).\\nThus, L (A, 0) is a nonempty subset of K m,1 that is and hence \u0002 z 1 +\u0002 closed under scalar multiplication and addition.\\nLemma 6.3 If A ∈ K n,m , b ∈ K n,1 and S ∈ K n,n , then L (A, b) ⊆ L (S A, Sb).\\nMoreover, if S is invertible, then L (A, b) = L (S A, Sb).\\nProof If \u0002 x ∈ L (A, b), then also S A\u0002 x = Sb, and thus \u0002 x ∈ L (S A, Sb), which shows that L (A, b) ⊆ L (S A, Sb).\\nIf S is invertible and \u0002 y ∈ L (S A, Sb), then\\nS A\u0002 y = Sb.\\nMultiplying from the left with S −1 yields A\u0002 y = b.\\nSince \u0002 y ∈ L (A, b), we have L (S A, Sb) ⊆ L (A, b).\\n\u0002\\nConsider the linear system of equations Ax = b.\\nBy Theorem 5.2 we can find a matrix S ∈ G L n (K ) such that S A is in echelon form.\\nLet \u0003 b = [\u0003 bi ] := Sb, then\\n\u0003\\nL (A, b) = L (S A, b) by Lemma 6.3, and the linear system S Ax = \u0003 b takes the form\\n⎤\\n⎡\\n⎡ ⎤\\n1 \u0002 0\\n0\\n0\\n\u0003 b1\\n\u0002\\n⎥\\n⎢\\n⎢\\n⎥\\n1\\n0 \u0002\\n⎥\\n⎢\\n..\\n⎢ ⎥\\n⎥\\n⎢\\n.\\n⎢\\n⎥\\n⎢\\n1\\n\u0002⎥\\n⎢ ⎥\\n⎥\\n⎢\\n⎢\\n⎥ x = ⎢ .. ⎥\\n⎢ 0\\n..\\n⎥.\\n⎥\\n⎢\\n.\\n⎢.⎥\\n0\\n0\\n⎥\\n⎢\\n⎢\\n⎥\\n0\\n⎥\\n⎢\\n⎢ ⎥\\n1\\n0\\n⎥\\n⎢\\n⎣\\n⎦\\n⎦\\n⎣\\n0\\n\u0003 bn</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80</td>\n",
       "      <td>6 Linear Systems of Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear system equation lemma let k n b k l b ø given x l b l b x l x z z l proof z l thus x z x l x z x z b b hence x z l b show x l l b z x let l b let x b b z x z x l show l b z l hence x l closer look set l clearly l ø z l λ k z λ z λ hence z l furthermore z z l z z z z z l thus l nonempty subset k hence z closed scalar multiplication addition lemma k n b k k n n l b l sb moreover invertible l b l sb proof x l b also x sb thus x l sb show l b l sb invertible l sb sb multiplying left yield b since l b l sb l b consider linear system equation ax b theorem find matrix g l n k echelon form let b bi sb l b l b lemma linear system ax b take form x bn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>6 Linear Systems of Equations\\n\\n75\\n\\nSuppose that rank(A) = r , and let j1 , j2 , . . . , jr be the pivot columns.\\nUsing a rightmultiplication with a permutation matrix we can move the r pivot columns of S A to the first r columns.\\nThis is achieved by\\nP T := [e j1 , . . . , e jr , e1 , . . . , e j1 −1 , e j1 +1 , . . . , e j2 −1 , e j2 +1 , . . . , e jr −1 , e jr +1 , . . . , em ] ∈ K m,m , which yields\\n\u0003 := S A P T =\\nA\\n\\n\u000312\\nIr\\nA\\n,\\n0n−r,r 0n−r,m−r\\n\\n\u000312 ∈ K r,m−r .\\nIf r = m, then we have A\\n\u000312 = [ ].\\nThis permutation leads to where A a simplification of the following presentation, but it is usually omitted in practical computations.\\n\u0003 =\u0003 b as (S A P T )(P x) = \u0003 b, or Ay b,\\nSince P T P = Im , we can write S Ax = \u0003 which has the form\\n⎡\\n⎤ ⎡\u0003 ⎤\\n⎡\\n⎤ y1 b1\\n⎢ .. ⎥ ⎢ .. ⎥\\n⎢ Ir\\n\u000312 ⎥ ⎢ . ⎥ ⎢ . ⎥\\nA\\n⎥ ⎢\\n⎥\\n⎢\\n⎥⎢\\n⎥\\n⎢\\n⎥ ⎢ yr ⎥ ⎢ \u0003\\n⎥ = ⎢ br ⎥.\\n⎢\\n⎥⎢\\n(6.1)\\n⎥\\n⎢\\n⎥ ⎢ yr +1 ⎥ ⎢\u0003 b\\n⎥ ⎢ r +1 ⎥\\n⎢\\n⎥⎢\\n⎣ 0n−r,r 0n−r,m−r ⎦ ⎢ .. ⎥ ⎢ .. ⎥\\n⎣ . ⎦ ⎣ . ⎦\\n\u0003 bn\\n\u000e\\n\u000f ym\\n\u000e \u000f\\n\u000e \u000f\\n\u0003\\n= A:=S\\nAPT\\n=y:=P x\\n\\n=\u0003 b:=Sb\\n\\nThe left-multiplication of x with P just means a different ordering of the unknowns x1 , . . . , xm .\\nThus, the solutions of Ax = b can be easily recovered from the solutions\\n\u0003 = \u0003\\n\u0003\u0003 y ∈ of Ay b, and vice versa: We have \u0002 y ∈ L ( A, b) if and only if \u0002 x := P T \u0002\\n\u0003\\nL (S A, b) = L (A, b).\\nThe solutions of (6.1) can now be determined using the extended coefficient matrix\\n\u0003\u0003\\n[ A, b] ∈ K n,m+1 ,\\n\u0003 Note that rank( A)\\n\u0003 ≤ which is obtained by attaching \u0003 b as an extra column to A.\\n\u0003\\n\u0003\\n\u0003\\n\u0003 rank([ A, b]), with equality if and only if br +1 = · · · = bn = 0.\\n\u0003 &lt; rank([ A,\\n\u0003\u0003 bn is nonzero.\\nThen\\nIf rank( A) b]), then at least one of \u0003 br +1 , . . . , \u0003\\n\u0003\\n(6.1) cannot have a solution, since all entries of A in the rows r + 1, . . . , n are zero.\\n\u0003 = rank([ A,\\n\u0003\u0003 bn = 0 and\\nIf, on the other hand, rank( A) b]), then \u0003 br +1 = · · · = \u0003\\n(6.1) can be written as\\n⎡ ⎤ ⎡ ⎤\\n⎡\\n⎤\\n\u0003 y1 b1 yr +1\\n⎢ .. ⎥ ⎢ .. ⎥ \u0003 ⎢ .. ⎥\\n⎣ . ⎦ = ⎣ . ⎦ − A12 ⎣ . ⎦ .\\n\u0003 yr ym br\\n\\n(6.2)</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81</td>\n",
       "      <td>6 Linear Systems of Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear system equation suppose rank r let jr pivot column using rightmultiplication permutation matrix move r pivot column first r column achieved p e e jr e e e e e jr e jr em k yield p ir r k r r permutation lead simplification following presentation usually omitted practical computation b p p x b ay b since p p im write ax form ir yr br yr b r r bn ym apt x b left-multiplication x p mean different ordering unknown xm thus solution ax b easily recovered solution ay b vice versa l b x p l b l b solution determined using extended coefficient matrix b k n note rank obtained attaching b extra column rank b equality br bn rank bn nonzero rank b least one br solution since entry row r n zero rank bn hand rank b br written yr yr ym br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>76\\n\\n6 Linear Systems of Equations\\n\\nThis representation implies, in particular, that\\n\u0003\u0003\\n\u0003 b1 , . . . , \u0003 br , 0, . . . , 0]T ∈ L ( A, b) \u0003= Ø.\\nb(m) := [\u0003\\n\u000e \u000f m−r\\n\\n\u0003\u0003\\n\u0003 0).\\nIn order to determine\\nFrom Lemma 6.2 we know that L ( A, b) = \u0003 b(m) +L ( A,\\n\u0003\\n\u0003\\n\u0003\\nL ( A, 0) we set b1 = · · · = br = 0 in (6.2), which yields\\n\u0003 0) =\\nL ( A,\\n\\n\u0010 ym ]T | \u0002 yr +1 , . . . , \u0002 ym arbitrary and\\n(6.3)\\n[\u0002 y1 , . . . , \u0002\\n\u0011\\nT\\nT\\n\u0003\\n[\u0002 y1 , . . . , \u0002 yr ] = 0 − A12 [\u0002 yr +1 , . . . , \u0002 ym ] .\\n\\n\u000312 = [ ], L ( A,\\n\u0003 0) = {0} and thus |L ( A,\\n\u0003\u0003\\nIf r = m, then A b)| = 1, i.e., the solution\\n\u0003\\n\u0003 of Ay = b is uniquely determined.\\nExample 6.4 For the extended coefficient matrix\\n⎡\\n\\n⎤\\n103\u0003 b1\\n\u0003\u0003\\n[ A, b] = ⎣ 0 1 4 \u0003 b2 ⎦ ∈ Q3,4\\n000\u0003 b3\\n\u0003 = rank([ A,\\n\u0003\u0003\\n\u0003\u0003 we have rank( A) b]) if and only if \u0003 b3 = 0.\\nIf \u0003 b3 \u0003= 0, then L ( A, b) = Ø.\\n\u0003 =\u0003 b can be written as\\nIf \u0003 b3 = 0, then Ay\\n\u0003\\n3 b y1\\n= \u00031 −\\n[y3 ].\\ny2\\n4 b2\\n\u0003\u0003\\nHence, \u0003 b(3) = [\u0003 b1 , \u0003 b2 , 0]T ∈ L ( A, b) and\\n\u0003 0) =\\nL ( A,\\n\\n\u0010\\n\\n\u0011 y2 , \u0002 y3 ]T | \u0002 y3 arbitrary and [\u0002 y1 , \u0002 y2 ]T = −[3, 4]T [\u0002 y3 ] .\\n[\u0002 y1 , \u0002\\n\\nSummarizing our considerations we have the following algorithm for solving a linear system of equations.\\nAlgorithm 6.5 Let A ∈ K n,m and b ∈ K n,1 be given.\\n(1) Determine S ∈ G L n (K ) such that S A is in echelon form and define \u0003 b := Sb.\\n(2a) If rank(S A) &lt; rank([S A, \u0003 b]), then L (S A, \u0003 b) = L (A, b) = Ø.\\n\u0003 := S A P T as in (6.1).\\n(2b) If r = rank(A) = rank([S A, \u0003 b]), then define A\\n\u0003\u0003\\n\u0003\u0003\\n\u0003 0), where L ( A,\\n\u0003 0) is\\nWe have \u0003 b(m) ∈ L ( A, b) and L ( A, b) = \u0003 b(m) + L ( A,\\nT\\n\u0003\\n\u0003 y |\u0002 y ∈ L ( A, b)}.\\ndetermined as in (6.3), as well as L (A, b) = {P \u0002\\n\u0003 and rank([A, b]) = rank([S A, \u0003\\nSince rank(A) = rank(S A) = rank( A) b]) =\\n\u0003\u0003 rank([ A, b]), the discussion above also yields the following result about the different cases of the solvability of a linear system of equations.</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82</td>\n",
       "      <td>6 Linear Systems of Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear system equation representation implies particular br l b b order determine lemma know l b b l set br yield l ym yr ym arbitrary yr yr ym l thus r b solution ay b uniquely determined example extended coefficient matrix b rank rank b l b ø b written ay b hence b l b l arbitrary summarizing consideration following algorithm solving linear system equation algorithm let k n b k given determine g l n k echelon form define b sb rank rank b l b l b ø p r rank rank b define l b l b l b b l l b determined well l b p rank b rank since rank rank rank b rank b discussion also yield following result different case solvability linear system equation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>6 Linear Systems of Equations\\n\\n77\\n\\nCorollary 6.6 For A ∈ K n,m and b ∈ K n,1 the following assertions hold:\\n(1) If rank(A) &lt; rank([A, b]), then L (A, b) = Ø.\\n(2) If rank(A) = rank([A, b]) = m, then |L (A, b)| = 1 (i.e., there exists a unique solution).\\n(3) If rank(A) = rank([A, b]) &lt; m, then there exist many solutions.\\nIf the field K has infinitely many elements (e.g., when K = Q, K = R or K = C), then there exist infinitely many pairwise distinct solutions.\\nThe different cases in Corollary 6.6 will be studied again in Example 10.8.\\nExample 6.7 Let K = Q and consider the linear system of equations Ax = b with\\n⎡\\n\\n1\\n⎢0\\n⎢\\nA=⎢\\n⎢1\\n⎣2\\n1\\n\\n2\\n1\\n0\\n3\\n1\\n\\n2\\n0\\n3\\n5\\n3\\n\\n⎡ ⎤\\n⎤\\n1\\n1\\n⎢0⎥\\n3⎥\\n⎢ ⎥\\n⎥\\n⎢ ⎥\\n0⎥\\n⎥, b = ⎢2⎥ .\\n⎣ 3⎦\\n⎦\\n4\\n2\\n3\\n\\nWe form [A, b] and apply the Gaussian elimination algorithm in order to transform\\nA into echelon form:\\n⎡\\n⎡\\n⎤\\n⎤\\n1 22 11\\n12211\\n⎢0 1 0 3 0⎥\\n⎢0 1 0 3 0⎥\\n⎢\\n⎢\\n⎥\\n⎥\\n⎢\\n⎥\\n⎥\\n[A, b] \u0003 ⎢ 0 −2 1 −1 1 ⎥ \u0003 ⎢\\n⎢0 0 1 5 1⎥\\n⎣ 0 −1 1 2 1 ⎦\\n⎣0 0 1 5 1⎦\\n0 −1 1 2 1\\n00151\\n⎡\\n⎡\\n⎤\\n⎤\\n12211\\n1 0 2 −5 1\\n⎢0 1 0 3 0⎥\\n⎢0 1 0 3 0⎥\\n⎢\\n⎢\\n⎥\\n⎥\\n⎢\\n⎥\\n⎥\\n\u0003 ⎢0 0 1 5 1⎥ \u0003 ⎢\\n⎢0 0 1 5 1⎥\\n⎣0 0 0 0 0⎦\\n⎣0 0 0 0 0⎦\\n00000\\n000 00\\n⎡\\n⎤\\n1 0 0 −15 −1\\n⎢0 1 0\\n3 0⎥\\n⎢\\n⎥\\n⎢\\n\u0003\\n5 1⎥\\n\u0003 ⎢0 0 1\\n⎥ = [S A|b].\\n⎣0 0 0\\n⎦\\n0 0\\n000\\n0 0\\nHere rank(S A) = rank([S A, \u0003 b]) = 3, and hence there exist solutions.\\nThe pivot\\n\u0003 = S A. Now columns are ji = i for i = 1, 2, 3, so that P = P T = I4 and A\\nS Ax = \u0003 b can be written as\\n⎤ ⎡\\n⎤\\n⎡ ⎤ ⎡\\n−1\\n−15 x1\\n⎣x2 ⎦ = ⎣ 0 ⎦ − ⎣ 3 ⎦ [x4 ].\\n1\\n5 x3</td>\n",
       "      <td>83.0</td>\n",
       "      <td>83</td>\n",
       "      <td>6 Linear Systems of Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear system equation corollary k n b k following assertion hold rank rank b l b ø rank rank b b exists unique solution rank rank b exist many solution field k infinitely many element k q k r k c exist infinitely many pairwise distinct solution different case corollary studied example example let k q consider linear system equation ax b b form b apply gaussian elimination algorithm order transform echelon form b rank rank b hence exist solution pivot column ji p p ax b written</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>78\\n\\n6 Linear Systems of Equations\\n\\nConsequently, \u0003 b(4) = [−1, 0, 1, 0]T ∈ L (A, b) and L (A, b) = \u0003 b(4) + L (A, 0), where\\nL (A, 0) =\\n\\n\u0010\\n\\n\u0011 x 4 ]T | \u0002 x4 arbitrary and [\u0002 x1 , \u0002 x2 , \u0002 x3 ]T = −[−15, 3, 5]T [\u0002 x4 ] .\\n[\u0002 x1 , . . . , \u0002\\n\\nExercises\\n6.1 Find a field K and matrices A ∈ K n,m , S ∈ K n,n and b ∈ K n,1 with L (A, b) \u0003=\\nL (S A, Sb).\\n6.2 Determine L (A, b) for the following A and b:\\n⎡\\n\\n1\\nA = ⎣1\\n1\\n⎡\\n1\\nA = ⎣1\\n1\\n⎡\\n1\\n⎢1\\nA=⎢\\n⎣1\\n1\\n⎡\\n1\\n⎢1\\nA=⎢\\n⎣1\\n1\\n\\n⎤\\n⎡\\n⎤\\n1 1\\n1\\n2 −1 ⎦ ∈ R3,3 , b = ⎣ −2 ⎦ ∈ R3,1 ,\\n−1 6\\n3\\n⎤\\n⎡\\n⎤\\n1\\n1\\n0\\n1\\n2 −1 −1⎦ ∈ R3,4 , b = ⎣ −2 ⎦ ∈ R3,1 ,\\n−1\\n6\\n2\\n3\\n⎡\\n⎤\\n⎤\\n1\\n1\\n1\\n⎢ −2 ⎥\\n2 −1⎥\\n4,3\\n4,1\\n⎥∈R , b=⎢\\n⎥\\n⎣ 3⎦ ∈ R ,\\n−1\\n6⎦\\n1\\n1\\n1\\n⎤\\n⎤\\n⎡\\n1\\n1\\n1\\n⎥\\n⎢\\n2 −1⎥\\n⎥ ∈ R4,3 , b = ⎢ −2 ⎥ ∈ R4,1 .\\n⎦\\n⎣\\n−1\\n6\\n3⎦\\n1\\n1\\n0\\n\\n6.3 Let α ∈ Q,\\n⎡\\n\\n⎤\\n⎡ ⎤\\n321\\n6\\nA = ⎣1 1 1⎦ ∈ Q3,3 , bα = ⎣ 3 ⎦ ∈ Q3,1 .\\n210\\nα\\nDetermine L (A, 0) and L (A, bα ) in dependence of α.\\n6.4 Let A ∈ K n,m and B ∈ K n,s .\\nFor i = 1, . . . , s denote by bi the ith column of\\nB. Show that the linear system of equations AX = B has at least one solution\\n\u0002\\nX ∈ K m,s if and only if rank(A) = rank([A, b1 ]) = rank([A, b2 ]) = · · · = rank([A, bs ]).\\nFind conditions under which this solution is unique.</td>\n",
       "      <td>84.0</td>\n",
       "      <td>84</td>\n",
       "      <td>6 Linear Systems of Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear system equation consequently b l b l b b l l x arbitrary exercise find field k matrix k n k n n b k l b l sb determine l b following b b b r b let α q bα α determine l l bα dependence α let k n b k n denote bi ith column b show linear system equation ax b least one solution x k rank rank rank rank b find condition solution unique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6 Linear Systems of Equations\\n\\n6.5 Let\\n\\n⎡\\n\\n0 β1\\n\\n79\\n\\n⎤\\n\\n⎡ ⎤ b1\\n⎢\\n⎥\\n.\\n.\\n⎢α2 0 . ⎥\\n⎢ .. ⎥ n,n n,1\\n⎢\\n⎥\\nA=⎢\\n⎥∈K , b=⎣.⎦∈K\\n⎣ . . . . . . βn ⎦ bn\\nαn 0 be given with βi , αi \u0003= 0 for all i.\\nDetermine a recursive formula for the entries of the solution of the linear system Ax = b.</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85</td>\n",
       "      <td>6 Linear Systems of Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear system equation let n n βn bn αn given βi αi determine recursive formula entry solution linear system ax b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Chapter 7\\n\\nDeterminants of Matrices\\n\\nThe determinant is a map that assigns to every square matrix A ∈ R n,n , where R is a commutative ring with unit, an element of R. This map has very interesting and important properties.\\nFor instance it yields a necessary and sufficient condition for the invertibility of A ∈ R n,n .\\nMoreover, it forms the basis for the definition of the characteristic polynomial of a matrix in Chap.\\n8.\\n\\n7.1 Definition of the Determinant\\nThere are several different approaches to define the determinant of a matrix.\\nWe use the constructive approach via permutations.\\nDefinition 7.1 Let n ∈ N be given.\\nA bijective map\\nσ : {1, 2, . . . , n} → {1, 2, . . . , n}, j \u0004→ σ( j), is called a permutation of the numbers {1, 2, . . . , n}.\\nWe denote the set of all these maps by Sn .\\nA permutation σ ∈ Sn can be written in the form\\n\u0002\\n\u0003\\nσ(1) σ(2) . . . σ(n) .\\nFor example S1 = {[1]}, S2 = {[1 2], [2 1]}, and\\nS3 = { [1 2 3], [1 3 2], [2 1 3], [2 3 1], [3 1 2], [3 2 1] }.\\nFrom Lemma 2.17 we know that |Sn | = n! = 1 · 2 · . . . · n.\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_7\\n\\n81</td>\n",
       "      <td>86.0</td>\n",
       "      <td>&lt;FEFF&gt;</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.1 Definition of the Determinant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter determinant matrix determinant map assigns every square matrix r n n r commutative ring unit element map interesting important property instance yield necessary sufficient condition invertibility r n n moreover form basis definition characteristic polynomial matrix chap definition determinant several different approach define determinant matrix use constructive approach via permutation definition let n n given bijective map σ n n j σ j called permutation number n denote set map sn permutation σ sn written form σ σ σ n example lemma know n springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>82\\n\\n7 Determinants of Matrices\\n\\nThe set Sn with the composition of maps “◦” forms a group (cp.\\nExercise 3.3), which is sometimes called the symmetric group.\\nThe neutral element in this group is the permutation [1 2 . . . n].\\nWhile S1 and S2 are commutative groups, the group Sn for n ≥ 3 is noncommutative.\\nAs an example consider n = 3 and the permutations σ1 = [2 3 1],\\nσ2 = [1 3 2].\\nThen\\nσ1 ◦ σ2 = [σ1 (σ2 (1)) σ1 (σ2 (2)) σ1 (σ2 (3))] = [σ1 (1) σ1 (3) σ1 (2)] = [2 1 3],\\nσ2 ◦ σ1 = [σ2 (σ1 (1)) σ2 (σ1 (2)) σ2 (σ1 (3))] = [σ2 (2) σ2 (3) σ2 (1)] = [3 2 1].\\nDefinition 7.2 Let n ≥ 2 and σ ∈ Sn .\\nA pair (σ(i), σ( j)) with 1 ≤ i &lt; j ≤ n and\\nσ(i) &gt; σ( j) is called an inversion of σ.\\nIf k is the number of inversions of σ, then sgn(σ) := (−1)k is called the sign of σ.\\nFor n = 1 we define sgn([1]) := 1= (−1)0 .\\nIn short, an inversion of a permutation σ is a pair that is “out of order”.\\nThe term inversion should not be confused with the inverse map σ −1 (which exists, since σ is bijective).\\nThe sign of a permutation is sometimes also called the signature.\\nExample 7.3 The permutation [2 3 1 4] ∈ S4 has the inversions (2, 1) and (3, 1), so that sgn([2 3 1 4]) = 1.\\nThe permutation [4 1 2 3] ∈ S4 has the inversions (4, 1),\\n(4, 2), (4, 3), so that sgn([4 1 2 3]) = −1.\\nWe can now define the determinant map.\\nDefinition 7.4 Let R be a commutative ring with unit and let n ∈ N. The map det : R n,n → R,\\n\\nA = [ai j ] \u0004→ det(A) :=\\n\\n\u0004\\nσ∈Sn sgn(σ) n\\n\u0005 ai,σ(i) ,\\n\\n(7.1) i=1 is called the determinant, and the ring element det(A) is called the determinant of A.\\nThe formula (7.1) for det(A) is called the signature formula of Leibniz.1 The term sgn(σ) in this definition is to be interpreted as an element of the ring R, i.e., either sgn(σ) = 1 ∈ R or sgn(σ) = −1 ∈ R, where −1 ∈ R is the unique additive inverse of the unit 1 ∈ R.\\nExample 7.5 For n = 1 we have A = [a11 ] and thus det(A) = sgn([1])a11 = a11 .\\nFor n = 2 we get det(A) = det\\n\\n\b\\n\u0006\u0007 a11 a12\\n= sgn([1 2])a11 a22 + sgn([2 1])a12 a21 a21 a22\\n\\n= a11 a22 − a12 a21 .\\n\\n1 Gottfried\\n\\nWilhelm Leibniz (1646–1716).</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.1 Definition of the Determinant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>determinant matrix set sn composition map form group cp exercise sometimes called symmetric group neutral element group permutation n commutative group group sn n noncommutative example consider n permutation definition let n σ sn pair σ σ j j n σ σ j called inversion σ k number inversion σ sgn σ k called sign σ n define sgn short inversion permutation σ pair order term inversion confused inverse map σ exists since σ bijective sign permutation sometimes also called signature example permutation inversion sgn permutation inversion sgn define determinant map definition let r commutative ring unit let n map det r n n r ai j det sgn σ n ai σ called determinant ring element det called determinant formula det called signature formula term sgn σ definition interpreted element ring r either sgn σ r sgn σ r r unique additive inverse unit example n thus det sgn n get det det sgn sgn gottfried wilhelm leibniz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>7.1 Definition of the Determinant\\n\\n83\\n\\nFor n = 3 we have the Sarrus rule2 : det(A) = a11 a22 a33 + a12 a23 a31 + a13 a21 a32\\n−a11 a23 a32 − a12 a21 a33 − a13 a22 a31 .\\nIn order to compute det(A) using the signature formula of Leibniz we have to form n! products with n factors each.\\nFor large n this is too costly even on modern computers.\\nAs we will see in Corollary 7.16, there are more efficient ways for computing det(A).\\nThe signature formula is mostly of theoretical relevance, since it represents the determinant of A explicitly in terms of the entries of A. Considering the n 2 entries as variables, we can interpret det(A) as a polynomial in these variables.\\nIf R = R or R = C, then standard techniques of Analysis show that det(A) is a continuous function of the entries of A.\\nWe will now study the group of permutations in more detail.\\nThe permutation\\nσ = [3 2 1] ∈ S3 has the inversions (3, 2), (3, 1) and (2, 1), so that sgn(σ) = −1.\\nMoreover,\\n\u0005\\n1≤i&lt; j≤3\\n\\nσ(2) − σ(1) σ(3) − σ(1) σ(3) − σ(2)\\nσ( j) − σ(i)\\n= j −i\\n2−1\\n3−1\\n3−2\\n=\\n\\n2−3 1−3 1−2\\n= (−1)3 = −1 = sgn(σ).\\n2−1 3−1 3−2\\n\\nThis observation can be generalized as follows.\\nLemma 7.6 For each σ ∈ Sn we have sgn(σ) =\\n\\n\u0005\\n1≤i&lt; j≤n\\n\\nσ( j) − σ(i)\\n.\\nj −i\\n\\n(7.2)\\n\\nProof If n = 1, then the left hand side of (7.2) is an empty product, which is defined to be 1 (cp.\\nSect.\\n3.2), so that (7.2) holds for n = 1.\\nLet n &gt; 1 and σ ∈ Sn with sgn(σ) = (−1)k , i.e., k is the number of pairs\\n(σ(i), σ( j)) with i &lt; j but σ(i) &gt; σ( j).\\nThen\\n\u0005\\n\\n(σ( j) − σ(i)) = (−1)k\\n\\n1≤i&lt; j≤n\\n\\n\u0005\\n1≤i&lt; j≤n\\n\\n|σ( j) − σ(i)| = (−1)k\\n\\n\u0005\\n\\n( j − i).\\n\\n1≤i&lt; j≤n\\n\\nIn the last equation we have used the fact that the two products have the same factors\\n(except possibly for their order).\\n\b\\n\\n2 Pierre\\n\\nFrédéric Sarrus (1798–1861).</td>\n",
       "      <td>88.0</td>\n",
       "      <td>88</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.1 Definition of the Determinant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>definition determinant n sarrus det order compute det using signature formula leibniz form n product n factor large n costly even modern computer see corollary efficient way computing det signature formula mostly theoretical relevance since represents determinant explicitly term entry considering n entry variable interpret det polynomial variable r r r c standard technique analysis show det continuous function entry study group permutation detail permutation σ inversion sgn σ moreover σ σ σ σ σ σ σ j σ j sgn σ observation generalized follows lemma σ sn sgn σ σ j σ j proof n left hand side empty product defined cp sect hold n let n σ sn sgn σ k k number pair σ σ j j σ σ j σ j σ k j σ k j last equation used fact two product factor except possibly order pierre frédéric sarrus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>84\\n\\n7 Determinants of Matrices\\n\\nTheorem 7.7 For all σ1 , σ2 ∈ Sn we have sgn(σ1 ◦ σ2 ) = sgn(σ1 ) sgn(σ2 ).\\nIn particular, sgn(σ −1 ) = sgn(σ) for all σ ∈ Sn .\\nProof By Lemma 7.6 we have\\n\u0005 sgn(σ1 ◦ σ2 ) =\\n\\n1≤i&lt; j≤n\\n\\n⎛\\n\\nσ1 (σ2 ( j)) − σ1 (σ2 (i)) j −i\\n\\n\u0005\\n\\n⎞⎛\\n\\n\u0005\\n\\n⎞\\n\\nσ1 (σ2 ( j)) − σ1 (σ2 (i)) ⎠ ⎝\\nσ2 ( j) − σ2 (i) ⎠\\nσ\\n( j)\\n−\\nσ\\n(i) j −i\\n2\\n2\\n1≤i&lt; j≤n\\n1≤i&lt; j≤n\\n⎛\\n⎞\\n\u0005\\nσ1 (σ2 ( j)) − σ1 (σ2 (i)) ⎠\\n=⎝ sgn(σ2 )\\nσ2 ( j) − σ2 (i)\\n1≤σ2 (i)&lt;σ2 ( j)≤n\\n⎛\\n⎞\\n\u0005 σ1 ( j) − σ1 (i)\\n⎠ sgn(σ2 )\\n=⎝ j −i\\n1≤i&lt; j≤n\\n\\n=⎝\\n\\n= sgn(σ1 ) sgn(σ2 ).\\nFor each σ ∈ Sn we have 1 = sgn([1 2 . . . n]) = sgn(σ ◦ σ −1 ) = sgn(σ) sgn(σ −1 ),\\n\b so that sgn(σ) = sgn(σ −1 ).\\nTheorem 7.7 shows that the map sgn is a homomorphism between the groups\\n(Sn , ◦) and ({1, −1}, ·), where the operation in the second group is the standard multiplication of the integers 1 and −1.\\nDefinition 7.8 A transposition is a permutation τ ∈ Sn , n ≥ 2, that exchanges exactly two distinct elements k, \u0002 ∈ {1, 2, . . . , n}, i.e., τ (k) = \u0002, τ (\u0002) = k and\\nτ ( j) = j for all j ∈ {1, 2, . . . , n} \\ {k, \u0002}.\\nObviously τ −1 = τ for every transposition τ ∈ Sn .\\nLemma 7.9 Let τ ∈ Sn be the transposition, that exchanges k and \u0002 for some\\n1 ≤ k &lt; \u0002 ≤ n.\\nThen τ has exactly 2(\u0002−k)−1 inversions and, hence, sgn(τ ) = −1.\\nProof We have \u0002 = k + j for a j ≥ 1 and thus τ is given by\\nτ = [1, . . . , k − 1, k + j, k + 1, . . . , k + ( j − 1), k, \u0002 + 1, . . . , n], where the points denote values of τ in increasing and thus “correct” order.\\nA simple counting argument shows that τ has exactly 2 j − 1 = 2(\u0002 − k) − 1 inversions. \b</td>\n",
       "      <td>89.0</td>\n",
       "      <td>89</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.1 Definition of the Determinant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>determinant matrix theorem sn sgn sgn sgn particular sgn σ sgn σ σ sn proof lemma sgn j j j j σ j σ j j sgn j j j sgn j sgn sgn σ sn sgn n sgn σ σ sgn σ sgn σ sgn σ sgn σ theorem show map sgn homomorphism group sn operation second group standard multiplication integer definition transposition permutation τ sn n exchange exactly two distinct element k n τ k τ k τ j j j n k obviously τ τ every transposition τ sn lemma let τ sn transposition exchange k k τ exactly inversion hence sgn τ proof k j j thus τ given τ k k j k k j k n point denote value τ increasing thus correct order simple counting argument show τ exactly j k inversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>7.2 Properties of the Determinant\\n\\n85\\n\\n7.2 Properties of the Determinant\\nIn this section we prove important properties of the determinant map.\\nLemma 7.10 For A ∈ R n,n the following assertions hold:\\n(1) For λ ∈ R,\\n\\n\u0006\u0007 det\\n\\n(2)\\n(3)\\n(4)\\n(5)\\n\\nλ \u0003\\n0n,1 A\\n\\n\b\\n\\n\u0006\u0007\\n= det\\n\\nλ 01,n\\n\u0003 A\\n\\n\b\\n= λ det(A).\\n\\n\u000en\\nIf A = [ai j ] is upper or lower triangular, then det(A) = i=1 aii .\\nIf A has a zero row or column, then det(A) = 0.\\nIf n ≥ 2 and A has two equal rows or two equal columns, then det(A) = 0.\\ndet(A) = det(A T ).\\n\\nProof\\n(1) Exercise.\\n(2) This follows by an application of (1) to the upper (or lower) triangular matrix A.\\n(3) If A has\u000e a zero row or column, then for every σ ∈ Sn at least one factor in the n ai,σ(i) is equal to zero and thus det(A) = 0.\\nproduct i=1\\n(4) Let the rows k and \u0002, with k &lt; \u0002, of A = [ai j ] be equal, i.e., ak j = a\u0002j for j = 1, . . . , n.\\nLet τ ∈ Sn be the transposition that exchanges the elements k and\\n\u0002, and let\\nTn := {σ ∈ Sn | σ(k) &lt; σ(\u0002)}.\\nSince the set Tn contains all permutations σ ∈ Sn for which σ(k) &lt; σ(\u0002), we have |Tn | = |Sn |/2 and\\nSn \\ Tn = {σ ◦ τ | σ ∈ Tn }.\\nMoreover, ai,(σ◦τ )(i)\\n\\n⎧\\n⎪\\n⎨ai,σ(i) , i = k, \u0002,\\n= ak,σ(\u0002) , i = k,\\n⎪\\n⎩ a\u0002,σ(k) , i = \u0002.\\n\\nWe have ak,σ(\u0002) = a\u0002,σ(\u0002) and a\u0002,σ(k) = ak,σ(k) , Thus, using Theorem 7.7 and\\nLemma 7.9, we obtain\\n\u0004\\nσ∈Sn \\Tn sgn(σ) n\\n\u0005 ai,σ(i) =\\n\\n\u0004 sgn(σ ◦ τ )\\n\\nσ∈Tn i=1\\n\\n=\\n\\n\u0004 ai,(σ◦τ )(i) i=1\\n\\n(−sgn(σ))\\n\\nσ∈Tn\\n\\n=− n\\n\u0005\\n\\n\u0004\\nσ∈Tn n\\n\u0005 ai,(σ◦τ )(i) i=1 sgn(σ) n\\n\u0005 i=1 ai,σ(i) .</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.2 Properties of the Determinant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>property determinant property determinant section prove important property determinant map lemma r n n following assertion hold λ r det λ det λ n λ det ai j upper lower triangular det aii zero row column det n two equal row two equal column det det det proof exercise follows application upper lower triangular matrix zero row column every σ sn least one factor n ai σ equal zero thus det product let row k k ai j equal ak j j let τ sn transposition exchange element k let tn σ sn σ k σ since set tn contains permutation σ sn σ k σ sn tn σ τ σ tn moreover ai σ k ak σ k σ k ak σ σ σ k ak σ k thus using theorem lemma obtain sgn σ n ai σ sgn σ τ ai σ n n ai sgn σ n ai σ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>86\\n\\n7 Determinants of Matrices\\n\\nThis implies det(A) =\\n\\n\u0004 sgn(σ)\\n\\nσ∈Sn\\n\\n=\\n\\n\u0004 n\\n\u0005 ai,σ(i) i=1 sgn(σ)\\n\\nσ∈Tn n\\n\u0005\\n\\n\u0004 ai,σ(i) + sgn(σ)\\n\\nσ∈Sn \\Tn i=1 n\\n\u0005 ai,σ(i) = 0.\\ni=1\\n\\nThe proof for the case of two equal columns is analogous.\\n(5) We observe first that\\n{ (σ(i), i) | 1 ≤ i ≤ n } = { (i, σ −1 (i)) | 1 ≤ i ≤ n } for every σ ∈ Sn .\\nTo see this, let i with 1 ≤ i ≤ n be fixed.\\nThen σ(i) = j if and only if i = σ −1 ( j).\\nThus, (σ(i), i) = ( j, i) is an element of the first set if and only if ( j, σ −1 ( j)) = ( j, i) is an element of the second set.\\nSince σ is bijective, the two sets are equal.\\nLet A = [ai j ] and A T = [bi j ] with bi j = a ji .\\nThen det(A T ) =\\n\\n\u0004 sgn(σ)\\n\\nσ∈Sn\\n\\n=\\n\\n\u0004\\n\u0004 bi,σ(i) = sgn(σ −1 ) n\\n\u0005 aσ(i),i = i=1 sgn(σ)\\n\\nσ∈Sn n\\n\u0005\\n\\n\u0004 sgn(σ)\\n\\nσ∈Sn i=1\\n\\nσ∈Sn\\n\\n= n\\n\u0005\\n\\n\u0004 n\\n\u0005 aσ(i),i i=1 sgn(σ −1 )\\n\\nσ∈Sn n\\n\u0005 ai,σ−1 (i) i=1 ai,σ(i) = det(A).\\ni=1\\n\\nHere we have used sgn(σ −1 ) (cp.\\nTheorem 7.7) and the fact that\\n\u000enthat sgn(σ) = \u000e n ai,σ−1 (i) have the same factors.\\n\b the two products i=1 aσ(i),i and i=1\\nExample 7.11 For the matrices\\n⎡\\n\\n⎤\\n123\\nA = ⎣0 4 5⎦ ,\\n006\\n\\n⎡\\n12\\nB = ⎣1 3\\n14\\n\\n⎡\\n⎤\\n⎤\\n0\\n112\\n0⎦ , C = ⎣1 1 3⎦\\n0\\n114 from Z3,3 we obtain det(A) = 1 · 4 · 6 = 24 by (2) in Lemma 7.10, and det(B) = det(C) = 0 by (3) and (4) in Lemma 7.10.\\nWe may also compute these determinants using the Sarrus rule from Example 7.5.\\nItem (2) in Lemma 7.10 shows in particular that det(In ) = 1 for the identity matrix In = [e1 , e2 , . . . , en ] ∈ R n,n .\\nFor this reason the determinant map is called normalized.</td>\n",
       "      <td>91.0</td>\n",
       "      <td>91</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.2 Properties of the Determinant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>determinant matrix implies det sgn σ n ai σ sgn σ n ai σ sgn σ n ai σ proof case two equal column analogous observe first σ n σ n every σ sn see let n fixed σ j σ j thus σ j element first set j σ j j element second set since σ bijective two set equal let ai j bi j bi j ji det sgn σ bi σ sgn σ n aσ sgn σ n sgn σ n n aσ sgn σ n ai ai σ det used sgn σ cp theorem fact sgn σ n ai factor two product aσ example matrix b c obtain det lemma det b det c lemma may also compute determinant using sarrus rule example item lemma show particular det identity matrix en r n n reason determinant map called normalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>7.2 Properties of the Determinant\\n\\n87\\n\\nFor σ ∈ Sn the matrix\\nPσ := [eσ(1) , eσ(2) , . . . , eσ(n) ] is called the permutation matrix associated with σ.\\nThis map from the group Sn to the group of permutation matrices in R n,n is bijective.\\nThe inverse of a permutation matrix is its transpose (cp.\\nTheorem 4.16) and we can easily check that\\nPσ−1 = PσT = Pσ−1 .\\nIf A = [a1 , a2 , . . . , an ] ∈ R n,n , i.e., a j ∈ R n,1 is the jth column of A, then\\nA Pσ = [aσ(1) , aσ(2) , . . . , aσ(n) ], i.e., the right-multiplication of A with Pσ exchanges the columns of A according to the permutation σ.\\nIf, on the other hand, ai ∈ R 1,n is the ith row of A, then\\n⎤ aσ(1)\\n⎢aσ(2) ⎥\\n⎥\\n⎢\\nPσT A = ⎢ . ⎥ ,\\n⎣ .. ⎦\\n⎡ aσ(n) i.e., the left-multiplication of A by PσT exchanges the rows of A according to the permutation σ.\\nWe next study the determinants of the elementary matrices.\\nLemma 7.12 (1) For σ ∈ Sn and the associated permutation matrix Pσ ∈ R n,n we have sgn(σ) = det(Pσ ).\\nIf n ≥ 2 and Pi j is defined as in (5.1), then det(Pi j ) =\\n−1.\\n(2) If Mi (λ) and G i j (λ) are defined as in (5.2) and (5.3), respectively, then det(Mi (λ)) = λ and det(G i j (λ)) = 1.\\nProof\\n(1) If \u0019\\nσ ∈ Sn and P\u0019σ = [ai j ] ∈ R n,n , then a\u0019σ( j), j = 1 for j = 1, 2, . . . , n, and all other entries of P\u0019σ are zero.\\nHence\\nT det(P\u0019\\nσ ) = det(P\u0019\\nσ )=\\n\\n\u0004\\nσ∈Sn sgn(σ) n\\n\u0005 j=1\\n\\n\u001a aσ( j), j = sgn(\u0019\\nσ)\\n\u001b n\\n\u0005 j=1 a\u0019\\nσ ).\\nσ ( j), j = sgn(\u0019\\n\u001a \u001b\u001c",
       " \u001d",
       "\\n=1\\n\\n=0 for σ =\u0019\\nσ\\n\\nThe permutation matrix Pi j is associated with the transposition that exchanges i and j.\\nHence, det(Pi j ) = −1 follows from Lemma 7.9.\\n(2) Since Mi (λ) and G i j (λ) are lower triangular matrices, the assertion follows from\\n(2) in Lemma 7.10.\\n\b</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.2 Properties of the Determinant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>property determinant σ sn matrix pσ eσ eσ eσ n called permutation matrix associated σ map group sn group permutation matrix r n n bijective inverse permutation matrix transpose cp theorem easily check pσt r n n j r jth column pσ aσ aσ aσ n right-multiplication pσ exchange column according permutation σ hand ai r n ith row aσ pσt aσ n left-multiplication pσt exchange row according permutation σ next study determinant elementary matrix lemma σ sn associated permutation matrix pσ r n n sgn σ det pσ n pi j defined det pi j mi λ g j λ defined respectively det mi λ λ det g j λ proof σ sn ai j r n n j j j n entry zero hence det σ det σ sgn σ n aσ j j sgn σ n σ σ j j sgn σ σ permutation matrix pi j associated transposition exchange j hence det pi j follows lemma since mi λ g j λ lower triangular matrix assertion follows lemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>88\\n\\n7 Determinants of Matrices\\n\\nThese results lead to some important computational rules for determinants.\\nLemma 7.13 For A ∈ R n,n , n ≥ 2, and λ ∈ R the following assertions hold:\\n(1) The multiplication of a row of A by λ leads to the multiplication of det(A) by λ: det(Mi (λ)A) = λ det(A) = det(Mi (λ)) det(A).\\n(2) The addition of the λ–multiple of a row of A to another row of A does not change det(A): det(G i j (λ)A) = det(A) = det(G i j (λ)) det(A), and det(G i j (λ)T A) = det(A) = det(G i j (λ)T ) det(A).\\n(3) Exchanging two rows of A changes the sign of det(A): det(Pi j A) = − det(A) = det(Pi j ) det A.\\nProof\\n\u0019 = Mi (λ)A = [\u0019\\n(1) If A = [amk ] and A amk ], then amk , m = i,\\n=\\nλamk , m = i,\\n\\n\u0019 amk and hence\\n\u0019 = det( A)\\n\\n\u0004 n\\n\u0005 sgn(σ)\\n\\nσ∈Sn m=1\\n\\n\u0019 am,σ(m) =\\n\\n\u0004\\nσ∈Sn sgn(σ) \u0019 ai,σ(i)\\n\u001a \u001b n\\n\u0005 m=1\\n=λai,σ(i) m =i\\n\\n\u0019 am,σ(m)\\n\u001a \u001b\u001c",
       " \u001d",
       "\\n=am,σ(m)\\n\\n= λ det(A).\\n\u0019 = G i j (λ)A = [\u0019\\n(2) If A = [amk ] and A amk ], then\\n\u0019 amk amk , m = j,\\n= a jk + λaik , m = j, and hence\\n\u0019 = det( A)\\n\\n\u0004 sgn(σ) (a j,σ( j) + λai,σ( j) )\\n\\nσ∈Sn\\n\\n=\\n\\n\u0004\\nσ∈Sn n\\n\u0005 am,σ(m) m=1 m= j sgn(σ) n\\n\u0005 m=1 am,σ(m) + λ\\n\\n\u0004\\nσ∈Sn sgn(σ)ai,σ( j) n\\n\u0005 am,σ(m) .\\nm=1 m= j\\n\\nThe first term is equal to det(A), and the second is equal to the determinant of a matrix with two equal columns, and thus equal to zero.\\nThe proof for the matrix\\nG i j (λ)T A is analogous.</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.2 Properties of the Determinant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>determinant matrix result lead important computational rule determinant lemma r n n n λ r following assertion hold multiplication row λ lead multiplication det λ det mi λ λ det det mi λ det addition row another row change det det g j λ det det g j λ det det g j λ det det g j λ det exchanging two row change sign det det pi j det det pi j det proof mi λ amk amk amk λamk amk hence det n sgn σ σ sgn σ ai σ n σ σ σ λ det g j λ amk amk amk amk j jk λaik j hence det sgn σ j σ j λai σ j n σ j sgn σ n σ λ sgn σ ai σ j n σ j first term equal det second equal determinant matrix two equal column thus equal zero proof matrix g j λ analogous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7.2 Properties of the Determinant\\n\\n89\\n\\n(3) The permutation matrix Pi j exchanges rows i and j of A, where i &lt; j.\\nThis exchange can be expressed by the following four elementary row operations:\\nMultiply row j by −1; add row i to row j; add the (−1)–multiple of row j to row i; add row i to row j.\\nTherefore,\\nPi j = G i j (1)(G i j (−1))T G i j (1)M j (−1).\\n(One may verify this also by carrying out the matrix multiplications.)\\nUsing (1) and (2) we obtain det(Pi j A) = det G i j (1)(G i j (−1))T G i j (1)M j (−1)A\\n= det(G i j (1)) det((G i j (−1))T ) det(G i j (1)) det(M j (−1)) det(A)\\n\b\\n= (−1) det(A).\\nSince det(A) = det(A T ) (cp.\\n(5) in Lemma 7.10), the results in Lemma 7.13 for the rows of A can be formulated analogously for the columns of A.\\nExample 7.14 Consider the matrices\\n⎡\\n13\\nA = ⎣1 2\\n12\\n\\n⎤\\n0\\n0⎦ ,\\n4\\n\\n⎡\\n\\n⎤\\n310\\nB = ⎣2 1 0⎦ ∈ Z3,3 .\\n214\\n\\nA simple calculation shows that det(A) = −4.\\nSince B is obtained from A by exchanging the first two columns we have det(B) = − det(A) = 4.\\nThe determinant map can be interpreted as a map of (R n,1 )n to R, i.e., as a map of the n columns of the matrix A ∈ R n,n to the ring R. If ai , a j ∈ R n,1 are two columns of A,\\nA = [. . . ai . . . a j . . .], then det(A) = − det([. . . a j . . . ai . . .]) by (3) in Lemma 7.13.\\nDue to this property the determinant map is called an alternating map of the columns of A. Analogously, the determinant map is an alternating map of the rows of A.\\n(1)\\n(2)\\n( j)\\n\" of A has the form λa + μa for some λ, μ ∈ R and a =\\n!\\nIf the kth row\\n( j)\\n( j)\\n1,n ak1 , . . . , akn ∈ R , j = 1, 2, then</td>\n",
       "      <td>94.0</td>\n",
       "      <td>94</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.2 Properties of the Determinant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>property determinant permutation matrix pi j exchange row j j exchange expressed following four elementary row operation multiply row j add row row j add row j row add row row j therefore pi j g j g j g j j one may verify also carrying matrix multiplication using obtain det pi j det g j g j g j j det g j det g j det g j det j det det since det det cp lemma result lemma row formulated analogously column example consider matrix b simple calculation show det since b obtained exchanging first two column det b det determinant map interpreted map r n r map n column matrix r n n ring ai j r two column ai j det det j ai lemma due property determinant map called alternating map column analogously determinant map alternating map row j form λa μa λ μ r kth row j j n akn r j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>90\\n\\n7 Determinants of Matrices\\n\\n⎛⎡\\n\\n⎤⎞\\n..\\n.\\nn\\n⎜⎢\\n⎥⎟\\n&amp;\u0005\\n%\\n\u0004\\n⎜⎢\\n⎥⎟\\n(1)\\n(2) det(A) = det ⎜⎢λa (1) + μa (2) ⎥⎟ = sgn(σ) λak,σ(k) + μak,σ(k) ai,σ(i)\\n⎝⎣\\n⎦⎠\\n..\\ni=1\\nσ∈Sn i =k\\n.\\n=λ\\n\\n\u0004\\nσ∈Sn\\n\\n(1) sgn(σ) ak,σ(k)\\n\\n⎛⎡ n\\n\u0005 i=1 i =k\\n\\n\u0004 ak,σ(k) + μ\\n\\nσ∈Sn\\n\\n(2) sgn(σ) ak,σ(k)\\n\\n⎛⎡\\n⎤⎞\\n⎤⎞\\n..\\n..\\n⎜⎢ . ⎥⎟\\n⎜⎢ . ⎥⎟\\n⎜⎢\\n⎜⎢\\n⎥⎟\\n⎥⎟\\n= λ det ⎜⎢a (1) ⎥⎟ + μ det ⎜⎢a (2) ⎥⎟ .\\n⎝⎣ . ⎦⎠\\n⎝⎣ . ⎦⎠\\n..\\n..\\nn\\n\u0005 ai,σ(i) i=1 i =k\\n\\nThis property is called the linearity of the determinant map with respect to the rows of A. Analogously we have the linearity with respect to the columns of A. Linear maps will be studied in detail in later chapters.\\nThe next result is called the multiplication theorem for determinants.\\nTheorem 7.15 If K is a field and A, B ∈ K n,n , then det(AB) = det(A) det(B).\\nMoreover, if A is invertible, then det(A−1 ) = (det(A))−1 .\\nProof By Theorem 5.2 we know that for A ∈ K n,n there exist invertible elementary\\n\u0019 = St . . .\\nS1 A is in echelon form.\\nBy Lemma 7.13 we matrices S1 , . . . , St such that A have\\n\u0019 det(A) = det(S1−1 ) · · · det(St−1 ) det( A), as well as\\n\u001f\\n\u0019 det(AB) = det S1−1 · · · St−1 AB\\n\u0019\\n= det(S1−1 ) · · · det(St−1 ) det( AB).\\n\u0019 and thus also AB\\n\u0019 have a zero\\nThere are two cases: If A is not invertible, then A\\n\u0019 = det( AB)\\n\u0019 row.\\nThen det( A)\\n= 0, which implies that det(A) = 0, and hence\\n\u0019 = In , det(AB) = 0 = det(A) det(B).\\nOn the other hand, if A is invertible, then A\\n\u0019 is in echelon form.\\nNow det(In ) = 1 again gives det(AB) = det(A) det(B).\\nsince A\\nFinally, if A is invertible, then 1 = det(In ) = det(A A−1 ) = det(A) det(A−1 ),\\n\b and hence det(A−1 ) = (det(A))−1 .\\nSince our proof relies on Theorem 5.2, which is valid for matrices over a field\\nK , we have formulated Theorem 7.15 for A, B ∈ K n,n .\\nHowever, the multiplication theorem for determinants also holds for matrices over a commutative ring R with unit.\\nA direct proof based on the signature formula of Leibniz can be found, for example, in the book “Advanced Linear Algebra” by Loehr [Loe14, Sect.\\n5.13].\\nThat book also contains a proof of the Cauchy-Binet formula for det(AB) with A ∈ R n,m and\\nB ∈ R m,n for n ≤ m.\\nBelow we will sometimes use that det(AB) = det(A) det(B)</td>\n",
       "      <td>95.0</td>\n",
       "      <td>95</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.2 Properties of the Determinant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>determinant matrix n det det μa sgn σ λak σ k μak σ k ai σ sgn σ ak σ k n ak σ k μ sgn σ ak σ k λ det μ det n ai σ property called linearity determinant map respect row analogously linearity respect column linear map studied detail later chapter next result called multiplication theorem determinant theorem k field b k n n det ab det det b moreover invertible det det proof theorem know k n n exist invertible elementary st echelon form lemma matrix st det det det det well det ab det ab det det det ab thus also ab zero two case invertible det ab row det implies det hence det ab det det b hand invertible echelon form det give det ab det det b since finally invertible det det det det hence det det since proof relies theorem valid matrix field k formulated theorem b k n n however multiplication theorem determinant also hold matrix commutative ring r unit direct proof based signature formula leibniz found example book advanced linear algebra loehr sect book also contains proof cauchy-binet formula det ab r n b r n n sometimes use det ab det det b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>7.2 Properties of the Determinant\\n\\n91 holds for all A, B ∈ R n,n , although we have shown the result in Theorem 7.15 only for A, B ∈ K n,n .\\nThe proof of Theorem 7.15 suggests that det(A) can be easily computed while transforming A ∈ K n,n into its echelon form using elementary row operations.\\nCorollary 7.16 For A ∈ K n,n let S1 , . . . , St ∈ K n,n be elementary matrices, such\\n\u0019 has a zero row and hence\\n\u0019 = St . . .\\nS1 A is in echelon form.\\nThen either A that A\\n−1\\n\u0019 det(A) = 0, or A = In and hence det(A) = (det(S1 )) · · · (det(St ))−1 .\\nAs shown in Theorem 5.4, every matrix A ∈ K n,n can be factorized as A = P LU , and hence det(A) = det(P) det(L) det(U ).\\nThe determinants of the matrices on the right hand side are easily computed, since these are permutation and triangular matrices.\\nAn LU -decomposition of a matrix A therefore yields an efficient way to compute det(A).\\nMATLAB-Minute.\\nLook at the matrices wilkinson(n) for n=2,3,. . .,10 in MATLAB.\\nCan you find a general formula for their entries?\\nFor n=2,3,. . .,10 compute\\nA=wilkinson(n)\\n[L,U,P]=lu(A) (LU -decomposition; cp. the MATLAB-Minute above Definition 5.6) det(L), det(U), det(P), det(P)∗det(L)∗det(U), det(A)\\nWhich permutation is associated with the computed matrix P?\\nWhy is det(A) an integer for odd n?\\n\\n7.3 Minors and the Laplace Expansion\\nWe now show that the determinant can be used for deriving formulas for the inverse of an invertible matrix and for the solution of linear systems of equations.\\nThese formulas are, however, more of theoretical than practical relevance.\\nDefinition 7.17 Let R be a commutative ring with unit and let A ∈ R n,n , n ≥ 2.\\nThen the matrix A( j, i) ∈ R n−1,n−1 that is obtained by deleting the jth row and ith column of A is called a minor 3 of A. The matrix adj(A) = [bi j ] ∈ R n,n with bi j := (−1)i+ j det(A( j, i)), is called the adjunct of A.\\nThe adjunct is also called adjungate or classical adjoint of A.\\n3 This term was introduced in 1850 by James Joseph Sylvester (1814–1897).</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.3 Minors and the Laplace Expansion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>property determinant hold b r n n although shown result theorem b k n n proof theorem suggests det easily computed transforming k n n echelon form using elementary row operation corollary k n n let st k n n elementary matrix zero row hence st echelon form either det hence det det det st shown theorem every matrix k n n factorized p lu hence det det p det l det u determinant matrix right hand side easily computed since permutation triangular matrix lu matrix therefore yield efficient way compute det matlab-minute look matrix wilkinson n matlab find general formula entry compute n l u p lu cp matlab-minute definition det l det u det p det p l u det permutation associated computed matrix p det integer odd n minor laplace expansion show determinant used deriving formula inverse invertible matrix solution linear system equation formula however theoretical practical relevance definition let r commutative ring unit let r n n n matrix j r obtained deleting jth row ith column called minor matrix adj bi j r n n bi j j det j called adjunct adjunct also called adjungate classical adjoint term introduced james joseph sylvester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>92\\n\\n7 Determinants of Matrices\\n\\nTheorem 7.18 For A ∈ R n,n , n ≥ 2, we have\\nA adj(A) = adj(A) A = det(A)In .\\nIn particular A is invertible if and only if det(A) ∈ R is invertible.\\nIn this case\\n(det(A))−1 = det(A−1 ) and A−1 = (det(A))−1 adj(A).\\nProof Let B = [bi j ] have the entries bi j = (−1)i+ j det(A( j, i)).\\nThen C = [ci j ] = adj(A)A satisfies ci j = n\\n\u0004 bik ak j = k=1 n\\n\u0004\\n\\n(−1)i+k det(A(k, i))ak j .\\nk=1\\n\\nLet a\u0002 be the \u0002th column of A and let\\n\u0019 i) := [a1 , . . . , ai−1 , ek , ai+1 , . . . , an ] ∈ R n,n ,\\nA(k, where ek is the kth column of the identity matrix In .\\nThen there exist permutation matrices P and Q that perform k − 1 row and i − 1 column exchanges, respectively, such that\\n\u0007\\n\b\\n\u0003\\n\u0019 i)Q = 1\\nP A(k,\\n.\\n0 A(k, i)\\nUsing (1) in Lemma 7.10 we obtain\\n\b\\n\u0003\\n1\\n\u0019 i)Q)\\n= det(P A(k,\\n0 A(k, i)\\n\u0019 i)) det(Q)\\n= det(P) det( A(k,\\n\u0006\u0007 det(A(k, i)) = det\\n\\n\u0019 i))\\n= (−1)(k−1)+(i−1) det( A(k,\\n\u0019 i)).\\n= (−1)k+i det( A(k,\\nThe linearity of the determinant with respect to the columns now gives ci j = n\\n\u0004\\n\u0019 i))\\n(−1)i+k (−1)k+i ak j det( A(k, k=1\\n\\n= det([a1 , . . . , ai−1 , a j , ai+1 , . . . , an ])\\n\u001e",
       "\\n0, i = j\\n= det(A), i = j\\n= δi j det(A), and thus adj(A)A = det(A)In .\\nAnalogously we can show that A adj(A) = det(A)In .</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.3 Minors and the Laplace Expansion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>determinant matrix theorem r n n n adj adj det particular invertible det r invertible case det det det adj proof let b bi j entry bi j j det j c ci j adj satisfies ci j n bik ak j n det k ak j let column let ek r n n k ek kth column identity matrix exist permutation matrix p q perform k row column exchange respectively q p k k using lemma obtain q det p k k det q det p det k det k det det k det k linearity determinant respect column give ci j n ak j det k det j j det j δi j det thus adj det analogously show adj det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>7.3 Minors and the Laplace Expansion\\n\\n93\\n\\nIf det(A) ∈ R is invertible, then\\nIn = (det(A))−1 adj(A)A = A(det(A))−1 adj(A), i.e., A is invertible with A−1 = (det(A))−1 adj(A).\\nIf, on the other hand, A is invertible, then\\n1 = det(In ) = det(A A−1 ) = det(A) det(A−1 ) = det(A−1 ) det(A), where we have used the multiplication theorem for determinants over R (cp. our comment following the proof of Theorem 7.15).\\nThus, det(A) is invertible with\\n\b\\n(det(A))−1 = det(A−1 ), and again A−1 = (det(A))−1 adj(A).\\nExample 7.19\\n(1) For\\n\\n\b\\n41\\n∈ Z2,2\\n21\\n\\n\u0007\\nA= we have det(A) = 2 and thus A is not invertible.\\nBut A is invertible when considered as an element of Q2,2 , since in this case det(A−1 ) = (det(A))−1 = 21 .\\n(2) For\\n\u0007\\n\b t −1 t −2\\nA=\\n∈ (Z[t])2,2 t t −1 we have det(A) = 1.\\nThe matrix A is invertible, since 1 ∈ Z[t] is invertible.\\nNote that if A ∈ R n,n is invertible, then Theorem 7.18 shows that A−1 can be obtained by inverting only one ring element, det(A).\\nWe now use Theorem 7.18 and the multiplication theorem for matrices over a commutative ring with unit to prove a result already announced in Sect.\\n4.2: In order\\n\u0019 ∈ R n,n is the (unique) inverse of A ∈ R n,n , only one of the two to show that A\\n\u0019 = In needs to be checked.\\n\u0019A = In or A A equations A\\n\u0019 ∈ R n,n exists with A\\n\u0019A = In or A A\\n\u0019 = In ,\\nCorollary 7.20 Let A ∈ R n,n .\\nIf a matrix A\\n\u0019 = A−1 .\\nthen A is invertible and A\\n\u0019A = In , then the multiplication theorem for determinants yields\\nProof If A\\n\u0019A) = det( A)\\n\u0019 det(A) = det(A) det( A),\\n\u0019\\n1 = det(In ) = det( A\\n\u0019 Thus also A is invertible i.e., det(A) ∈ R is invertible with (det(A))−1 = det( A).\\n−1 and has a unique inverse A .\\nFor n = 1 this is obvious and for n ≥ 2 it was shown\\n\u0019A = In from the right with A−1 we in Theorem 7.18.\\nIf we multiply the equation A\\n−1\\n\u0019 get A = A .\\n\u0019 = In is analogous.\\n\b\\nThe proof starting from A A</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.3 Minors and the Laplace Expansion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>minor laplace expansion det r invertible det adj det adj invertible det adj hand invertible det det det det det det used multiplication theorem determinant r cp comment following proof theorem thus det invertible det det det adj example det thus invertible invertible considered element since case det det z det matrix invertible since z invertible note r n n invertible theorem show obtained inverting one ring element det use theorem multiplication theorem matrix commutative ring unit prove result already announced sect order r n n unique inverse r n n one two show need checked equation r n n exists corollary let r n n matrix invertible multiplication theorem determinant yield proof det det det det det det thus also invertible det r invertible det det unique inverse n obvious n shown right theorem multiply equation get analogous proof starting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>94\\n\\n7 Determinants of Matrices\\n\\nLet us summarize the invertibility criteria for a square matrix over a field that we have shown so far:\\nA ∈ G L n (K )\\n\\nTheorem 5.2\\n\\n⇐⇒\\n\\nThe echelon form of A is the identity matrix In\\n\\nDefinition 5.10\\n\\n⇐⇒ clear\\n\\n⇐⇒ rank(A) = n rank(A) = rank([A, b]) = n for all b ∈ K n,1\\n\\nAlgorithm 6.6\\n\\n|L (A, b)| = 1 for all b ∈ K n,1\\n\\nTheorem 7.18 det(A) = 0.\\n\\n⇐⇒\\n⇐⇒\\n\\n(7.3)\\n\\nAlternatively we obtain:\\nA∈\\n/ G L n (K )\\n\\nTheorem 5.2\\n\\n⇐⇒\\n\\nThe echelon form of A has at least one zero row\\n\\nDefinition 5.10\\n\\n⇐⇒ clear\\n\\n⇐⇒ rank(A) &lt; n rank([A, 0]) &lt; n\\n\\nAlgorithm 6.6\\n\\nL (A, 0) = {0}\\n\\nTheorem 7.18 det(A) = 0.\\n\\n⇐⇒\\n⇐⇒\\n\\n(7.4)\\n\\nIn the fields Q, R and C we have the (usual) absolute value | · | of numbers and can formulate the following useful invertibility criterion for matrices.\\nTheorem 7.21 If A ∈ K n,n with K ∈ {Q, R, C} is diagonally dominant, i.e., if\\n|aii | &gt; n\\n\u0004\\n\\n|ai j | for all i = 1, . . . , n, j=1 j =i then det(A) = 0.\\nProof We prove the assertion by contraposition, i.e., by showing that det(A) = 0 implies that A is not diagonally dominant.\\nIf det(A) = 0, then L (A, 0) = {0}, i.e., the homogeneous linear system of xn ]T = 0.\\nLet ' xm be an equations Ax = 0 has at least one solution ' x = [' x1 , . . . , ' entry of ' x with maximal absolute value, i.e., |' xm | ≥ |' x j | for all j = 1, . . . , n.\\nIn x = 0 is given by particular, we then have |' xm | &gt; 0.\\nThe mth row of A' x1 + am2' x2 + . . . + amn' xn = 0 am1'\\n\\n⇔ amm ' xm = − n\\n\u0004 am j ' xj.\\nj=1 j =m\\n\\nWe now take absolute values on both sides and use the triangle inequality, which yields</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.3 Minors and the Laplace Expansion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>determinant matrix let u summarize invertibility criterion square matrix field shown far g l n k theorem echelon form identity matrix definition clear rank n rank rank b n b k algorithm b b k theorem det alternatively obtain g l n k theorem echelon form least one zero row definition clear rank n rank n algorithm l theorem det field q r c usual absolute value number formulate following useful invertibility criterion matrix theorem k n n k q r c diagonally dominant n j n j det proof prove assertion contraposition showing det implies diagonally dominant det l homogeneous linear system xn let xm equation ax least one solution x entry x maximal absolute value xm x j j x given particular xm mth row amn xn amm xm n j xj j take absolute value side use triangle inequality yield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>7.3 Minors and the Laplace Expansion\\n\\n|amm | |' xm | ≤ n\\n\u0004\\n\\n|am j | |' xj| ≤ j=1 j =m\\n\\n95 n\\n\u0004\\n\\n|am j ||' xm |, hence |amm | ≤ n\\n\u0004 j=1 j =m\\n\\n|am j |, j=1 j =m\\n\\n\b so that A not diagonally dominant.\\nThe converse of this theorem does not hold: For example, the matrix\\nA=\\n\\n\u0007 \b\\n12\\n∈ Q2,2 ,\\n10 has det(A) = −2 = 0, but A is not diagonally dominant.\\nFrom Theorem 7.18 we obtain the Laplace expansion4 of the determinant, which is particularly useful when A contains many zero entries (cp.\\nExample 7.24 below).\\nCorollary 7.22 For A ∈ R n,n , n ≥ 2, the following assertions hold:\\n(1) For each i = 1, 2, . . . , n we have det(A) = n\\n\u0004\\n(−1)i+ j ai j det(A(i, j)).\\nj=1\\n\\n(Laplace expansion of det(A) with respect to the ith row A.)\\n(2) For each j = 1, 2, . . . , n we have det(A) = n\\n\u0004\\n(−1)i+ j ai j det(A(i, j)).\\ni=1\\n\\n(Laplace expansion of det(A) with respect to the jth column of A.)\\nProof The two expansions for det(A) follow immediately by comparison of the diagonal entries in the matrix equations det(A) In = A adj(A) and det(A) In = adj(A) A.\\n\b\\nThe Laplace expansions allows a recursive definition of the determinant: For A ∈\\nR n,n with n ≥ 2, let det(A) be defined as in (1) or (2) in Corollary 7.22.\\nWe can choose an arbitrary row or column of A. The formula for det(A) then contains only matrices of size (n−1)×(n−1).\\nFor each of these we can use the Laplace expansion again, now expressing each determinant in terms of determinants of (n − 2) × (n − 2) matrices.\\nWe can do this recursively until only 1 × 1 matrices remain.\\nFor A = [a11 ] ∈ R 1,1 we define det(A) := a11 .\\nFinally we state Cramer’s rule,5 which gives an explicit formula for the solution of a linear system in form of determinants.\\nThis rule is only of theoretical value, because in order to compute the n components of the solution it requires the evaluation of n + 1 determinants of n × n matrices.\\n4 Pierre-Simon\\n5 Gabriel\\n\\nLaplace (1749–1827) published this expansion in 1772.\\nCramer (1704–1752).</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.3 Minors and the Laplace Expansion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>minor laplace expansion xm n j j n j xm hence n j j j diagonally dominant converse theorem hold example matrix det diagonally dominant theorem obtain laplace determinant particularly useful contains many zero entry cp example corollary r n n n following assertion hold n det n j ai j det j laplace expansion det respect ith row j n det n j ai j det j laplace expansion det respect jth column proof two expansion det follow immediately comparison diagonal entry matrix equation det adj det adj laplace expansion allows recursive definition determinant r n n n let det defined corollary choose arbitrary row column formula det contains matrix size use laplace expansion expressing determinant term determinant n n matrix recursively matrix remain r define det finally state cramer give explicit formula solution linear system form determinant rule theoretical value order compute n component solution requires evaluation n determinant n n matrix pierre-simon gabriel laplace published expansion cramer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>96\\n\\n7 Determinants of Matrices\\n\\nCorollary 7.23 Let K be a field, A ∈ G L n (K ) and b ∈ K n,1 .\\nThen the unique solution of the linear system of equations Ax = b is given by xn ]T = A−1 b = (det(A))−1 adj(A)b,\\n' x = [' x1 , . . . , ' with\\n' xi = det[a1 , . . . , ai−1 , b, ai+1 , . . . , an ]\\n, i = 1, . . . , n.\\ndet(A)\\n\\nExample 7.24 Consider\\n⎡\\n13\\n⎢1 2\\nA=⎢\\n⎣1 2\\n12\\n\\n0\\n0\\n1\\n3\\n\\n⎤\\n⎡ ⎤\\n0\\n1\\n⎢2⎥\\n0⎥\\n4,4\\n⎥ ∈ Q , b = ⎢ ⎥ ∈ Q4,1 .\\n⎣1⎦\\n0⎦\\n1\\n0\\n\\nThe Laplace expansion with respect to the last column yields\\n⎛⎡\\n⎤⎞\\n\u0006\u0007 \b\\n130\\n13 det(A) = 1 · det ⎝⎣1 2 0⎦⎠ = 1 · 1 · det\\n= 1 · 1 · (−1) = −1.\\n12\\n121\\nThus, A is invertible and Ax = b has a unique solution ' x = A−1 b ∈ Q4,1 , which by\\nCramer’s rule has the following entries:\\n⎛⎡\\n\\n' x1\\n\\n' x2\\n\\n' x3\\n\\n' x4\\n\\n⎤⎞\\n1300\\n⎜⎢2 2 0 0⎥⎟\\n⎢\\n⎥⎟\\n= det ⎜\\n⎝⎣1 2 1 0⎦⎠ / det(A) = −4/(−1) = 4,\\n0231\\n⎛⎡\\n⎤⎞\\n1100\\n⎜⎢1 2 0 0⎥⎟\\n⎢\\n⎥⎟\\n= det ⎜\\n⎝⎣1 1 1 0⎦⎠ / det(A) = 1/(−1) = −1,\\n1031\\n⎛⎡\\n⎤⎞\\n1310\\n⎜⎢1 2 2 0⎥⎟\\n⎢\\n⎥⎟\\n= det ⎜\\n⎝⎣1 2 1 0⎦⎠ / det(A) = 1/(−1) = −1,\\n1201\\n⎛⎡\\n⎤⎞\\n1301\\n⎜⎢1 2 0 2⎥⎟\\n⎢\\n⎥⎟\\n= det ⎜\\n⎝⎣1 2 1 1⎦⎠ / det(A) = −1/(−1) = 1.\\n1230</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.3 Minors and the Laplace Expansion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>determinant matrix corollary let k field g l n k b k unique solution linear system equation ax b given xn b det adj b x xi det b det example consider q b laplace expansion respect last column yield det det det thus invertible ax b unique solution x b cramer rule following entry det det det det det det det det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>7.3 Minors and the Laplace Expansion\\n\\n97\\n\\nExercises\\n7.1 A permutation σ ∈ Sn is called an r -cycle if there exists a subset {i 1 , . . . , ir } ⊆\\n{1, 2, . . . , n} with r ≥ 1 elements and\\nσ(i k ) = i k+1 for k = 1, 2, . . . , r − 1, σ(ir ) = i 1 , σ(i) = i for i ∈\\n/ {i 1 , . . . , ir }.\\n\\nWe write an r -cycle as σ = (i 1 , i 2 , . . . , ir ).\\nIn particular, a transposition τ ∈ Sn is a 2-cycle.\\n(a) Let n = 4 and the 2-cycles τ1,2 = (1, 2), τ2,3 = (2, 3) and τ3,4 = (3, 4) be\\n−1\\n, and τ1,2 ◦ τ2,3 ◦ τ3,4 .\\ngiven.\\nCompute τ1,2 ◦ τ2,3 , τ1,2 ◦ τ2,3 ◦ τ1,2\\n(b) Let n ≥ 4 and σ = (1, 2, 3, 4).\\nDetermine σ j for j = 2, 3, 4, 5.\\n(c) Show that the inverse of the cycle (i 1 , . . . , ir ) is given by (ir , . . . , i 1 ).\\n(d) Show that two cycles with disjoint elements, i.e. (i 1 , . . . , ir ) and ( j1 , . . . , js ) with {i 1 , . . . , ir } ∩ { j1 , . . . , js } = Ø, commute.\\n(e) Show that every permutation σ ∈ Sn can be written as product of disjoint cycles that are, except for the order, uniquely determined by σ.\\n7.2 Prove Lemma 7.10 (1) using (7.1).\\n7.3 Show that the group homomorphism sgn : (Sn , ◦) → ({1, −1}, ·) satisfies the following assertions:\\n(a) The set An = {σ ∈ Sn | sgn(σ) = 1} is a subgroup of Sn (cp.\\nExercise 3.8).\\n(b) For all σ ∈ An and π ∈ Sn we have π ◦ σ ◦ π −1 ∈ An .\\n7.4 Compute the determinants of the following matrices:\\n(a) A = [en , en−1 , . . . , e1 ] ∈ Zn,n , where ei is the ith column of the identity matrix.\\n\u0002 \u0003\\n(b) B = bi j ∈ Zn,n with\\n⎧\\n⎪ for |i − j| = 0,\\n⎨2 bi j = −1 for |i − j| = 1,\\n⎪\\n⎩\\n0 for |i − j| ≥ 2.\\n(c)\\n\\n⎡\\n\\n1\\n⎢e\\n⎢ 2\\n⎢e\\n⎢ 3\\nC =⎢\\n⎢e 4\\n⎢e\\n⎢\\n⎣e 6\\n0\\n\\n0 1\\n0 eπ\\n1 17\\n31\\n0 −e\\n0 10001\\n√\\n2\\n0\\n0 1\\n\\n0 0\\n√4 √5\\n6 7\\nπ e\\n0 π −1\\n0 0\\n0 0\\n\\n⎤\\n0 √0\\n⎥\\n√1 √ π ⎥\\n8 10⎥\\n⎥\\n7,7\\n0 πe ⎥\\n⎥∈R .\\n2 ⎥\\n0 e π⎥\\n0 −1 ⎦\\n0 0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>102</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.3 Minors and the Laplace Expansion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>minor laplace expansion exercise permutation σ sn called r exists subset ir n r element σ k k r σ ir σ ir write r σ ir particular transposition τ sn let n given compute b let n σ determine σ j j c show inverse cycle ir given ir show two cycle disjoint element ir j ir j ø commute e show every permutation σ sn written product disjoint cycle except order uniquely determined σ prove lemma using show group homomorphism sgn sn satisfies following assertion set σ sn sgn σ subgroup sn cp exercise b σ π sn π σ π compute determinant following matrix en zn n ei ith column identity matrix b b bi j zn n bi j c c eπ π e π π πe e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>98\\n\\n7 Determinants of Matrices\\n\\n(d) The 4 × 4 Wilkinson matrix 6 (cp. the MATLAB-Minute at the end of\\nSect.\\n7.2).\\n7.5 Construct matrices A, B ∈ Rn,n for some n ≥ 2 and with det(A + B) = det(A) + det(B).\\n7.6 Let R be a commutative ring with unit, n ≥ 2 and A ∈ R n,n .\\nShow that the following assertions hold:\\n(a)\\n(b)\\n(c)\\n(d)\\n(e)\\n(f)\\n(g) adj(In ) = In .\\nadj(AB) = adj(B)adj(A), if A and B ∈ R n,n are invertible.\\nadj(λA) = λn−1 adj(A) for all λ ∈ R.\\nadj(A T ) = adj(A)T .\\ndet(adj(A)) = (det(A))n−1 , if A is invertible.\\nadj(adj(A)) = det(A)n−2 A.\\nadj(A−1 ) = adj(A)−1 , if A is invertible.\\n\\nCan one drop the requirement of invertibility in (b) or (e)?\\n1\\n7.7 Let n ≥ 2 and A = [ai j ] ∈ Rn,n with ai j = xi +y for some x1 , . . . , xn , j y1 , . . . , yn ∈ R. Hence, in particular, xi + y j = 0 for all i, j.\\n(Such a matrix A is called a Cauchy matrix.7 )\\n(a) Show that\\n\\n\u000e det(A) =\\n\\n1≤i&lt; j≤n (x j − x i )(y j −\\n\u000en i, j=1 (x i + y j ) yi )\\n\\n.\\n\\n(b) Use (a) to derive a formula for the determinant of the n × n Hilbert matrix\\n(cp. the MATLAB-Minute above Definition 5.6).\\n7.8 Let R be a commutative ring with unit.\\nIf α1 , . . . , αn ∈ R, n ≥ 2, then\\n⎤\\nα1 · · · α1n−1\\nα2 · · · α2n−1 ⎥\\n⎥ n,n\\n.. ⎥ ∈ R\\n..\\n. ⎦\\n.\\n1 αn · · · αnn−1\\n\\n⎡\\n1\\n\" ⎢1\\n!\\n⎢\\nVn := αij−1 = ⎢ .\\n⎣ ..\\nis called a Vandermonde matrix.8\\n(a) Show that\\n\\n\u0005 det(Vn ) =\\n\\n1≤i&lt; j≤n\\n\\n6 James\\n\\nHardy Wilkinson (1919–1986).\\nLouis Cauchy (1789–1857).\\n8 Alexandre-Théophile Vandermonde (1735–1796).\\n7 Augustin\\n\\n(α j − αi ).</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.3 Minors and the Laplace Expansion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>determinant matrix wilkinson matrix cp matlab-minute end sect construct matrix b rn n n det b det det b let r commutative ring unit n r n n show following assertion hold b c e f g adj adj ab adj b adj b r n n invertible adj λa adj λ adj adj det adj det invertible adj adj det adj adj invertible one drop requirement invertibility b e let n ai j rn n ai j xi xn j yn hence particular xi j j matrix called cauchy show det x j x j x j yi b use derive formula determinant n n hilbert matrix cp matlab-minute definition let r commutative ring unit αn r n n n r αn vn called vandermonde show det vn james hardy wilkinson louis cauchy alexandre-théophile vandermonde augustin α j αi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>7.3 Minors and the Laplace Expansion\\n\\n99\\n\\n(b) Let K be a field and let K [t]≤n−1 be the set of polynomials in the variable t of degree at most n − 1.\\nShow that two polynomials p, q ∈ K [t]≤n−1 are equal if there exist pairwise distinct β1 , . . . , βn ∈ K with p(β j ) = q(β j ).\\n7.9 Show the following assertions:\\n(a) Let K be a field with 1 + 1 = 0 and let A ∈ K n,n with A T = −A. If n is odd, then det(A) = 0.\\n(b) If A ∈ G L n (R) with A T = A−1 , then det(A) ∈ {1, −1}.\\n7.10 Let K be a field and\\n\\n\u0007\\n\\nA11 A12\\nA=\\nA21 A22\\n\\n\b for some A11 ∈ K n 1 ,n 1 , A12 ∈ K n 1 ,n 2 , A21 ∈ K n 2 ,n 1 , A22 ∈ K n 2 ,n 2 .\\nShow the following assertions:\\n\u001f\\n(a) If A11 ∈ G L n 1 (K ), then det(A) = det(A11 ) det A22 − A21 A−1\\n11 A12 .\\n\u001f\\n(b) If A22 ∈ G L n 2 (K ), then det(A) = det(A22 ) det A11 − A12 A−1\\n22 A21 .\\n(c) If A21 = 0, then det(A) = det(A11 ) det(A22 ).\\nCan you show this also when the matrices are defined over a commutative ring with unit?\\n7.11 Construct matrices A11 , A12 , A21 , A22 ∈ Rn,n for n ≥ 2 with\\n\b\\n\u0006\u0007\\nA11 A12\\n= det(A11 ) det(A22 ) − det(A12 ) det(A21 ).\\ndet\\nA21 A22\\n7.12 Let A = [ai j ] ∈ G L n (R) with ai j ∈ Z for i, j = 1, . . . , n.\\nShow that the following assertions hold:\\n(a) A−1 ∈ Qn,n .\\n(b) A−1 ∈ Zn,n if and only if det(A) ∈ {−1, 1}.\\n(c) The linear system of equations Ax = b has a unique solution ' x ∈ Zn,1 for every b ∈ Zn,1 if and only if det(A) ∈ {−1, 1}.\\n7.13 Show that G = {A ∈ Zn,n | det(A) ∈ {−1, 1} } is a subgroup of G L n (Q).</td>\n",
       "      <td>104.0</td>\n",
       "      <td>104</td>\n",
       "      <td>7 Determinants of Matrices</td>\n",
       "      <td>7.3 Minors and the Laplace Expansion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>minor laplace expansion b let k field let k set polynomial variable degree n show two polynomial p q k equal exist pairwise distinct βn k p β j q β j show following assertion let k field let k n n n odd det b g l n r det let k field k n n k n n k n n k n n show following assertion g l n k det det det b g l n k det det det c det det det show also matrix defined commutative ring unit construct matrix rn n n det det det det det let ai j g l n r ai j z j show following assertion hold qn n b zn n det c linear system equation ax b unique solution x every b det show g zn n det subgroup g l n q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Chapter 8\\n\\nThe Characteristic Polynomial and Eigenvalues of Matrices\\n\\nWe have already characterized matrices using their rank and their determinant.\\nIn this chapter we use the determinant map in order to assign to every square matrix a unique polynomial that is called the characteristic polynomial of the matrix.\\nThis polynomial contains important information about the matrix.\\nFor example, one can read off the determinant and thus see whether the matrix is invertible.\\nEven more important are the roots of the characteristic polynomial, which are called the eigenvalues of the matrix.\\n\\n8.1 The Characteristic Polynomial and the Cayley-Hamilton Theorem\\nLet R be a commutative ring with unit and let R[t] be the corresponding ring of polynomials (cp.\\nExample 3.17).\\nFor A = [ai j ] ∈ R n,n we set\\n⎡\\n⎤ t − a11 −a12\\n···\\n−a1n\\n⎢\\n.. ⎥\\n⎢ −a21 t − a22 . . .\\n. ⎥\\n⎥ ∈ (R[t])n,n .\\nt In − A := ⎢\\n⎢ ..\\n⎥\\n.\\n.\\n..\\n. . −a\\n⎣ .\\n⎦ n−1,n\\n−an1\\n· · · −an,n−1 t − ann\\nThe entries of the matrix t In − A are elements of the commutative ring with unit\\nR[t], where the diagonal entries are polynomials of degree 1, and the other entries are constant polynomials.\\nUsing Definition 7.4 we can form the determinant of the matrix t In − A, which is an element of R[t].\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_8\\n\\n101</td>\n",
       "      <td>105.0</td>\n",
       "      <td>&lt;FEFF&gt;</td>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.1 The Characteristic Polynomial  and the Cayley-Hamilton Theorem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter characteristic polynomial eigenvalue matrix already characterized matrix using rank determinant chapter use determinant map order assign every square matrix unique polynomial called characteristic polynomial matrix polynomial contains important information matrix example one read determinant thus see whether matrix invertible even important root characteristic polynomial called eigenvalue matrix characteristic polynomial cayley-hamilton theorem let r commutative ring unit let r corresponding ring polynomial cp example ai j r n n set r n n n ann entry matrix element commutative ring unit r diagonal entry polynomial degree entry constant polynomial using definition form determinant matrix element r springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>102\\n\\n8 The Characteristic Polynomial and Eigenvalues of Matrices\\n\\nDefinition 8.1 Let R be a commutative ring with unit and A ∈ R n,n .\\nThen\\nPA := det(t In − A) ∈ R[t] is called the characteristic polynomial of A.\\nExample 8.2 If n = 1 and A = [a11 ], then\\nPA = det(t I1 − A) = det([t − a11 ]) = t − a11 .\\nFor n = 2 and\\nA=\\n\\n\b a11 a12 a21 a22 we obtain\\n\b t − a11 −a12\\n−a21 t − a22\\n\\nPA = det\\n\\n= t 2 − (a11 + a22 )t + (a11 a22 − a12 a21 ).\\n\\nUsing Definition 7.4 we see that the general form of PA for a matrix A ∈ R n,n is given by n\\n\u000e\\n\u000f sgn(σ)\\nδi,σ(i) t − ai,σ(i) .\\n(8.1)\\nPA =\\nσ∈Sn i=1\\n\\nThe following lemma presents basic properties of the characteristic polynomial.\\nLemma 8.3 For A ∈ R n,n we have PA = PAT and\\nPA = t n − αn−1 t n−1 + . . . + (−1)n−1 α1 t + (−1)n α0 with αn−1 =\\n\\n\u0010n i=1 aii and α0 = det(A).\\n\\nProof Using (5) in Lemma 7.10 we obtain\\nPA = det(t In − A) = det((t In − A)T ) = det(t In − A T ) = PAT .\\nUsing PA as in (8.1) we see that n\\n\\nPA = n\\n\\n(t − aii ) + i=1 sgn(σ)\\nσ∈Sn\\nσ\u0003 =[1 ··· n] i=1\\n\\n\u000e\\n\\n\u000f\\nδi,σ(i) t − ai,σ(i) .</td>\n",
       "      <td>106.0</td>\n",
       "      <td>106</td>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.1 The Characteristic Polynomial  and the Cayley-Hamilton Theorem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>characteristic polynomial eigenvalue matrix definition let r commutative ring unit r n n pa det r called characteristic polynomial example n pa det det n obtain pa det using definition see general form pa matrix r n n given n sgn σ δi σ ai σ pa following lemma present basic property characteristic polynomial lemma r n n pa pat pa n n aii det proof using lemma obtain pa det det det pat using pa see n pa n aii sgn σ n δi σ ai σ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>8.1 The Characteristic Polynomial and the Cayley-Hamilton Theorem\\n\\n103\\n\\nThe first term on the right hand side is of the form\\n\u0011 n t −\\n\\n\u0012 aii t n−1 + (polynomial of degree ≤ n − 2), n i=1 and the second term is a polynomial of degree ≤ n − 2.\\nThus, αn−1 = claimed.\\nMoreover, Definition 8.1 yields\\n\\n\u0010n i=1 aii as\\n\\nPA (0) = det(−A) = (−1)n det(A), so that α0 = det(A).\\n\\n\u0006\\n\u0005\\n\\nThis lemma shows that the characteristic polynomial of A ∈ R n,n always is of degree n.\\nThe coefficient of t n is 1 ∈ R. Such a polynomial is called monic.\\nThe coefficient of t n−1 is given by the sum of the diagonal entries of A. This quantity is called the trace of A, i.e., n trace(A) := aii .\\ni=1\\n\\nThe following lemma shows that for every monic polynomial p ∈ R[t] of degree n ≥ 1 there exists a matrix A ∈ R n,n with PA = p.\\nLemma 8.4 If n ∈ N and p = t n + βn−1 t n−1 + . . . + β0 ∈ R[t], then p is the characteristic polynomial of the matrix\\n⎡\\n⎤\\n0\\n−β0\\n⎢ ..\\n.. ⎥\\n⎢1 .\\n. ⎥\\n⎥ ∈ R n,n .\\nA=⎢\\n⎢ .\\n⎥\\n.\\n⎣\\n.\\n0 −βn−2 ⎦\\n1 −βn−1\\n(For n = 1 we have A = [−β0 ].)\\nThe matrix A is called the companion matrix of p.\\nProof We prove the assertion by induction on n.\\nFor n = 1 we have p = t + β0 , A = [−β0 ] and PA = det([t + β0 ]) = p.\\nLet the assertion hold for some n ≥ 1.\\nWe consider p = t n+1 + βn t n + . . . + β0 and\\n⎡\\n⎤\\n0\\n−β0\\n⎢ ..\\n.. ⎥\\n⎢1 .\\n. ⎥\\n⎢\\n⎥ ∈ R n+1,n+1 .\\nA=⎢\\n⎥\\n.\\n⎣ . .\\n0 −βn−1 ⎦\\n1 −βn</td>\n",
       "      <td>107.0</td>\n",
       "      <td>107</td>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.1 The Characteristic Polynomial  and the Cayley-Hamilton Theorem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>characteristic polynomial cayley-hamilton theorem first term right hand side form n aii polynomial degree n n second term polynomial degree n thus claimed moreover definition yield aii pa det n det det lemma show characteristic polynomial r n n always degree coefficient n polynomial called monic coefficient given sum diagonal entry quantity called trace n trace aii following lemma show every monic polynomial p r degree n exists matrix r n n pa lemma n n p n r p characteristic polynomial matrix r n n n matrix called companion matrix proof prove assertion induction n p pa det let assertion hold n consider p βn n r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>104\\n\\n8 The Characteristic Polynomial and Eigenvalues of Matrices\\n\\nUsing the Laplace expansion with respect to the first row (cp.\\nCorollary 7.22) and the induction hypothesis we get\\nPA = det(t In+1 − A)\\n⎛⎡\\n⎤⎞ t\\nβ0\\n⎜⎢\\n.. ⎥⎟\\n⎟\\n⎜⎢−1 . . .\\n. ⎥\\n⎢\\n⎥⎟\\n= det ⎜\\n⎟\\n⎜⎢\\n⎥\\n..\\n⎝⎣\\n. t βn−1 ⎦⎠\\n−1 t + βn\\n⎛⎡\\n⎤⎞\\n⎛⎡\\n⎤⎞ t\\nβ1\\n−1 t\\n⎜⎢\\n⎥⎟\\n⎜⎢\\n.. ⎥⎟\\n.. ..\\n⎜⎢−1 . . .\\n⎥⎟\\n⎟\\n⎜⎢\\n. .\\n. ⎥ n+2\\n⎢\\n⎜\\n⎥⎟\\n⎢\\n⎟\\n⎜\\n⎥\\n= t · det ⎜⎢\\n+ (−1)\\n· β0 · det ⎜⎢\\n⎥⎟\\n⎟\\n⎥\\n..\\n..\\n⎝⎣\\n⎝⎣\\n. t ⎦⎠\\n. t βn−1 ⎦⎠\\n−1\\n−1 t + βn\\n= t · (t n + βn t n−1 + . . . + β1 ) + (−1)2n+2 β0\\n= t n+1 + βn t n + . . . + β1 t + β0\\n= p.\\n\\n\u0006\\n\u0005\\n\\nExample 8.5 The polynomial p = (t − 1)3 = t 3 − 3t 2 + 3t − 1 ∈ Z[t] has the companion matrix\\n⎡\\n⎤\\n00 1\\nA = ⎣ 1 0 −3 ⎦ ∈ Z3,3 .\\n01 3\\nThe identity matrix I3 has the characteristic polynomial\\nPI3 = det(t I3 − I3 ) = (t − 1)3 = PA .\\nThus, different matrices may have the same characteristic polynomial.\\nIn Example 3.17 we have seen how to evaluate a polynomial p ∈ R[t] at a scalar\\nλ ∈ R. Analogously, we can evaluate p at a matrix M ∈ R m,m (cp.\\nExercise 4.8).\\nFor p = βn t n + βn−1 t n−1 + . . . + β0 ∈ R[t] we define p(M) := βn M n + βn−1 M n−1 + . . . + β0 Im ∈ R m,m , where the multiplication on the right hand side is the scalar multiplication of β j ∈ R and M j ∈ R m,m , j = 0, 1, . . . , n.\\n(Recall that M 0 = Im .)\\nEvaluating a given polynomial at matrices M ∈ R m,m therefore defines a map from R m,m to R m,m .</td>\n",
       "      <td>108.0</td>\n",
       "      <td>108</td>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.1 The Characteristic Polynomial  and the Cayley-Hamilton Theorem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>characteristic polynomial eigenvalue matrix using laplace expansion respect first row cp corollary induction hypothesis get pa det det βn det det βn n βn βn n example polynomial p z companion matrix identity matrix characteristic polynomial det pa thus different matrix may characteristic polynomial example seen evaluate polynomial p r scalar λ analogously evaluate p matrix r cp exercise p βn n r define p βn n im r multiplication right hand side scalar multiplication β j r j r j recall im evaluating given polynomial matrix r therefore defines map r r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>8.1 The Characteristic Polynomial and the Cayley-Hamilton Theorem\\n\\n105\\n\\nIn particular, using (8.1), the characteristic polynomial PA of A ∈ R n,n satisfies n\\n\\nPA (M) = sgn(σ)\\nσ∈Sn\\n\\n\u000e\\n\u000f\\nδi,σ(i) M − ai,σ(i) Im for all M ∈ R m,m .\\ni=1\\n\\nNote that for M ∈ R n,n and PA = det(t In − A) the “obvious” equation PA (M) = det(M − A) is wrong.\\nBy definition, PA (M) ∈ R n,n and det(M − A) ∈ R, so that the two expressions cannot be the same, even for n = 1.\\nThe following result is called the Cayley-Hamilton theorem.1\\nTheorem 8.6 For every matrix A ∈ R n,n and its characteristic polynomial PA ∈\\nR[t] we have PA (A) = 0 ∈ R n,n .\\nProof For n = 1 we have A = [a11 ] and PA = t − a11 , so that PA (A) = [a11 ] −\\n[a11 ] = [0].\\nLet now n ≥ 2 and let ei be the ith column of the identity matrix In ∈ R n,n .\\nThen\\nAei = a1i e1 + a2i e2 + . . . + ani en , i = 1, . . . , n, which is equivalent to n\\n\\n(A − aii In )ei +\\n\\n(−a ji In )e j = 0, i = 1, . . . , n.\\nj=1 j\u0003 =i\\n\\nThe last n equations can be written as\\n⎡\\n\\n⎤⎡ ⎤ ⎡ ⎤\\n0 e1\\n⎥ ⎢e2 ⎥ ⎢0⎥\\n⎥⎢ ⎥ ⎢ ⎥\\n0.\\n⎥ ⎢ .. ⎥ = ⎢ .. ⎥ , or Bε = \u0019\\n⎦ ⎣ . ⎦ ⎣.⎦\\n0\\n· · · A − ann In en\\n\\nA − a11 In −a21 In · · ·\\n⎢ −a12 In A − a22 In · · ·\\n⎢\\n⎢\\n..\\n..\\n⎣\\n.\\n.\\n−a1n In\\n\\n−a2n In\\n\\n−an1 In\\n−an2 In\\n..\\n.\\n\\nHence B ∈ (R[A])n,n with R[A] := { p(A) | p ∈ R[t]} ⊂ R n,n .\\nThe set R[A] forms a commutative ring with unit given by the identity matrix In (cp.\\nExercise 4.8).\\nUsing\\nTheorem 7.18 we obtain adj(B)B = det(B)\u0019\\nIn ,\\n\\n= 2 and claimed that he had verified it for n = 3.\\nHe did not feel it necessary to give a proof for general n.\\nSir William Rowan Hamilton\\n(1805–1865) proved the theorem for the case n = 4 in 1853 in the context of his investigations of quaternions.\\nOne of the first proofs for general n was given by Ferdinand Georg Frobenius (1849–\\n1917) in 1878.\\nJames Joseph Sylvester (1814–1897) coined the name of the theorem in 1884 by calling it the “no-little-marvelous Hamilton-Cayley theorem”.\\n\\n1 Arthur Cayley (1821–1895) showed this theorem in 1858 for n</td>\n",
       "      <td>109.0</td>\n",
       "      <td>109</td>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.1 The Characteristic Polynomial  and the Cayley-Hamilton Theorem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>characteristic polynomial cayley-hamilton theorem particular using characteristic polynomial pa r n n satisfies n pa sgn σ δi σ ai σ im r note r n n pa det obvious equation pa det wrong definition pa r n n det r two expression even n following result called cayley-hamilton theorem every matrix r n n characteristic polynomial pa r pa r n n proof n pa pa let n let ei ith column identity matrix r n n aei ani en n equivalent n aii ei ji e j last n equation written bε ann en hence b r n n r p p r r n n set r form commutative ring unit given identity matrix cp exercise using theorem obtain adj b b det b claimed verified n feel necessary give proof general sir william rowan hamilton proved theorem case n context investigation quaternion one first proof general n given ferdinand georg frobenius james joseph sylvester coined name theorem calling no-little-marvelous hamilton-cayley theorem arthur cayley showed theorem n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>106\\n\\n8 The Characteristic Polynomial and Eigenvalues of Matrices where det(B) ∈ R[A] and \u0019\\nIn is the identity matrix in (R[A])n,n .\\n(This matrix has n times the identity matrix In on its diagonal.)\\nMultiplying this equation from the right by ε yields adj(B)Bε = det(B)\u0019\\nIn ε, which implies that det(B) = 0 ∈ R n,n .\\nFinally, using Lemma 8.3 gives n\\n\\n0 = det(B) =\\n\\n(δi,σ(i) A − aσ(i),i In ) sgn(σ)\\nσ∈Sn i=1 n\\n\\n=\\n\\n(δσ(i),i A − aσ(i),i In ) sgn(σ)\\nσ∈Sn i=1\\n\\n= PAT (A)\\n= PA (A),\\n\u0006\\n\u0005 which completes the proof.\\n\\n8.2 Eigenvalues and Eigenvectors\\nIn this section we present an introduction to the topic of eigenvalues and eigenvectors of square matrices over a field K .\\nThese concepts will be studied in more detail in later chapters.\\nDefinition 8.7 Let A ∈ K n,n .\\nIf λ ∈ K and v ∈ K n,1 \\ {0} satisfy Av = λv, then λ is called an eigenvalue of A and v is called an eigenvector of A corresponding to λ.\\nWhile by definition v = 0 can never be an eigenvector of a matrix, λ = 0 may be an eigenvalue.\\nFor example,\\n\b\\n\\n1 −1\\n−1 1\\n\\n\b\\n\b\\n1\\n1\\n= 0\\n.\\n1\\n1\\n\\nIf v is an eigenvector corresponding to the eigenvalue λ of A and α ∈ K \\ {0}, then\\nαv \u0003= 0 and\\nA (αv) = α (Av) = α (λv) = λ (αv).\\nThus, also αv is an eigenvector of A corresponding to λ.</td>\n",
       "      <td>110.0</td>\n",
       "      <td>110</td>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.2 Eigenvalues and Eigenvectors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>characteristic polynomial eigenvalue matrix det b r identity matrix r n n matrix n time identity matrix diagonal multiplying equation right ε yield adj b bε det b ε implies det b r n n finally using lemma give n det b δi σ aσ sgn σ n δσ aσ sgn σ pat pa completes proof eigenvalue eigenvectors section present introduction topic eigenvalue eigenvectors square matrix field k concept studied detail later chapter definition let k n n λ k v k satisfy av λv λ called eigenvalue v called eigenvector corresponding λ definition v never eigenvector matrix λ may eigenvalue example v eigenvector corresponding eigenvalue λ α k αv αv α av α λv λ αv thus also αv eigenvector corresponding λ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>8.2 Eigenvalues and Eigenvectors\\n\\n107\\n\\nTheorem 8.8 For A ∈ K n,n the following assertions hold:\\n(1) λ is an eigenvalue of A if and only if λ is a root of the characteristic polynomial of A, i.e., PA (λ) = 0 ∈ K .\\n(2) λ = 0 is an eigenvalue of A if and only if det(A) = 0.\\n(3) λ is an eigenvalue of A if and only if λ is an eigenvalue of A T .\\nProof\\n(1) The equation PA (λ) = det(λIn − A) = 0 holds if and only if the matrix λIn − A is not invertible (cp.\\n(7.4)), and this is equivalent to L (λIn − A, 0) \u0003= {0}.\\nx = 0, or\\nThis, however, means that there exists a vector \u0019 x \u0003= 0 with (λIn − A)\u0019\\nA\u0019 x = λ\u0019 x.\\n(2) By (1), λ = 0 is an eigenvalue of A if and only if PA (0) = 0.\\nThe assertion now follows from PA (0) = (−1)n det(A) (cp.\\nLemma 8.3).\\n\u0006\\n\u0005\\n(3) This follows from (1) and PA = PAT (cp.\\nLemma 8.3).\\nWhether a matrix A ∈ K n,n has eigenvalues or not may depend on the field K over which A is considered.\\nExample 8.9 The matrix\\n\b\\nA=\\n\\n01\\n∈ R2,2\\n−1 0 has the characteristic polynomial PA = t 2 + 1 ∈ R[t].\\nThis polynomial does not have roots, since the equation t 2 + 1 = 0 has no (real) solutions.\\nIf we consider A as an element of C2,2 , then PA ∈ C[t] has the roots i and −i.\\nThen these two complex numbers are the eigenvalues of A.\\nItem (3) in Theorem 8.8 shows that A and A T have the same eigenvalues.\\nAn eigenvector of A, however, may not be an eigenvector of A T .\\nExample 8.10 The matrix\\n\b\\nA=\\n\\n33\\n∈ R2,2\\n11 has the characteristic polynomial PA = t 2 −4t = t ·(t −4), and hence its eigenvalues are 0 and 4.\\nWe have\\n\b\\n\b\\n\b\\n\b\\n\b\\n1\\n2\\n1\\n1\\n1\\n\u0003= λ\\n= and A T\\n=0\\nA\\n−1\\n2\\n−1\\n−1\\n−1</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111</td>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.2 Eigenvalues and Eigenvectors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eigenvalue eigenvectors theorem k n n following assertion hold λ eigenvalue λ root characteristic polynomial pa λ k λ eigenvalue det λ eigenvalue λ eigenvalue proof equation pa λ det λin hold matrix λin invertible cp equivalent l λin x however mean exists vector x λin x x λ eigenvalue pa assertion follows pa n det cp lemma follows pa pat cp lemma whether matrix k n n eigenvalue may depend field k considered example matrix characteristic polynomial pa r polynomial root since equation real solution consider element pa c root two complex number eigenvalue item theorem show eigenvalue eigenvector however may eigenvector example matrix characteristic polynomial pa hence eigenvalue λ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>108\\n\\n8 The Characteristic Polynomial and Eigenvalues of Matrices for all λ ∈ R. Thus, [1, −1]T is an eigenvector of A corresponding to the eigenvalue 0, but it is not an eigenvector of A T .\\nOn the other hand,\\n\b\\nA\\n\\nT\\n\\n1\\n=0\\n−3\\n\\n\b\\n\\n1\\n−3\\n\\n\b\\n\\n\b\\n\b\\n1\\n−6\\n1 and A\\n=\\n\u0003= λ\\n−3\\n−2\\n−3 for all λ ∈ R. Thus, [1, −3]T is an eigenvector of A T corresponding to the eigenvalue 0, but it is not an eigenvector of A.\\nTheorem 8.8 implies further criteria for the invertibility of A ∈ K n,n (cp.\\n(7.3)):\\nA ∈ G L n (K ) ⇔ 0 is not an eigenvalue of A\\n⇔ 0 is not a root of PA .\\nDefinition 8.11 Two matrices A, B ∈ K n,n are called similar, if there exists a matrix\\nZ ∈ G L n (K ) with A = Z B Z −1 .\\nOne can easily show that this defines an equivalence relation on the set K n,n (cp.\\nthe proof following Definition 5.13).\\nTheorem 8.12 If two matrices A, B ∈ K n,n are similar, then PA = PB .\\nProof If A = Z B Z −1 , then the multiplication theorem for determinants yields\\nPA = det(t In − A) = det(t In − Z B Z −1 ) = det(Z (t In − B)Z −1 )\\n= det(Z ) det(t In − B) det(Z −1 ) = det(t In − B) det(Z Z −1 )\\n= PB\\n\u0006\\n\u0005\\n\\n(cp. the remarks below Theorem 7.15).\\n\\nTheorem 8.12 and (1) in Theorem 8.8 show that two similar matrices have the same eigenvalues.\\nThe condition that A and B are similar is sufficient, but not necessary for PA = PB .\\nExample 8.13 Let\\n\b\\n\\n11\\nA=\\n,\\n01\\n\\n\b\\nB=\\n\\n10\\n= I2 .\\n01\\n\\nThen PA = (t − 1)2 = PB , but for every matrix Z ∈ G L n (K ) we have Z B Z −1 =\\nI2 \u0003= A. Thus, we have PA = PB although A and B are not similar (cp. also\\nExample 8.5).</td>\n",
       "      <td>112.0</td>\n",
       "      <td>112</td>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.2 Eigenvalues and Eigenvectors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>characteristic polynomial eigenvalue matrix λ thus eigenvector corresponding eigenvalue eigenvector hand λ λ thus eigenvector corresponding eigenvalue eigenvector theorem implies criterion invertibility k n n cp g l n k eigenvalue root pa definition two matrix b k n n called similar exists matrix z g l n k z b z one easily show defines equivalence relation set k n n cp proof following definition theorem two matrix b k n n similar pa pb proof z b z multiplication theorem determinant yield pa det det z b z det z b z det z det b det z det b det z z pb cp remark theorem theorem theorem show two similar matrix eigenvalue condition b similar sufficient necessary pa pb example let pa pb every matrix z g l n k z b z thus pa pb although b similar cp also example</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>8.2 Eigenvalues and Eigenvectors\\n\\n109\\n\\nMATLAB-Minute.\\nThe roots of a polynomial p = αn t n + αn−1 t n−1 + . . . + α0 can be computed\\n(or approximated) in MATLAB using the command roots(p), where p is a\\n1×(n +1) matrix with the entries p(i)= αn+1−i for i = 1, . . . , n +1.\\nCompute roots(p) for the monic polynomial p = t 3 − 3t 2 + 3t − 1 ∈ R[t] and display the output using format long.\\nWhat are the exact roots of p and how large is the numerical error in the computation of the roots using roots(p)?\\nForm the matrix A=compan(p) and compare its structure with the one of the companion matrix from Lemma 8.4.\\nCan you transfer the proof of Lemma 8.4 to the structure of the matrix A?\\nCompute the eigenvalues of A with the command eig(A) and compare the output with the one of roots(p).\\nWhat do you observe?\\n\\n8.3 Eigenvectors of Stochastic Matrices\\nWe now consider the eigenvalue problem presented in Sect.\\n1.1 in the context of the PageRank algorithm.\\nThe mathematical modeling leads to the equations (1.1), which can be written in the form Ax = x.\\nHere A = [ai j ] ∈ Rn,n (n is the number of documents) satisfies n ai j ≥ 0 and ai j = 1 for j = 1, . . . , n.\\ni=1\\n\\nSuch a matrix A is called column-stochastic.\\nNote that A is column-stochastic if and only if A T is row-stochastic.\\nSuch matrices also occurred in the car insurance application considered in Sect.\\n1.2 and Example 4.7.\\nWe want to determine x =\\n[x1 , . . . , xn ]T ∈ Rn,1 \\ {0} with Ax = x, where the entry xi describes the importance of document i.\\nThe importance values should be nonnegative, i.e., xi ≥ 0 for i =\\n1, . . . , n.\\nThus, we want to determine an entrywise nonnegative eigenvector of A corresponding to the eigenvalue λ = 1.\\nWe first check whether this problem has a solution, and then study whether the solution is unique.\\nOur presentation is based on the article [BryL06].\\nLemma 8.14 A column-stochastic matrix A ∈ Rn,n has an eigenvector corresponding to the eigenvalue 1.\\nProof Since A is column-stochastic, we have A T [1, . . . , 1]T = [1, . . . , 1]T , so that 1 is an eigenvalue of A T .\\nNow (3) in Theorem 8.8 shows that also A has the eigenvalue\\n1, and hence there exists a corresponding eigenvector.\\n\u0006\\n\u0005</td>\n",
       "      <td>113.0</td>\n",
       "      <td>113</td>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.3 Eigenvectors of Stochastic Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eigenvalue eigenvectors matlab-minute root polynomial p αn n computed approximated matlab using command root p p n matrix entry p n compute root p monic polynomial p r display output using format long exact root p large numerical error computation root using root p form matrix p compare structure one companion matrix lemma transfer proof lemma structure matrix compute eigenvalue command eig compare output one root p observe eigenvectors stochastic matrix consider eigenvalue problem presented sect context pagerank algorithm mathematical modeling lead equation written form ax x ai j rn n n number document satisfies n ai j ai j j matrix called column-stochastic note column-stochastic row-stochastic matrix also occurred car insurance application considered sect example want determine x xn ax x entry xi describes importance document importance value nonnegative xi thus want determine entrywise nonnegative eigenvector corresponding eigenvalue λ first check whether problem solution study whether solution unique presentation based article lemma column-stochastic matrix rn n eigenvector corresponding eigenvalue proof since column-stochastic eigenvalue theorem show also eigenvalue hence exists corresponding eigenvector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>110\\n\\n8 The Characteristic Polynomial and Eigenvalues of Matrices\\n\\nA matrix with real entries is called positive, if all its entries are positive.\\nLemma 8.15 If A ∈ Rn,n is positive and column-stochastic and if x ∈ Rn,1 is an eigenvector of A corresponding to the eigenvalue 1, then either x or −x is positive.\\nProof If x = [x1 , . . . , xn ]T is an eigenvector of A = [ai j ] corresponding to the eigenvalue 1, then n xi = ai j x j , i = 1, . . . , n.\\nj=1\\n\\nSuppose that not all entries of x are positive or not all entries of x are negative.\\nThen there exists at least one index k with\\n\u001a\\n|xk |= \u001a n\\n\\n\u001a ak j x j \u001a &lt; j=1 n ak j |x j |, j=1 which implies n n n\\n\\n|xi | &lt; i=1 n n ai j |x j | = i=1 j=1 n j=1 i=1 n n\\n\\n|x j | · ai j |x j | = j=1 ai j i=1\\n\\n\u001b \u001c",
       "\u001d",
       " \u001e",
       "\\n\\n=\\n\\n|x j |.\\nj=1\\n\\n=1\\n\\nThis is impossible, so that indeed x or −x must be positive.\\n\\n\u0006\\n\u0005\\n\\nWe can now prove the following uniqueness result.\\nTheorem 8.16 If A ∈ Rn,n is positive and column-stochastic, then there exists a\\n\u0010n xi = 1 and Ax = x.\\nunique positive x = [x1 , . . . , xn ]T ∈ Rn,1 with i=1\\nProof By Lemma 8.15, A has a least one positive eigenvector corresponding to the\\n\u001f\\n\u001f\\nT\\nT eigenvalue 1.\\nSuppose that x (1) = x1(1) , . . . , xn(1) and x (2) = x1(2) , . . . , xn(2)\\n\u0010n\\n( j) are two such eigenvectors.\\nSuppose that these are normalized by i=1 xi = 1, j = 1, 2.\\nThis assumption can be made without loss of generality, since every nonzero multiple of an eigenvector is still an eigenvector.\\nWe will show that x (1) = x (2) .\\nFor α ∈ R we define x(α) := x (1) + αx (2) ∈ Rn,1 , then\\nAx(α) = Ax (1) + α Ax (2) = x (1) + αx (2) = x(α).\\nα) is equal to zero and thus, by\\nIf !\\nα := −x1(1) /x1(2) , then the first entry of x(!\\nLemma 8.15, x(!\\nα) cannot be an eigenvector of A corresponding to the eigenvalue 1.\\nNow Ax(!\\nα) = x(!\\nα) implies that x(!\\nα) = 0, and hence\\nα xi(2) = 0, i = 1, . . . , n.\\nxi(1) + !\\n\\n(8.2)</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114</td>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.3 Eigenvectors of Stochastic Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>characteristic polynomial eigenvalue matrix matrix real entry called positive entry positive lemma rn n positive column-stochastic x eigenvector corresponding eigenvalue either x positive proof x xn eigenvector ai j corresponding eigenvalue n xi ai j x j suppose entry x positive entry x negative exists least one index k n ak j x j n ak j j implies n n n n n ai j j n n n j ai j j ai j j impossible indeed x must positive prove following uniqueness result theorem rn n positive column-stochastic exists xi ax unique positive x xn proof lemma least one positive eigenvector corresponding eigenvalue suppose x xn x xn j two eigenvectors suppose normalized xi j assumption made without loss generality since every nonzero multiple eigenvector still eigenvector show x x α r define x α x αx ax α ax α ax x αx x α α equal zero thus α first entry x lemma x α eigenvector corresponding eigenvalue ax α x α implies x α hence α xi xi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>8.3 Eigenvectors of Stochastic Matrices\\n\\n111\\n\\nSumming up these n equations yields n xi(1) + !\\nα i=1\\n\\n\u001b \u001c",
       "\u001d",
       " \u001e",
       "\\n=1 n xi(2) = 0, i=1\\n\\n\u001b \u001c",
       "\u001d",
       " \u001e",
       "\\n=1 so that !\\nα = −1.\\nFrom (8.2) we get xi(1) = xi(2) for i = 1, . . . , n, and therefore\\n(1)\\n\u0006\\n\u0005 x = x (2) .\\nThe unique positive eigenvector x in Theorem 8.16 is called the Perron eigenvector 2 of the positive matrix A. The theory of eigenvalues and eigenvectors of positive\\n(or more general nonnegative) matrices is an important area of Matrix Theory, since these matrices arise in many applications.\\nBy construction, the matrix A ∈ Rn,n in the PageRank algorithm is columnstochastic but not positive, since there are (usually many) entries ai j = 0.\\nIn order to obtain a uniquely solvable problem one can use the following trick:\\nLet S = [si j ] ∈ Rn,n with si j = 1/n.\\nObviously, S is positive and columnstochastic.\\nFor a real number α ∈ (0, 1] we define the matrix\\n\u0019\\nA(α)\\n:= (1 − α)A + αS.\\nThis matrix is positive and column-stochastic, and hence it has a unique positive eigenvector \u0019 u corresponding to the eigenvalue 1.\\nWe thus have\\nα\\n\u0019 u = (1 − α)A\u0019\\n\u0019 u = A(α)\u0019 u + αS\u0019 u = (1 − α)A\u0019 u + [1, . . . , 1]T.\\nn\\nFor a very large number of documents (e.g. the entire internet) the number α/n is very small, so that (1 − α)A\u0019 u ≈\u0019 u .\\nTherefore a solution of the eigenvalue problem\\n\u0019 u =\u0019\\nA(α)\u0019 u for small α potentially gives a good approximation of a u ∈ Rn,1 that satisfies Au = u.\\nThe practical solution of the eigenvalue problem with the matrix\\n\u0019\\nA(α) is a topic of the field of Numerical Linear Algebra.\\nThe matrix S represents a link structure where all document are mutually linked\\n\u0019 and thus all documents are equally important.\\nThe matrix A(α)\\n= (1 − α)A + αS therefore models the following internet “surfing behavior”: A user follows a proposed link with the probability 1−α and an arbitrary link with the probability α.\\nOriginally,\\nGoogle Inc. used the value α = 0.15.\\n\\n2 Oskar\\n\\nPerron (1880–1975).</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115</td>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.3 Eigenvectors of Stochastic Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eigenvectors stochastic matrix summing n equation yield n xi α n xi α get xi xi n therefore x x unique positive eigenvector x theorem called perron eigenvector positive matrix theory eigenvalue eigenvectors positive general nonnegative matrix important area matrix theory since matrix arise many application construction matrix rn n pagerank algorithm columnstochastic positive since usually many entry ai j order obtain uniquely solvable problem one use following trick let si j rn n si j obviously positive columnstochastic real number α define matrix α α αs matrix positive column-stochastic hence unique positive eigenvector u corresponding eigenvalue thus α u α u α u u α u n large number document entire internet number small α u u therefore solution eigenvalue problem u α u small α potentially give good approximation u satisfies au u practical solution eigenvalue problem matrix α topic field numerical linear algebra matrix represents link structure document mutually linked thus document equally important matrix α α αs therefore model following internet surfing behavior user follows proposed link probability arbitrary link probability α originally google used value α oskar perron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>112\\n\\n8 The Characteristic Polynomial and Eigenvalues of Matrices\\n\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n8.1 Determine the characteristic polynomials of the following matrices over Q:\\n\b\\nA=\\n\\n20\\n,\\n02\\n\\n\b\\nB=\\n\\n⎡\\n\\n\b\\n\\n44\\n21\\n, C=\\n,\\n−1 0\\n02\\n\\n⎤\\n2 0 −1\\nD = ⎣ 0 2 0⎦.\\n−4 0 2\\n\\nVerify the Cayley-Hamilton theorem in each case by direct computation.\\nAre two of the matrices A, B, C similar?\\n8.2 Let R be a commutative ring with unit and n ≥ 2.\\n(a) Show that for every A ∈ G L n (R) there exists a polynomial p ∈ R[t] of degree at most n − 1 with adj(A) = p(A).\\nConclude that A−1 = q(A) holds for a polynomial q ∈ R[t] of degree at most n − 1.\\n(b) Let A ∈ R n,n .\\nApply Theorem 7.18 to the matrix t In − A ∈ (R[t])n,n and derive an alternative proof of the Cayley-Hamilton theorem from the formula det(t In − A) In = (t In − A) adj(t In − A).\\n8.3 Let A ∈ K n,n be a matrix with Ak = 0 for some k ∈ N. (Such a matrix is called nilpotent.)\\n(a) Show that λ = 0 is the only eigenvalue of A.\\n(b) Determine PA and show that An = 0.\\nn\\n\"\\n(Hint: You may assume that PA has the form (t −λi ) for some λ1 , . . . , λn i=1\\n\\n∈ K .)\\n(c) Show that μIn − A is invertible if and only if μ ∈ K \\ {0}.\\n(d) Show that (In − A)−1 = In + A + A2 + . . . + An−1 .\\n8.4 Determine the eigenvalues and corresponding eigenvectors of the following matrices over R:\\n⎡\\n⎤\\n⎡\\n⎡\\n⎤\\n⎤\\n0 −1 0 0\\n111\\n3 8 16\\n⎢1 0 0 0⎥\\n⎥\\nA = ⎣0 1 1⎦ , B = ⎣ 0 7 8 ⎦ , C = ⎢\\n⎣ 0 0 −2 1 ⎦ .\\n001\\n0 −4 −5\\n0 0 0 −2\\nIs there any difference when you consider A, B, C as matrices over C?\\n8.5 Let n ≥ 3 and ε ∈ R. Consider the matrix\\n⎤\\n1 1\\n⎢ .. .. ⎥\\n⎢\\n. . ⎥\\n⎥\\nA(ε) = ⎢\\n⎢\\n.. ⎥\\n⎣\\n.\\n1⎦\\nε\\n1\\n⎡</td>\n",
       "      <td>116.0</td>\n",
       "      <td>116</td>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.3 Eigenvectors of Stochastic Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>characteristic polynomial eigenvalue matrix exercise following exercise k arbitrary field determine characteristic polynomial following matrix q verify cayley-hamilton theorem case direct computation two matrix b c similar let r commutative ring unit n show every g l n r exists polynomial p r degree n adj p conclude q hold polynomial q r degree n b let r n n apply theorem matrix r n n derive alternative proof cayley-hamilton theorem formula det adj let k n n matrix ak k matrix called nilpotent show λ eigenvalue b determine pa show n hint may assume pa form λn k c show μin invertible μ k show determine eigenvalue corresponding eigenvectors following matrix r b c difference consider b c matrix c let n ε consider matrix ε ε</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>8.3 Eigenvectors of Stochastic Matrices\\n\\n113 as an element of Cn,n and determine all eigenvalues in dependence of ε.\\nHow many pairwise distinct eigenvalues does A(ε) have?\\n8.6 Determine the eigenvalues and corresponding eigenvectors of\\n⎡\\n\\n⎤\\n2 2−a\\n2−a\\n2 − a ⎦ ∈ R3,3 ,\\nA = ⎣0 4 − a\\n0 −4 + 2a −2 + 2a\\n\\n⎡\\n\\n⎤\\n110\\nB = ⎣1 0 1⎦ ∈ (Z/2Z)3,3 .\\n011\\n\\n(For simplicity, the elements of Z/2Z are here denoted by k instead of [k].)\\n8.7 Let A ∈ K n,n , B ∈ K m,m , n ≥ m, and C ∈ K n,m with rank(C) = m and\\nAC = C B. Show that then every eigenvalue of B is an eigenvalue of A.\\n8.8 Show the following assertions:\\n(a) trace(λ A + μB) = λ trace(A) + μ trace(B) holds for all λ, μ ∈ K and\\nA, B ∈ K n,n .\\n(b) trace(AB) = trace(B A) holds for all A, B ∈ K n,n .\\n(c) If A, B ∈ K n,n are similar, then trace(A) = trace(B).\\n8.9 Prove or disprove the following statements:\\n(a) There exist matrices A, B ∈ K n,n with trace(AB) \u0003= trace(A) trace(B).\\n(b) There exist matrices A, B ∈ K n,n with AB − B A = In .\\n8.10 Suppose that the matrix A = [ai j ] ∈ Cn,n has only real entries ai j .\\nShow that if λ ∈ C\\R is an eigenvalue of A with corresponding eigenvector v =\\n[ν1 , . . . , νn ]T ∈ Cn,1 , then also λ is an eigenvalue of A with corresponding eigenvector v := [ν 1 , . . . , ν n ]T .</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117</td>\n",
       "      <td>8 The Characteristic Polynomial  and Eigenvalues of Matrices</td>\n",
       "      <td>8.3 Eigenvectors of Stochastic Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eigenvectors stochastic matrix element cn n determine eigenvalue dependence ε many pairwise distinct eigenvalue ε determine eigenvalue corresponding eigenvectors b simplicity element denoted k instead k let k n n b k n c k n rank c ac c b show every eigenvalue b eigenvalue show following assertion trace λ μb λ trace μ trace b hold λ μ k b k n n b trace ab trace b hold b k n n c b k n n similar trace trace b prove disprove following statement exist matrix b k n n trace ab trace trace b b exist matrix b k n n ab b suppose matrix ai j cn n real entry ai j show λ eigenvalue corresponding eigenvector v νn also λ eigenvalue corresponding eigenvector v ν ν n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Chapter 9\\n\\nVector Spaces\\n\\nIn the previous chapters we have focussed on matrices and their properties.\\nWe have defined algebraic operations with matrices and derived important concepts associated with them, including their rank, determinant, characteristic polynomial, and eigenvalues.\\nIn this chapter we place these concepts in a more abstract framework by introducing the idea of a vector space.\\nMatrices form one of the most important examples of vector spaces, and properties of certain (namely, finite dimensional) vector spaces can be studied in a transparent way using matrices.\\nIn the next chapter we will study (linear) maps between vector spaces, and there the connection with matrices will play a central role as well.\\n\\n9.1 Basic Definitions and Properties of Vector Spaces\\nWe begin with the definition of a vector space over a field K .\\nDefinition 9.1 Let K be a field.\\nA vector space over K , or shortly K -vector space, is a set V with two operations,\\n+ : V × V → V,\\n\\n(v, w) \u0003→ v + w,\\n\\n(addition)\\n\\n· : K × V → V,\\n\\n(λ, v) \u0003→ λ · v,\\n\\n(scalar multiplication) that satisfy the following:\\n(1) (V, +) is a commutative group.\\n(2) For all v, w ∈ V and λ, μ ∈ K the following assertions hold:\\n(a)\\n(b)\\n(c)\\n(d)\\n\\nλ · (μ · v) = (λμ) · v.\\n1 · v = v.\\nλ · (v + w) = λ · v + λ · w.\\n(λ + μ) · v = λ · v + μ · v.\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_9\\n\\n115</td>\n",
       "      <td>118.0</td>\n",
       "      <td>&lt;FEFF&gt;</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.1 Basic Definitions and Properties of Vector Spaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter vector space previous chapter focussed matrix property defined algebraic operation matrix derived important concept associated including rank determinant characteristic polynomial eigenvalue chapter place concept abstract framework introducing idea vector space matrix form one important example vector space property certain namely finite dimensional vector space studied transparent way using matrix next chapter study linear map vector space connection matrix play central role well basic definition property vector space begin definition vector space field k definition let k field vector space k shortly k space set v two operation v v v v w v w addition k v v λ v λ v scalar multiplication satisfy following v commutative group v w v λ μ k following assertion hold b c λ μ v λμ v λ v w λ v λ λ μ v λ v μ springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>116\\n\\n9 Vector Spaces\\n\\nAn element v ∈ V is called a vector,1 an element λ ∈ K is called a scalar.\\nAgain, we usually omit the sign of the scalar multiplication, i.e., we usually write\\nλv instead of λ · v. If it is clear from the context (or not important) which field we are using, we often omit the explicit reference to K and simply write vector space instead of K -vector space.\\nExample 9.2\\n(1) The set K n,m with the matrix addition and the scalar multiplication forms a\\nK -vector space.\\nFor obvious reasons, the elements of K n,1 and K 1,m are sometimes called column and row vectors, respectively.\\n(2) The set K [t] forms a K -vector space, if the addition is defined as in Example 3.17 (usual addition of polynomials) and the scalar multiplication for p = α0 + α1 t + . . . + αn t n ∈ K [t] is defined by\\nλ · p := (λ α0 ) + (λ α1 )t + . . . + (λ αn )t n .\\n(3) The continuous and real valued functions defined on a real interval [α, β] with the pointwise addition and scalar multiplication, i.e.,\\n( f + g)(x) := f (x) + g(x) and (λ · f )(x) := λ f (x), form an R-vector space.\\nThis can be shown by using that the addition of two continuous functions as well as the multiplication of a continuous function by a real number yield again a continuous function.\\nSince, by definition, (V, +) is a commutative group, we already know some vector space properties from the theory of groups (cp.\\nChap.\\n3).\\nIn particular, every vector space contains a unique neutral element (with respect to addition) 0V , which is called the null vector.\\nEvery vector v ∈ V has a unique (additive) inverse −v ∈ V with v + (−v) = v − v = 0V .\\nAs usual, we will write v − w instead of v + (−w).\\nLemma 9.3 Let V be a K -vector space.\\nIf 0 K and 0V are the neutral (null) elements of K and V, respectively, then the following assertions hold:\\n(1) 0 K · v = 0V for all v ∈ V.\\n(2) λ · 0V = 0V for all λ ∈ K .\\n(3) −(λ · v) = (−λ) · v = λ · (−v) for all v ∈ V and λ ∈ K .\\n\\n1 This term was introduced in 1845 by Sir William Rowan Hamilton (1805–1865) in the context of his quaternions.\\nIt is motivated by the Latin verb “vehi” (“vehor”, “vectus sum”) which means to ride or drive.\\nAlso the term “scalar” was introduced by Hamilton; see the footnote on the scalar multiplication (4.2).</td>\n",
       "      <td>119.0</td>\n",
       "      <td>119</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.1 Basic Definitions and Properties of Vector Spaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vector space element v v called element λ k called scalar usually omit sign scalar multiplication usually write λv instead λ clear context important field using often omit explicit reference k simply write vector space instead k space example set k n matrix addition scalar multiplication form k space obvious reason element k k sometimes called column row vector respectively set k form k space addition defined example usual addition polynomial scalar multiplication p αn n k defined λ p λ λ λ αn n continuous real valued function defined real interval α β pointwise addition scalar multiplication f g x f x g x λ f x λ f x form r-vector space shown using addition two continuous function well multiplication continuous function real number yield continuous function since definition v commutative group already know vector space property theory group cp chap particular every vector space contains unique neutral element respect addition called null vector every vector v v unique additive inverse v v v v usual write v w instead v lemma let v k space k neutral null element k v respectively following assertion hold k v v λ λ k λ v v λ v v λ k term introduced sir william rowan hamilton context quaternion motivated latin verb vehi vehor vectus sum mean ride drive also term scalar introduced hamilton see footnote scalar multiplication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>9.1 Basic Definitions and Properties of Vector Spaces\\n\\n117\\n\\nProof\\n(1) For all v ∈ V we have 0 K · v = (0 K + 0 K ) · v = 0 K · v + 0 K · v. Adding −(0 K · v) on both sides of this identity gives 0V = 0 K · v.\\n(2) For all λ ∈ K we have λ · 0V = λ · (0V + 0V ) = λ · 0V + λ · 0V .\\nAdding −(λ · 0V ) on both sides of this identity gives 0V = λ · 0V .\\n(3) For all λ ∈ K and v ∈ V we have λ · v + (−λ) · v = (λ − λ) · v = 0 K · v = 0V , as well as λ · v + λ · (−v) = λ · (v − v) = λ · 0V = 0V .\\n\u0006\\n\u0005\\nIn the following we will write 0 instead of 0 K and 0V when it is clear which null element is meant.\\nAs in groups, rings and fields we can identify substructures in vector spaces that are again vector spaces.\\nDefinition 9.4 Let (V, +, ·) be a K -vector space and let U ⊆ V. If (U, +, ·) is a\\nK -vector space, then it is called a subspace of (V, +, ·).\\nA substructure must be closed with respect to the given operations, which here are addition and scalar multiplication.\\nLemma 9.5 (U, +, ·) is a subspace of the K -vector space (V, +, ·) if and only if\\nØ \b= U ⊆ V and the following assertions hold:\\n(1) v + w ∈ U for all v, w ∈ U,\\n(2) λv ∈ U for all λ ∈ K and v ∈ U.\\n\u0006\\n\u0005\\n\\nProof Exercise.\\nExample 9.6\\n\\n(1) Every vector space V has the trivial subspaces U = V and U = {0}.\\n(2) Let A ∈ K n,m and U = L (A, 0) ⊆ K m,1 , i.e., U is the solution set of the homogeneous linear system Ax = 0.\\nWe have 0 ∈ U, so U is not empty.\\nIf v, w ∈ U, then\\nA(v + w) = Av + Aw = 0 + 0 = 0, i.e., v + w ∈ U. Furthermore, for all λ ∈ K ,\\nA(λ v) = λ (Av) = λ 0 = 0, i.e., λv ∈ U. Hence, U is a subspace of K m,1 .\\n(3) For every n ∈ N0 the set K [t]≤n := { p ∈ K [t] | deg( p) ≤ n} is a subspace of\\nK [t].\\nDefinition 9.7 Let V be a K -vector space, n ∈ N, and v1 , . . . , vn ∈ V. A vector of the form n\\n\u0002\\nλi vi ∈ V\\nλ1 v1 + . . . + λn vn = i=1</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.1 Basic Definitions and Properties of Vector Spaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic definition property vector space proof v v k v k k v k v k adding k v side identity give k λ k λ λ λ λ adding λ side identity give λ λ k v v λ v v λ λ v k v well λ v λ λ v v λ following write instead k clear null element meant group ring field identify substructure vector space vector space definition let v k space let u u k space called subspace v substructure must closed respect given operation addition scalar multiplication lemma u subspace k space v ø u v following assertion hold v w u v w u λv u λ k v u proof exercise example every vector space v trivial subspace u v u let k n u l k u solution set homogeneous linear system ax u u empty v w u v w av aw v w u furthermore λ k λ v λ av λ λv u hence u subspace k every n set k p k deg p n subspace k definition let v k space n n vn vector form n λi vi v λn vn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>118\\n\\n9 Vector Spaces is called a linear combination of v1 , . . . , vn with the coefficients λ1 , . . . , λn ∈ K .\\nThe (linear) span of v1 , . . . , vn is the set span{v1 , . . . , vn } := n\\n\u0003\u0002\\n\\n\u0004\\nλi vi | λ1 , . . . , λn ∈ K .\\ni=1\\n\\nLet M be a set and suppose that for every m ∈ M we have a vector vm ∈ V. Let the set of all these vectors, called the system of these vectors, be denoted by {vm }m∈M .\\nThen the (linear) span of the system {vm }m∈M , denoted by span {vm }m∈M , is defined as the set of all vectors v ∈ V that are linear combinations of finitely many vectors of the system.\\nThis definition can be consistently extended to the case n = 0.\\nIn this case v1 , . . . , vn is a list of length zero, or an empty list.\\nIf we define the empty sum of vectors as 0 ∈ V, then we obtain span{v1 , . . . , vn } = span Ø = {0}.\\nIf in the following we consider a list of vectors v1 , . . . , vn or a set of vectors\\n{v1 , . . . , vn }, we usually mean that n ≥ 1.\\nThe case of empty list and the associated zero vector space V = {0} will sometimes be discussed separately.\\nExample 9.8 The vector space K 1,3 = {[α1 , α2 , α3 ] | α1 , α2 , α3 ∈ K } is spanned by the vectors [1, 0, 0], [0, 1, 0], [0, 0, 1].\\nThe set {[α1 , α2 , 0] | α1 , α2 ∈ K } forms a subspace of K 1,3 that is spanned by the vectors [1, 0, 0], [0, 1, 0].\\nLemma 9.9 If V is a vector space and v1 , . . . , vn ∈ V, then span{v1 , . . . , vn } is a subspace of V.\\nProof It is clear that Ø \b= span{v1 , . . . , vn } ⊆ V. Furthermore, span{v1 , . . . , vn } is by definition closed with respect to addition and scalar multiplication, so that (1) and\\n(2) in Lemma 9.5 are satisfied.\\n\u0006\\n\u0005\\n\\n9.2 Bases and Dimension of Vector Spaces\\nWe will now discuss the central theory of bases and dimension of vector spaces, and start with the concept of linear independence.\\nDefinition 9.10 Let V be a K -vector space.\\n(1) The vectors v1 , . . . , vn ∈ V are called linearly independent if the equation n\\n\u0002\\n\\nλi vi = 0 with λ1 , . . . , λn ∈ K i=1\\n\\n\u0005n\\nλi vi = 0 always implies that λ1 = · · · = λn = 0.\\nOtherwise, i.e., when i=1 holds for some scalars λ1 , . . . , λn ∈ K that are not all equal to zero, then the vectors v1 , . . . , vn are called linearly dependent.</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.2 Bases and Dimension of Vector Spaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vector space called linear combination vn coefficient λn k linear span vn set span vn n λi vi λn k let set suppose every vector vm let set vector called system vector denoted vm linear span system vm denoted span vm defined set vector v v linear combination finitely many vector system definition consistently extended case n case vn list length zero empty list define empty sum vector v obtain span vn span ø following consider list vector vn set vector vn usually mean n case empty list associated zero vector space v sometimes discussed separately example vector space k k spanned vector set k form subspace k spanned vector lemma v vector space vn v span vn subspace proof clear ø span vn furthermore span vn definition closed respect addition scalar multiplication lemma satisfied base dimension vector space discus central theory base dimension vector space start concept linear independence definition let v k space vector vn v called linearly independent equation n λi vi λn k λi vi always implies λn otherwise hold scalar λn k equal zero vector vn called linearly dependent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>9.2 Bases and Dimension of Vector Spaces\\n\\n119\\n\\n(2) The empty list is linear independent.\\n(3) If M is a set and for every m ∈ M we have a vector vm ∈ V, the corresponding system {vm }m∈M is called linearly independent when finitely many vectors of the system are always linearly independent in the sense of (1).\\nOtherwise the system is called linearly dependent.\\nThe vectors v1 , . . . , vn are linearly independent if and only if the zero vector can be linearly combined only in the trivial way 0 = 0 · v1 + . . . + 0 · vn .\\nConsequently, if one of these vectors is the zero vector, then v1 , . . . , vn are linearly dependent.\\nA single vector v is linearly independent if and only if v \b= 0.\\nThe following result gives a useful characterization of the linear independence of finitely many (but at least two) given vectors.\\nLemma 9.11 The vectors v1 , . . . , vn , n ≥ 2, are linearly independent if and only if no vector vi , i = 1, . . . , n, can be written as a linear combination of the others.\\nProof We prove the assertion by contraposition.\\nThe vectors v1 , . . . , vn are linearly dependent if and only if n\\n\u0002\\nλi vi = 0 i=1 with at least one scalar λ j \b= 0.\\nEquivalently, vj = − n\\n\u0002\\n(λ−1 j λi ) vi , i=1 i\b = j so that v j is a linear combination of the other vectors.\\n\\n\u0006\\n\u0005\\n\\nUsing the concept of linear independence we can now define the concept of the basis of a vector space.\\nDefinition 9.12 Let V be a vector space.\\n(1) A set {v1 , . . . , vn } ⊆ V is called a basis of V, when v1 , . . . , vn are linearly independent and span{v1 , . . . , vn } = V.\\n(2) The set Ø is the basis of the zero vector space V = {0}.\\n(3) Let M be a set and suppose that for every m ∈ M we have a vector vm ∈ V. The set {vm | m ∈ M} is called a basis of V if the corresponding system {vm }m∈M is linearly independent and span {vm }m∈M = V.\\nIn short, a basis is a linearly independent spanning set of a vector space.\\nExample 9.13\\n(1) Let E i j ∈ K n,m be the matrix with entry 1 in position (i, j) and all other entries 0\\n(cp.\\nSect.\\n5.1).\\nThen the set</td>\n",
       "      <td>122.0</td>\n",
       "      <td>122</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.2 Bases and Dimension of Vector Spaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>base dimension vector space empty list linear independent set every vector vm v corresponding system vm called linearly independent finitely many vector system always linearly independent sense otherwise system called linearly dependent vector vn linearly independent zero vector linearly combined trivial way vn consequently one vector zero vector vn linearly dependent single vector v linearly independent v following result give useful characterization linear independence finitely many least two given vector lemma vector vn n linearly independent vector vi n written linear combination others proof prove assertion contraposition vector vn linearly dependent n λi vi least one scalar λ j equivalently vj n j λi vi j v j linear combination vector using concept linear independence define concept basis vector space definition let v vector space set vn v called basis v vn linearly independent span vn set ø basis zero vector space v let set suppose every vector vm set vm called basis v corresponding system vm linearly independent span vm short basis linearly independent spanning set vector space example let e j k n matrix entry position j entry cp sect set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>120\\n\\n9 Vector Spaces\\n\\n{E i j | 1 ≤ i ≤ n and 1 ≤ j ≤ m}\\n\\n(9.1) is a basis of the vector space K n,m (cp.\\n(1) in Example 9.2): The matrices E i j ∈\\nK n,m , 1 ≤ i ≤ n and 1 ≤ j ≤ m, are linearly independent, since\\n0= m n \u0002\\n\u0002\\n\\nλi j E i j = [λi j ] i=1 j=1 implies that λi j = 0 for i = 1, . . . , n and j = 1, . . . , m.\\nFor any A = [ai j ] ∈\\nK n,m we have\\nA= m n \u0002\\n\u0002 ai j E i j , i=1 j=1 and hence span{E i j | 1 ≤ i ≤ n and 1 ≤ j ≤ m} = K n,m .\\nThe basis (9.1) is called the canonical or standard basis of the vector space\\nK n,m .\\nFor m = 1 we denote the canonical basis vectors of K n,1 by\\n⎡ ⎤\\n⎡ ⎤\\n⎡ ⎤\\n1\\n0\\n0\\n⎢0⎥\\n⎢1⎥\\n⎢ .. ⎥\\n⎢ ⎥\\n⎢ ⎥\\n⎢.⎥\\n⎢ ⎥\\n⎢ ⎥\\n⎢ ⎥ e1 := ⎢0⎥ , e2 := ⎢0⎥ , . . . , en := ⎢0⎥ .\\n⎢ .. ⎥\\n⎢ .. ⎥\\n⎢ ⎥\\n⎣.⎦\\n⎣.⎦\\n⎣0⎦\\n0\\n0\\n1\\nThese vectors are also called unit vectors; they are the n columns of the identity matrix In .\\n(2) A basis of the vector space K [t] (cp.\\n(2) in Example 9.2) is given by the set\\n{t m | m ∈ N0 }, since the corresponding system {t m }m∈N0 is linearly independent, and every polynomial p ∈ K [t] is a linear combination of finitely many vectors of the system.\\nThe next result is called the basis extension theorem.\\nTheorem 9.14 Let V be a vector space and let v1 , . . . , vr , w1 , . . . , w\u0002 ∈ V, where r, \u0002 ∈ N0 .\\nIf v1 , . . . , vr are linearly independent and span{v1 , . . . , vr , w1 , . . . , w\u0002 } =\\nV, then the set {v1 , . . . , vr } can be extended to a basis of V using vectors from the set {w1 , . . . , w\u0002 }.\\nProof Note that for r = 0 the list v1 , . . . , vr is empty and hence linearly independent due to (2) in Definition 9.10.\\nWe prove the assertion by induction on \u0002.\\nIf \u0002 = 0, then span{v1 , . . . , vr } = V, and the linear independence of {v1 , . . . , vr } shows that this set is a basis of V.</td>\n",
       "      <td>123.0</td>\n",
       "      <td>123</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.2 Bases and Dimension of Vector Spaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vector space e j n j basis vector space k n cp example matrix e j k n n j linearly independent since n λi j e j λi j implies λi j n j ai j k n n ai j e j hence span e j n j k n basis called canonical standard basis vector space k n denote canonical basis vector k en vector also called unit vector n column identity matrix basis vector space k cp example given set since corresponding system linearly independent every polynomial p k linear combination finitely many vector system next result called basis extension theorem theorem let v vector space let vr v r vr linearly independent span vr v set vr extended basis v using vector set proof note r list vr empty hence linearly independent due definition prove assertion induction span vr v linear independence vr show set basis v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>9.2 Bases and Dimension of Vector Spaces\\n\\n121\\n\\nLet the assertion hold for some \u0002 ≥ 0.\\nSuppose that v1 , . . . , vr , w1 , . . . , w\u0002+1 ∈ V are given, where v1 , . . . , vr are linearly independent and span{v1 , . . . , vr , w1 , . . . , w\u0002+1 } = V. If {v1 , . . . , vr } already is a basis of V, then we are done.\\nSuppose, therefore, that span{v1 , . . . , vr } ⊂ V. Then there exists at least one j, 1 ≤ j ≤ \u0002 + 1,\\n/ span{v1 , . . . , vr }.\\nIn particular, we have w j \b= 0.\\nThen such that w j ∈\\nλw j + r\\n\u0002\\n\\nλi vi = 0 i=1 implies that λ = 0 (otherwise we would have w j ∈ span{v1 , . . . , vr }) and, therefore, λ1 = · · · = λr = 0 due to the linear independence of v1 , . . . , vr .\\nThus, v1 , . . . , vr , w j are linearly independent.\\nBy the induction hypothesis we can extend the set {v1 , . . . , vr , w j } to a basis of V using vectors from the set\\n\u0006\\n\u0005\\n{w1 , . . . , w\u0002+1 } \\ {w j }, which contains \u0002 elements.\\nExample 9.15 Consider the vector space V = K [t]≤3 (cp.\\n(3) in Example 9.6) and the vectors v1 = t, v2 = t 2 , v3 = t 3 .\\nThese vectors are linearly independent, but {v1 , v2 , v3 } is not a basis of V, since span{v1 , v2 , v3 } \b= V. For example, the\\n/ vectors w1 = t 2 + 1 and w2 = t 3 − t 2 − 1 are elements of V, but w1 , w2 ∈ span{v1 , v2 , v3 }.\\nWe have span{v1 , v2 , v3 , w1 , w2 } = V. If we extend {v1 , v2 , v3 } by w1 , then we get the linearly independent vectors v1 , v2 , v3 , w1 which indeed span V.\\nThus, {v1 , v2 , v3 , w1 } is a basis of V.\\nBy the basis extension theorem every vector space that is spanned by finitely many vectors has a basis consisting of finitely many elements.\\nA central result of the theory of vector spaces is that every such basis has the same number of elements.\\nIn order to show this result we first prove the following exchange lemma.\\n\u0005m\\nλi vi ∈\\nLemma 9.16 Let V be a vector space, let v1 , . . . , vm ∈ V and let w = i=1\\nV with λ1 \b= 0.\\nThen span{w, v2 , . . . , vm } = span{v1 , v2 , . . . , vm }.\\nProof By assumption we have v1 = λ−1\\n1 w− m\\n\u0002\\n\\nλ−1\\n1 λi vi .\\ni=2\\n\\nIf y ∈ span{v1 , . . . , vm }, say y =\\n\u000e y = γ1 λ−1\\n1 w− m\\n\u0002 i=1\\n\\nγi vi , then\\n\u000f\\n\\nλ−1\\n1 λi vi i=2 m\\n\u0002 w +\\n= γ1 λ−1\\n1\\n\\n\u0005m\\n\\n+ m\\n\u0002\\n\\nγi vi i=2\\n\\nγi − γ1 λ−1\\n1 λi vi ∈ span{w, v2 , . . . , vm }.\\ni=2\\n\\nIf, on the other hand, y = α1 w +\\n\\n\u0005m i=2\\n\\nαi vi ∈ span{w, v2 , . . . , vm }, then</td>\n",
       "      <td>124.0</td>\n",
       "      <td>124</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.2 Bases and Dimension of Vector Spaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>base dimension vector space let assertion hold suppose vr v given vr linearly independent span vr vr already basis v done suppose therefore span vr exists least one j j span vr particular w j w j λw j r λi vi implies λ otherwise would w j span vr therefore λr due linear independence vr thus vr w j linearly independent induction hypothesis extend set vr w j basis v using vector set w j contains element example consider vector space v k cp example vector vector linearly independent basis v since span example vector element v span span extend get linearly independent vector indeed span thus basis basis extension theorem every vector space spanned finitely many vector basis consisting finitely many element central result theory vector space every basis number element order show result first prove following exchange lemma λi vi lemma let v vector space let vm v let w v span w vm span vm proof assumption λi vi span vm say γi vi λi vi w γi vi γi λi vi span w vm hand w αi vi span w vm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>122\\n\\n9 Vector Spaces y = α1\\n\\n\u000e m\\n\u0002\\n\\n\u000f\\nλi vi\\n\\n+ i=1 m\\n\u0002\\n\\nαi vi i=2\\n\\n= α1 λ1 v1 + m\\n\u0002\\n\\n(α1 λi + αi ) vi ∈ span{v1 , . . . , vm }, i=2 and thus span{w, v2 , . . . , vm } = span{v1 , v2 , . . . , vm }.\\n\\n\u0006\\n\u0005\\n2\\n\\nUsing this lemma we now prove the exchange theorem.\\n\\nTheorem 9.17 Let W = {w1 , . . . , wn } and U = {u 1 , . . . , u m } be finite subsets of a vector space, and let w1 , . . . , wn be linearly independent.\\nIf W ⊆ span{u 1 , . . . , u m }, then n ≤ m, and n elements of U , if numbered appropriately the elements u 1 , . . . , u n , can be exchanged against n elements of W in such a way that span{w1 , . . . , wn , u n+1 , . . . , u m } = span{u 1 , . . . , u n , u n+1 , . . . , u m }.\\n\u0005m\\nλi u i for some scalars λ1 , . . . , λm that\\nProof By assumption we have w1 = i=1 are not all zero (otherwise w1 = 0, which contradicts the linear independence of w1 , . . . , wn ).\\nAfter an appropriate renumbering we have λ1 \b= 0, and Lemma 9.16 yields span{w1 , u 2 , . . . , u m } = span{u 1 , u 2 , . . . , u m }.\\nSuppose that for some r , 1 ≤ r ≤ n −1, we have exchanged the vectors u 1 , . . . , u r against w1 , . . . , wr so that span{w1 , . . . , wr , u r +1 , . . . , u m } = span{u 1 , . . . , u r , u r +1 , . . . , u m }.\\nIt is then clear that r ≤ m.\\nBy assumption we have wr +1 ∈ span{u 1 , . . . , u m }, and thus wr +1 = r\\n\u0002 i=1\\n\\nλi wi + m\\n\u0002\\n\\nλi u i i=r +1 for some scalars λ1 , . . . , λm .\\nOne of the scalars λr +1 , . . . , λm must be nonzero (otherwise wr +1 ∈ span{w1 , . . . , wr }, which contradicts the linear independence of w1 , . . . , wm ).\\nAfter an appropriate renumbering we have λr +1 \b= 0, and Lemma 9.16 yields span{w1 , . . . , wr +1 , u r +2 , . . . , u m } = span{w1 , . . . , wr , u r +1 , . . . , u m }.\\nIf we continue this construction until r = n − 1, then we obtain\\n2 In the literature, his theorem is sometimes called the Steinitz exchange theorem after Ernst Steinitz\\n\\n(1871–1928).\\nThe result was first proved in 1862 by Hermann Günther Graßmann (1809–1877).</td>\n",
       "      <td>125.0</td>\n",
       "      <td>125</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.2 Bases and Dimension of Vector Spaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vector space λi vi αi vi λi αi vi span vm thus span w vm span vm using lemma prove exchange theorem theorem let w wn u u u finite subset vector space let wn linearly independent w span u u n n element u numbered appropriately element u u n exchanged n element w way span wn u u span u u n u u λi u scalar λm proof assumption zero otherwise contradicts linear independence wn appropriate renumbering lemma yield span u u span u u u suppose r r n exchanged vector u u r wr span wr u r u span u u r u r u clear r assumption wr span u u thus wr r λi wi λi u scalar λm one scalar λr λm must nonzero otherwise wr span wr contradicts linear independence wm appropriate renumbering λr lemma yield span wr u r u span wr u r u continue construction r n obtain literature theorem sometimes called steinitz exchange theorem ernst steinitz result first proved hermann günther graßmann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>9.2 Bases and Dimension of Vector Spaces\\n\\n123 span{w1 , . . . , wn , u n+1 , . . . , u m } = span{u 1 , . . . , u n , u n+1 , . . . u m }, where in particular n ≤ m.\\n\\n\u0006\\n\u0005\\n\\nUsing this fundamental theorem, the following result about the unique number of basis elements is a simple corollary.\\nCorollary 9.18 If a vector space V is spanned by finitely many vectors, then V has a basis consisting of finitely many elements, and any two bases of V have the same number of elements.\\nProof The assertion is clear for V = {0} (cp.\\n(2) in Definition 9.12).\\nLet V = span{v1 , . . . , vm } with v1 \b= 0.\\nBy Theorem 9.14, we can extend span{v1 } using elements of {v2 , . . . , vm } to a basis of V. Thus, V has a basis with finitely many elements.\\nLet U := {u 1 , . . . , u \u0002 } and W := {w1 , . . . , wk } be two such bases.\\nThen\\nW ⊆ V = span{u 1 , . . . , u \u0002 }\\n\\nTheorem 9.18\\n\\n=⇒ k ≤ \u0002,\\n\\nU ⊆ V = span{w1 , . . . , wk }\\n\\nTheorem 9.18\\n\\n\u0002 ≤ k,\\n\\n=⇒ and thus \u0002 = k.\\n\\n\u0006\\n\u0005\\n\\nWe can now define the dimension of a vector space.\\nDefinition 9.19 If there exists a basis of a K -vector space V that consists of finitely many elements, then V is called finite dimensional, and the unique number of basis elements is called the dimension of V. We denote the dimension by dim K (V) or dim(V), if it is clear which field is meant.\\nIf V is not spanned by finitely many vectors, then V is called infinite dimensional, and we write dim K (V) = ∞.\\nNote that the zero vector space V = {0} has the basis Ø and thus it has dimension zero (cp.\\n(2) in Definition 9.12).\\nIf V is a finite dimensional vector space and if v1 , . . . , vm ∈ V with m &gt; dim(V), then the vectors v1 , . . . , vm must be linearly dependent.\\n(If these vectors were linearly independent, then we could extend them via Theorem 9.14 to a basis of V that would contain more than dim(V) elements.)\\nExample 9.20 The set in (9.1) forms a basis of the vector space K n,m .\\nThis basis has n · m elements, and hence dim(K n,m ) = n · m.\\nOn the other hand, the vector space\\nK [t] is not spanned by finitely many vectors (cp.\\n(2) in Example 9.13) and hence it is infinite dimensional.\\nExample 9.21 Let V be the vector space of continuous and real valued functions on the real interval [0, 1] (cp.\\n(3) in Example 9.2).\\nDefine for n = 1, 2, . . . the function f n ∈ V by</td>\n",
       "      <td>126.0</td>\n",
       "      <td>126</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.2 Bases and Dimension of Vector Spaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>base dimension vector space span wn u u span u u n u u particular n using fundamental theorem following result unique number basis element simple corollary corollary vector space v spanned finitely many vector v basis consisting finitely many element two base v number element proof assertion clear v cp definition let v span vm theorem extend span using element vm basis thus v basis finitely many element let u u u w wk two base w v span u u theorem k u v span wk theorem k thus define dimension vector space definition exists basis k space v consists finitely many element v called finite dimensional unique number basis element called dimension denote dimension dim k v dim v clear field meant v spanned finitely many vector v called infinite dimensional write dim k v note zero vector space v basis ø thus dimension zero cp definition v finite dimensional vector space vm v dim v vector vm must linearly dependent vector linearly independent could extend via theorem basis v would contain dim v element example set form basis vector space k n basis n element hence dim k n n hand vector space k spanned finitely many vector cp example hence infinite dimensional example let v vector space continuous real valued function real interval cp example define n function f n v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>124\\n\\n9 Vector Spaces f n (x) =\\n\\n⎧\\n⎪\\n⎪\\n⎪\\n⎪\\n⎪\\n⎪\\n⎪\\n⎨\\n⎪\\n⎪\\n⎪\\n⎪\\n⎪\\n⎪\\n⎪\\n⎩\\n\\n1\\n2\\n\\n\u0014\\n\\n1 j\\n\\n+\\n\\n1 j+1\\n\\n\u0015 x&lt;\\n\\n0,\\n\\n1 n\\n\\n2n(n + 1)x − 2n,\\n−2n(n + 1)x + 2n + 2,\\n\\nEvery linear combination at\\n\\n0, k\\n\u0005\\n\\n&lt; x,\\n\\n1 n+1\\n1\\n2\\n\\n1\\n, n+1\\n\\n1 n\\n\\n≤x≤\\n+\\n\\n1 n+1\\n\\n1\\n2\\n\\n1 n\\n\\n+\\n\\n1 n+1\\n\\n,\\n\\n&lt; x ≤ n1 .\\n\\nλ j f j is a continuous function that has the value λ j j=1\\n\\n.\\nThus, the equation k\\n\u0005\\n\\nλ j f j = 0 ∈ V implies that all λ j must be j=1 zero, so that f 1 , . . . , f k ∈ V are linearly independent for all k ∈ N. Consequently, dim(V) = ∞.\\n\\n9.3 Coordinates and Changes of the Basis\\nWe will now study the linear combinations of basis vectors of a finite dimensional vector space.\\nIn particular, we will study what happens with a linear combination if we change to another basis of the vector space.\\nLemma 9.22 If {v1 , . . . , vn } is a basis of a K -vector space V, then for every v ∈ V there exist uniquely determined scalars λ1 , . . . , λn ∈ K with v = λ1 v1 + . . . + λn vn .\\nThese scalars are called the coordinates of v with respect to the basis {v1 , . . . , vn }.\\n\u0005n\\n\u0005n\\nProof Let v = i=1\\nλi vi = i=1\\nμi vi for some scalars λi , μi ∈ K , i = 1, . . . , n, then n\\n\u0002\\n(λi − μi )vi .\\n0=v−v = i=1</td>\n",
       "      <td>127.0</td>\n",
       "      <td>127</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.3 Coordinates and Changes of the Basis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vector space f n x j x n n x n x every linear combination k x n n x λ j f j continuous function value λ j thus equation k λ j f j v implies λ j must zero f f k v linearly independent k consequently dim v coordinate change basis study linear combination basis vector finite dimensional vector space particular study happens linear combination change another basis vector space lemma vn basis k space v every v v exist uniquely determined scalar λn k v λn vn scalar called coordinate v respect basis vn proof let v λi vi μi vi scalar λi μi k n n λi μi vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>9.3 Coordinates and Changes of the Basis\\n\\n125\\n\\nThe linear independence of v1 , . . . , vn implies that λi = μi for i = 1, . . . , n.\\n\\n\u0006\\n\u0005\\n\\nBy definition, the coordinates of a vector depend on the given basis.\\nIn particular, they depend on the ordering (or numbering) of the basis vectors.\\nBecause of this, some authors distinguish between the basis as “set”, i.e., a collection of elements without a particular ordering, and an “ordered basis”.\\nIn this book we will keep the set notation for a basis {v1 , . . . , vn }, where the indices indicate the ordering of the basis vectors.\\nLet V be a K -vector space, v1 , . . . , vn ∈ V (they need not be linearly independent) and v = λ1 v1 + . . . + λn vn for some coefficients λ1 , . . . , λn ∈ K .\\nLet us write\\n⎤\\nλ1\\n⎢ ⎥\\n(v1 , . . . , vn ) ⎣ ... ⎦ := λ1 v1 + . . . + λn vn .\\n⎡\\n\\n(9.2)\\n\\nλn\\n\\nHere (v1 , . . . , vn ) is an n-tuple over V, i.e.,\\n. . × V\u0019 .\\n(v1 , . . . , vn ) ∈ V n = V\\n\u0016 × .\u0017\u0018 n times\\n\\nFor n = 1 we have V 1 = V. We then skip the parentheses and write v instead of\\n(v) for a 1-tuple.\\nThe notation (9.2) formally defines a “multiplication” as map from\\nV n × K n,1 to V.\\nFor all α ∈ K we have\\n⎤\\n⎡\\nαλ1\\n⎥\\n⎢\\nα · v = (α · λ1 )v1 + . . . + (α · λn )vn = (v1 , . . . , vn ) ⎣ ... ⎦ .\\nαλn\\n\\nIf μ1 , . . . , μn ∈ K and\\n⎤\\nμ1\\n⎢ ⎥ u = μ1 v1 + . . . + μn vn = (v1 , . . . , vn ) ⎣ ... ⎦ ,\\n⎡\\n\\nμn then\\n⎤\\nλ1 + μ1\\n⎥\\n⎢ v + u = (λ1 + μ1 )v1 + . . . + (λn + μn )vn = (v1 , . . . , vn ) ⎣ ... ⎦ .\\n⎡\\n\\nλn + μn</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.3 Coordinates and Changes of the Basis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coordinate change basis linear independence vn implies λi μi definition coordinate vector depend given basis particular depend ordering numbering basis vector author distinguish basis set collection element without particular ordering ordered basis book keep set notation basis vn index indicate ordering basis vector let v k space vn v need linearly independent v λn vn coefficient λn k let u write vn λn vn λn vn n-tuple v vn v n v n time n v skip parenthesis write v instead v notation formally defines multiplication map v n k α k α v α α λn vn vn αλn μn k u μn vn vn μn v u λn μn vn vn λn μn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>126\\n\\n9 Vector Spaces\\n\\nThis shows that if vectors are given by linear combinations, then the operations scalar multiplication and addition correspond to operations with the coefficients of the vectors with respect to the linear combinations.\\nWe can further extend this notation.\\nLet A = [ai j ] ∈ K n,m and let\\n⎡ ⎤ a1 j\\n⎢ .. ⎥ u j = (v1 , . . . , vn ) ⎣ . ⎦ , j = 1, . . . , m.\\nan j\\nThen we write the m linear combinations for u 1 , . . . , u m as the system\\n(u 1 , . . . , u m ) =: (v1 , . . . , vn )A.\\n\\n(9.3)\\n\\nOn both sides of this equation we have elements of V m .\\nThe right-multiplication of an arbitrary n-tuple (v1 , . . . , vn ) ∈ V n with a matrix A ∈ K n,m thus corresponds to forming m linear combinations of the vectors v1 , . . . , vn , with the corresponding coefficients given by the entries of A. Formally, this defines a “multiplication” as a map from V n × K n,m to V m .\\nLemma 9.23 Let V be a K -vector space, let v1 , . . . , vn ∈ V be linearly independent, let A ∈ K n,m , and let (u 1 , . . . , u m ) = (v1 , . . . , vn )A. Then the vectors u 1 , . . . , u m are linearly independent if and only if rank(A) = m.\\n\u0006\\n\u0005\\n\\nProof Exercise.\\nNow consider also a matrix B = [bi j ] ∈ K m,\u0002 .\\nUsing (9.3) we obtain\\n(u 1 , . . . , u m )B = ((v1 , . . . , vn )A)B.\\nLemma 9.24 In the previous notation,\\n((v1 , . . . , vn )A)B = (v1 , . . . , vn )(AB).\\n\\n\u0006\\n\u0005\\n\\nProof Exercise.\\n\\nLet {v1 , . . . , vn } and {w1 , . . . , wn } be bases of V and let v ∈ V. By Lemma 9.22 there exist (unique) coordinates λ1 , . . . , λn and μ1 , . . . , μn , respectively, with\\n⎤\\n⎡ ⎤\\nλ1\\nμ1\\n⎢ .. ⎥\\n⎢ .. ⎥ v = (v1 , . . . , vn ) ⎣ . ⎦ = (w1 , . . . , wn ) ⎣ . ⎦ .\\n⎡\\n\\nλn\\n\\nμn\\n\\nWe will now describe a method for transforming the coordinates λ1 , . . . , λn with respect to the basis {v1 , . . . , vn } into the coordinates μ1 , . . . , μn with respect to the basis {w1 , . . . , wn }, and vice versa.</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.3 Coordinates and Changes of the Basis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vector space show vector given linear combination operation scalar multiplication addition correspond operation coefficient vector respect linear combination extend notation let ai j k n let j u j vn j j write linear combination u u system u u vn side equation element v right-multiplication arbitrary n-tuple vn v n matrix k n thus corresponds forming linear combination vector vn corresponding coefficient given entry formally defines multiplication map v n k n v lemma let v k space let vn v linearly independent let k n let u u vn vector u u linearly independent rank proof exercise consider also matrix b bi j k using obtain u u b vn lemma previous notation vn b vn ab proof exercise let vn wn base v let v lemma exist unique coordinate λn μn respectively v vn wn λn μn describe method transforming coordinate λn respect basis vn coordinate μn respect basis wn vice versa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>9.3 Coordinates and Changes of the Basis\\n\\n127\\n\\nFor every basis vector v j , j = 1, . . . , n, there exist (unique) coordinates pi j , i = 1, . . . , n, such that\\n⎤ p1 j\\n⎢ ⎥ v j = (w1 , . . . , wn ) ⎣ ... ⎦ ,\\n⎡ j = 1, . . . , n.\\npn j\\nDefining P = [ pi j ] ∈ K n,n we can write these n equations for the vectors v j analogous to (9.3) as\\n(v1 , . . . , vn ) = (w1 , . . . , wn ) P.\\n\\n(9.4)\\n\\nIn the same way, for every basis vector w j , j = 1, . . . , n, there exist (unique) coordinates qi j , i = 1, . . . , n, such that\\n⎡ ⎤ q1 j\\n⎢ .. ⎥ w j = (v1 , . . . , vn ) ⎣ . ⎦ , j = 1, . . . , n.\\nqn j\\nIf we set Q = [qi j ] ∈ K n,n , then analogously to (9.4) we get\\n(w1 , . . . , wn ) = (v1 , . . . , vn )Q.\\nThus,\\n(w1 , . . . , wn ) = (v1 , . . . , vn )Q = ((w1 , . . . , wn )P)Q = (w1 , . . . , wn )(P Q), which implies that\\n(w1 , . . . , wn )(In − P Q) = (0, . . . , 0).\\nThis means that the n linear combinations of the basis vectors w1 , . . . , wn , with their corresponding coordinates given by the entries of the n columns of In − P Q, are all equal to the zero vector.\\nSince the basis vectors are linearly independent, all coordinates must be zero, and hence In − P Q = 0 ∈ K n,n , or P Q = In .\\nAnalogously we obtain the equation Q P = In .\\nTherefore the matrix P ∈ K n,n is invertible with\\nP −1 = Q. Furthermore, we have\\n⎤\\n⎡ ⎤\\n⎛ ⎡ ⎤⎞\\nλ1\\nλ1\\nλ1\\n⎢ .. ⎥\\n⎢ .. ⎥\\n⎜ ⎢ .. ⎥⎟ v = (v1 , . . . , vn ) ⎣ . ⎦ = ((w1 , . . . , wn )P) ⎣ . ⎦ = (w1 , . . . , wn ) ⎝ P ⎣ . ⎦⎠ .\\n⎡\\n\\nλn\\n\\nλn\\n\\nλn</td>\n",
       "      <td>130.0</td>\n",
       "      <td>130</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.3 Coordinates and Changes of the Basis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coordinate change basis every basis vector v j j n exist unique coordinate pi j n j v j wn j pn j defining p pi j k n n write n equation vector v j analogous vn wn way every basis vector w j j n exist unique coordinate qi j n j w j vn j qn j set q qi j k n n analogously get wn vn q thus wn vn q wn p q wn p q implies wn p q mean n linear combination basis vector wn corresponding coordinate given entry n column p q equal zero vector since basis vector linearly independent coordinate must zero hence p q k n n p q analogously obtain equation q p therefore matrix p k n n invertible p q furthermore v vn wn p wn p λn λn λn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>128\\n\\n9 Vector Spaces\\n\\nDue to the uniqueness of the coordinates of v with respect to the basis {w1 , . . . , wn } we obtain\\n⎡ ⎤\\n⎡ ⎤\\n⎡ ⎤\\n⎡ ⎤\\nλ1\\nλ1\\nμ1\\nμ1\\n⎢ .. ⎥\\n⎢ .. ⎥\\n⎢ .. ⎥\\n−1 ⎢ .. ⎥\\n=\\nP\\n, or\\n=\\nP\\n⎣.⎦\\n⎣.⎦\\n⎣.⎦\\n⎣ . ⎦.\\nμn\\n\\nλn\\n\\nλn\\n\\nμn\\n\\nHence a multiplication with the matrix P transforms the coordinates of v with respect to the basis {v1 , . . . , vn } into those with respect to the basis {w1 , . . . , wn }; a multiplication with P −1 yields the inverse transformation.\\nTherefore, P and P −1 are called coordinate transformation matrices.\\nWe can summarize the results obtained above as follows.\\nTheorem 9.25 Let {v1 , . . . , vn } and {w1 , . . . , wn } be bases of a K -vector space V.\\nThen the uniquely determined matrix P ∈ K n,n is (9.4) is invertible and yields the coordinate transformation from {v1 , . . . , vn } to {w1 , . . . , wn }: If\\n⎤\\n⎡ ⎤\\nλ1\\nμ1\\n⎢ .. ⎥\\n⎢ .. ⎥ v = (v1 , . . . , vn ) ⎣ . ⎦ = (v1 , . . . , vn ) ⎣ . ⎦ ,\\n⎡\\n\\nλn then\\n\\nμn\\n\\n⎡\\n\\n⎡ ⎤\\n⎤\\nμ1\\nλ1\\n⎢ .. ⎥\\n⎢ .. ⎥\\n⎣ . ⎦ = P ⎣ . ⎦.\\nμn\\n\\nλn\\n\\nExample 9.26 Consider the vector space V = R2 = {(α1 , α2 ) | α1 , α2 ∈ R} with the entrywise addition and scalar multiplication.\\nA basis of V is given by the set\\n{e1 = (1, 0), e2 = (0, 1)}, and we have (α1 , α2 ) = α1 e1 +α2 e2 for all (α1 , α2 ) ∈ V.\\nAnother basis of V is the set {v1 = (1, 1), v2 = (1, 2)}.\\nThe corresponding coordinate transformation matrices can be obtained from the defining equations (v1 , v2 ) =\\n(e1 , e2 )P and (e1 , e2 ) = (v1 , v2 )Q as\\n!\\n11\\nP=\\n,\\n12\\n\\nQ=P\\n\\n−1\\n\\n!\\n2 −1\\n=\\n.\\n−1 1\\n\\n9.4 Relations Between Vector Spaces and Their Dimensions\\nOur first result describes the relation between a vector space and a subspace.\\nLemma 9.27 If V is a finite dimensional vector space and U ⊆ V is a subspace, then dim(U) ≤ dim(V) with equality if and only if U = V.</td>\n",
       "      <td>131.0</td>\n",
       "      <td>131</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.4 Relations Between Vector Spaces and Their Dimensions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vector space due uniqueness coordinate v respect basis wn obtain p p μn λn λn μn hence multiplication matrix p transforms coordinate v respect basis vn respect basis wn multiplication p yield inverse transformation therefore p p called coordinate transformation matrix summarize result obtained follows theorem let vn wn base k space uniquely determined matrix p k n n invertible yield coordinate transformation vn wn v vn vn λn μn p μn λn example consider vector space v r entrywise addition scalar multiplication basis v given set another basis v set corresponding coordinate transformation matrix obtained defining equation p q relation vector space dimension first result describes relation vector space subspace lemma v finite dimensional vector space u v subspace dim u dim v equality u v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>9.4 Relations Between Vector Spaces and Their Dimensions\\n\\n129\\n\\nProof Let U ⊆ V and let {u 1 , . . . , u m } be a basis of U, where {u 1 , . . . , u m } = Ø for U = {0}.\\nUsing Theorem 9.14 we can extend this set to a basis of V. If U is a proper subset of V, then at least one basis vector needs to be added and hence dim(U) &lt; dim(V).\\nIf U = V, then every basis of V is also a basis of U, and thus dim(U) = dim(V).\\n\u0006\\n\u0005\\nIf U1 and U2 are subspaces of a vector space V, then their intersection is given by\\nU1 ∩ U2 = {u ∈ V | u ∈ U1 ∧ u ∈ U2 }\\n(cp.\\nDefinition 2.6).\\nThe sum of the two subspaces is defined as\\nU1 + U2 := {u 1 + u 2 ∈ V | u 1 ∈ U1 ∧ u 2 ∈ U2 }.\\nLemma 9.28 If U1 and U2 are subspaces of a vector space V, then the following assertions hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n\\nU1 ∩ U2 and U1 + U2 are subspaces of V.\\nU1 + U1 = U1 .\\nU1 + {0} = U1 .\\nU1 ⊆ U1 + U2 , with equality if and only if U2 ⊆ U1 .\\n\u0006\\n\u0005\\n\\nProof Exercise.\\nAn important result is the following dimension formula for subspaces.\\n\\nTheorem 9.29 If U1 and U2 are finite dimensional subspaces of a vector space V, then dim(U1 ∩ U2 ) + dim(U1 + U2 ) = dim(U1 ) + dim(U2 ).\\nProof Let {v1 , . . . , vr } be a basis of U1 ∩ U2 .\\nWe extend this set to a basis\\n{v1 , . . . , vr , w1 , . . . , w\u0002 } of U1 and to a basis {v1 , . . . , vr , x1 , . . . , xk } of U2 , where we assume that r, \u0002, k ≥ 1.\\n(If one of the lists is empty, then the following argument is easily modified.)\\nIf suffices to show that {v1 , . . . , vr , w1 , . . . , w\u0002 , x1 , . . . , xk } is a basis of U1 + U2 .\\nObviously, span{v1 , . . . , vr , w1 , . . . , w\u0002 , x1 , . . . , xk } = U1 + U2 , and hence it suffices to show that v1 , . . . , vr , w1 , . . . , w\u0002 , x1 , . . . , xk are linearly independent.\\nLet r\\n\u0002 k\\n\u0002\\n\u0002\\n\u0002\\nλi vi +\\nμi wi +\\nγi xi = 0, i=1 i=1 i=1</td>\n",
       "      <td>132.0</td>\n",
       "      <td>132</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.4 Relations Between Vector Spaces and Their Dimensions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relation vector space dimension proof let u v let u u basis u u u ø u using theorem extend set basis u proper subset v least one basis vector need added hence dim u dim v u v every basis v also basis u thus dim u dim v subspace vector space v intersection given u v u u cp definition sum two subspace defined u u v u u lemma subspace vector space v following assertion hold subspace equality proof exercise important result following dimension formula subspace theorem finite dimensional subspace vector space v dim dim dim dim proof let vr basis extend set basis vr basis vr xk assume r k one list empty following argument easily modified suffices show vr xk basis obviously span vr xk hence suffices show vr xk linearly independent let r k λi vi μi wi γi xi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>130\\n\\n9 Vector Spaces then k\\n\u0002\\n\\n\u000e\\nγi xi = − r\\n\u0002 i=1\\n\\nλi vi + i=1\\n\\n\u0002\\n\u0002\\n\\n\u000f\\nμi wi .\\ni=1\\n\\nOn the left hand side of this equation we have,\\n\u0005k by definition, a vector in U2 ; on the\\nγi xi ∈ U1 ∩ U2 .\\nBy construction, right hand side a vector in U1 .\\nTherefore, i=1\\nU1 ∩U2 and the vectors v1 , . . . , vr , w1 , . . . , w\u0002 are however, {v1 , . . . , vr } is a basis of\u0005\\n\u0002\\nμi wi = 0 implies that μ1 = · · · = μ\u0002 = 0.\\nlinearly independent.\\nTherefore, i=1\\nBut then also r\\n\u0002\\n\\nλi vi + i=1 k\\n\u0002\\n\\nγi xi = 0, i=1 and hence λ1 = · · · = λr = γ1 = · · · = γk = 0 due to the linear independence of\\n\u0006\\n\u0005 v1 , . . . , vr , x1 , . . . , xk .\\nIf at least one of the subspaces in Theorem 9.29 is infinite dimensional, then the assertion is still formally correct, since in this case dim(U1 + U2 ) = ∞ and dim(U1 ) + dim(U2 ) = ∞.\\nExample 9.30 For the subspaces\\nU1 = {[α1 , α2 , 0] | α1 , α2 ∈ K }, U2 = {[0, α2 , α3 ] | α2 , α3 ∈ K } ⊂ K 1,3 we have dim(U1 ) = dim(U2 ) = 2,\\nU1 ∩ U2 = {[0, α2 , 0] | α2 ∈ K }, dim(U1 ∩ U2 ) = 1,\\nU1 + U2 = K 1,3 , dim(U1 + U2 ) = 3.\\n\\nThe above definition of the sum can be extended to an arbitrary (but finite) number of subspaces: If U1 , . . . , Uk , k ≥ 2, are subspaces of the vector space V, then we define\\nU1 + . . . + Uk = k\\n\u0002\\n\\nU j := j=1 k\\n\u0003\u0002\\n\\n\u0004 u j | u j ∈ U j , j = 1, . . . , k .\\nj=1\\n\\nThis sum is called direct, if\\nUi ∩ k\\n\u0002\\n\\nU j = {0} for i = 1, . . . , k, j=1 j\b =i and in this case we write the (direct) sum as</td>\n",
       "      <td>133.0</td>\n",
       "      <td>133</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.4 Relations Between Vector Spaces and Their Dimensions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vector space k γi xi r λi vi μi wi left hand side equation definition vector γi xi construction right hand side vector therefore vector vr however vr basis μi wi implies linearly independent therefore also r λi vi k γi xi hence λr γk due linear independence vr xk least one subspace theorem infinite dimensional assertion still formally correct since case dim dim dim example subspace k k k dim dim k dim k dim definition sum extended arbitrary finite number subspace uk k subspace vector space v define uk k u j k u j u j u j j k sum called direct ui k u j k case write direct sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>9.4 Relations Between Vector Spaces and Their Dimensions\\n\\nU1 ⊕ . . . ⊕ Uk = k\\n\"\\n\\n131\\n\\nUj.\\nj=1\\n\\nIn particular, a sum U1 + U2 of two subspaces U1 , U2 ⊆ V is direct if U1 ∩ U2 = {0}.\\nThe following theorem presents two equivalent characterizations of the direct sum of subspaces.\\nTheorem 9.31 If U = U1 + . . . + Uk is a sum of k ≥ 2 subspaces of a vector space\\nV, then the following assertions are equivalent:\\n\u0005\\n(1) The sum U is direct, i.e., Ui ∩ j\b=i U j = {0} for i = 1, . . . , k.\\n\u0005\\n(2) Every vector u ∈ U has a representation of the form u = kj=1 u j with uniquely determined u j ∈ U j for j = 1, . . . , k.\\n\u0005k\\n(3) j=1 u j = 0 with u j ∈ U j for j = 1, . . . , k implies that u j = 0 for j =\\n1, . . . , k.\\nProof\\n\\n\u0005\\n\u0005\\n(1) ⇒ (2): Let u = kj=1 u j = kj=1 # u j with u j , # u j ∈ U j , j = 1, . . . , k.\\nFor every i = 1, . . . , k we then have ui = − ui − #\\n\\n\u0002\\n\\n(u j − # u j ) ∈ Ui ∩ j\b=i\\n\\n\u0002\\n\\nUj.\\nj\b=i\\n\\n\u0005 u i = 0, and hence u i = # u i for\\nNow Ui ∩ j\b=i U j = {0} implies that u i − # i = 1, . . . , k.\\n(2) ⇒ (3): This is obvious.\\n\u0005\\n\u0005\\n∩ j\b=i U j .\\nThen u =\\n(3) ⇒ (1): For a given i, let u ∈ Ui\u0005 j\b=i u j for some\\n−u\\n+ u\\n=\\n0.\\nIn particular, this implies that u j ∈ U j , j \b= i, and hence j\b=i j\\n\u0005\\n\u0006\\n\u0005 u = 0, and thus Ui ∩ j\b=i U j = {0}.\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n9.1.\\nWhich of the following sets (with the usual addition and scalar multiplication) are R-vector spaces?\\n\u0003\\n\\n\u0004 \u0003\\n\u0004\\n[α1 , α2 ] ∈ R1,2 | α1 = α2 , [α1 , α2 ] ∈ R1,2 | α12 + α22 = 1 ,\\n\u0004 \u0003\\n\u0004\\n\u0003\\n[α1 , α2 ] ∈ R1,2 | α1 ≥ α2 , [α1 , α2 ] ∈ R1,2 | α1 − α2 = 0 and 2α1 + α2 = 0 .\\n\\nDetermine, if possible, a basis and the dimension.\\n9.2.\\nDetermine a basis of the R-vector space C and dimR (C).\\nDetermine a basis of the C-vector space C and dimC (C).\\n9.3.\\nShow that a1 , . . . , an ∈ K n,1 are linearly independent if and only if det([a1 , …, an ]) \b= 0.</td>\n",
       "      <td>134.0</td>\n",
       "      <td>134</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.4 Relations Between Vector Spaces and Their Dimensions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relation vector space dimension uk k uj particular sum two subspace v direct following theorem present two equivalent characterization direct sum subspace theorem u uk sum k subspace vector space v following assertion equivalent sum u direct ui u j every vector u u representation form u u j uniquely determined u j u j j u j u j u j j k implies u j j proof let u u j u j u j u j u j j every k ui ui u j u j ui uj u hence u u ui u j implies u obvious u j u given let u u j u particular implies u j u j j hence j u thus ui u j exercise following exercise k arbitrary field following set usual addition scalar multiplication r-vector space determine possible basis dimension determine basis r-vector space c dimr c determine basis c-vector space c dimc c show k linearly independent det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>132\\n\\n9 Vector Spaces\\n\\n9.4.\\nLet V be a K -vector space, \u0003 a nonempty set and Map(\u0003, V) the set of maps from \u0003 to V. Show that Map(\u0003, V) with the operations\\n+ : Map(\u0003, V) × Map(\u0003, V) → Map(\u0003, V), ( f, g) \u0003→ f + g, with ( f + g)(x) := f (x) + g(x) for all x ∈ \u0003,\\n· : K × Map(\u0003, V) → Map(\u0003, V), (λ, f ) \u0003→ λ · f, with(λ · f )(x) := λ f (x) for all x ∈ \u0003, is a K -vector space.\\n9.5.\\nShow that the functions sin and cos in Map(R, R) are linearly independent.\\n9.6.\\nLet V be a vector space with n = dim(V) ∈ N and let v1 , . . . , vn ∈ V. Show that the following statements are equivalent:\\n(1) v1 , . . . , vn are linearly independent.\\n(2) span{v1 , . . . , vn } = V.\\n(3) {v1 , . . . , vn } is a basis of V.\\n9.7.\\nShow that (K n,m , +, ·) is a K -vector space (cp.\\n(1) in Example 9.2).\\nFind a subspace of this K -vector space.\\n9.8.\\nShow that (K [t], +, ·) is a K -vector space (cp.\\n(2) in Example 9.2).\\nShow further that K [t]≤n is a subspace of K [t] (cp.\\n(3) in Example 9.6) and determine dim(K [t]≤n ).\\n9.9.\\nShow that the polynomials p1 = t 5 + t 4 , p2 = t 5 − 7t 3 , p3 = t 5 − 1, p4 = t 5 + 3t are linearly independent in Q[t]≤5 and extend { p1 , p2 , p3 , p4 } to a basis of Q[t]≤5 .\\n9.10.\\nLet n ∈ N and n\\n\u0003\u0002\\n\u0004 j $\\nαi j t1i t2 $ αi j ∈ K .\\nK [t1 , t2 ] := i, j=0\\n\\n9.11.\\n9.12.\\n9.13.\\n9.14.\\n9.15.\\n\\nAn element of K [t1 , t2 ] is called bivariate polynomial over K in the unknowns t1 and t2 .\\nDefine a scalar multiplication and an addition so that K [t1 , t2 ] becomes a vector space.\\nDetermine a basis of K [t1 , t2 ].\\nShow Lemma 9.5.\\nLet A ∈ K n,m and b ∈ K n,1 .\\nIs the solution set L (A, b) of Ax = b a subspace of K m,1 ?\\nLet A ∈ K n,n and let λ ∈ K be an eigenvalue of A. Show that the set {v ∈\\nK n,1 | Av = λv} is a subspace of K n,1 .\\nLet A ∈ K n,n and let λ1 \b= λ2 be two eigenvalues of A. Show that any two associated eigenvectors v1 and v2 are linearly independent.\\nShow that B = {B1 , B2 , B3 , B4 } and C = {C1 , C2 , C3 , C4 } with\\nB1 =\\n\\n!\\n!\\n!\\n!\\n11\\n10\\n10\\n11\\n, B2 =\\n, B3 =\\n, B4 =\\n00\\n00\\n10\\n01</td>\n",
       "      <td>135.0</td>\n",
       "      <td>135</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.4 Relations Between Vector Spaces and Their Dimensions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vector space let v k space nonempty set map v set map show map v operation map v map v map v f g f g f g x f x g x x k map v map v λ f λ f λ f x λ f x x k space show function sin co map r r linearly independent let v vector space n dim v n let vn show following statement equivalent vn linearly independent span vn vn basis show k n k space cp example find subspace k space show k k space cp example show k subspace k cp example determine dim k show polynomial linearly independent q extend basis q let n n n j αi j αi j k k element k called bivariate polynomial k unknown define scalar multiplication addition k becomes vector space determine basis k show lemma let k n b k solution set l b ax b subspace k let k n n let λ k eigenvalue show set v k av λv subspace k let k n n let two eigenvalue show two associated eigenvectors linearly independent show b c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>9.4 Relations Between Vector Spaces and Their Dimensions\\n\\n133 and\\nC1 =\\n\\n!\\n!\\n!\\n!\\n10\\n10\\n10\\n01\\n, C2 =\\n, C3 =\\n, C4 =\\n01\\n10\\n00\\n10 are bases of the vector space K 2,2 , and determine corresponding coordinate transformation matrices.\\n9.16.\\nExamine the elements of the following sets for linear independence in the vector space K [t]≤3 :\\nU1 = {t, t 2 + 2t, t 2 + 3t + 1, t 3 }, U2 = {1, t, t + t 2 , t 2 + t 3 },\\nU3 = {1, t 2 − t, t 2 + t, t 3 }.\\n\\n9.17.\\n\\n9.18.\\n9.19.\\n9.20.\\n9.21.\\n9.22.\\n9.23.\\n\\n9.24.\\n\\nDetermine the dimensions of the subspaces spanned by the elements of U1 ,\\nU2 , U3 .\\nIs one of these sets a basis of K [t]≤3 ?\\nShow that the set of sequences {(α1 , α2 , α3 , . . .) | αi ∈ Q, i ∈ N} with entrywise addition and scalar multiplication forms an infinite dimensional vector space, and determine a basis system.\\nProve Lemma 9.23.\\nProve Lemma 9.24.\\nProve Lemma 9.28.\\nLet U1 , U2 be finite dimensional subspaces of a vector space V. Show that the sum U1 + U2 is direct if dim(U1 + U2 ) = dim(U1 ) + dim(U2 ).\\nLet U1 , . . . , Uk , k ≥ 3, be finite dimensional subspaces of a vector space V.\\nSuppose that Ui ∩ U j = {0} for all i \b= j.\\nIs the sum U1 + . . . + Uk direct?\\nLet U be a subspace of a finite dimensional vector space V. Show that there\\n# with U ⊕ U\\n# = V. (The subspace U\\n# is called a exists another subspace U complement of U.)\\nDetermine three subspaces U1 , U2 , U3 of V = R3,1 with U2 \b= U3 and V =\\nU1 ⊕ U2 = U1 ⊕ U3 .\\nIs there a subspace U1 of V with a uniquely determined complement?</td>\n",
       "      <td>136.0</td>\n",
       "      <td>136</td>\n",
       "      <td>9 Vector Spaces</td>\n",
       "      <td>9.4 Relations Between Vector Spaces and Their Dimensions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relation vector space dimension base vector space k determine corresponding coordinate transformation matrix examine element following set linear independence vector space k determine dimension subspace spanned element one set basis k show set sequence αi q n entrywise addition scalar multiplication form infinite dimensional vector space determine basis system prove lemma prove lemma prove lemma let finite dimensional subspace vector space show sum direct dim dim dim let uk k finite dimensional subspace vector space suppose ui u j j sum uk direct let u subspace finite dimensional vector space show u u subspace u called exists another subspace u complement u determine three subspace v v subspace v uniquely determined complement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Chapter 10\\n\\nLinear Maps\\n\\nIn this chapter we study maps between vector spaces that are compatible with the two vector space operations, addition and scalar multiplication.\\nThese maps are called linear maps or homomorphisms.\\nWe first investigate their most important properties and then show that in the case of finite dimensional vector spaces every linear map can be represented by a matrix, when bases in the respective spaces have been chosen.\\nIf the bases are chosen in a clever way, then we can read off important properties of a linear map from its matrix representation.\\nThis central idea will arise frequently in later chapters.\\n\\n10.1 Basic Definitions and Properties of Linear Maps\\nWe start our investigations with the definition of linear maps between vector spaces.\\nDefinition 10.1 Let V and W be K -vector spaces.\\nA map f : V → W is called linear, when\\n(1) f (λv) = λ f (v), and\\n(2) f (v + w) = f (v) + f (w), hold for all v, w ∈ V and λ ∈ K .\\nThe set of all these maps is denoted by L(V, W).\\nA linear map f : V → W is also called a linear transformation or (vector space) homomorphism.\\nA bijective linear map is called an isomorphism.\\nIf there exists an isomorphism between V and W, then the spaces V and W are called isomorphic, which we denote by\\nV∼\\n= W.\\nA map f ∈ L(V, V) is called an endomorphism, and a bijective endomorphism is called an automorphism.\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_10\\n\\n135</td>\n",
       "      <td>137.0</td>\n",
       "      <td>&lt;FEFF&gt;</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.1 Basic Definitions and Properties of Linear Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter linear map chapter study map vector space compatible two vector space operation addition scalar multiplication map called linear map homomorphism first investigate important property show case finite dimensional vector space every linear map represented matrix base respective space chosen base chosen clever way read important property linear map matrix representation central idea arise frequently later chapter basic definition property linear map start investigation definition linear map vector space definition let v w k space map f v w called linear f λv λ f v f v w f v f w hold v w v λ k set map denoted l v w linear map f v w also called linear transformation vector space homomorphism bijective linear map called isomorphism exists isomorphism v w space v w called isomorphic denote map f l v v called endomorphism bijective endomorphism called automorphism springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>136\\n\\n10 Linear Maps\\n\\nIt is an easy exercise to show that the conditions (1) and (2) in Definition 10.1 hold if and only if f (λv + μw) = λ f (v) + μ f (w) holds for all λ, μ ∈ K and v, w ∈ V.\\nExample 10.2\\n(1) Every matrix A ∈ K n,m defines a map\\nA : K m,1 → K n,1 , x \u0005→ Ax.\\nThis map is linear, since\\nA(λx) = λAx for all x ∈ K m,1 and λ ∈ K ,\\nA(x + y) = Ax + Ay for all x, y ∈ K m,1\\n(cp.\\nLemmas 4.3 and 4.4).\\n\u0002n aii , is linear (cp.\\n(2) The map trace : K n,n → K , A = [ai j ] \u0005→ trace(A) := i=1\\nExercise 8.8).\\n(3) The map f : Q[t]≤3 → Q[t]≤2 , α3 t 3 + α2 t 2 + α1 t + α0 \u0005→ 2α2 t 2 + 3α1 t + 4α0 , is linear.\\n(Show this as an exercise).\\nThe map g : Q[t]≤3 → Q[t]≤2 , α3 t 3 + α2 t 2 + α1 t + α0 \u0005→ α2 t 2 + α1 t + α02 , is not linear.\\nFor example, if p1 = t + 2 and p2 = t + 1, then g( p1 + p2 ) =\\n2t + 9 \u0007= 2t + 5 = g( p1 ) + g( p2 ).\\nThe set of linear maps between vector spaces forms a vector space itself.\\nLemma 10.3 Let V and W be K -vector spaces.\\nFor f, g ∈ L(V, W) and λ ∈ K define f + g and λ · f by\\n( f + g)(v) := f (v) + g(v),\\n(λ · f )(v) := λ f (v), for all v ∈ V. Then (L(V, W), +, ·) is a K -vector space.\\nProof Cp.\\nExercise 9.4.\\n\\n\b\\n\\nThe next result deals with the existence and uniqueness of linear maps.\\nTheorem 10.4 Let V and W be K -vector spaces, let {v1 , . . . , vm } be a basis of V, and let w1 , . . . , wm ∈ W. Then there exists a unique linear map f ∈ L(V, W) with f (vi ) = wi for i = 1, . . . , m.</td>\n",
       "      <td>138.0</td>\n",
       "      <td>138</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.1 Basic Definitions and Properties of Linear Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map easy exercise show condition definition hold f λv μw λ f v μ f w hold λ μ k v w example every matrix k n defines map k k x ax map linear since λx λax x k λ k x ax ay x k cp lemma aii linear cp map trace k n n k ai j trace exercise map f q q linear show exercise map g q q linear example g g g set linear map vector space form vector space lemma let v w k space f g l v w λ k define f g λ f f g v f v g v λ f v λ f v v l v w k space proof cp exercise next result deal existence uniqueness linear map theorem let v w k space let vm basis v let wm exists unique linear map f l v w f vi wi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>10.1 Basic Definitions and Properties of Linear Maps\\n\\n137\\n\\n(v)\\nProof For every v ∈ V there exist (unique) coordinates λ(v)\\n1 , . . . , λm with v =\\n\u0002m (v) i=1 λi vi (cp.\\nLemma 9.22).\\nWe define the map f : V → W by f (v) := m\\n\u0003\\n\\nλi(v) wi for all v ∈ V.\\ni=1\\n\\nBy definition, f (vi ) = wi for i = 1, . . . , m.\\n\u0002m\\n(λ λi(v) )vi ,\\nWe next show that f is linear.\\nFor every λ ∈ K we have λv = i=1 and hence m m\\n\u0003\\n\u0003\\n(λ λi(v) )wi = λ\\nλi(v) wi = λ f (v).\\nf (λv) = i=1\\n\\nIf u =\\n\\n\u0002m i=1 i=1\\n\\nλi(u) vi ∈ V, then v + u = f (v + u) = m\\n\u0003\\n\\n(λi(v)\\n\\n+ i=1\\n\\nλi(u) )wi\\n\\n=\\n\\n\u0002m\\n\\n(v) i=1 (λi m\\n\u0003\\n\\nλi(v) wi\\n\\n+ λi(u) )vi , and hence\\n+ m\\n\u0003 i=1\\n\\nλi(u) wi = f (v) + f (u).\\ni=1\\n\\nThus, f ∈ L(V, W).\\nSuppose that g ∈ L(V, W) also satisfies g(vi ) = wi for i = 1, . . . , m.\\nThen for\\n\u0002m (v) every v = i=1\\nλi vi we have f (v) = f m\\n\u0004\u0003 i=1 m m m m\\n\u0004\u0003\\n\u0005 \u0003\\n\u0005\\n\u0003\\n\u0003\\nλi(v) vi =\\nλi(v) f (vi ) =\\nλi(v) wi =\\nλi(v) g(vi ) = g\\nλi(v) vi = g(v), i=1 i=1 i=1 i=1 and hence f = g, so that f is indeed uniquely determined.\\n\\n\b\\n\\nTheorem 10.4 shows that the map f ∈ L(V, W) is uniquely determined by the images of f at the given basis vectors of V. Note that the image vectors w1 , . . . , wm ∈\\nW may be linearly dependent, and that W may be infinite dimensional.\\nIn Definition 2.12 we have introduced the image and pre-image of a map.\\nWe next recall these definitions for completeness and introduce the kernel of a linear map.\\nDefinition 10.5 If V and W are K -vector spaces and f ∈ L(V, W), then the kernel and the image of f are defined by ker( f ) := {v ∈ V | f (v) = 0}, im( f ) := { f (v) | v ∈ V}.\\nFor w ∈ W the pre-image of w in the space V is defined by f −1 (w) := f −1 ({w}) = {v ∈ V | f (v) = w}.\\nThe kernel of a linear map is sometimes called the null space (or nullspace) of the map, and some authors use the notation null( f ) instead of ker( f ).</td>\n",
       "      <td>139.0</td>\n",
       "      <td>139</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.1 Basic Definitions and Properties of Linear Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic definition property linear map v proof every v v exist unique coordinate λ v λm v v λi vi cp lemma define map f v w f v λi v wi v definition f vi wi λ λi v vi next show f linear every λ k λv hence λ λi v wi λ λi v wi λ f v f λv u λi u vi v v u f v u λi v λi u wi v λi λi v wi λi u vi hence λi u wi f v f u thus f l v w suppose g l v w also satisfies g vi wi v every v λi vi f v f λi v vi λi v f vi λi v wi λi v g vi g λi v vi g v hence f g f indeed uniquely determined theorem show map f l v w uniquely determined image f given basis vector note image vector wm w may linearly dependent w may infinite dimensional definition introduced image pre-image map next recall definition completeness introduce kernel linear map definition v w k space f l v w kernel image f defined ker f v v f v im f f v v v w w pre-image w space v defined f w f w v v f v w kernel linear map sometimes called null space nullspace map author use notation null f instead ker f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>138\\n\\n10 Linear Maps\\n\\nNote that the pre-image f −1 (w) is a set, and that f −1 here does not mean the inverse map of f (cp.\\nDefinition 2.12).\\nIn particular, we have f −1 (0) = ker( f ), and if w ∈\\n/ im( f ), then f −1 (w) = Ø,\\nExample 10.6 For A ∈ K n,m and the corresponding map A ∈ L(K m,1 , K n,1 ) from\\n(1) in Example 10.2 we have ker(A) = {x ∈ K m,1 | Ax = 0} and im(A) = {Ax | x ∈ K m,1 }.\\nNote that ker(A) = L (A, 0) (cp.\\nDefinition 6.1).\\nLet a j ∈ K n,1 denote the jth column of A, j = 1, . . . , m.\\nFor x = [x1 , . . . , xm ]T ∈ K m,1 we then can write\\nAx = m\\n\u0003 xjaj.\\nj=1\\n\\nClearly, 0 ∈ ker(A).\\nMoreover, we see from the representation of Ax that ker(A) =\\n{0} if and only if the columns of A are linearly independent.\\nThe set im(A) is given by the linear combinations of the columns of A, i.e., im(A) = span{a1 , . . . , am }.\\nLemma 10.7 If V and W are K -vector spaces, then for every f ∈ L(V, W) the following assertions hold: f (0) = 0 and f (−v) = − f (v) for all v ∈ V.\\nIf f is an isomorphism, then f −1 ∈ L(W, V).\\nker( f ) is a subspace of V and im( f ) is a subspace of W.\\nf is surjective if and only if im( f ) = W.\\nf is injective if and only if ker( f ) = {0}.\\nIf f is injective and if v1 , . . . , vm ∈ V are linearly independent, then f (v1 ), . . . , f (vm ) ∈ W are linearly independent.\\n(7) If v1 , . . . , vm ∈ V are linearly dependent, then f (v1 ), . . . , f (vm ) ∈ W are linearly dependent, or, equivalently, if f (v1 ), . . . , f (vm ) ∈ W are linearly independent, then v1 , . . . , vm ∈ V are linearly independent.\\n(8) If w ∈ im( f ) and if u ∈ f −1 (w) is arbitrary, then\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\n(6) f −1 (w) = u + ker( f ) := {u + v | v ∈ ker( f )}.\\nProof\\n(1) We have f (0V ) = f (0 K · 0V ) = 0 K · f (0V ) = 0V as well as f (v) + f (−v) = f (v + (−v)) = f (0) = 0 for all v ∈ V.\\n(2) The existence of the inverse map f −1 : W → V is guaranteed by Theorem 2.20, so we just have to show that f −1 is linear.\\nIf w1 , w2 ∈ W, then there exist uniquely determined v1 , v2 ∈ V with w1 = f (v1 ) and w2 = f (v2 ).\\nHence, f −1 (w1 + w2 ) = f −1 ( f (v1 ) + f (v2 )) = f −1 ( f (v1 + v2 )) = v1 + v2\\n= f −1 (w1 ) + f −1 (w2 ).</td>\n",
       "      <td>140.0</td>\n",
       "      <td>140</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.1 Basic Definitions and Properties of Linear Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map note pre-image f w set f mean inverse map f cp definition particular f ker f w im f f w ø example k n corresponding map l k k example ker x k ax im ax x k note ker l cp definition let j k denote jth column j x xm k write ax xjaj clearly ker moreover see representation ax ker column linearly independent set im given linear combination column im span lemma v w k space every f l v w following assertion hold f f f v v f isomorphism f l w v ker f subspace v im f subspace f surjective im f f injective ker f f injective vm v linearly independent f f vm w linearly independent vm v linearly dependent f f vm w linearly dependent equivalently f f vm w linearly independent vm v linearly independent w im f u f w arbitrary f w u ker f u v v ker f proof f f k k f well f v f f v f v existence inverse map f w v guaranteed theorem show f linear w exist uniquely determined v f f hence f f f f f f f f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>10.1 Basic Definitions and Properties of Linear Maps\\n\\n139\\n\\nMoreover, for every λ ∈ K we have f −1 (λw1 ) = f −1 (λ f (v1 )) = f −1 ( f (λv1 )) = λv1 = λ f −1 (w1 ).\\n(3) and (4) are obvious from the corresponding definitions.\\n(5) Let f be injective and v ∈ ker( f ), i.e., f (v) = 0.\\nFrom (1) we know that f (0) = 0.\\nSince f (v) = f (0), the injectivity of f yields v = 0.\\nSuppose now that ker( f ) = {0} and let u, v ∈ V with f (u) = f (v).\\nThen f (u − v) = 0, i.e., u −\u0002 v ∈ ker( f ), which implies u − v = 0, i.e., u = v.\\nm\\nλi f (vi ) = 0.\\nThe linearity of f yields\\n(6) Let i=1 f m\\n\u0004\u0003 m\\n\u0005\\n\u0003\\nλi vi = 0, i.e.,\\nλi vi ∈ ker( f ).\\ni=1 i=1\\n\\n\u0002m\\nλi vi = 0 by (5), and hence λ1 = · · · =\\nSince f is injective, we have i=1\\nλm = 0 due to the linear independence of v1 , . . . , vm .\\nThus, f (v1 ), . . . , f (vm ) are linearly independent.\\n\u0002m\\nλi vi = 0 for some λ1 , . . . , λm ∈\\n(7) If v1 , . . . , vm are linearly dependent, then i=1\\nK that \u0002 are not all equal to zero.\\nApplying f on both sides and using the linearity m\\nλi f (vi ) = 0, hence f (v1 ), . . . , f (vm ) are linearly dependent.\\nyields i=1\\n(8) Let w ∈ im( f ) and u ∈ f −1 (w).\\nIf v ∈ f −1 (w), then f (v) = f (u), and thus f (v − u) = 0, i.e., v − u ∈ ker( f ) or v ∈ u + ker( f ).\\nThis shows that f −1 (w) ⊆ u + ker( f ).\\nIf, on the other hand, v ∈ u +ker( f ), then f (v) = f (u) = w, i.e., v ∈ f −1 (w).\\n\b\\nThis shows that u + ker( f ) ⊆ f −1 (w).\\nExample 10.8 Consider a matrix A ∈ K n,m and the corresponding map A ∈\\nL(K m,1 , K n,1 ) from (1) in Example 10.2.\\nFor a given b ∈ K n,1 we have A−1 (b) =\\nL (A, b).\\nIf b ∈\\n/ im(A), then L (A, b) = Ø (case (1) in Corollary 6.6).\\nNow suppose that b ∈ im(A) and let \u0006 x ∈ L (A, b) be arbitrary.\\nThen (8) in Lemma 10.7 yields\\nL (A, b) = \u0006 x + ker(A), which is the assertion of Lemma 6.2.\\nIf ker(A) = {0}, i.e., the columns of A are linearly independent, then |L (A, b)| = 1 (case (2) in Corollary 6.6).\\nIf ker(A) \u0007= {0}, i.e., the columns of A are linearly dependent, then |L (A, b)| &gt; 1 (case (3) in\\nCorollary 6.6).\\nIf {w1 , . . . , w\u0002 } is a basis of ker(A), then\\n\u0002\\n\u0007\\n\u0003\\n\b\\nλi wi \b λ1 , . . . , λ\u0002 ∈ K .\\nL (A, b) = \u0006 x+ i=1\\n\\nThus, the solutions of Ax = b depend of \u0002 ≤ m parameters.</td>\n",
       "      <td>141.0</td>\n",
       "      <td>141</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.1 Basic Definitions and Properties of Linear Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic definition property linear map moreover every λ k f f λ f f f λ f obvious corresponding definition let f injective v ker f f v know f since f v f injectivity f yield v suppose ker f let u v v f u f v f u v u v ker f implies u v u λi f vi linearity f yield let f λi vi λi vi ker f λi vi hence since f injective λm due linear independence vm thus f f vm linearly independent λi vi λm vm linearly dependent k equal zero applying f side using linearity λi f vi hence f f vm linearly dependent yield let w im f u f w v f w f v f u thus f v u v u ker f v u ker f show f w u ker f hand v u f f v f u w v f w show u ker f f w example consider matrix k n corresponding map l k k example given b k b l b b im l b ø case corollary suppose b im let x l b arbitrary lemma yield l b x ker assertion lemma ker column linearly independent b case corollary ker column linearly dependent b case corollary basis ker λi wi k l b thus solution ax b depend parameter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>140\\n\\n10 Linear Maps\\n\\nThe following result, which gives an important dimension formula for linear maps, is also known as the rank-nullity theorem: The dimension of the image of f is equal to the rank of a matrix associated with f (cp.\\nTheorem 10.22 below), and the dimension of the kernel (or null space) of f is sometimes called the nullity1 of f .\\nTheorem 10.9 Let V and W be K -vector spaces and let V be finite dimensional.\\nThen for every f ∈ L(V, W) we have the dimension formula dim(V) = dim(im( f )) + dim(ker( f )).\\nProof Let v1 , . . . , vn ∈ V. If f (v1 ), . . . , f (vn ) ∈ W are linearly independent, then by (7) in Lemma 10.7 also v1 , . . . , vn are linearly independent, and thus dim(im( f )) ≤ dim(V).\\nSince ker( f ) ⊆ V, we have dim(ker( f )) ≤ dim(V), so that im( f ) and ker( f ) are both finite dimensional.\\nLet {w1 , . . . , wr } and {v1 , . . . , vk } be bases of im( f ) and ker( f ), respectively, and let u 1 ∈ f −1 (w1 ), . . . , u r ∈ f −1 (wr ).\\nWe will show that {u 1 , . . . , u r , v1 , . . . , vk } is a basis of V, which then implies the assertion.\\nIf v ∈ V, then\u0002by Lemma 9.22 there\u0002exist (unique) coordinates μ1 , . . . , μr ∈\\nK with f (v) = ri=1 μi wi .\\nLet v := ri=1 μi u i , then f (v) = f (v), and hence\\n\u0002k v − v ∈ ker( f ), which gives v − v = i=1\\nλi vi for some (unique) coordinates\\nλ1 , . . . , λk ∈ K .\\nTherefore, v=v + k\\n\u0003 r\\n\u0003\\n\\nλi vi = i=1\\n\\nμi u i + i=1 k\\n\u0003\\n\\nλi vi , i=1 and thus v ∈ span{u 1 , . . . , u r , v1 , . . . , vk }.\\nSince {u 1 , . . . , u r , v1 , . . . , vk } ⊂ V, we have\\nV = span{u 1 , . . . , u r , v1 , . . . , vk }, and it remains to show that u 1 , . . . , u r , v1 , . . . , vk are linearly independent.\\nIf r\\n\u0003\\n\\nαi u i + k\\n\u0003 i=1\\n\\nβi vi = 0, i=1 then\\n0 = f (0) = f r\\n\u0003 i=1\\n\\nαi u i + k\\n\u0003 i=1\\n\\nβi vi\\n\\n= r\\n\u0003\\n\\nαi f (u i ) = i=1 r\\n\u0003\\n\\nαi wi i=1 and thus α1 = · · · = αr = 0, because w1 , . . . , wr are linearly independent.\\nFinally,\\n\b the linear independence of v1 , . . . , vk implies that β1 = · · · = βk = 0.\\n1 This term was introduced in 1884 by James Joseph Sylvester (1814–1897).</td>\n",
       "      <td>142.0</td>\n",
       "      <td>142</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.1 Basic Definitions and Properties of Linear Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map following result give important dimension formula linear map also known rank-nullity theorem dimension image f equal rank matrix associated f cp theorem dimension kernel null space f sometimes called f theorem let v w k space let v finite dimensional every f l v w dimension formula dim v dim im f dim ker f proof let vn f f vn w linearly independent lemma also vn linearly independent thus dim im f dim v since ker f v dim ker f dim v im f ker f finite dimensional let wr vk base im f ker f respectively let u f u r f wr show u u r vk basis v implies assertion v v lemma unique coordinate μr k f v μi wi let v μi u f v f v hence v v ker f give v v λi vi unique coordinate λk k therefore k r λi vi μi u k λi vi thus v span u u r vk since u u r vk v v span u u r vk remains show u u r vk linearly independent r αi u k βi vi f f r αi u k βi vi r αi f u r αi wi thus αr wr linearly independent finally linear independence vk implies βk term introduced james joseph sylvester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>10.1 Basic Definitions and Properties of Linear Maps\\n\\n141\\n\\nExample 10.10\\n(1) For the linear map\\n⎤\\n⎡ ⎤\\n\u0012\\n\u0012 α1\\n\u0011\\n\u0011\\nα1\\n1\\n0\\n1\\nα1 + α3\\n2,1\\n⎦\\n⎦\\n⎣\\n⎣\\nα2 \u0005→\\nα2 =\\n,\\n→Q ,\\nα1 + α3\\n101\\nα3\\nα3\\n⎡ f : Q3,1 we have\\n\\n⎧⎡\\n⎫\\n⎤\b\\n\u0014\\n\u0013\u0011 \u0012 \b\\nα1 \b\b\\n⎨\\n⎬\\nα \b\b\\n⎣ α2 ⎦ \b α1 , α2 ∈ Q .\\nα\\n∈\\nQ\\n, ker( f\\n)\\n= im( f ) =\\n\b\\nα \b\\n⎩\\n⎭\\n−α1 \b\\nHence dim(im( f )) = 1 and dim(ker( f )) = 2, so that indeed dim(im( f )) + dim(ker( f )) = dim(Q3,1 ).\\n(2) If A ∈ K n,m and A ∈ L(K m,1 , K n,1 ) are as in (1) in Example 10.2, then m = dim(K m,1 ) = dim(ker(A)) + dim(im(A)).\\nThus, dim(im(A)) = m if and only if dim(ker(A)) = 0.\\nThis holds if and only if ker(A) = {0}, i.e., if and only if the columns of A are linearly independent (cp.\\nExample 10.6).\\nIf, on the other hand, dim(im(A)) &lt; m, then dim(ker(A)) = m − dim(im(A)) &gt; 0, and thus ker(A) \u0007= {0}.\\nIn this case the columns of A are linearly dependent, since there exists an x ∈ K m,1 \\ {0} with Ax = 0.\\nCorollary 10.11 If V and W are K -vector spaces with dim(V) = dim(W) ∈ N and if f ∈ L(V, W), then the following statements are equivalent:\\n(1) f is injective.\\n(2) f is surjective.\\n(3) f is bijective.\\nProof If (3) holds, then (1) and (2) hold by definition.\\nWe now show that (3) is implied by (1) as well as by (2).\\nIf f is injective, then ker( f ) = {0} (cp.\\n(5) in Lemma 10.7) and the dimension formula of Theorem 10.9 yields dim(W) = dim(V) = dim(im( f )).\\nThus, im( f ) =\\nW (cp.\\nLemma 9.27), so that f is also surjective.\\nIf f is surjective, i.e., im( f ) = W, then the dimension formula and dim(W) = dim(V) yield dim(ker( f )) = dim(V) − dim(im( f )) = dim(W) − dim(im( f )) = 0.\\nThus, ker( f ) = {0}, so that f is also injective.\\n\\n\b\\n\\nUsing Theorem 10.9 we can also characterize when two finite dimensional vector spaces are isomorphic.</td>\n",
       "      <td>143.0</td>\n",
       "      <td>143</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.1 Basic Definitions and Properties of Linear Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic definition property linear map example linear map f α q α q ker f im f α hence dim im f dim ker f indeed dim im f dim ker f dim k n l k k example dim k dim ker dim im thus dim im dim ker hold ker column linearly independent cp example hand dim im dim ker dim im thus ker case column linearly dependent since exists x k ax corollary v w k space dim v dim w n f l v w following statement equivalent f injective f surjective f bijective proof hold hold definition show implied well f injective ker f cp lemma dimension formula theorem yield dim w dim v dim im f thus im f w cp lemma f also surjective f surjective im f w dimension formula dim w dim v yield dim ker f dim v dim im f dim w dim im f thus ker f f also injective using theorem also characterize two finite dimensional vector space isomorphic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>142\\n\\n10 Linear Maps\\n\\nCorollary 10.12 Two finite dimensional K -vector spaces V and W are isomorphic if and only if dim(V) = dim(W).\\nProof If V ∼\\n= W, then there exists a bijective map f ∈ L(V, W).\\nBy (4) and (5) in\\nLemma 10.7 we have im( f ) = W and ker( f ) = {0}, and the dimension formula of\\nTheorem 10.9 yields dim(V) = dim(im( f )) + dim(ker( f )) = dim(W) + dim({0}) = dim(W).\\nLet now dim(V) = dim(W).\\nWe need to show that there exists a bijective f ∈\\nL(V, W).\\nLet {v1 , . . . , vn } and {w1 , . . . , wn } be bases of V and W. By Theorem 10.4 there exists a unique f ∈ L(V, W) with f (vi ) = wi , i = 1, . . . , n.\\nIf v = λ1 v1 +\\n. . . + λn vn ∈ ker( f ), then\\n0 = f (v) = f (λ1 v1 + . . . + λn vn ) = λ1 f (v1 ) + . . . + λn f (vn )\\n= λ 1 w1 + . . . + λn wn .\\nSince w1 , . . . , wn are linearly independent, we have λ1 = · · · = λn = 0, hence v = 0 and ker( f ) = {0}.\\nThus, f is injective.\\nMoreover, the dimension formula yields dim(V) = dim(im( f )) = dim(W) and, therefore, im( f ) = W (cp.\\nLemma 9.27), so that f is also surjective.\\n\b\\nExample 10.13\\n(1) The vector spaces K n,m and K m,n both have the dimension n·m and are therefore isomorphic.\\nAn isomorphism is given by the linear map A \u0005→ A T .\\n(2) The R-vector spaces R1,2 and C = {x + iy | x, y ∈ R} both have the dimension 2 and are therefore isomorphic.\\nAn isomorphism is given by the linear map\\n[x, y] \u0005→ x + iy.\\n(3) The vector spaces Q[t]≤2 and Q1,3 both have dimension 3 and are therefore isomorphic.\\nAn isomorphism is given by the linear map α2 t 2 + α1 t + α0 \u0005→\\n[α2 , α1 , α0 ].\\nAlthough Mathematics is a formal and exact science, where smallest details matter, one sometimes uses an “abuse of notation” in order to simplify the presentation.\\nWe have used this for example in the inductive existence proof of the echelon form in Theorem 5.2.\\nThere we kept, for simplicity, the indices of the larger matrix A(1) in\\n(2) the smaller matrix A(2) = [ai(2) had, of course, an entry in position j ].\\nThe matrix A\\n(2)\\n(2)\\n.\\nKeeping the indices in the\\n(1, 1), but this entry was denoted by a22 rather than a11 induction made the argument much less technical, while the proof itself remained formally correct.\\nAn abuse of notation should always be justified and should not be confused with a “misuse” of notation.\\nIn the field of Linear Algebra a justification is often given by an isomorphism that identifies vector spaces with each other.\\nFor example, the constant polynomials over a field K , i.e., polynomials of the form αt 0 with α ∈ K , are often written simply as α, i.e., as elements of the field itself.\\nThis is justified since</td>\n",
       "      <td>144.0</td>\n",
       "      <td>144</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.1 Basic Definitions and Properties of Linear Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map corollary two finite dimensional k space v w isomorphic dim v dim w proof v w exists bijective map f l v w lemma im f w ker f dimension formula theorem yield dim v dim im f dim ker f dim w dim dim w let dim v dim w need show exists bijective f l v w let vn wn base v theorem exists unique f l v w f vi wi v λn vn ker f f v f λn vn f λn f vn λ λn wn since wn linearly independent λn hence v ker f thus f injective moreover dimension formula yield dim v dim im f dim w therefore im f w cp lemma f also surjective example vector space k n k n dimension therefore isomorphic isomorphism given linear map r-vector space c x iy x r dimension therefore isomorphic isomorphism given linear map x x iy vector space q dimension therefore isomorphic isomorphism given linear map although mathematics formal exact science smallest detail matter one sometimes us abuse notation order simplify presentation used example inductive existence proof echelon form theorem kept simplicity index larger matrix smaller matrix ai course entry position j matrix keeping index entry denoted rather induction made argument much le technical proof remained formally correct abuse notation always justified confused misuse notation field linear algebra justification often given isomorphism identifies vector space example constant polynomial field k polynomial form αt α k often written simply α element field justified since</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>10.1 Basic Definitions and Properties of Linear Maps\\n\\n143\\n\\nK [t]≤0 and K are isomorphic K -vector spaces (of dimension 1).\\nWe already used this identification above.\\nSimilarly, we have identified the vector space V with V 1 and written just v instead of (v) in Sect.\\n9.3.\\nAnother common example in the literature is the notation K n that in our text denotes the set of n-tuples with elements from\\nK , but which is often used for the (matrix) sets of the “column vectors” K n,1 or the\\n“row vectors” K 1,n .\\nThe actual meaning then should be clear from the context.\\nAn attentive reader can significantly benefit from the simplifications due to such abuses of notation.\\n\\n10.2 Linear Maps and Matrices\\nLet V and W be finite dimensional K -vector spaces with bases {v1 , . . . , vm } and\\n{w1 , . . . , wn }, respectively, and let f ∈ L(V, W).\\nBy Lemma 9.22, for every f (v j ) ∈\\nW, j = 1, . . . , m, there exist (unique) coordinates ai j ∈ K , i = 1, . . . , n, with f (v j ) = a1 j w1 + . . . + an j wn .\\nWe define A := [ai j ] ∈ K n,m and write, similarly to (9.3), the m equations for the vectors f (v j ) as\\n(10.1)\\n( f (v1 ), . . . , f (vm )) = (w1 , . . . , wn )A.\\nThe matrix A is determined uniquely by f and the given bases of V and W.\\nIf v = λ1 v1 + . . . + λm vm ∈ V, then f (v) = f (λ1 v1 + . . . + λm vm ) = λ1 f (v1 ) + . . . + λm f (vm )\\n⎡ ⎤\\nλ1\\n= ( f (v1 ), . . . , f (vm )) ⎣ ... ⎦\\nλm\\n⎤\\n\\n⎡\\n\\nλ1\\n= ((w1 , . . . , wn ) A) ⎣ ... ⎦\\nλm\\n⎛ ⎡ ⎤⎞\\nλ1\\n= (w1 , . . . , wn ) ⎝ A ⎣ ... ⎦⎠ .\\nλm\\n\\nThe coordinates of f (v) with respect to the given basis of W are therefore given by\\n⎤\\nλ1\\n.\\nA ⎣ .. ⎦ .\\nλm\\n⎡</td>\n",
       "      <td>145.0</td>\n",
       "      <td>145</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.2 Linear Maps and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic definition property linear map k k isomorphic k space dimension already used identification similarly identified vector space v v written v instead v sect another common example literature notation k n text denotes set n-tuples element k often used matrix set column vector k row vector k n actual meaning clear context attentive reader significantly benefit simplification due abuse notation linear map matrix let v w finite dimensional k space base vm wn respectively let f l v w lemma every f v j w j exist unique coordinate ai j k n f v j j j wn define ai j k n write similarly equation vector f v j f f vm wn matrix determined uniquely f given base v v λm vm v f v f λm vm f λm f vm f f vm λm wn λm wn λm coordinate f v respect given basis w therefore given λm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>144\\n\\n10 Linear Maps\\n\\nThus, we can compute the coordinates of f (v) simply by multiplying the coordinates of v with A. This motivates the following definition.\\nDefinition 10.14 The uniquely determined matrix in (10.1) is called the matrix representation of f ∈ L(V, W) with respect to the bases B1 = {v1 , . . . , vm } of V and\\nB2 = {w1 , . . . , wn } of W. We denote this matrix by [ f ] B1 ,B2 .\\nThe construction of the matrix representation and Definition 10.14 can be consistently extended to the case that (at least) one of the K -vector spaces has dimension zero.\\nIf, for instance, m = dim(V) ∈ N and W = {0}, then f (v j ) = 0 for every basis vector v j of V. Thus, every vector f (v j ) is an empty linear combination of vector of the basis Ø of W. The matrix representation of f then is an empty matrix of size 0 × m.\\nIf also V = {0}, then the matrix representation of f is an empty matrix of size 0 × 0.\\nThere are many different notations for the matrix representation of linear maps in the literature.\\nThe notation should reflect that the matrix depends on the linear map f and the given bases B1 and B2 .\\nExamples of alternative notations are [ f ] BB12 and\\nM( f ) B1 ,B2 (where “M” means “matrix”).\\nAn important special case is obtained for V = W, hence in particular m = n, and f = IdV , the identity on V. We then obtain\\n(v1 , . . . , vn ) = (w1 , . . . , wn )[IdV ] B1 ,B2 ,\\n\\n(10.2) so that [IdV ] B1 ,B2 is exactly the matrix P in (9.4), i.e., the coordinate transformation matrix in Theorem 9.25.\\nOn the other hand,\\n(w1 , . . . , wn ) = (v1 , . . . , vn ) [IdV ] B2 ,B1 , and thus\\n\u001f\\n[IdV ] B1 ,B2\\n\\n−1\\n\\n= [IdV ] B2 ,B1 .\\n\\nExample 10.15\\n(1) Consider the vector space Q[t]≤1 with the bases B1 = {1, t} and B2 = {t +\\n1, t − 1}.\\nThen the linear map f : Q[t]≤1 → Q[t]≤1 , α1 t + α0 \u0005→ 2α1 t + α0 , has the matrix representations\\n!\\n!\\n\"\\n\u0012\\n1\\n3\\n1\\n10\\n2\\n2\\n, [ f ] B1 ,B2 =\\n=\\n=\\n,\\n[ f\\n]\\nB2 ,B2\\n1\\n02\\n− 21 1\\n2\\n\u0011\\n\\n[ f ] B1 ,B1\\n\\n1\\n2\\n3\\n2\\n\\n\"\\n.\\n\\n(2) For the vector space K [t]≤n with the basis B = {t 0 , t 1 , . . . , t n } and the linear map</td>\n",
       "      <td>146.0</td>\n",
       "      <td>146</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.2 Linear Maps and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map thus compute coordinate f v simply multiplying coordinate v motivates following definition definition uniquely determined matrix called matrix representation f l v w respect base vm v wn denote matrix f construction matrix representation definition consistently extended case least one k space dimension zero instance dim v n w f v j every basis vector v j thus every vector f v j empty linear combination vector basis ø matrix representation f empty matrix size also v matrix representation f empty matrix size many different notation matrix representation linear map literature notation reflect matrix depends linear map f given base example alternative notation f f mean matrix important special case obtained v w hence particular n f idv identity obtain vn wn idv idv exactly matrix p coordinate transformation matrix theorem hand wn vn idv thus idv idv example consider vector space q base linear map f q q matrix representation f f f vector space k basis b n linear map</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>10.2 Linear Maps and Matrices\\n\\n145 f : K [t]≤n → K [t]≤n ,\\nαn t n + αn−1 t n−1 + . . . + α1 t + α0 \u0005→ α0 t n + α1 t n−1 + . . . + αn−1 t + αn , we have f (t j ) = t n− j for j = 0, 1, . . . , n, so that\\n⎡\\n\\n1\\n\\n⎤\\n\\n[ f ] B,B = ⎣ . . . ⎦ ∈ K n+1,n+1 .\\n1\\nThus, [ f ] B,B is a permutation matrix.\\nTheorem 10.16 Let V and W be finite dimensional K -vector spaces with bases\\nB1 = {v1 , . . . , vm } and B2 = {w1 , . . . , wn }, respectively.\\nThen the map\\nL(V, W) → K n,m , f \u0005→ [ f ] B1 ,B2 , is an isomorphism.\\nHence L(V, W) ∼\\n= K n,m and dim(L(V, W)) = dim(K n,m ) = n · m.\\nProof In this proof we denote the map f \u0005→ [ f ] B1 ,B2 by mat, i.e., mat( f ) =\\n[ f ] B1 ,B2 .\\nWe first show that this map is linear.\\nLet f, g ∈ L(V, W), mat( f ) = [ f i j ] and mat(g) = [gi j ].\\nFor j = 1, . . . , m we have\\n( f + g)(v j ) = f (v j ) + g(v j ) = n\\n\u0003 f i j wi + i=1 n\\n\u0003 gi j wi = i=1 n\\n\u0003\\n( f i j + gi j )wi , i=1 and thus mat( f + g) = [ f i j + gi j ] = [ f i j ] + [gi j ] = mat( f ) + mat(g).\\nFor λ ∈ K and j = 1, . . . , m we have\\n(λ f )(v j ) = λ f (v j ) = λ n\\n\u0003 i=1 f i j wi = n\\n\u0003\\n(λ f i j )wi , i=1 and thus mat(λ f ) = [λ f i j ] = λ [ f i j ] = λ mat( f ).\\nIt remains to show that mat is bijective.\\nIf f ∈ ker(mat), i.e., mat( f ) = 0 ∈ K n,m , then f (v j ) = 0 for j = 1, . . . , m.\\nThus, f (v) = 0 for all v ∈ V, so that f = 0\\n(the zero map) and mat is injective (cp.\\n(5) in Lemma 10.7).\\nIf, on the other hand,\\nA = [ai j ] ∈ K n,m is arbitrary, we define the linear map f : V → W via f (v j ) :=\\n\u0002 n i=1 ai j wi , j = 1, . . . , m (cp. the proof of Theorem 10.4).\\nThen mat( f ) = A and hence mat is also surjective (cp.\\n(4) in Lemma 10.7).\\nCorollary 10.12 now shows that dim(L(V, W)) = dim(K n,m ) = n · m (cp. also\\nExample 9.20).\\n\b</td>\n",
       "      <td>147.0</td>\n",
       "      <td>147</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.2 Linear Maps and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map matrix f k k αn n n αn f j j j n f b b k thus f b b permutation matrix theorem let v w finite dimensional k space base vm wn respectively map l v w k n f f isomorphism hence l v w k n dim l v w dim k n n proof proof denote map f f mat mat f f first show map linear let f g l v w mat f f j mat g gi j j f g v j f v j g v j n f j wi n gi j wi n f j gi j wi thus mat f g f j gi j f j gi j mat f mat g λ k j λ f v j λ f v j λ n f j wi n λ f j wi thus mat λ f λ f j λ f j λ mat f remains show mat bijective f ker mat mat f k n f v j j thus f v v v f zero map mat injective cp lemma hand ai j k n arbitrary define linear map f v w via f v j n ai j wi j cp proof theorem mat f hence mat also surjective cp lemma corollary show dim l v w dim k n n cp also example</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>146\\n\\n10 Linear Maps\\n\\nTheorem 10.16 shows, in particular, that f, g ∈ L(V, W) satisfy f = g if and only if [ f ] B1 ,B2 = [g] B1 ,B2 holds for given bases B1 of V and B2 of W. Thus, we can prove the equality of linear maps via the equality of their matrix representations.\\nWe now consider the map from the elements of a finite dimensional vector space to their coordinates with respect to a given basis.\\nLemma 10.17 If B = {v1 , . . . , vn } is a basis of a K -vector space V, then the map\\n⎡\\n\\n\u0003B\\n\\n⎤\\nλ1\\n: V → K n,1 , v = λ1 v1 + . . . + λn vn \u0005→ \u0003 B (v) := ⎣ ... ⎦ ,\\nλn is an isomorphism, called the coordinate map of V with respect to the basis B.\\nProof The linearity of \u0003 B is clear.\\nMoreover, we obviously have \u0003 B (V) = K n,1 , i.e., \u0003 B is surjective.\\nIf v ∈ ker(\u0003 B ), i.e., λ1 = · · · = λn = 0, then v = 0, so that\\n\b ker(\u0003 B ) = {0} and \u0003 B is also injective (cp.\\n(5) in Lemma 10.7).\\nExample 10.18 have\\n\\nIn the vector space K [t]≤n with the basis B = {t 0 , t 1 , . . . , t n } we\\n\\n⎤\\nα0\\n⎢α1 ⎥ n+1\\n⎥\\n\u0003 B (αn t n + αn−1 t n−1 + . . . + α1 t + α0 ) = ⎢\\n⎣ ... ⎦ ∈ K .\\n⎡\\n\\nαn\\n\\nOn the other hand, the basis B = {t n , t n−1 , . . . , t 0 } yields\\n⎤\\nαn\\n⎢αn−1 ⎥ n+1\\n⎥\\n\u0003 B (αn t n + αn−1 t n−1 + . . . + α1 t + α0 ) = ⎢\\n⎣ ... ⎦ ∈ K .\\n⎡\\n\\nα0\\n\\nIf B1 and B2 are bases of the finite dimensional vector spaces V and W, respectively, then we can illustrate the meaning and the construction of the matrix representation [ f ] B1 ,B2 of f ∈ L(V, W) in the following commutative diagram:\\n\\nV\\n\u0003 B1 f\\n\\n/W\\n\\n\u0003 B2\\n[ f ] B1 ,B2 \u000f\\n/\\nK m,1\\nK n,1\\n\u000f\\n\\nWe see that different compositions of maps yield the same result.\\nIn particular, we have\\n(10.3) f = \u0003−1\\nB2 ◦ [ f ] B1 ,B2 ◦ \u0003 B1 ,</td>\n",
       "      <td>148.0</td>\n",
       "      <td>148</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.2 Linear Maps and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map theorem show particular f g l v w satisfy f g f g hold given base v thus prove equality linear map via equality matrix representation consider map element finite dimensional vector space coordinate respect given basis lemma b vn basis k space v map v k v λn vn b v λn isomorphism called coordinate map v respect basis b proof linearity b clear moreover obviously b v k b surjective v ker b λn v ker b b also injective cp lemma example vector space k basis b n b αn n k αn hand basis b n yield αn b αn n k base finite dimensional vector space v w respectively illustrate meaning construction matrix representation f f l v w following commutative diagram v f f k k see different composition map yield result particular f f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>10.2 Linear Maps and Matrices\\n\\n147 where the matrix [ f ] B1 ,B2 ∈ K n,m is interpreted as a linear map from K m,1 to K n,1 , and we use that the coordinate map \u0003 B2 is bijective and hence invertible.\\nIn the same way we obtain\\n\u0003 B2 ◦ f = [ f ] B1 ,B2 ◦ \u0003 B1 , i.e.,\\n\u0003 B2 ( f (v)) = [ f ] B1 ,B2 \u0003 B1 (v) for all v ∈ V.\\n\\n(10.4)\\n\\nIn words, the coordinates of f (v) with respect to the basis B2 of W are given by the product of [ f ] B1 ,B2 and the coordinates of v with respect to the basis B1 of V.\\nWe next show that the consecutive application of linear maps corresponds to the multiplication of their matrix representations.\\nTheorem 10.19 Let V, W and X be K -vector spaces.\\nIf f ∈ L(V, W) and g ∈\\nL(W, X ), then g ◦ f ∈ L(V, X ).\\nMoreover, if V, W and X are finite dimensional with respective bases B1 , B2 and B3 , then\\n[g ◦ f ] B1 ,B3 = [g] B2 ,B3 [ f ] B1 ,B2 .\\nProof Let h := g ◦ f .\\nWe show first that h ∈ L(V, X ).\\nFor u, v ∈ V and λ, μ ∈ K we have h(λu + μv) = g( f (λu + μv)) = g(λ f (u) + μ f (v))\\n= λg( f (u)) + μg( f (v)) = λh(u) + μh(v).\\nNow let B1 = {v1 , . . . , vm }, B2 = {w1 , . . . , wn } and B3 = {x1 , . . . , xs }.\\nIf\\n[ f ] B1 ,B2 = [ f i j ] and [g] B2 ,B3 = [gi j ], then for j = 1, . . . , m we have h(v j ) = g( f (v j )) = g n\\n\u0003 f k j wk\\n\\n= k=1\\n\\n= s n\\n\u0003\\n\u0003 i=1 k=1 f k j gik xi = n\\n\u0003 s\\n\u0003 k=1 n\\n\u0003 i=1 k=1 f k j g(wk ) = k=1 gik f k j\\n\\n%\\n\\n&amp;'\\n=: h i j n\\n\u0003 fk j s\\n\u0003 gik xi i=1 xi .\\n(\\n\\nThus, [h] B1 ,B3 = [h i j ] = [gi j ] [ f i j ] = [g] B2 ,B3 [ f ] B1 ,B2 .\\n\\n\b\\n\\nUsing this theorem we can study how a change of the bases affects the matrix representation of a linear map.</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.2 Linear Maps and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map matrix matrix f k n interpreted linear map k k use coordinate map bijective hence invertible way obtain f f f v f v v word coordinate f v respect basis w given product f coordinate v respect basis next show consecutive application linear map corresponds multiplication matrix representation theorem let v w x k space f l v w g l w x g f l v x moreover v w x finite dimensional respective base g f g f proof let h g f show first h l v x u v v λ μ k h λu μv g f λu μv g λ f u μ f v λg f u μg f v λh u μh v let vm wn x f f j g gi j j h v j g f v j g n f k j wk n f k j gik xi n n f k j g wk gik f k j h j n fk j gik xi xi thus h h j gi j f j g f using theorem study change base affect matrix representation linear map</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>148\\n\\n10 Linear Maps\\n\\nCorollary 10.20 Let V and W be finite dimensional K -vector spaces with bases\\nB1 , B1 of V and B2 , B2 of W. If f ∈ L(V, W), then\\n[ f ] B1 ,B2 = [IdW ] B2 ,B2 [ f ] B1 , B2 [IdV ] B1 , B1 .\\n\\n(10.5)\\n\\nIn particular, the matrices [ f ] B1 ,B2 and [ f ] B1 , B2 are equivalent.\\nProof Applying Theorem 10.19 twice to the identity f = IdW ◦ f ◦ IdV yields\\n[ f ] B1 ,B2 = [(IdW ◦ f ) ◦ IdV ] B1 ,B2\\n= [IdW ◦ f ] B1 ,B2 [IdV ] B1 , B1\\n= [IdW ] B2 ,B2 [ f ] B1 , B2 [IdV ] B1 , B1 .\\nThe matrices [ f ] B1 ,B2 and [ f ] B1 , B2 are equivalent, since both [IdW ] B2 ,B2 and [IdV ] B1 , B1 are invertible.\\n\b\\nIf V = W, B1 = B2 , and B1 = B2 , then (10.5) becomes\\n[ f ] B1 ,B1 = [IdV ] B1 ,B1 [ f ] B1 , B1 [IdV ] B1 , B1 = ([IdV ] B1 , B1 )−1 [ f ] B1 , B1 [IdV ] B1 , B1 .\\nThus, the matrix representations [ f ] B1 ,B1 and [ f ] B1 , B1 of the endomorphism f ∈\\nL(V, V) are similar (cp.\\nDefinition 8.11).\\nThe following commutative diagram illustrates Corollary 10.20:\\n\\n[ f ] B1 ,B2\\n\\nK m,1 bEE \u0003\\nEE B1\\nEE\\nEE\\n\\n[IdV ] B\\n\\n1 , B1\\n\\n\u000f\\n\\n\u0003B yV y\\n1 yy y y y| y\\n\\nK m,1\\n\\n/\\nK n,1\\n\u0003 B2 yyy&lt; O f /\\nW\\n\\n[ f ]B\\n\\n1 , B2 y yy yy\\n\\n(10.6)\\n\\n[IdW ] B\\n\\nEE \u0003\\nEE B2\\nEE\\nEE\\n\"\\n/ n,1\\n\\n2 ,B2\\n\\nK\\n\\nAnalogously to (10.3) we have\\n−1 f = \u0003−1\\nB2 ◦ [ f ] B1 ,B2 ◦ \u0003 B1 = \u0003 B ◦ [ f ] B1 , B2 ◦ \u0003 B1 .\\n2\\n\\nExample 10.21 For the following bases of the vector space Q2,2 ,\\n\u0013\u0011\\n\\n\u0012\\n10\\nB1 =\\n,\\n00\\n\u0013\u0011 \u0012\\n10\\n,\\nB2 =\\n01\\n\\n\u0011 \u0012\\n01\\n,\\n00\\n\u0011 \u0012\\n10\\n,\\n00\\n\\n\u0011 \u0012\\n00\\n,\\n10\\n\u0011 \u0012\\n11\\n,\\n00\\n\\n\u0011 \u0012\u0014\\n00\\n,\\n01\\n\u0011 \u0012\u0014\\n00\\n,\\n10</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.2 Linear Maps and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map corollary let v w finite dimensional k space base v f l v w f idw f idv particular matrix f f equivalent proof applying theorem twice identity f idw f idv yield f idw f idv idw f idv idw f idv matrix f f equivalent since idw idv invertible v w becomes f idv f idv idv f idv thus matrix representation f f endomorphism f l v v similar cp definition following commutative diagram illustrates corollary f k bee ee ee ee idv b yv yy k k yyy f w f b yy yy idw b ee ee ee ee k analogously f f b f example following base vector space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>10.2 Linear Maps and Matrices\\n\\n149 we have the coordinate transformation matrices\\n⎡\\n0 0\\n⎢ 1 −1\\n⎢\\n[IdV ] B1 ,B2 = ⎣\\n0 1\\n0 0\\n\\n⎤\\n0 1\\n0 −1 ⎥\\n⎥\\n0 0⎦\\n1 0 and\\n⎡\\n\\n[IdV ] B2 ,B1 = ([IdV ] B1 ,B2 )−1\\n\\n11\\n⎢0 0\\n⎢\\n=⎣\\n00\\n10\\n\\n1\\n1\\n0\\n0\\n\\n⎤\\n0\\n0⎥\\n⎥.\\n1⎦\\n0\\n\\nThe coordinate maps are\\n⎡ ⎤\\n⎡\\n⎤ a11 a22\\n)\u0011\\n\u0012*\\n)\u0011\\n\u0012*\\n⎢a12 ⎥\\n⎢a11 − a12 − a22 ⎥ a11 a12 a11 a12\\n⎥\\n⎥,\\n=⎢\\n=⎢\\n\u0003 B1\\n⎣a21 ⎦ , \u0003 B2 a21 a22\\n⎣\\n⎦ a21 a22 a12 a22 a21 and one can easily verify that\\n\u0003 B2\\n\\n)\u0011\\n)\u0011\\n\u0012*\\n\u0012* a11 a12 a11 a12\\n= ([IdV ] B1 ,B2 ◦ \u0003 B1 )\\n.\\na21 a22 a21 a22\\n\\nTheorem 10.22 Let V and W be K -vector spaces with dim(V) = m and dim(W) = n, respectively.\\nThen there exist bases B1 of V and B2 of W such that\\n\u0011\\n[ f ] B1 ,B2 =\\n\\n\u0012\\nIr 0\\n∈ K n,m ,\\n0 0 where 0 ≤ r = dim(im( f )) ≤ min{n, m}.\\nFurthermore, r = rank(F), where F is the matrix representation of f with respect to arbitrary bases of V and W, and we define rank( f ) := rank(F) = dim(im( f )).\\nProof Let B1 = {v1 , . . . , vm } and B2 = {w1 , . . . , wn } be two arbitrary bases of V and W, respectively.\\nLet r := rank([ f ] B1 , B2 ).\\nThen by Theorem 5.11 there exist invertible matrices Q ∈ K n,n and Z ∈ K m,m with\\n\u0011\\nQ [ f ] B1 , B2 Z =\\n\\n\u0012\\nIr 0\\n,\\n0 0\\n\\n(10.7)</td>\n",
       "      <td>151.0</td>\n",
       "      <td>151</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.2 Linear Maps and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map matrix coordinate transformation matrix idv idv idv coordinate map one easily verify idv theorem let v w k space dim v dim w n respectively exist base v w f ir k n r dim im f min n furthermore r rank f f matrix representation f respect arbitrary base v w define rank f rank f dim im f proof let vm wn two arbitrary base v w respectively let r rank f theorem exist invertible matrix q k n n z k q f z ir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>150\\n\\n10 Linear Maps where r = rank([ f ] B1 , B2 ) ≤ min{n, m}.\\nLet us introduce two new bases B1 =\\n{v1 , . . . , vm } and B2 = {w1 , . . . , wn } of V and W via\\n(v1 , . . . , vm ) := (v1 , . . . , vm )Z ,\\n(w1 , . . . , wn ) := (w1 , . . . , wn )Q −1 , hence (w1 , . . . , wn ) = (w1 , . . . , wn )Q.\\nThen, by construction,\\nZ = [IdV ] B1 , B1 ,\\n\\nQ = [IdW ] B2 ,B2 .\\n\\nFrom (10.7) and Corollary 10.20 we obtain\\n\u0011\\n\\nIr 0\\n0 0\\n\\n\u0012\\n= [IdW ] B2 ,B2 [ f ] B1 , B2 [IdV ] B1 , B1 = [ f ] B1 ,B2 .\\n\\nWe thus have found bases B1 and B2 that yield the desired matrix representation of f .\\nEvery other choice of bases leads, by Corollary 10.20, to an equivalent matrix which therefore also has rank r .\\nIt remains to show that r = dim(im( f )).\\nThe structure of the matrix [ f ] B1 ,B2 shows that\\n+ f (v j ) = w j , 1 ≤ j ≤ r,\\n0, r + 1 ≤ j ≤ m.\\n\\nTherefore, vr +1 , . . . , vm ∈ ker( f ), which implies that dim(ker( f )) ≥ m − r .\\nOn the other hand, w1 , . . . , w j ∈ im( f ) and thus dim(im( f )) ≥ r .\\nTheorem 10.9 yields dim(V) = m = dim(im( f )) + dim(ker( f )), and hence dim(ker( f )) = m − r and dim(im( f )) = r .\\n\\n\b\\n\\nExample 10.23 For A ∈ K n,m and the corresponding map A ∈ L(K m,1 , K n,1 ) from\\n(1) in Examples 10.2 and 10.6, we have im(A) = span{a1 , . . . , am }.\\nThus, rank(A) is equal to the number of linearly independent columns of A. Since rank(A) = rank(A T ) (cp.\\n(4) in Theorem 5.11), this number is equal to the number of linearly independent rows of A.\\nTheorem 10.22 is a first example of a general strategy that we will use several times in the following chapters:\\nBy choosing appropriate bases, the matrix representation should reveal a desired information about a linear map in an efficient way.\\nIn Theorem 10.22 this information is the rank of the linear map f , i.e., the dimension of its image.\\nThe dimension formula for linear maps can be generalized to the composition of maps as follows.</td>\n",
       "      <td>152.0</td>\n",
       "      <td>152</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.2 Linear Maps and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map r rank f min n let u introduce two new base vm wn v w via vm vm z wn wn q hence wn wn q construction z idv q idw corollary obtain ir idw f idv f thus found base yield desired matrix representation f every choice base lead corollary equivalent matrix therefore also rank r remains show r dim im f structure matrix f show f v j w j j r r j therefore vr vm ker f implies dim ker f r hand w j im f thus dim im f r theorem yield dim v dim im f dim ker f hence dim ker f r dim im f r example k n corresponding map l k k example im span thus rank equal number linearly independent column since rank rank cp theorem number equal number linearly independent row theorem first example general strategy use several time following chapter choosing appropriate base matrix representation reveal desired information linear map efficient way theorem information rank linear map f dimension image dimension formula linear map generalized composition map follows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>10.2 Linear Maps and Matrices\\n\\n151\\n\\nTheorem 10.24 If V, W and X are finite dimensional K -vector spaces, f ∈ L(V, W) and g ∈ L(W, X ), then dim(im(g ◦ f )) = dim(im( f )) − dim(im( f ) ∩ ker(g)).\\nProof Let g := g|im( f ) be the restriction of g to the image of f , i.e., the map g ∈ L(im( f ), X ), v \u0005→ g(v).\\nApplying Theorem 10.9 to g yields dim(im( f )) = dim(im(g)) + dim(ker(g)).\\nNow im(g) = {g(v) ∈ X | v ∈ im( f )} = im(g ◦ f ) and ker(g) = {v ∈ im( f ) | g(v) = 0} = im( f ) ∩ ker(g),\\n\b imply the assertion.\\n\\nNote that Theorem 10.22 with V = W, f = IdV , and g ∈ L(V, X ) gives dim(im(g)) = dim(V) − dim(ker(g), which is equivalent to Theorem 10.9.\\nIf we interpret matrices A ∈ K n,m and B ∈ K s,n as linear maps, then Theorem 10.24 implies the equation rank(B A) = rank(A) − dim(im(A) ∩ ker(B)).\\nFor the special case K = R and B = A T we have the following result.\\nCorollary 10.25 If A ∈ Rn,m , then rank(A T A) = rank(A).\\nProof Let w = [ω1 , . . . , ωn ]T ∈ im(A) ∩ ker(A T ).\\nThen w = Ay for a vector y ∈ Rm,1 .\\nMultiplying this equation from the left by A T , and using that w ∈ ker(A T ), we obtain 0 = A T w = A T Ay, which implies\\n0 = y A Ay = w w =\\nT\\n\\nT\\n\\nT n\\n\u0003\\n\\nω 2j .\\nj=1\\n\\nSince this holds only for w = 0, we have im(A) ∩ ker(A T ) = {0}.\\n\\n\b</td>\n",
       "      <td>153.0</td>\n",
       "      <td>153</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.2 Linear Maps and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map matrix theorem v w x finite dimensional k space f l v w g l w x dim im g f dim im f dim im f ker g proof let g f restriction g image f map g l im f x v g v applying theorem g yield dim im f dim im g dim ker g im g g v x v im f im g f ker g v im f g v im f ker g imply assertion note theorem v w f idv g l v x give dim im g dim v dim ker g equivalent theorem interpret matrix k n b k n linear map theorem implies equation rank b rank dim im ker b special case k r b following result corollary rn rank rank proof let w ωn im ker w ay vector multiplying equation left using w ker obtain w ay implies ay w w n ω since hold w im ker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>152\\n\\n10 Linear Maps\\n\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n\\n⎡\\n⎤\\n201\\n10.1 Consider the linear map on R3,1 given by the matrix A = ⎣2 1 0⎦ ∈ R3,3 .\\n411\\nDetermine ker(A), dim(ker(A)) and dim(im(A)).\\n10.2 Construct a map f ∈ L(V, W) such that for linearly independent vectors v1 , . . . , vr ∈ V the images f (v1 ), . . . , f (vr ) ∈ W are linearly dependent.\\n10.3 The map f : R[t]≤n → R[t]≤n−1 ,\\nαn t n + αn−1 t n−1 + . . . + α1 t + α0 \u0005→ nαn t n−1 + (n − 1)αn−1 t n−2 + . . . + 2α2 t + α1 , is called the derivative of the polynomial p ∈ R[t]≤n with respect to the variable t.\\nShow that⎧⎡ f is⎤linear determine\\n⎡ and\\n⎤ ⎡\\n⎤⎫ ker( f ) and im( f ).\\n\u0013\u0011 \u0012 \u0011 \u0012\u0014\\n0\\n0 ⎬\\n⎨ 1\\n1\\n0\\n10.4 For the bases B1 = ⎣ 0 ⎦ , ⎣ 1 ⎦ , ⎣ 0 ⎦ of R3,1 and B2 =\\n,\\n0\\n1\\n⎩\\n⎭\\n0\\n0\\n1\\n3,1\\n2,1 of R2,1 , let\\n\u0011\\n\u0012 f ∈ L(R , R ) have the matrix representation [ f ] B1 ,B2 =\\n0 23\\n.\\n1 −2 0\\n⎧⎡\\n⎤ ⎡ ⎤ ⎡\\n⎤⎫\\n2\\n1\\n−1 ⎬\\n⎨\\n(a) Determine [ f ] B1 , B2 for the bases B1 = ⎣ 1 ⎦ , ⎣ 0 ⎦ , ⎣ 2 ⎦ of\\n⎩\\n⎭\\n−1\\n3\\n1\\n\u0013\u0011 \u0012 \u0011\\n\u0012\u0014\\n1\\n1\\n, of R2,1 .\\nR3,1 and B2 =\\n1\\n−1\\n(b) Determine the coordinates of f ([4, 1, 3]T ) with respect to the basis B2 .\\n10.5 Construct a map f ∈ L(K [t], K [t]) with the following properties:\\n(1) f ( pq) = ( f ( p))q + p( f (q)) for all p, q ∈ K [t].\\n(2) f (t) = 1.\\nIs this map uniquely determined by these properties or are there further maps with the same properties?\\n10.6 Let α ∈ K and A ∈ K n,n .\\nShow that the maps\\nK [t] → K , p \u0005→ p(α), and\\n\\nK [t] → K m,m , p \u0005→ p(A), are linear and justify the name evaluation homomorphism for this map.\\n10.7 Let S ∈ G L n (K ).\\nShow that the map f : K n,n → K n,n , A \u0005→ S −1 AS is an isomorphism.</td>\n",
       "      <td>154.0</td>\n",
       "      <td>154</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.2 Linear Maps and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map exercise following exercise k arbitrary field consider linear map given matrix determine ker dim ker dim im construct map f l v w linearly independent vector vr v image f f vr w linearly dependent map f r r αn n nαn n called derivative polynomial p r respect variable show f determine ker f im f base let f l r r matrix representation f determine f base b determine coordinate f respect basis construct map f l k k following property f pq f p q p f q p q k f map uniquely determined property map property let α k k n n show map k k p p α k k p p linear justify name evaluation homomorphism map let g l n k show map f k n n k n n isomorphism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>10.2 Linear Maps and Matrices\\n\\n153\\n\\n10.8 Let K be a field with 1 + 1 \u0007= 0 and let A ∈ K n,n .\\nConsider the map f : K n,1 → K , x \u0005→ x T Ax.\\nIs f a linear map?\\nShow that f = 0 if and only if A + A T = 0.\\n10.9 Let V be a Q-vector space with the basis B1 = {v1 , . . . , vn } and let f ∈\\nL(V, V) be defined by\\n+ f (v j ) = v j + v j+1 , j = 1, . . . , n − 1, j = n.\\nv1 + vn ,\\n\\n(a) Determine [ f ] B1 ,B1 .\\n(b) Let B2 = {w1 , . . . , wn } with w j = jvn+1− j , j = 1, . . . , n.\\nShow that\\nB2 is a basis of V. Determine the coordinate transformation matrices\\n[IdV ] B1 ,B2 and [IdV ] B2 ,B1 , as well as the matrix representations [ f ] B1 ,B2 and [ f ] B2 ,B2 .\\n10.10 Can you extend Theorem 10.19 consistently to the case W = {0}?\\nWhat are the properties of the matrices [g ◦ f ] B1 ,B3 , [g] B2 ,B3 and [ f ] B1 ,B2 ?\\n10.11 Consider the map f : R[t]≤n → R[t]≤n+1 ,\\nαn t n + αn−1 t n−1 + . . . + α1 t + α0 \u0005→\\n\\n1\\nαn t n+1 n+1\\n1\\n1\\n+ αn−1 t n + . . . + α1 t 2 + α0 t.\\nn\\n2\\n\\n(a) Show that f is linear.\\nDetermine ker( f ) and im( f ).\\n(b) Choose bases B1 , B2 in the two vector spaces and verify that for your choice rank([ f ] B1 ,B2 ) = dim(im( f )) holds.\\n10.12 Let α1 , . . . , αn ∈ R, n ≥ 2, be pairwise distinct numbers and let n polynomials in R[t] be defined by pj = n )\\n, k=1 k\u0007 = j\\n\\n*\\n1\\n(t − αk ) ,\\nα j − αk j = 1, . . . , n.\\n\\n(a) Show that the set B ={ p1 , . . . , pn } is a basis of R[t]≤n−1 .\\n(This basis is called the Lagrange basis2 of R[t]≤n−1 .)\\n(b) Show that the corresponding coordinate map is given by\\n\\n2 Joseph-Louis de Lagrange (1736–1813).</td>\n",
       "      <td>155.0</td>\n",
       "      <td>155</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.2 Linear Maps and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map matrix let k field let k n n consider map f k k x x ax f linear map show f let v q-vector space basis vn let f l v v defined f v j v j v j n j vn determine f b let wn w j j j show basis determine coordinate transformation matrix idv idv well matrix representation f f extend theorem consistently case w property matrix g f g f consider map f r r αn n αn n n show f linear determine ker f im f b choose base two vector space verify choice rank f dim im f hold let αn r n pairwise distinct number let n polynomial r defined pj n j αk α j αk j show set b pn basis r basis called lagrange r b show corresponding coordinate map given joseph-louis de lagrange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>154\\n\\n10 Linear Maps\\n\\n⎡\\n\\n\u0003 B : R[t]≤n−1 → Rn,1 ,\\n\\n⎤ p(α1 ) p \u0005→ ⎣ ... ⎦ .\\np(αn )\\n\\n(Hint: You can use Exercise 7.8 (b).)\\n10.13 Verify different paths in the commutative diagram (10.6) for the vector spaces\\n2,2\\n2,2 and bases\\n\u0011 of \u0012Example 10.21 and linear map f : Q → Q , A \u0005→ F A with\\n11\\n.\\nF=\\n−1 1</td>\n",
       "      <td>156.0</td>\n",
       "      <td>156</td>\n",
       "      <td>10 Linear Maps</td>\n",
       "      <td>10.2 Linear Maps and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear map b r p p p αn hint use exercise b verify different path commutative diagram vector space base linear map f q q f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Chapter 11\\n\\nLinear Forms and Bilinear Forms\\n\\nIn this chapter we study different classes of maps between one or two K -vector spaces and the one dimensional K -vector space defined by the field K itself.\\nThese maps play an important role in many areas of Mathematics, including Analysis, Functional\\nAnalysis and the solution of differential equations.\\nThey will also be essential for the further developments in this book: Using bilinear and sesquilinear forms, which are introduced in this chapter, we will define and study Euclidean and unitary vector spaces in Chap.\\n12.\\nLinear forms and dual spaces will be used in the existence proof of the Jordan canonical form in Chap.\\n16.\\n\\n11.1 Linear Forms and Dual Spaces\\nWe start with the set of linear maps from a K -vector space to the vector space K .\\nDefinition 11.1 If V is a K -vector space, then f ∈ L(V, K ) is called a linear form on V. The K -vector space V ∗ := L(V, K ) is called the dual space of V.\\nA linear form is sometimes called a linear functional or a one-form, which stresses that it (linearly) maps into a one dimensional vector space.\\nExample 11.2 If V is the R-vector space of the continuous and real valued functions on the real interval [α, β] and if γ ∈ [α, β], then the two maps f 1 : V → R, g \u0005→ g(γ),\\n\u0002 β f 2 : V → R, g \u0005→ g(x)d x,\\nα are linear forms on V.\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_11\\n\\n155</td>\n",
       "      <td>157.0</td>\n",
       "      <td>157</td>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.1 Linear Forms and Dual Spaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter linear form bilinear form chapter study different class map one two k space one dimensional k space defined field k map play important role many area mathematics including analysis functional analysis solution differential equation also essential development book using bilinear sesquilinear form introduced chapter define study euclidean unitary vector space chap linear form dual space used existence proof jordan canonical form chap linear form dual space start set linear map k space vector space k definition v k space f l v k called linear form k space v l v k called dual space linear form sometimes called linear functional one-form stress linearly map one dimensional vector space example v r-vector space continuous real valued function real interval α β γ α β two map f v r g g γ β f v r g g x x α linear form springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>156\\n\\n11 Linear Forms and Bilinear Forms\\n\\nIf dim(V) = n, then dim(V ∗ ) = n by Theorem 10.16.\\nLet B1 = {v1 , . . . , vn } be a basis of V and let B2 = {1} be a basis of the K -vector space K .\\nIf f ∈ V ∗ , then f (vi ) = αi for some αi ∈ K , i = 1, . . . , n, and\\n[ f ] B1 ,B2 = [α1 , . . . , αn ] ∈ K 1,n .\\nFor an element v = n\\n\u0003\\n\\nλi vi ∈ V we have i=1 f (v) = f n\\n\u0004\u0005 i=1\\n\\nλi vi\\n\\n\u0006\\n\\n⎡\\n\\n⎤\\nλ1\\n⎢ ⎥\\n=\\nλi f (vi ) =\\nλi αi = [α1 , . . . , αn ] ⎣ ... ⎦\\n\u0007\\n\b i=1 i=1\\nλn\\n∈K 1,n\\n\u0007\b n\\n\u0005 n\\n\u0005\\n\\n∈K n,1\\n\\n= [ f ] B1 ,B2 \u0002 B1 (v), where we have identified the isomorphic vector spaces K and K 1,1 with each other.\\nFor a given basis of a finite dimensional vector space V we will now construct a special, uniquely determined basis of the dual space V ∗ .\\nTheorem 11.3 If V is K -vector space \u0012with the basis B = {v1 , . . . , vn }, then there\\n\u0011 exists a unique basis B ∗ = v1∗ , . . . , vn∗ of V ∗ such that vi∗ (v j ) = δi j , i, j = 1, . . . , n, which is called the dual basis of B.\\nProof By Theorem 10.4, a unique linear map from V to K can be constructed by prescribing its images at the given basis B. Thus, for each i = 1, . . . , n, there exists a unique map vi∗ ∈ L(V, K ) with vi∗ (v j ) = δi j , j = 1, . . . , n.\\nIt remains to show that B ∗ := {v1∗ , . . . , vn∗ } is a basis of V ∗ .\\nIf λ1 , . . . , λn ∈ K are such that n\\n\u0005\\nλi vi∗ = 0V ∗ ∈ V ∗ , i=1 then\\n0 = 0V ∗ (v j ) = n\\n\u0005\\n\\nλi vi∗ (v j ) = λ j , j = 1, . . . , n.\\ni=1\\n\\nThus, v1∗ , . . . , vn∗ are linearly independent, and dim(V ∗ ) = n implies that B ∗ is a\\n\u0006\\n\u0007 basis of V ∗ (cp.\\nExercise 9.6).\\nn,1\\nExample\\n11.4\\n\u0012 Consider V = K with ∗the canonical basis B = {e1 , \u0013. . ∗. \u0014, en }.\\nIf\\n\u0011 ∗\\n∗ e1 , . . . , en is the dual basis of B, then ei (e j ) = δi j , which shows that ei B,{1} = eiT ∈ K 1,n , i = 1, . . . , n.</td>\n",
       "      <td>158.0</td>\n",
       "      <td>158</td>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.1 Linear Forms and Dual Spaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear form bilinear form dim v n dim v n theorem let vn basis v let basis k space k f v f vi αi αi k n f αn k n element v n λi vi v f v f n λi vi λi f vi λi αi αn λn n n n f v identified isomorphic vector space k k given basis finite dimensional vector space v construct special uniquely determined basis dual space v theorem v k space basis b vn exists unique basis b v v j δi j j n called dual basis b proof theorem unique linear map v k constructed prescribing image given basis b thus n exists unique map l v k v j δi j j remains show b basis v λn k n λi v v j n λi v j λ j j thus linearly independent dim v n implies b basis v cp exercise example consider v k canonical basis b en en dual basis b ei e j δi j show ei b eit k n n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>11.1 Linear Forms and Dual Spaces\\n\\n157\\n\\nDefinition 11.5 Let V and W be K -vector spaces with their respective dual spaces\\nV ∗ and W ∗ , and let f ∈ L(V, W).\\nThen f ∗ : W ∗ → V ∗ , h \u0005→ f ∗ (h) := h ◦ f, is called the dual map of f .\\nWe next derive some properties of the dual map.\\nLemma 11.6 If V, W and X are K -vector spaces, then the following assertions hold:\\n(1) If f ∈ L(V, W), then the dual map f ∗ is linear, hence f ∗ ∈ L(W ∗ , V ∗ ).\\n(2) If f ∈ L(V, W) and g ∈ L(W, X ), then (g ◦ f )∗ ∈ L(X ∗ , V ∗ ) and (g ◦ f )∗ = f ∗ ◦ g∗ .\\n(3) If f ∈ L(V, W) is bijective, then f ∗ ∈ L(W ∗ , V ∗ ) is bijective and ( f ∗ )−1 =\\n( f −1 )∗ .\\nProof (1) If h 1 , h 2 ∈ W ∗ , λ1 , λ2 ∈ K , then f ∗ (λ1 h 1 + λ2 h 2 ) = (λ1 h 1 + λ2 h 2 ) ◦ f = (λ1 h 1 ) ◦ f + (λ2 h 2 ) ◦ f\\n= λ1 (h 1 ◦ f ) + λ2 (h 2 ◦ f ) = λ1 f ∗ (h 1 ) + λ2 f ∗ (h 2 ).\\n\\n\u0007\\n\u0006\\n(2) and (3) are exercises.\\nAs the following theorem shows, the concepts of the dual map and the transposed matrix are closely related.\\nTheorem 11.7 Let V and W be finite dimensional K -vector spaces with bases\\nB1 and B2 , respectively.\\nLet B1∗ and B2∗ be the corresponding dual bases.\\nIf f ∈ L(V, W), then\\n[ f ∗ ] B2∗ ,B1∗ = ([ f ] B1 ,B2 )T .\\n\u0011\\n\u0012\\nProof \u0011Let B1 = {v\u00121 , . . . , vm }, B2 = {w1 , . . . , wn }, and let B1∗ = v1∗ , . . . , vm∗ ,\\nB2∗ = w1∗ , . . . , wn∗ .\\nLet [ f ] B1 ,B2 = [ai j ] ∈ K n,m , i.e., f (v j ) = n\\n\u0005 ai j wi , j = 1, . . . , m, i=1 and [ f ∗ ] B2∗ ,B1∗ = [bi j ] ∈ K m,n , i.e., m\\n\u0015 \u0016 \u0005 f ∗ w ∗j = bi j vi∗ , i=1 j = 1, . . . , n.</td>\n",
       "      <td>159.0</td>\n",
       "      <td>159</td>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.1 Linear Forms and Dual Spaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear form dual space definition let v w k space respective dual space v w let f l v w f w v h f h h f called dual map f next derive property dual map lemma v w x k space following assertion hold f l v w dual map f linear hence f l w v f l v w g l w x g f l x v g f f f l v w bijective f l w v bijective f f proof h h w k f h h h h f h f h f h f h f f h f h exercise following theorem show concept dual map transposed matrix closely related theorem let v w finite dimensional k space base respectively let corresponding dual base f l v w f f proof vm wn let let f ai j k n f v j n ai j wi j f bi j k n f w bi j j n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>158\\n\\n11 Linear Forms and Bilinear Forms\\n\\nFor every pair (k, \u0003) with 1 ≤ k ≤ n and 1 ≤ \u0003 ≤ m we then have ak\u0003 = n\\n\u0005 ai\u0003 wk∗ (wi ) = wk∗ n\\n\u0004\u0005 i=1\\n\\n= m\\n\u0004\u0005\\n\\n\u0006\\n\u0015 \u0016 ai\u0003 wi = wk∗ ( f (v\u0003 )) = f ∗ wk∗ (v\u0003 ) i=1 m\\n\u0006\\n\u0005 bik vi∗ (v\u0003 ) = bik vi∗ (v\u0003 ) i=1 i=1\\n\\n= b\u0003k , where we have used the definition of the dual map as well as wk∗ (wi ) = δki and\\n\u0007\\n\u0006 vi∗ (v\u0003 ) = δi\u0003 .\\nBecause of the close relationship between the transposed matrix and the dual map, some authors call the dual map f ∗ the transpose of the linear map f .\\nApplied to matrices, Lemma 11.6 and Theorem 11.7 yield the following rules known from Chap.\\n4:\\n(AB)T = B T A T for A ∈ K n,m and B ∈ K m,\u0003 , and\\n(A−1 )T = (A T )−1 for A ∈ G L n (K ).\\nExample 11.8 For the two bases of R2,1 ,\\n\u0017\\n\u0018 \u0019\\n\u0018 \u0019\u001a\\n1\\n0\\nB1 = v1 =\\n, v2 =\\n,\\n0\\n2\\n\\n\u0017\\n\u0018 \u0019\\n\u0018 \u0019\u001a\\n1\\n1\\nB2 = w1 =\\n, w2 =\\n,\\n0\\n1 the elements of the corresponding dual bases are given by\\n\u0019\\nα1\\n\u0005→ α1 + 0,\\nα2\\n\u0018 \u0019\\nα1\\n\u0005→ α1 − α2 ,\\n→ R,\\nα2 v1∗ : R2,1 → R, w1∗ : R2,1\\n\\n\u0018\\n\\n\u0019\\n1\\nα1\\n\u0005→ 0 + α2 ,\\nα2\\n2\\n\u0018 \u0019\\nα1\\n\u0005→ 0 + α2 .\\n→ R,\\nα2 v2∗ : R2,1 → R, w2∗ : R2,1\\n\\n\u0018\\n\\nThe matrix representations of these maps are\\n\u0013 \u0014\\n\u0013 ∗\u0014 v1 B1 ,{1} = 1 0 ,\\n\u0013 ∗\u0014\\n\u0013 \u0014 w1 B2 ,{1} = 1 0 ,\\n\\n\u0013 ∗\u0014\\n\u0013 \u0014 v2 B1 ,{1} = 0 1 ,\\n\u0013 ∗\u0014\\n\u0013 \u0014 w2 B2 ,{1} = 0 1 .\\n\\nFor the linear map\\n\u0019\\n\u0018\\n\u0019\\nα + α2\\nα1\\n\u0005→ 1\\n,\\nα2\\n3α2\\n\\n\u0018 f : R2,1 → R2,1 ,</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160</td>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.1 Linear Forms and Dual Spaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear form bilinear form every pair k k n n wi n wi f f bik bik used definition dual map well wi δki close relationship transposed matrix dual map author call dual map f transpose linear map f applied matrix lemma theorem yield following rule known chap ab b k n b k g l n k example two base element corresponding dual base given r r r r matrix representation map linear map α f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>11.1 Linear Forms and Dual Spaces\\n\\n\u0018 we have\\n[ f ] B1 ,B2 =\\n\\n159\\n\\n\u0018\\n\u0019\\n\u0019\\n1 −4\\n10\\n, [ f ∗ ] B2∗ ,B1∗ =\\n.\\n0 6\\n−4 6\\n\\n11.2 Bilinear Forms\\nWe now consider special maps from a pair of K -vector spaces to the K -vector space\\nK.\\nDefinition 11.9 Let V and W be K -vector spaces.\\nA map β : V × W → K is called a bilinear form on V × W, when\\n(1) β(v1 + v2 , w) = β(v1 , w) + β(v2 , w),\\n(2) β(v, w1 + w2 ) = β(v, w1 ) + β(v, w2 ),\\n(3) β(λv, w) = β(v, λw) = λβ(v, w), hold for all v, v1 , v2 ∈ V, w, w1 , w2 ∈ W, and λ ∈ K .\\nA bilinear form β is called non-degenerate in the first variable, if β(v, w) = 0 for all w ∈ W implies that v = 0.\\nAnalogously, it is called non-degenerate in the second variable, if β(v, w) = 0 for all v ∈ V implies that w = 0.\\nIf β is non-degenerate in both variables, then β is called non-degenerate and the spaces V, W are called a dual pair with respect to β.\\nIf V = W, then β is called a bilinear form on V. If additionally β(v, w) =\\nβ(w, v) holds for all v, w ∈ V, then β is called symmetric.\\nOtherwise, β is called nonsymmetric.\\nExample 11.10\\n(1) If A ∈ K n,m , then\\nβ : K m,1 × K n,1 → K , (v, w) \u0005→ w T Av, is a bilinear form on K m,1 × K n,1 that is non-degenerate if and only if n = m and A ∈ G L n (K ), (cp.\\nExercise 11.10).\\n(2) The bilinear form\\nβ : R2,1 × R2,1 → R, (x, y) \u0005→ y T\\n\\n\u0018 \u0019\\n11 x,\\n11 is degenerate in both variables: For \u001b x = [1, −1]T , we have β(\u001b x , y) = 0 for all\\n2,1\\nT y = [1, −1] we have β(x, \u001b y) = 0 for all x ∈ R2,1 .\\nThe set of y ∈ R ; for \u001b all x = [x1 , x2 ]T ∈ R2,1 with β(x, x) = 1 is equal to the solution set of the quadratic equation in two variables x12 + 2x1 x2 + x22 = 1, or (x1 + x2 )2 = 1, for x1 , x2 ∈ R. Geometrically, this set is given by the two straight lines x1 + x2 = 1 and x1 + x2 = −1 in the cartesian coordinate system of R2 .</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161</td>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.2 Bilinear Forms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear form dual space f f bilinear form consider special map pair k space k space definition let v w k space map β v w k called bilinear form v w β w β w β w β v β v β v β λv w β v λw λβ v w hold v v w w λ k bilinear form β called non-degenerate first variable β v w w w implies v analogously called non-degenerate second variable β v w v v implies w β non-degenerate variable β called non-degenerate space v w called dual pair respect β v w β called bilinear form additionally β v w β w v hold v w v β called symmetric otherwise β called nonsymmetric example k n β k k k v w w av bilinear form k k non-degenerate n g l n k cp exercise bilinear form β r x x degenerate variable x β x β x x set r x β x x equal solution set quadratic equation two variable geometrically set given two straight line cartesian coordinate system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>160\\n\\n11 Linear Forms and Bilinear Forms\\n\\n(3) If V is a K -vector space, then\\nβ : V × V ∗ → K , (v, f ) \u0005→ f (v), is a bilinear form on V × V ∗ , since\\nβ(v1 + v2 , f ) = f (v1 + v2 ) = f (v1 ) + f (v2 ) = β(v1 , f ) + β(v2 , f ),\\nβ(v, f 1 + f 2 ) = ( f 1 + f 2 )(v) = f 1 (v) + f 2 (v) = β(v, f 1 ) + β(v, f 2 ),\\nβ(λv, f ) = f (λv) = λ f (v) = λβ(v, f ) = (λ f )(v) = β(v, λ f ), hold for all v, v1 , v2 ∈ V, f, f 1 , f 2 ∈ V ∗ and λ ∈ K .\\nThis bilinear form is non-degenerate and thus V, V ∗ are a dual pair with respect to β (cp.\\nExercise\\n11.11 for the case dim(V) ∈ N).\\nDefinition 11.11 Let V and W be K -vector spaces with bases B1 = {v1 , . . . , vm } and B2 = {w1 , . . . , wn }, respectively.\\nIf β is a bilinear form on V × W, then\\n[β] B1 ×B2 = [bi j ] ∈ K n,m , bi j := β(v j , wi ), is called the matrix representation of β with respect to the bases B1 and B2 .\\n\u0003\\n\u0003n\\nIf v = mj=1 λ j v j ∈ V and w = i=1\\nμi wi ∈ W, then\\nβ(v, w) = m \u0005 n\\n\u0005\\n\\nλ j μi β(v j , wi ) = j=1 i=1 n\\n\u0005 i=1\\n\\nμi m\\n\u0005\\n\\n\u0015\\n\u0016T bi j λ j = \u0002 B2 (w) [β] B1 ×B2 \u0002 B1 (v), j=1 where we have used the coordinate map from Lemma 10.17.\\n\u0012\\n\u0012\\n\u0011\\n\u0011\\nExample 11.12 If B1 = e1(m) , . . . , em(m) and B2 = e1(n) , . . . , en(n) are the canonical bases of K m,1 and K n,1 , respectively, and if β is the bilinear form from (1) in\\nExample 11.10 with A = [ai j ] ∈ K n,m , then [β] B1 ×B2 = [bi j ], where\\n\u0015\\n\u0015 \u0016T\\n(n) \u0016\\n= ai j ,\\n= ei(n) Ae(m) bi j = β e(m) j , ei j and hence [β] B1 ×B2 = A.\\nThe following result shows that symmetric bilinear forms have symmetric matrix representations.\\nLemma 11.13 For a bilinear form β on a finite dimensional vector space V the following statements are equivalent:\\n(1) β is symmetric.\\n(2) For every basis B of V the matrix [β] B×B is symmetric.\\n(3) There exists a basis B of V such that [β] B×B is symmetric.</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162</td>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.2 Bilinear Forms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear form bilinear form v k space β v v k v f f v bilinear form v v since β f f f f β f β f β v f f f f v f v f v β v f β v f β λv f f λv λ f v λβ v f λ f v β v λ f hold v v f f f v λ k bilinear form non-degenerate thus v v dual pair respect β cp exercise case dim v n definition let v w k space base vm wn respectively β bilinear form v w β bi j k n bi j β v j wi called matrix representation β respect base v λ j v j v w μi wi w β v w n λ j μi β v j wi n μi bi j λ j w β v used coordinate map lemma example em n en n canonical base k k respectively β bilinear form example ai j k n β bi j n ai j ei n ae bi j β e j ei j hence β following result show symmetric bilinear form symmetric matrix representation lemma bilinear form β finite dimensional vector space v following statement equivalent β symmetric every basis b v matrix β symmetric exists basis b v β symmetric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>11.2 Bilinear Forms\\n\\n161\\n\\n\u0007\\n\u0006\\n\\nProof Exercise.\\n\\nWe will now analyze the effect of a basis change on the matrix representation of a bilinear form.\\nTheorem 11.14 Let V and W be finite dimensional K -vector spaces with bases\\nB1 , \u001c",
       "\\nB1 of V and B2 , \u001c",
       "\\nB2 of W. If β is a bilinear form on V × W, then\\n\u0015\\n\u0016T\\n[β] B1 ×B2 = [IdW ] B2 , \u001c",
       "\\n[β] \u001c",
       "\\nB2\\nB2 [IdV ] B1 , \u001c",
       "\\nB1 .\\nB1 × \u001c",
       "\\nProof Let B1 = {v1 , . . . , vm }, \u001c",
       "\\nB1 = { v1 , . . . , vm }, B2 = {w1 , . . . , wn }, \u001c",
       "\\nB2 = \u001c",
       "n }, and\\n{ w1 , . . . , w v1 , . . . , vm )P, where P = [ pi j ] = [IdV ] B1 , \u001c",
       "\\n(v1 , . . . , vm ) = (\u001c",
       "\\nB1 ,\\n(w1 , . . . , wn ) = ( w1 , . . . , w \u001c",
       "n )Q, where Q = [qi j ] = [IdW ] B2 , \u001c",
       "\\nB2 .\\n\u001c",
       "\\n\u001c",
       "\\nWith [β] vj, w \u001c",
       "i ), we then have\\nB1 × \u001c",
       "\\nB2 = [bi j ], where bi j = β(\u001c",
       "\\nβ(v j , wi ) = β m\\n\u0004\u0005 pk j vk , n\\n\u0005 q\u0003i\\n\\n\u0003=1\\n\\n\u0006 q\u0003i w\\n\u001c",
       "\u0003 =\\n\\n\u0003=1 k=1\\n\\n= n\\n\u0005 m\\n\u0005 n\\n\u0005\\n\u0003=1 q\u0003i m\\n\u0005\\n\\nβ( vk , w\\n\u001c",
       "\u0003 ) pk j k=1 b\u0003k pk j k=1\\n\\n⎤T\\n⎡\\n⎤ p1 j q1i\\n⎢ . ⎥\\n⎢ ⎥\\n= ⎣ ... ⎦ [β] \u001c",
       "\\nB2 ⎣ .. ⎦ ,\\nB1 × \u001c",
       "\\n⎡ qni pm j which implies that [β] B1 ×B2 = Q T [β] \u001c",
       "\\nB2 P, and hence the assertion follows.\\nB1 × \u001c",
       "\\n\\n\u0007\\n\u0006\\n\\nIf V = W and B1 , B2 are two bases of V, then we obtain the following special case of Theorem 11.14:\\n\u0016T\\n\u0015\\n[β] B1 ×B1 = [IdV ] B1 ,B2 [β] B2 ×B2 [IdV ] B1 ,B2 .\\nThe two matrix representations [β] B1 ×B1 and [β] B2 ×B2 of β in this case are congruent, which we formally define as follows.\\nDefinition 11.15 If for two matrices A, B ∈ K n,n there exists a matrix Z ∈ G L n (K ) with B = Z T AZ , then A and B are called congruent.\\nLemma 11.16 Congruence is an equivalence relation on the set K n,n .\\nProof Exercise.\\n\\n\u0007\\n\u0006</td>\n",
       "      <td>163.0</td>\n",
       "      <td>163</td>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.2 Bilinear Forms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bilinear form proof exercise analyze effect basis change matrix representation bilinear form theorem let v w finite dimensional k space base v β bilinear form v w β idw β idv proof let vm vm wn n w vm p p pi j idv vm wn w n q q qi j idw β vj w bi j bi j β β v j wi β pk j vk n w n n β vk w pk j pk j j β qni pm j implies β q β p hence assertion follows v w two base v obtain following special case theorem β idv β idv two matrix representation β β β case congruent formally define follows definition two matrix b k n n exists matrix z g l n k b z az b called congruent lemma congruence equivalence relation set k n n proof exercise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>162\\n\\n11 Linear Forms and Bilinear Forms\\n\\n11.3 Sesquilinear Forms\\nFor complex vector spaces we introduce another special class of forms.\\nDefinition 11.17 Let V and W be C-vector spaces.\\nA map s : V × W → C is called a sesquilinear form on V × W, when\\n(1)\\n(2)\\n(3)\\n(4) s(v1 + v2 , w) = s(v1 , w) + s(v2 , w), s(λv, w) = λs(v, w), s(v, w1 + w2 ) = s(v, w1 ) + s(v, w2 ), s(v, λw) = λs(v, w), hold for all v, v1 , v2 ∈ V, w, w1 , w2 ∈ W and λ ∈ C.\\nIf V = W, then s is called a sesquilinear form on V. If additionally s(v, w) = s(w, v) holds for all v, w ∈ V, then s is called Hermitian.1\\nThe prefix sesqui is Latin and means “one and a half”.\\nNote that a sesquilinear form is linear in the first variable and semilinear (“half linear”) in the second variable.\\nThe following result characterizes Hermitian sesquilinear forms.\\nLemma 11.18 A sesquilinear form on the C-vector space V is Hermitian if and only if s(v, v) ∈ R for all v ∈ V.\\nProof If s is Hermitian then, in particular, s(v, v) = s(v, v) for all v ∈ V, and thus s(v, v) ∈ R.\\nIf, on the other hand, v, w ∈ V, then by definition s(v + w, v + w) = s(v, v) + s(v, w) + s(w, v) + s(w, w), s(v + iw, v + iw) = s(v, v) + is(w, v) − is(v, w) + s(w, w).\\n\\n(11.1)\\n(11.2)\\n\\nThe first equation implies that s(v, w) + s(w, v) ∈ R, since s(v + w, v + w), s(v, v), s(w, w) ∈ R by assumption.\\nThe second equation implies analogously that is(w, v) − is(v, w) ∈ R. Therefore, s(v, w) + s(w, v) = s(v, w) + s(w, v),\\n−is(v, w) + is(w, v) = is(v, w) − is(w, v).\\nMultiplying the second equation with i and adding the resulting equation to the first\\n\u0007\\n\u0006 we obtain s(v, w) = s(w, v)\\nCorollary 11.19 For a sesquilinear form s on the C-vector space V we have\\n2 s(v, w) = s(v + w, v + w) + is(v + iw, v + iw) − (i + 1) (s(v, v) + s(w, w)).\\nfor all v, w ∈ V.\\n1 Charles\\n\\nHermite (1822–1901).</td>\n",
       "      <td>164.0</td>\n",
       "      <td>164</td>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.3 Sesquilinear Forms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear form bilinear form sesquilinear form complex vector space introduce another special class form definition let v w c-vector space map v w c called sesquilinear form v w w w w λv w λs v w v v v v λw λs v w hold v v w w λ v w called sesquilinear form additionally v w w v hold v w v called prefix sesqui latin mean one half note sesquilinear form linear first variable semilinear half linear second variable following result characterizes hermitian sesquilinear form lemma sesquilinear form c-vector space v hermitian v v r v proof hermitian particular v v v v v v thus v v hand v w v definition v w v w v v v w w v w w v iw v iw v v w v v w w w first equation implies v w w v r since v w v w v v w w r assumption second equation implies analogously w v v w therefore v w w v v w w v v w w v v w w v multiplying second equation adding resulting equation first obtain v w w v corollary sesquilinear form c-vector space v v w v w v w v iw v iw v v w w v w charles hermite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>11.3 Sesquilinear Forms\\n\\n163\\n\\nProof The result follows from multiplication of (11.2) with i and adding the result to (11.1).\\n\u0007\\n\u0006\\nCorollary 11.19 shows that a sesquilinear form on a C-vector space V is uniquely determined by the values of s(v, v) for all v ∈ V.\\nDefinition 11.20 The Hermitian transpose of A = [ai j ] ∈ Cn,m is the matrix\\nA H := [a i j ]T ∈ Cm,n .\\nIf A = A H , then A is called Hermitian.\\nIf a matrix A has real entries, then obviously A H = A T .\\nThus, a real symmetric matrix is also Hermitian.\\nIf A = [ai j ] ∈ Cn,n is Hermitian, then in particular aii = a ii for i = 1, . . . , n, i.e., Hermitian matrices have real diagonal entries.\\nThe Hermitian transposition satisfies similar rules as the (usual) transposition\\n(cp.\\nLemma 4.6).\\n\u001b ∈ Cn,m , B ∈ Cm,\u0003 and λ ∈ C the following assertions\\nLemma 11.21 For A, A hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n\\n(A H ) H = A.\\n\u001bH .\\n\u001b H = AH + A\\n(A + A)\\n(λ A) H = λ A H .\\n(AB) H = B H A H .\\n\u0007\\n\u0006\\n\\nProof Exercise.\\nExample 11.22 For A ∈ Cn,m the map s : Cm,1 × Cn,1 → C, (v, w) \u0005→ w H Av, is a sesquilinear form.\\n\\nThe matrix representation of a sesquilinear form is defined analogously to the matrix representation of bilinear forms (cp.\\nDefinition 11.11).\\nDefinition 11.23 Let V and W be C-vector spaces with bases B1 = {v1 , . . . , vm } and B2 = {w1 , . . . , wn }, respectively.\\nIf s is a sesquilinear form on V × W, then\\n[s] B1 ×B2 = [bi j ] ∈ Cn,m , bi j := s(v j , wi ), is called the matrix representation of s with respect to the bases B1 and B2 .\\n\u0012\\n\u0012\\n\u0011\\n\u0011\\nExample 11.24 If B1 = e1(m) , . . . , em(m) and B2 = e1(n) , . . . , en(n) are the canonical bases of Cm,1 and Cn,1 , respectively, and s is the sesquilinear form of Example 11.22 with A = [ai j ] ∈ Cn,m , then [s] B1 ×B2 = [bi j ] with</td>\n",
       "      <td>165.0</td>\n",
       "      <td>165</td>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.3 Sesquilinear Forms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sesquilinear form proof result follows multiplication adding result corollary show sesquilinear form c-vector space v uniquely determined value v v v definition hermitian transpose ai j cn matrix h j cm n h called hermitian matrix real entry obviously h thus real symmetric matrix also hermitian ai j cn n hermitian particular aii ii n hermitian matrix real diagonal entry hermitian transposition satisfies similar rule usual transposition cp lemma cn b cm λ c following assertion lemma hold h h h ah λ h λ h ab h b h h proof exercise example cn map c v w w h av sesquilinear form matrix representation sesquilinear form defined analogously matrix representation bilinear form cp definition definition let v w c-vector space base vm wn respectively sesquilinear form v w bi j cn bi j v j wi called matrix representation respect base example em n en n canonical base respectively sesquilinear form example ai j cn bi j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>164\\n\\n11 Linear Forms and Bilinear Forms\\n\\n\u0015\\n\u0015 \u0016T\\n\u0015 \u0016H\\n(n) \u0016 bi j = s e(m)\\n= ei(n) Ae(m)\\n= ai j\\n= ei(n) Ae(m) j , ei j j and, hence, [s] B1 ×B2 = A.\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n11.1.\\nLet V be a finite dimensional K -vector space and v ∈ V. Show that f (v) = 0 for all f ∈ V ∗ if and only if v = 0.\\n11.2.\\nConsider the basis B = {10, t − 1, t 2 − t} of the 3-dimensional vector space\\nR[t]≤2 .\\nCompute the dual basis B ∗ to B.\\n\u0012\\n\u0011\\n11.3.\\nLet V be an n-dimensional K -vector space and let v1∗ , . . . , vn∗ be a basis of V ∗ .\\nProve or disprove: There exists a unique basis {v1 , . . . , vn } of V with vi∗ (v j ) = δi j .\\n11.4.\\nLet V be a finite dimensional K -vector space and let f, g ∈ V ∗ with f = 0.\\nShow that g = λ f for a λ ∈ K \\ {0} holds if and only if ker( f ) = ker(g).\\nIs it possible to omit the assumption f = 0?\\n11.5.\\nLet V be a K -vector space and let U be a subspace of V. The set\\nU 0 := { f ∈ V ∗ | f (u) = 0 for all u ∈ U} is called the annihilator of U. Show the following assertions:\\n(a) U 0 is a subspace of V ∗ .\\n(b) For subspaces U1 , U2 of V we have\\n(U1 + U2 )0 = U10 ∩ U20 , (U1 ∩ U2 )0 = U10 + U20 , and if U1 ⊆ U2 , then U20 ⊆ U10 .\\n(c) If W is a K -vector space and f ∈ L(V, W), then ker( f ∗ ) = (im( f ))0 .\\n11.6.\\nProve Lemma 11.6 (2) and (3).\\n11.7.\\nLet V and W be K -vector spaces.\\nShow that the set of all bilinear forms on\\nV × W with the operations\\n+ : (β1 + β2 )(v, w) := β1 (v, w) + β2 (v, w),\\n· : (λ · β)(v, w) := λ · β(v, w), is a K -vector space.\\n11.8.\\nLet V and W be K -vector spaces with bases {v1 , . . . , vm } and {w1 , . . . , wn } and corresponding dual bases {v1∗ , . . . , vm∗ } and {w1∗ , . . . , wn∗ }, respectively.\\nFor i = 1, . . . , m and j = 1, . . . , n let\\nβi j : V × W → K , (v, w) \u0005→ vi∗ (v)w ∗j (w).\\n(a) Show that βi j is a bilinear form on V × W.</td>\n",
       "      <td>166.0</td>\n",
       "      <td>166</td>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.3 Sesquilinear Forms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear form bilinear form n bi j e ei n ae ai j ei n ae j ei j j hence exercise following exercise k arbitrary field let v finite dimensional k space v show f v f v v consider basis b vector space r compute dual basis b b let v n-dimensional k space let basis v prove disprove exists unique basis vn v v j δi j let v finite dimensional k space let f g v f show g λ f λ k hold ker f ker g possible omit assumption f let v k space let u subspace set u f v f u u u called annihilator u show following assertion u subspace v b subspace v c w k space f l v w ker f im f prove lemma let v w k space show set bilinear form v w operation v w v w v w λ β v w λ β v w k space let v w k space base vm wn corresponding dual base respectively j n let βi j v w k v w v w w show βi j bilinear form v w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>11.3 Sesquilinear Forms\\n\\n165\\n\\n(b) Show that the set {βi j | i = 1, . . . , m, j = 1, . . . , n} is a basis of the\\nK -vector space of bilinear forms on V × W (cp.\\nExercise 11.7) and determine the dimension of this space.\\n11.9.\\nLet V be the R-vector space of the continuous and real valued functions on the real interval [α, β].\\nShow that\\n\u0002 β f (x)g(x)d x,\\nβ : V × V → R, ( f, g) \u0005→\\nα is a symmetric bilinear form on V. Is β degenerate?\\n11.10.\\nShow that the map β from (1) in Example 11.10 is a bilinear form, and show that it is non-degenerate if and only if n = m and A ∈ G L n (K ).\\n11.11.\\nLet V be a finite dimensional K -vector space.\\nShow that V, V ∗ is a dual pair with respect to the bilinear form β from (3) in Example 11.10, i.e., that the bilinear form β is non-degenerate.\\n11.12.\\nLet V be a finite dimensional K -vector space and let U ⊆ V and W ⊆ V ∗ be subspaces with dim(U) = dim(W) ≥ 1.\\nProve or disprove: The spaces\\nU, W form a dual pair with respect to the bilinear form β : U × W → K ,\\n(v, h) \u0005→ h(v).\\n11.13.\\nLet V and W be finite dimensional K -vector spaces with the bases B1 and\\nB2 , respectively, and let β be a bilinear form on V × W.\\n\\n11.14.\\n11.15.\\n11.16.\\n\\n11.17.\\n\\n(a) Show that the following statements are equivalent:\\n(1) [β] B1 ×B2 is not invertible.\\n(2) β is degenerate in the second variable.\\n(3) β is degenerate in the first variable.\\n(b) Conclude from (a): β is non-degenerate if and only if [β] B1 ×B2 is invertible.\\nProve Lemma 11.16.\\nProve Lemma 11.13.\\nFor a bilinear form β on a K -vector space V, the map qβ : V → K , v \u0005→ β(v, v), is called the quadratic form induced by β.\\nShow the following assertion:\\nIf 1+1 = 0 in K and β is symmetric, then β(v, w) = 21 (qβ (v +w)−qβ (v)− qβ (w)) holds for all v, w ∈ V.\\nShow that a sesquilinear form s on a C-vector space V satisfies the polarization identity s(v, w) =\\n\\n\u0016\\n1\u0015 s(v +w, v +w)−s(v −w, v −w)+is(v +iw, v +iw)−is(v −iw, v −iw)\\n4 for all v, w ∈ V.\\n11.18.\\nConsider the following maps from C3,1 × C3,1 to C:\\n(a) β1 (x, y) = 3x1 x 1 + 3y1 y 1 + x2 y 3 − x3 y 2 ,\\n(b) β2 (x, y) = x1 y 2 + x2 y 3 + x3 y 1 ,\\n(c) β3 (x, y) = x1 y2 + x2 y3 + x3 y1 ,</td>\n",
       "      <td>167.0</td>\n",
       "      <td>167</td>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.3 Sesquilinear Forms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sesquilinear form b show set βi j j n basis k space bilinear form v w cp exercise determine dimension space let v r-vector space continuous real valued function real interval α β show β f x g x x β v v r f g α symmetric bilinear form β degenerate show map β example bilinear form show non-degenerate n g l n k let v finite dimensional k space show v v dual pair respect bilinear form β example bilinear form β non-degenerate let v finite dimensional k space let u v w v subspace dim u dim w prove disprove space u w form dual pair respect bilinear form β u w k v h h v let v w finite dimensional k space base respectively let β bilinear form v show following statement equivalent β invertible β degenerate second variable β degenerate first variable b conclude β non-degenerate β invertible prove lemma prove lemma bilinear form β k space v map qβ v k v β v v called quadratic form induced β show following assertion k β symmetric β v w qβ v v qβ w hold v w show sesquilinear form c-vector space v satisfies polarization identity v w v v v v v v v v v w consider following map c x x b x c x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>166\\n\\n11 Linear Forms and Bilinear Forms\\n\\n(d) β4 (x, y) = 3x1 y 1 + x1 y 2 + x2 y 1 + 2ix2 y 3 − 2ix3 y 2 + x3 y 3 .\\nWhich of these are bilinear forms or sesquilinear forms on C3,1 ?\\nTest whether the bilinear form is symmetric or the sesquilinear form is Hermitian, and derive the corresponding matrix representations with respect to the canonical basis B1 = {e1 , e2 , e3 } and the basis B2 = {e1 , e1 + ie2 , e2 + ie3 }.\\n11.19.\\nProve Lemma 11.21.\\n11.20.\\nLet A ∈ Cn,n be Hermitian.\\nShow that s : Cn,1 × Cn,1 , (v, w) \u0005→ w H Av, is a Hermitian sesquilinear form on Cn,1 .\\n11.21.\\nLet V be a finite dimensional C-vector space with the basis B, and let s be a sesquilinear form on V. Show that s is Hermitian if and only if [s] B×B is\\nHermitian.\\n11.22.\\nShow the following assertions for A, B ∈ Cn,n :\\n(a) If A H = −A, then the eigenvalues of A are purely imaginary.\\n(b) If A H = −A, then trace(A2 ) ≤ 0 and (trace(A))2 ≤ 0.\\n(c) If A H = A and B H = B, then trace((AB)2 ) ≤ trace(A2 B 2 ).</td>\n",
       "      <td>168.0</td>\n",
       "      <td>168</td>\n",
       "      <td>11 Linear Forms and Bilinear Forms</td>\n",
       "      <td>11.3 Sesquilinear Forms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear form bilinear form x bilinear form sesquilinear form test whether bilinear form symmetric sesquilinear form hermitian derive corresponding matrix representation respect canonical basis basis prove lemma let cn n hermitian show v w w h av hermitian sesquilinear form let v finite dimensional c-vector space basis b let sesquilinear form show hermitian hermitian show following assertion b cn n h eigenvalue purely imaginary b h trace trace c h b h b trace ab trace b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Chapter 12\\n\\nEuclidean and Unitary Vector Spaces\\n\\nIn this chapter we study vector spaces over the fields R and C. Using the definition of bilinear and sesquilinear forms, we introduce scalar products on such vector spaces.\\nScalar products allow the extension of well-known concepts from elementary geometry, such as length and angles, to abstract real and complex vector spaces.\\nThis, in particular, leads to the idea of orthogonality and to orthonormal bases of vector spaces.\\nAs an example for the importance of these concepts in many applications we study least-squares approximations.\\n\\n12.1 Scalar Products and Norms\\nWe start with the definition of a scalar product and the Euclidean or unitary vector spaces.\\nDefinition 12.1 Let V be a K -vector space, where either K = R or K = C. A map\\n\u0002·, ·\u0003 : V × V → K , (v, w) \u0005→ \u0002v, w\u0003, is called a scalar product on V, when the following properties hold:\\n(1) If K = R, then \u0002·, ·\u0003 is a symmetric bilinear form.\\nIf K = C, then \u0002·, ·\u0003 is an Hermitian sesquilinear form.\\n(2) \u0002·, ·\u0003 is positive definite, i.e., \u0002v, v\u0003 ≥ 0 holds for all v ∈ V, with equality if and only if v = 0.\\nAn R-vector space with a scalar product is called a Euclidean vector space1 , and a\\nC-vector space with a scalar product is called a unitary vector space.\\nScalar products are sometimes called inner products.\\nNote that \u0002v, v\u0003 is nonnegative and real also when V is a C-vector space.\\nIt is easy to see that a subspace U of\\n1 Euclid of Alexandria (approx.\\n300 BC).\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_12\\n\\n167</td>\n",
       "      <td>169.0</td>\n",
       "      <td>169</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.1 Scalar Products and Norms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter euclidean unitary vector space chapter study vector space field r using definition bilinear sesquilinear form introduce scalar product vector space scalar product allow extension well-known concept elementary geometry length angle abstract real complex vector space particular lead idea orthogonality orthonormal base vector space example importance concept many application study least-squares approximation scalar product norm start definition scalar product euclidean unitary vector space definition let v k space either k r k map v v k v w called scalar product v following property hold k r symmetric bilinear form k c hermitian sesquilinear form positive definite hold v v equality v r-vector space scalar product called euclidean vector c-vector space scalar product called unitary vector space scalar product sometimes called inner product note nonnegative real also v c-vector space easy see subspace u euclid alexandria approx bc springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>168\\n\\n12 Euclidean and Unitary Vector Spaces a Euclidean or unitary vector space V is again a Euclidean or unitary vector space, respectively, when the scalar product on the space V is restricted to the subspace U.\\nExample 12.2\\n(1) A scalar product on Rn,1 is given by\\n\u0002v, w\u0003 := w T v.\\nIt is called the standard scalar product of Rn,1 .\\n(2) A scalar product on Cn,1 is given by\\n\u0002v, w\u0003 := w H v.\\nIt is called the standard scalar product of Cn,1 .\\n(3) For both K = R and K = C,\\n\u0002A, B\u0003 := Spur(B H A) is a scalar product on K n,m .\\n(4) A scalar product on the vector space of the continuous and real valued functions on the real interval [α, β] is given by\\n\u0002\\n\u0002 f, g\u0003 :=\\n\\nα\\n\\nβ f (x)g(x)d x.\\n\\nWe will now show how to use the Euclidean or unitary structure of a vector space in order to introduce geometric concepts such as the length of a vector or the angle between vectors.\\nAs a motivation of a general concept of length we have the absolute value of real numbers, i.e., the map | · | : R → R, x \u0005→ |x|.\\nThis map has the following properties:\\n(1) |λx| = |λ| · |x| for all λ, x ∈ R.\\n(2) |x| ≥ 0 for all x ∈ R, with equality if and only if x = 0.\\n(3) |x + y| ≤ |x| + |y| for all x, y ∈ R.\\nThese properties are generalized to real or complex vector spaces as follows.\\nDefinition 12.3 Let V be a K -vector space, where either K = R or K = C. A map\\n·\\n\\n: V → R, v \u0005→ v ,</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.1 Scalar Products and Norms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>euclidean unitary vector space euclidean unitary vector space v euclidean unitary vector space respectively scalar product space v restricted subspace u example scalar product given w called standard scalar product scalar product given w h called standard scalar product k r k c spur b h scalar product k n scalar product vector space continuous real valued function real interval α β given f α β f x g x x show use euclidean unitary structure vector space order introduce geometric concept length vector angle vector motivation general concept length absolute value real number map r r x map following property λ x x r equality x x property generalized real complex vector space follows definition let v k space either k r k map v r v v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>12.1 Scalar Products and Norms\\n\\n169 is called a norm on V, when for all v, w ∈ V and λ ∈ K the following properties hold:\\n(1)\\n(2)\\n(3)\\n\\nλv = |λ| · v .\\nv ≥ 0, with equality if and only if v = 0.\\nv + w ≤ v + w (triangle inequality).\\n\\nA K -vector space on which a norm is defined is called a normed space.\\nExample 12.4\\n(1) If \u0002·, ·\u0003 is the standard scalar product on Rn,1 , then v := \u0002v, v\u00031/2 = (v T v)1/2 defines a norm that is called the Euclidean norm of Rn,1 .\\n(2) If \u0002·, ·\u0003 is the standard scalar product on Cn,1 , then v := \u0002v, v\u00031/2 = (v H v)1/2 defines a norm that is called the Euclidean norm of Cn,1 .\\n(This is common terminology, although the space itself is unitary and not Euclidean.)\\n(3) For both K = R and K = C,\\nA\\n\\nF\\n\\n:= (trace(A H A))1/2 = n \u0004 m\\n\u0003\u0004\\n\\n|ai j |2\\n\\n\u00051/2 i=1 j=1 is a norm on K n,m that is called the Frobenius norm2 of K n,m .\\nFor m = 1 the\\nFrobenius norm is equal to the Euclidean norm of K n,1 .\\nMoreover, the Frobenius norm of K n,m is equal to the Euclidean norm of K nm,1 (or K nm ), if we identify these vector spaces via an isomorphism.\\nObviously, we have A F = A T F = A H F for all A ∈ K n,m .\\n(4) If V is the vector space of the continuous and real valued functions on the real interval [α, β], then\\n\u0003\u0002 β\\n\u00051/2\\n( f (x))2 d x f := \u0002 f, f \u00031/2 =\\nα is a norm on V that is called the L 2 -norm.\\n(5) Let K = R or K = C, and let p ∈ R, p ≥ 1 be given.\\nThen for v = [ν1 , . . . , νn ]T ∈ K n,1 the p-norm of K n,1 is defined by v p\\n\\n:= n\\n\u0003\u0004 i=1\\n\\n2 Ferdinand\\n\\nGeorg Frobenius (1849–1917).\\n\\n|νi | p\\n\\n\u00051/ p\\n\\n.\\n\\n(12.1)</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.1 Scalar Products and Norms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scalar product norm called norm v v w v λ k following property hold λv v v equality v v w v w triangle inequality k space norm defined called normed space example standard scalar product v v v defines norm called euclidean norm standard scalar product v v h v defines norm called euclidean norm common terminology although space unitary euclidean k r k c f trace h n j norm k n called frobenius k n frobenius norm equal euclidean norm k moreover frobenius norm k n equal euclidean norm k k nm identify vector space via isomorphism obviously f f h f k n v vector space continuous real valued function real interval α β β f x x f f f α norm v called l let k r k c let p r p given v νn k p-norm k defined v p n ferdinand georg frobenius p p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>170\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\nFor p = 2 this is the Euclidean norm on K n,1 .\\nFor this norm we typically omit the index 2 and write · instead of · 2 (as in (1) and (2) above).\\nTaking the limit p → ∞ in (12.1), we obtain the ∞-norm of K n,1 , given by v\\n\\n∞\\n\\n= max |νi |.\\n1≤i≤n\\n\\nThe following figures illustrate the unit circle in R2,1 with respect to the p-norm, i.e., the set of all v ∈ R2,1 with v p = 1, for p = 1, p = 2 and p = ∞:\\n\\n(6) For K = R or K = C the p-norm of K n,m is defined by\\nA p\\n\\n:= sup v∈K m,1 \\{0}\\n\\nAv p\\n.\\nv p\\n\\nHere we use the p-norm of K m,1 in the denominator and the p-norm of K n,1 in the numerator.\\nThe notation sup means supremum, i.e., the least upper bound that is known from Analysis.\\nOne can show that the supremum is attained by a vector v, and thus we may write max instead of sup in the definition above.\\nIn particular, for A = [ai j ] ∈ K n,m we have\\nA\\n\\nA\\n\\n1\\n\\n∞\\n\\n= max\\n\\n1≤ j≤m\\n\\n= max\\n\\n1≤i≤n n\\n\u0004 i=1 m\\n\u0004\\n\\n|ai j |,\\n\\n|ai j |.\\nj=1\\n\\nThese norms are called maximum column sum and maximum row sum norm of K n,m , respectively.\\nWe easily see that A 1 = A T ∞ = A H ∞ and\\nA ∞ = A T 1 = A H 1 .\\nHowever, for the matrix\\n\u0006\\nA=\\n\\n1/2 −1/4\\n−1/2 2/3\\n\\n\u0007\\n∈ R2,2 we have A 1 = 1 and A ∞ = 7/6.\\nThus, this matrix A satisfies A 1 &lt; A ∞ and A T ∞ &lt; A T 1 .\\nThe 2-norm of matrices will be considered further in\\nChap.\\n19.</td>\n",
       "      <td>172.0</td>\n",
       "      <td>172</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.1 Scalar Products and Norms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>euclidean unitary vector space p euclidean norm k norm typically omit index write instead taking limit p obtain k given v max following figure illustrate unit circle respect p-norm set v v p p p p k r k c p-norm k n defined p sup av p v p use p-norm k denominator p-norm k numerator notation sup mean supremum least upper bound known analysis one show supremum attained vector v thus may write max instead sup definition particular ai j k n max max n j j norm called maximum column sum maximum row sum norm k n respectively easily see h h however matrix thus matrix satisfies matrix considered chap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>12.1 Scalar Products and Norms\\n\\n171\\n\\nThe norms in the above examples (1)–(4) have the form v = \u0002v, v\u00031/2 , where\\n\u0002·, ·\u0003 is a given scalar product.\\nWe will show now that the map v \u0005→ \u0002v, v\u00031/2 always defines a norm.\\nOur proof is based on the following theorem.\\nTheorem 12.5 If V is a Euclidean or unitary vector space with the scalar product\\n\u0002·, ·\u0003, then\\n|\u0002v, w\u0003|2 ≤ \u0002v, v\u0003 · \u0002w, w\u0003 for all v, w ∈ V,\\n(12.2) with equality if and only if v, w are linearly dependent.\\nProof The inequality is trivial for w = 0.\\nThus, let w = 0 and let\\nλ :=\\n\\n\u0002v, w\u0003\\n.\\n\u0002w, w\u0003\\n\\nThen\\n0 ≤ \u0002v − λw, v − λw\u0003 = \u0002v, v\u0003 − λ\u0002v, w\u0003 − λ\u0002w, v\u0003 − λ(−λ)\u0002w, w\u0003\\n\u0002v, w\u0003\\n|\u0002v, w\u0003|2\\n\u0002v, w\u0003\\n\u0002v, w\u0003 +\\n\u0002w, w\u0003\\n\u0002v, w\u0003 −\\n\u0002w, w\u0003\\n\u0002w, w\u0003\\n\u0002w, w\u00032\\n|\u0002v, w\u0003|2\\n,\\n= \u0002v, v\u0003 −\\n\u0002w, w\u0003\\n\\n= \u0002v, v\u0003 − which implies (12.2).\\nIf v, w are linearly dependent, then v = λw for a scalar λ, and hence\\n|\u0002v, w\u0003|2 = |\u0002λw, w\u0003|2 = |λ\u0002w, w\u0003|2 = |λ|2 |\u0002w, w\u0003|2 = λλ \u0002w, w\u0003 \u0002w, w\u0003\\n= \u0002λw, λw\u0003 \u0002w, w\u0003 = \u0002v, v\u0003 \u0002w, w\u0003.\\nOn the other hand, let |\u0002v, w\u0003|2 = \u0002v, v\u0003\u0002w, w\u0003.\\nIf w = 0, then v, w are linearly dependent.\\nIf w = 0, then we define λ as above and get\\n\u0002v − λw, v − λw\u0003 = \u0002v, v\u0003 −\\n\\n|\u0002v, w\u0003|2\\n= 0.\\n\u0002w, w\u0003\\n\\nSince the scalar product is positive definite, we have v − λw = 0, and thus v, w are linearly dependent.\\nThe inequality (12.2) is called Cauchy-Schwarz inequality.3 It is an important tool in Analysis, in particular in the estimation of approximation and interpolation errors.\\n\\n3 Augustin\\n\\nLouis Cauchy (1789–1857) and Hermann Amandus Schwarz (1843–1921).</td>\n",
       "      <td>173.0</td>\n",
       "      <td>173</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.1 Scalar Products and Norms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scalar product norm norm example form v given scalar product show map v always defines norm proof based following theorem theorem v euclidean unitary vector space scalar product v w v equality v w linearly dependent proof inequality trivial w thus let w let λ λw v λ implies v w linearly dependent v λw scalar λ hence λλ hand let w v w linearly dependent w define λ get λw v since scalar product positive definite v λw thus v w linearly dependent inequality called cauchy-schwarz important tool analysis particular estimation approximation interpolation error augustin louis cauchy hermann amandus schwarz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>172\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\nCorollary 12.6 If V is a Euclidean or unitary vector space with the scalar product\\n\u0002·, ·\u0003, then the map\\n: V → R, v \u0005→ v := \u0002v, v\u00031/2 ,\\n\\n· is a norm on V that is called the norm induced by the scalar product.\\nProof We have to prove the three defining properties of the norm.\\nSince \u0002·, ·\u0003 is positive definite, we have v ≥ 0, with equality if and only if v = 0.\\nIf v ∈ V and\\nλ ∈ K (where in the Euclidean case K = R and in the unitary case K = C), then\\nλv\\n\\n2\\n\\n= \u0002λv, λv\u0003 = λλ\u0002v, v\u0003 = |λ|2 \u0002v, v\u0003, and hence λv = |λ| v .\\nIn order to show the triangle inequality, we use the\\nCauchy-Schwarz inequality and the fact that Re(z) ≤ |z| for every complex number z.\\nFor all v, w ∈ V we have v+w\\n\\n2\\n\\n= \u0002v + w, v + w\u0003 = \u0002v, v\u0003 + \u0002v, w\u0003 + \u0002w, v\u0003 + \u0002w, w\u0003\\n= \u0002v, v\u0003 + \u0002v, w\u0003 + \u0002v, w\u0003 + \u0002w, w\u0003\\n= v\\n\\n2\\n\\n+ 2 Re(\u0002v, w\u0003) + w\\n\\n≤ v\\n\\n2\\n\\n+ 2 |\u0002v, w\u0003| + w\\n\\n2\\n\\n≤ v\\n\\n2\\n\\n+2 v\\n\\n2 w + w\\n\\n2\\n\\n= ( v + w )2 , and thus v + w ≤ v + w .\\n\\n12.2 Orthogonality\\nWe will now use the scalar product to introduce angles between vectors.\\nAs motivation we consider the Euclidean vector space R2,1 with the standard scalar product and the induced Euclidean norm v = \u0002v, v\u00031/2 .\\nThe Cauchy-Schwarz inequality shows that\\n−1 ≤\\n\\n\u0002v, w\u0003\\n≤ 1 for all v, w ∈ R2,1 \\ {0}.\\nv w\\n\\nIf v, w ∈ R2,1 \\ {0}, then the angle between v and w is the uniquely determined real number ϕ ∈ [0, π] with cos(ϕ) =\\n\\n\u0002v, w\u0003\\n.\\nv w</td>\n",
       "      <td>174.0</td>\n",
       "      <td>174</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.2 Orthogonality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>euclidean unitary vector space corollary v euclidean unitary vector space scalar product map v r v v norm v called norm induced scalar product proof prove three defining property norm since positive definite v equality v v v λ k euclidean case k r unitary case k c λv hence λv v order show triangle inequality use cauchy-schwarz inequality fact z every complex number z v w v w v v w v w v v w w v w thus v w v w orthogonality use scalar product introduce angle vector motivation consider euclidean vector space standard scalar product induced euclidean norm v cauchy-schwarz inequality show v w v w v w angle v w uniquely determined real number ϕ π co ϕ v w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>12.2 Orthogonality\\n\\n173\\n\\nThe vectors v, w are orthogonal if ϕ = π/2, so that cos(ϕ) = 0.\\nThus, v, w are orthogonal if and only if \u0002v, w\u0003 = 0.\\nAn elementary calculation now leads to the cosine theorem for triangles: v−w\\n\\n2\\n\\n= \u0002v − w, v − w\u0003 = \u0002v, v\u0003 − 2\u0002v, w\u0003 + \u0002w, w\u0003\\n= v\\n\\n2\\n\\n+ w\\n\\n2\\n\\n−2 v w cos(ϕ).\\n\\nIf v, w are orthogonal, i.e., \u0002v, w\u0003 = 0, then the cosine theorem implies the\\nPythagorean theorem4 : v−w\\n\\n2\\n\\n= v\\n\\n2\\n\\n+ w 2.\\n\\nThe following figures illustrate the cosine theorem and the Pythagorean theorem for vectors in R2,1 :\\n\\nIn the following definition we generalize the ideas of angles and orthogonality.\\nDefinition 12.7 Let V be a Euclidean or unitary vector space with the scalar product\\n\u0002·, ·\u0003.\\n(1) In the Euclidean case, the angle between two vectors v, w ∈ V \\ {0} is the uniquely determined real number ϕ ∈ [0, π] with cos(ϕ) =\\n\\n\u0002v, w\u0003\\n.\\nv w\\n\\n(2) Two vectors v, w ∈ V are called orthogonal, if \u0002v, w\u0003 = 0.\\n(3) A basis {v1 , . . . , vn } of V is called an orthogonal basis, if\\n\u0002vi , v j \u0003 = 0, i, j = 1, . . . , n and i = j.\\nIf, furthermore, vi = 1, i = 1, . . . , n, where v = \u0002v, v\u00031/2 is the norm induced by the scalar product, then\\n{v1 , . . . , vn } is called an orthonormal basis of V. (For an orthonormal basis we therefore have \u0002vi , v j \u0003 = δi j .)\\n4 Pythagoras of Samos (approx.\\n570–500 BC).</td>\n",
       "      <td>175.0</td>\n",
       "      <td>175</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.2 Orthogonality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>orthogonality vector v w orthogonal ϕ co ϕ thus v w orthogonal elementary calculation lead cosine theorem triangle w v v w v w co ϕ v w orthogonal cosine theorem implies pythagorean v w following figure illustrate cosine theorem pythagorean theorem vector following definition generalize idea angle orthogonality definition let v euclidean unitary vector space scalar product euclidean case angle two vector v w v uniquely determined real number ϕ π co ϕ v w two vector v w v called orthogonal basis vn v called orthogonal basis v j j n j furthermore vi n v norm induced scalar product vn called orthonormal basis orthonormal basis therefore v j δi j pythagoras samos approx bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>174\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\nNote that the terms in (1)−(3) are defined with respect to the given scalar product.\\nDifferent scalar products yield different angles between vectors.\\nIn particular, the orthogonality of two given vectors may be lost when we consider a different scalar product.\\nExample 12.8 The standard basis vectors e1 , e2 ∈ R2,1 are orthogonal and {e1 , e2 } is an orthonormal basis of R2,1 with respect to the standard scalar product (cp.\\n(1) in\\nExample 12.2).\\nConsider the symmetric and invertible matrix\\n\u0006 \u0007\\n21\\nA=\\n∈ R2,2 ,\\n12 which defines a symmetric and non-degenerate bilinear form on R2,1 by\\n(v, w) \u0005→ w T Av\\n(cp.\\n(1) in Example 11.10).\\nThis bilinear form is positive definite, since for all v =\\n[ν1 , ν2 ]T ∈ R2,1 we have v T Av = ν12 + ν22 + (ν1 + ν2 )2 .\\nThe bilinear form therefore is a scalar product on R2,1 , which we denote by \u0002·, ·\u0003 A .\\nWe denote the induced norm by · A .\\nWith respect to the scalar product \u0002·, ·\u0003 A the vectors e1 , e2 satisfy\\n\u0002e1 , e1 \u0003 A = e1T Ae1 = 2, \u0002e2 , e2 \u0003 A = e2T Ae2 = 2, \u0002e1 , e2 \u0003 A = e2T Ae1 = 1.\\n2,1\\nClearly, {e1 , e2 } is not an\\n√ orthonormal basis of R with respect to \u0002·, ·\u0003 A .\\nAlso note that e1 A = e2 A = 2.\\nOn the other hand, the vectors v1 = [1, 1]T and v2 = [−1, 1]T satisfy\\n\\n\u0002v1 , v1 \u0003 A = v1T Av1 = 6, \u0002v2 , v2 \u0003 A = v2T Av2 = 2, \u0002v1 , v2 \u0003 A = v2T Av1 = 0.\\n√\\n√\\nHence v1 A = 6 and v2 A = 2, so that {6−1/2 v1 , 2−1/2 v2 } is an orthonormal basis of R2,1 with respect to the scalar product \u0002·, ·\u0003 A\\nWe now show that every finite dimensional Euclidean or unitary vector space has an orthonormal basis.\\nTheorem 12.9 Let V be a Euclidean or unitary vector space with the basis\\n{v1 , . . . , vn }.\\nThen there exists an orthonormal basis {u 1 , . . . , u n } of V with span{u 1 , . . . , u k } = span{v1 , . . . , vk }, k = 1, . . . , n.</td>\n",
       "      <td>176.0</td>\n",
       "      <td>176</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.2 Orthogonality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>euclidean unitary vector space note term defined respect given scalar product different scalar product yield different angle vector particular orthogonality two given vector may lost consider different scalar product example standard basis vector orthogonal orthonormal basis respect standard scalar product cp example consider symmetric invertible matrix defines symmetric non-degenerate bilinear form v w w av cp example bilinear form positive definite since v v av bilinear form therefore scalar product denote denote induced norm respect scalar product vector satisfy clearly orthonormal basis r respect also note hand vector satisfy hence orthonormal basis respect scalar product show every finite dimensional euclidean unitary vector space orthonormal basis theorem let v euclidean unitary vector space basis vn exists orthonormal basis u u n v span u u k span vk k n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>12.2 Orthogonality\\n\\n175\\n\\nProof We give the proof by induction on dim(V) = n.\\nIf n = 1, then we set u 1 := v1 −1 v1 .\\nThen u 1 = 1, and {u 1 } is an orthonormal basis of V with span{u 1 } = span{v1 }.\\nLet the assertion hold for an n ≥ 1.\\nLet dim(V) = n + 1 and let {v1 , . . . , vn+1 } be a basis of V. Then Vn := span{v1 , . . . , vn } is an n-dimensional subspace of V. By the induction hypothesis there exists an orthonormal basis {u 1 , . . . , u n } of Vn with span{u 1 , . . . , u k } = span{v1 , . . . , vk } for k = 1, . . . , n.\\nWe define\\n\b u n+1 := vn+1 − n\\n\u0004\\n\u0002vn+1 , u k \u0003u k , u n+1 := ||\b u n+1 ||−1\b u n+1 .\\nk=1\\n\\nSince vn+1 ∈\\n/ Vn = span{u 1 , . . . , u n }, we must have \b u n+1 = 0, and Lemma 9.16 yields span{u 1 , . . . , u n+1 } = span{v1 , . . . , vn+1 }.\\nFor j = 1, . . . , n we have u n+1\\n\u0002u n+1 , u j \u0003 = \u0002 \b\\n\\n−1\\n\\n\b u n+1 , u j \u0003\\n\\n= \b u n+1\\n\\n−1\\n\\n= \b u n+1\\n\\n−1 n\\n\u0004\\n\u0002vn+1 , u k \u0003 \u0002u k , u j \u0003\\n\u0002vn+1 , u j \u0003 − k=1\\n\\n\u0002vn+1 , u j \u0003 − \u0002vn+1 , u j \u0003\\n\\n= 0.\\nu n+1\\nFinally, \u0002u n+1 , u n+1 \u0003 = \b\\n\\n−2\\n\\n\u0002\b u n+1 , \b u n+1 \u0003 = 1 which completes the proof.\\n\\nThe proof of Theorem 12.9 shows how a given basis {v1 , . . . , vn } can be orthonormalized, i.e., transformed into an orthonormal basis {u 1 , . . . , u n } with span{u 1 , . . . , u k } = span{v1 , . . . , vk }, k = 1, . . . , n.\\nThe resulting algorithm is called the Gram-Schmidt method 5 :\\nAlgorithm 12.10 Given a basis {v1 , . . . , vn } of V.\\n(1) Set u 1 := v1 −1 v1 .\\n(2) For j = 1, . . . , n − 1 set\\n\b u j+1 := v j+1 − u j+1 := \b u j+1\\n\\n5 Jørgen j\\n\u0004\\n\u0002v j+1 , u k \u0003u k , k=1\\n−1\\n\\n\b u j+1 .\\n\\nPedersen Gram (1850–1916) and Erhard Schmidt (1876–1959).</td>\n",
       "      <td>177.0</td>\n",
       "      <td>177</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.2 Orthogonality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>orthogonality proof give proof induction dim v n set u u u orthonormal basis v span u span let assertion hold n let dim v n let basis vn span vn n-dimensional subspace induction hypothesis exists orthonormal basis u u n vn span u u k span vk k define u n u k k u u u since vn span u u n must u lemma yield span u u span j n u u j u u j u u n u k k u j u j u j u j u finally u u u completes proof proof theorem show given basis vn orthonormalized transformed orthonormal basis u u n span u u k span vk k resulting algorithm called gram-schmidt method algorithm given basis vn set u j n set u v u u jørgen j u k k u pedersen gram erhard schmidt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>176\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\nA slight reordering and combination of steps in the Gram-Schmidt method yields\\n⎛\\n\\n⎞ v1 \u0002v2 , u 1 \u0003 . . . \u0002vn , u 1 \u0003\\n⎜\\n⎟\\n..\\n..\\n⎜\\n⎟\\n.\\n.\\n\b u2\\n⎟.\\n(v1 , v2 , . . . , vn ) = (u 1 , u 2 , . . . , u n ) ⎜\\n⎜\\n⎟\\n..\\n\u000e\u000f\\n\u0010\\n\u000e\u000f\\n\u0010⎝\\n⎠\\n.\\n\u0002v\\n, u\\n\u0003 n n−1\\n∈V n\\n∈V n\\n\b un\\nThe upper triangular matrix on the right hand side is the coordinate transformation matrix from the basis {v1 , . . . , vn } to the basis {u 1 , . . . , u n } of V (cp.\\nTheorem 9.25 or 10.2).\\nThus, we have shown the following result.\\nTheorem 12.11 If V is a finite dimensional Euclidean or unitary vector space with a given basis B1 , then the Gram-Schmidt method applied to B1 yields an orthonormal basis B2 of V, such that [IdV ] B1 ,B2 is an invertible upper triangular matrix.\\nConsider an m-dimensional subspace of Rn,1 or Cn,1 with the standard scalar product \u0002·, ·\u0003, and write the m vectors of an orthonormal basis {q1 , . . . , qm } as columns of a matrix, Q := [q1 , . . . , qm ].\\nThen we obtain in the real case\\nQ T Q = [qiT q j ] = [\u0002q j , qi \u0003] = [δ ji ] = Im , and analogously in the complex case\\nQ H Q = [qiH q j ] = [\u0002q j , qi \u0003] = [δ ji ] = Im .\\nIf, on the other hand, Q T Q = Im or Q H Q = Im for a matrix Q ∈ Rn,m or Q ∈ Cn,m , respectively, then the m columns of Q form an orthonormal basis (with respect to the standard scalar product) of an m-dimensional subspace of Rn,1 or Cn,1 , respectively.\\nA “matrix version” of Theorem 12.11 can therefore be formulated as follows.\\nCorollary 12.12 Let K = R or K = C and let v1 , . . . , vm ∈ K n,1 be linearly independent.\\nThen there exists a matrix Q ∈ K n,m with its m columns being orthonormal with respect to the standard scalar product of K n,1 , i.e., Q T Q = Im for K = R or\\nQ H Q = Im for K = C, and an upper triangular matrix R ∈ G L m (K ), such that\\n[v1 , . . . , vm ] = Q R.\\n\\n(12.3)\\n\\nThe factorization (12.3) is called a Q R-decomposition of the matrix [v1 , . . . , vm ].\\nThe Q R-decomposition has many applications in Numerical Mathematics (cp.\\nExample 12.16 below).\\nLemma 12.13 Let K = R or K = C and let Q ∈ K n,m be a matrix with orthonormal columns with respect to the standard scalar product of K n,1 .\\nThen v = Qv holds for all v ∈ K m,1 .\\n(Here · is the Euclidean norm of K m,1 and of K n,1 .)</td>\n",
       "      <td>178.0</td>\n",
       "      <td>178</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.2 Orthogonality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>euclidean unitary vector space slight reordering combination step gram-schmidt method yield u u vn u u u n u n n n un upper triangular matrix right hand side coordinate transformation matrix basis vn basis u u n v cp theorem thus shown following result theorem v finite dimensional euclidean unitary vector space given basis gram-schmidt method applied yield orthonormal basis v idv invertible upper triangular matrix consider m-dimensional subspace standard scalar product write vector orthonormal basis qm column matrix q qm obtain real case q q qit q j j qi δ ji im analogously complex case q h q qih q j j qi δ ji im hand q q im q h q im matrix q rn q cn respectively column q form orthonormal basis respect standard scalar product m-dimensional subspace respectively matrix version theorem therefore formulated follows corollary let k r k c let vm k linearly independent exists matrix q k n column orthonormal respect standard scalar product k q q im k r q h q im k c upper triangular matrix r g l k vm q factorization called q r-decomposition matrix vm q r-decomposition many application numerical mathematics cp example lemma let k r k c let q k n matrix orthonormal column respect standard scalar product k v qv hold v k euclidean norm k k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>12.2 Orthogonality\\n\\n177\\n\\nProof For K = C we have v\\n\\n2\\n\\n= \u0002v, v\u0003 = v H v = v H (Q H Q)v = \u0002Qv, Qv\u0003 = Qv 2 , and the proof for K = R is analogous.\\nWe now introduce two important classes of matrices.\\nDefinition 12.14\\n(1) A matrix Q ∈ Rn,n whose columns form an orthonormal basis with respect to the standard scalar product of Rn,1 is called orthogonal.\\n(2) A matrix Q ∈ Cn,n whose columns form an orthonormal basis with respect to the standard scalar product of Cn,1 is called unitary.\\nA matrix Q = [q1 , . . . , qn ] ∈ Rn,n is therefore orthogonal if and only if\\nQ T Q = [qiT q j ] = [\u0002q j , qi \u0003] = [δ ji ] = In .\\nIn particular, an orthogonal matrix Q is invertible with Q −1 = Q T (cp.\\nCorollary 7.20).\\nThe equation Q Q T = In means that the n rows of Q form an orthonormal basis of R1,n (with respect to the scalar product \u0002v, w\u0003 := wv T ).\\nAnalogously, a unitary matrix Q ∈ Cn,n is invertible with Q −1 = Q H and\\nH\\nQ Q = In = Q Q H .\\nThe n columns of Q form an orthonormal basis of C1,n .\\nLemma 12.15 The sets O(n) of orthogonal and U(n) of unitary n ×n matrices form subgroups of G L n (R) and G L n (C), respectively.\\nProof We consider only O(n); the proof for U(n) is analogous.\\nSince every orthogonal matrix is invertible, we have that O(n) ⊂ G L n (R).\\nThe identity matrix In is orthogonal, and hence In ∈ O(n) = Ø.\\nIf Q ∈ O(n), then also\\nQ T = Q −1 ∈ O(n), since (Q T )T Q T = Q Q T = In .\\nFinally, if Q 1 , Q 2 ∈ O(n), then\\n(Q 1 Q 2 )T (Q 1 Q 2 ) = Q 2T (Q 1T Q 1 )Q 2 = Q 2T Q 2 = In , and thus Q 1 Q 2 ∈ O(n).\\nExample 12.16 In many applications measurements or samples lead to a data set that is represented by tuples (τi , μi ) ∈ R2 , i = 1, . . . , m.\\nHere τ1 &lt; · · · &lt; τm , are the pairwise distinct measurement points and μ1 , . . . , μm are the corresponding measurements.\\nIn order to approximate the given data set by a simple model, one can try to construct a polynomial p of small degree so that the values p(τ1 ), . . . , p(τm ) are as close as possible to the measurements μ1 , . . . , μm .\\nThe simplest case is a real polynomial of degree (at most) 1.\\nGeometrically, this corresponds to the construction of a straight line in R2 that has a minimal distance</td>\n",
       "      <td>179.0</td>\n",
       "      <td>179</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.2 Orthogonality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>orthogonality proof k c v v h v v h q h q v qv proof k r analogous introduce two important class matrix definition matrix q rn n whose column form orthonormal basis respect standard scalar product called orthogonal matrix q cn n whose column form orthonormal basis respect standard scalar product called unitary matrix q qn rn n therefore orthogonal q q qit q j j qi δ ji particular orthogonal matrix q invertible q q cp corollary equation q q mean n row q form orthonormal basis n respect scalar product wv analogously unitary matrix q cn n invertible q q h h q q q q h n column q form orthonormal basis n lemma set n orthogonal u n unitary n matrix form subgroup g l n r g l n c respectively proof consider n proof u n analogous since every orthogonal matrix invertible n g l n r identity matrix orthogonal hence n ø q n also q q n since q q q q finally q q n q q q q q q q q q q thus q q n example many application measurement sample lead data set represented tuples τi μi τm pairwise distinct measurement point μm corresponding measurement order approximate given data set simple model one try construct polynomial p small degree value p p τm close possible measurement μm simplest case real polynomial degree geometrically corresponds construction straight line minimal distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>178\\n\\n12 Euclidean and Unitary Vector Spaces to the given points, as shown in the figure below (cp.\\nSect.\\n1.4).\\nThere are many possibilities to measure the distance.\\nIn the following we will describe one of them in more detail and use the Gram-Schmidt method, or the Q R-decomposition, for the construction of the straight line.\\nIn Statistics this method is called linear regression.\\n\\nA real polynomial of degree (at most) 1 has the form p = αt + β and we are looking for coefficients α, β ∈ R with p(τi ) = ατi + β ≈ μi , i = 1, . . . , m.\\nUsing matrices we can write this problem as\\n⎡\\n\\n⎤\\n⎡ ⎤\\n1 \u0006 \u0007\\nμ1\\n\u0006 \u0007\\n.. ⎥ α ≈ ⎢ .. ⎥ or [v , v ] α ≈ y.\\n⎣ . ⎦\\n1 2\\n.⎦ β\\nβ\\nμm\\nτm 1\\n\\nτ1\\n⎢ ..\\n⎣ .\\n\\nAs mentioned above, there are different possibilities for interpreting the symbol “≈”.\\nIn particular, there are different norms in which we can measure the distance between the given values μ1 , . . . , μm and the polynomial values p(τ1 ), . . . , p(τm ).\\nHere we will use the Euclidean norm · and consider the minimization problem\\n\u001d",
       "\\n\u001d",
       "\\n\u0006 \u0007\\n\u001d",
       "\\n\u001d",
       "\\nα min [v1 , v2 ]\\n− y\u001d",
       "\\n\u001d",
       ".\\nβ\\nα,β∈R \u001d",
       "\\nThe vectors v1 , v2 ∈ Rm,1 are linearly independent, since the entries of v1 are pairwise distinct, while all entries of v2 are equal.\\nLet\\n[v1 , v2 ] = [q1 , q2 ]R be a Q R-decomposition.\\nWe extend the vectors q1 , q2 ∈ Rm,1 to an orthonormal basis {q1 , q2 , q3 , . . . , qm } of Rm,1 .\\nThen Q = [q1 , . . . , qm ] ∈ Rm,m is an orthogonal matrix and</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.2 Orthogonality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>euclidean unitary vector space given point shown figure cp sect many possibility measure distance following describe one detail use gram-schmidt method q r-decomposition construction straight line statistic method called linear regression real polynomial degree form p αt β looking coefficient α β r p τi ατi β μi using matrix write problem α v v α β β μm τm mentioned different possibility interpreting symbol particular different norm measure distance given value μm polynomial value p p τm use euclidean norm consider minimization problem α min β α vector linearly independent since entry pairwise distinct entry equal let r q r-decomposition extend vector orthonormal basis qm q qm rm orthogonal matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>12.2 Orthogonality\\n\\n179\\n\\n\u001d",
       "\\n\u001d",
       "\\n\u0006 \u0007\\n\u001d",
       "\\n\u001d",
       "\\nα\\n\u001d",
       " = min\\n[v\\n− y min \u001d",
       "\\n, v\\n]\\n1 2\\n\u001d",
       "\\n\u001d",
       " α,β∈R\\nβ\\nα,β∈R\\n\\n\u001d",
       "\\n\u001d",
       "\\n\u0006 \u0007\\n\u001d",
       "\\n\u001d",
       "\\n\u001d",
       " [q1 , q2 ]R α − y \u001d",
       "\\n\u001d",
       "\\n\u001d",
       "\\nβ\\n\u001d",
       " \u0006\\n\u001d",
       "\\n\u0007\u0006 \u0007\\n\u001d",
       "\\n\u001d",
       "\\nR\\nα\\n\u001d",
       "\\n= min \u001d",
       "\\nQ\\n− y\\n\u001d",
       "\\n\u001d",
       "\\n0m−2,2 β\\nα,β∈R\\n\u001d",
       " \u001e",
       "\u0006\\n\u0007\u0006 \u0007\\n\u001f\u001d",
       "\\n\u001d",
       "\\n\u001d",
       "\\nR\\nα\\nT\\n\u001d",
       "\\nQ\\n= min y\\n−\\nQ\\n\u001d",
       "\\n\u001d",
       "\\n0m−2,2 β\\nα,β∈R\\n\u001d",
       "\\n⎡ T ⎤\u001d",
       "\\n\u001d",
       "⎡ \u0006 \u0007 ⎤ q1 y \u001d",
       "\\n\u001d",
       "\\n\u001d",
       "\\n⎢ T ⎥\u001d",
       "\\n\u001d",
       " R α\\n⎥ ⎢q2 y ⎥\u001d",
       "\\n\u001d",
       "⎢\\nβ\\n⎥ ⎢ T ⎥\u001d",
       "\\n\u001d",
       "⎢\\n⎥ ⎢\\n\u001d",
       "⎢\\n⎥\u001d",
       "\\n= min \u001d",
       "⎢ 0 ⎥ − ⎢q3 y ⎥\u001d",
       " .\\nα,β∈R \u001d",
       "⎢\\n.. ⎥ ⎢ . ⎥\u001d",
       "\\n\u001d",
       "⎣ . ⎦ ⎢ . ⎥\u001d",
       "\\n\u001d",
       "\\n⎣ . ⎦\u001d",
       "\\n\u001d",
       "\\n\u001d",
       "\\n0 qT y m\\n\\nHere we have used that Q Q T = Im and Qv = v for all v ∈ Rm,1 .\\nThe upper triangular matrix R is invertible and thus the minimization problem is solved by\\n!\\n\"\\n\u0006 \u0007\\nT q y\\nα\\n1\\n.\\n= R −1 T\\nβ q2 y\\nUsing the definition of the Euclidean norm, we can write the minimizing property of the polynomial p := αt + β as\\n\u001d",
       "\\n\u001d",
       "2 \u0004\\n\u0006 \u0007 m\\n\u001d",
       "\\n\u001d",
       "\\n\u001d",
       " [v1 , v2 ] α − y \u001d",
       " =\\n( p(τi ) − μi )2\\n\u001d",
       "\\n\u001d",
       "\\nβ i=1\\n\\n= min\\n\\nα,β∈R m\\n\u0003\u0004\\n\\n\u0005\\n((ατi + β) − μi )2 .\\ni=1\\n\\nSince the polynomial p minimizes the sum of squares of the distances between the measurements μi and the polynomial values p(τi ), this polynomial yields a least squares approximation of the measurement values.\\nConsider the example from Sect.\\n1.4.\\nIn the four quarters of a year, a company has profits of 10, 8, 9, 11 million Euros.\\nUnder the assumption that the profits grows linearly, i.e., like a straight line, the goal is to estimate the profit in the last quarter of the following year.\\nThe given data leads to the approximation problem\\n⎡\\n\\n1\\n⎢2\\n⎢\\n⎣3\\n4\\n\\n⎤\\n⎡ ⎤\\n1 \u0006 \u0007\\n10\\n\u0006 \u0007\\n⎢8⎥\\n1⎥\\nα\\nα\\n⎥\\n⎢\\n⎥\\n≈ y.\\n≈ ⎣ ⎦ or [v1 , v2 ]\\n1⎦ β\\nβ\\n9\\n1\\n11</td>\n",
       "      <td>181.0</td>\n",
       "      <td>181</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.2 Orthogonality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>orthogonality α min v min v α β α r α β r α min q β α r α q min q β α r α β min α qt used q q im qv v v upper triangular matrix r invertible thus minimization problem solved q α r β using definition euclidean norm write minimizing property polynomial p αt β α p τi μi β min α ατi β μi since polynomial p minimizes sum square distance measurement μi polynomial value p τi polynomial yield least square approximation measurement value consider example sect four quarter year company profit million euro assumption profit grows linearly like straight line goal estimate profit last quarter following year given data lead approximation problem α α β β</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>180\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\nThe numerical computation of a Q R-decomposition of [v1 , v2 ] yields\\n\u0006 \u0007 \u0006√ 1 √ \u0007−1 ! √1\\nα\\n30 3 √30\\n30\\n=\\n√2\\nβ\\n0 13 6\\n6\\n\u000e\u000f\\n\u0010\\n=R −1\\n\\n⎡\\n\\n⎤\\n10\\n\u0006 \u0007\\n√2 √3\\n√4\\n⎢ 8⎥\\n0.4\\n30\\n30\\n30\\n⎢\\n⎥\\n⎣ 9 ⎦ = 8.5 ,\\n√1\\n√1\\n0\\n−\\n6\\n6\\n\u000e\u000f\\n\u0010 11\\n\"\\n\\n=[q1 ,q2 ]T and the resulting profit estimate for the last quarter of the following year is p(8) =\\n11.7, i.e., 11.7 million Euros.\\n\\nMATLAB-Minute.\\nIn Example 12.16 one could imagine that the profit grows quadratically instead of linearly.\\nDetermine, analogously to the procedure in Example 12.16, a polynomial p = αt 2 + βt + γ that solves the least squares problem\\n4\\n\u0004 i=1\\n\\n( p(τi ) − μi )2 = min\\n\\nα,β,γ∈R\\n\\n4 \u0003\\n\u0004\\n(ατi2 + βτi + γ) − μi\\n\\n2\\n\\n.\\ni=1\\n\\nUse the MATLAB command qr for computing a Q R-decomposition, and determine the estimated profit in the last quarter of the following year.\\n\\nWe will now analyze the properties of orthonormal bases in more detail.\\nLemma 12.17 If V is a Euclidean or unitary vector space with the scalar product\\n\u0002·, ·\u0003 and the orthonormal basis {u 1 , . . . , u n }, then v= n\\n\u0004\\n\u0002v, u i \u0003u i i=1 for all v ∈ V.\\nProof#For every v ∈ V there exist uniquely determined coordinates\\n#nλ1 , . . . , λn with n\\nλi u i .\\nFor every j = 1, . . . , n we then have \u0002v, u j \u0003 = i=1\\nλi \u0002u i , u j \u0003 = v = i=1\\nλj.\\nThe coordinates \u0002v, u i \u0003, i = 1, . . . , n, of v with respect to an orthonormal basis\\n6\\n{u 1 , . . . , u n } are often called\\n#n the Fourier coefficients of v with respect to this basis.\\nThe representation v = i=1\\n\u0002v, u i \u0003u i is called the (abstract) Fourier expansion of v in the given orthonormal basis.\\n\\n6 Jean\\n\\nBaptiste Joseph Fourier (1768–1830).</td>\n",
       "      <td>182.0</td>\n",
       "      <td>182</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.2 Orthogonality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>euclidean unitary vector space numerical computation q r-decomposition yield α β resulting profit estimate last quarter following year p million euro matlab-minute example one could imagine profit grows quadratically instead linearly determine analogously procedure example polynomial p αt βt γ solves least square problem p τi μi min α β βτi γ μi use matlab command qr computing q r-decomposition determine estimated profit last quarter following year analyze property orthonormal base detail lemma v euclidean unitary vector space scalar product orthonormal basis u u n n u v proof every v v exist uniquely determined coordinate λn n λi u every j n u j λi u j v λj coordinate u n v respect orthonormal basis u u n often called n fourier coefficient v respect basis representation v u called abstract fourier expansion v given orthonormal basis jean baptiste joseph fourier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>12.2 Orthogonality\\n\\n181\\n\\nCorollary 12.18 If V is a Euclidean or unitary vector space with the scalar product\\n\u0002·, ·\u0003 and the orthonormal basis {u 1 , . . . , u n }, then the following assertions hold:\\n#n\\n#n\\n(1) \u0002v, w\u0003 = i=1\\n\u0002v, u i \u0003\u0002u i , w\u0003 = i=1\\n\u0002v, u i \u0003\u0002w, u i \u0003 for all v, w ∈ V (Parseval’s identity7 ).# n\\n|\u0002v, u i \u0003|2 for all v ∈ V (Bessel’s identity8 ).\\n(2) \u0002v, v\u0003 = i=1\\nProof\\n(1) We have v =\\n\\n#n\\n\\n\u0002v, w\u0003 = i=1 \u0002v, u i \u0003u i , and thus n n n\\n% \u0004\\n$\u0004\\n\u0004\\n\u0002v, u i \u0003u i , w =\\n\u0002v, u i \u0003\u0002u i , w\u0003 =\\n\u0002v, u i \u0003\u0002w, u i \u0003.\\ni=1 i=1 i=1\\n\\n(2) is a special case of (1) for v = w.\\nBy Bessel’s identity, every vector v ∈ V satisfies v\\n\\n2\\n\\n= \u0002v, v\u0003 = n\\n\u0004\\n\\n|\u0002v, u i \u0003|2 ≥ max |\u0002v, u i \u0003|2 , i=1\\n\\n1≤i≤n where · is the norm induced by the scalar product.\\nThe absolute value of each coordinate of v with respect to an orthonormal basis of V is therefore bounded by the norm of v. This property does not hold for a general basis of V.\\nExample 12.19 Consider V = R2,1 with the standard scalar product and the Euclidean norm, then for every real ε = 0 the set\\n&amp;\u0006 \u0007 \u0006 \u0007'\\n1\\n1\\n,\\n0\\nε is a basis of V. For every vector v = [ν1 , ν2 ]T we then have\\n\u0006 \u0007\\n\u0006 \u0007\\n\u0003\\nν2 \u0005 1\\nν2 1 v = ν1 −\\n+\\n.\\n0\\nε\\nε ε\\nIf |ν1 |, |ν2 | are moderate numbers and if |ε| is (very) small, then |ν1 − ν2 /ε| and\\n|ν2 /ε| are (very) large.\\nIn numerical algorithms such a situation can lead to significant problems (e.g. due to roundoff errors) that are avoided when orthonormal bases are used.\\n\\n7 Marc-Antoine\\n8 Friedrich\\n\\nParseval (1755–1836).\\nWilhelm Bessel (1784–1846).</td>\n",
       "      <td>183.0</td>\n",
       "      <td>183</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.2 Orthogonality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>orthogonality corollary v euclidean unitary vector space scalar product orthonormal basis u u n following assertion hold n n u u u v w v parseval n u v v bessel proof v n u thus n n n u w u u u special case v bessel identity every vector v v satisfies v n u max u norm induced scalar product absolute value coordinate v respect orthonormal basis v therefore bounded norm property hold general basis example consider v standard scalar product euclidean norm every real ε set ε basis every vector v v ε ε ε moderate number small large numerical algorithm situation lead significant problem due roundoff error avoided orthonormal base used marc-antoine friedrich parseval wilhelm bessel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>182\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\nDefinition 12.20 Let V be a Euclidean or unitary vector space with the scalar product\\n\u0002·, ·\u0003, and let U ⊆ V be a subspace.\\nThen\\nU ⊥ := {v ∈ V | \u0002v, u\u0003 = 0 for all u ∈ U} is called the orthogonal complement of U (in V).\\nLemma 12.21 The orthogonal complement U ⊥ is a subspace of V.\\nProof Exercise.\\nLemma 12.22 If V is an n-dimensional Euclidean or unitary vector space, and if\\nU ⊆ V is an m-dimensional subspace, then dim(U ⊥ ) = n − m and V = U ⊕ U ⊥ .\\nProof We know that m ≤ n (cp.\\nLemma 9.27).\\nIf m = n, then U = V, and thus\\nU ⊥ = V ⊥ = {v ∈ V | \u0002v, u\u0003 = 0 for all u ∈ V} = {0}, so that the assertion is trivial.\\nThus let m &lt; n and let {u 1 , . . . , u m } be an orthonormal basis of U. We extend this basis to a basis of V and apply the Gram-Schmidt method in order to obtain an orthonormal basis {u 1 , . . . , u m , u m+1 , . . . , u n } of V. Then span{u m+1 , . . . , u n } ⊆ U ⊥ and therefore V = U + U ⊥ .\\nIf w ∈ U ∩ U ⊥ , then \u0002w, w\u0003 = 0, and hence w = 0, since the scalar product is positive definite.\\nThus, U ∩ U ⊥ = {0}, which implies that\\nV = U ⊕ U ⊥ and dim(U ⊥ ) = n − m (cp.\\nTheorem 9.29).\\nIn particular, we have\\nU ⊥ = span{u m+1 , . . . , u n }.\\n\\n12.3 The Vector Product in R3,1\\nIn this section we consider a further product on the vector space R3,1 that is frequently used in Physics and Electrical Engineering.\\nDefinition 12.23 The vector product or cross product in R3,1 is the map\\nR3,1 ×R3,1 → R3,1 , (v, w) \u0005 → v×w := [ν2 ω3 − ν3 ω2 , ν3 ω1 − ν1 ω3 , ν1 ω2 − ν2 ω1 ]T , where v = [ν1 , ν2 , ν3 ]T and w = [ω1 , ω2 , ω3 ]T .\\nIn contrast to the scalar product, the vector product of two elements of the vector space R3,1 is not a scalar but again a vector in R3,1 .\\nUsing the canonical basis vectors of R3,1 , e1 = [1, 0, 0]T , e2 = [0, 1, 0]T , e3 = [0, 0, 1]T ,</td>\n",
       "      <td>184.0</td>\n",
       "      <td>184</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.3 The Vector Product in mathbbR3,1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>euclidean unitary vector space definition let v euclidean unitary vector space scalar product let u v subspace u v v u u called orthogonal complement u v lemma orthogonal complement u subspace proof exercise lemma v n-dimensional euclidean unitary vector space u v m-dimensional subspace dim u n v u u proof know n cp lemma n u v thus u v v v u v assertion trivial thus let n let u u orthonormal basis u extend basis basis v apply gram-schmidt method order obtain orthonormal basis u u u u n span u u n u therefore v u u w u u hence w since scalar product positive definite thus u u implies v u u dim u n cp theorem particular u span u u n vector product section consider product vector space frequently used physic electrical engineering definition vector product cross product map v w v w contrast scalar product vector product two element vector space scalar vector using canonical basis vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>12.3 The Vector Product in R3,1\\n\\n183 we can write the vector product as\\n\u0007\u001f\\n\u0007\u001f\\n\u0007\u001f\\n\u001e",
       "\u0006\\n\u001e",
       "\u0006\\n\u001e",
       "\u0006\\nν1 ω 1\\nν1 ω 1\\nν2 ω2 e1 − det e2 + det e3 .\\nv × w = det\\nν3 ω 3\\nν3 ω 3\\nν2 ω 2\\nLemma 12.24 The vector product is linear in both components and for all v, w ∈\\nR3,1 the following properties hold:\\n(1) v × w = −w × v, i.e., the vector product is anti commutative or alternating.\\n(2) v × w 2 = v 2 w 2 − \u0002v, w\u00032 , where \u0002·, ·\u0003 is the standard scalar product and · the Euclidean norm of R3,1 .\\n(3) \u0002v, v × w\u0003 = \u0002w, v × w\u0003 = 0, where \u0002·, ·\u0003 is the standard scalar product of R3,1 .\\nProof Exercise.\\nBy (2) and the Cauchy-Schwarz inequality (12.2), it follows that v × w = 0 holds if and only if v, w are linearly dependent.\\nFrom (3) we obtain\\n\u0002λv + μw, v × w\u0003 = λ\u0002v, v × w\u0003 + μ\u0002w, v × w\u0003 = 0, for arbitrary λ, μ ∈ R. If v, w are linearly independent, then the product v × w is orthogonal to the plane through the origin spanned by v and w in R3,1 , i.e., v × w ∈ {λv + μw | λ, μ ∈ R}⊥ .\\nGeometrically, there are two possibilities:\\n\\nThe positions of the three vectors v, w, v ×w on the left side of this figure correspond to the “right-handed orientation” of the usual coordinate system of R3,1 , where the canonical basis vectors e1 , e2 , e3 are associated with thumb, index finger and middle finger of the right hand.\\nThis motivates the name right-hand rule.\\nIn order to explain this in detail, one needs to introduce the concept of orientation, which we omit here.\\nIf ϕ ∈ [0, π] is the angle between the vectors v, w, then\\n\u0002v, w\u0003 = v w cos(ϕ)</td>\n",
       "      <td>185.0</td>\n",
       "      <td>185</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.3 The Vector Product in mathbbR3,1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vector product write vector product ω ω det det v w det ω ω ω lemma vector product linear component v w following property hold v w v vector product anti commutative alternating v w v w standard scalar product euclidean norm v v standard scalar product proof exercise cauchy-schwarz inequality follows v w hold v w linearly dependent obtain μw v v v arbitrary λ μ v w linearly independent product v w orthogonal plane origin spanned v w v w λv μw λ μ r geometrically two possibility position three vector v w v left side figure correspond right-handed orientation usual coordinate system canonical basis vector associated thumb index finger middle finger right hand motivates name right-hand rule order explain detail one need introduce concept orientation omit ϕ π angle vector v w v w co ϕ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>184\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\n(cp.\\nDefinition 12.7) and we can write (2) in Lemma 12.24 as v×w\\n\\n2\\n\\n= v\\n\\n2 w\\n\\n2\\n\\n− v\\n\\n2 w cos2 (ϕ) = v\\n\\n2\\n\\n2 w\\n\\n2 sin2 (ϕ), so that v×w = v w sin(ϕ).\\n\\nA geometric interpretation of this equation is the following: The norm of the vector product of v and w is equal to the area of the parallelogram spanned by v and w.\\nThis interpretation is illustrated in the following figure:\\n\\nExercises\\n12.1 Let V be a finite dimensional real or complex vector space.\\nShow that there exists a scalar product on V.\\n12.2 Show that the maps defined in Example 12.2 are scalar products on the corresponding vector spaces.\\n12.3 Let \u0002·, ·\u0003 be an arbitrary scalar product on Rn,1 .\\nShow that there exists a matrix\\nA ∈ Rn,n with \u0002v, w\u0003 = w T Av for all v, w ∈ Rn,1 .\\n12.4 Let V be a finite dimensional R- or C-vector space.\\nLet s1 and s2 be scalar products on V with the following property: If v, w ∈ V satisfy s1 (v, w) = 0, then also s2 (v, w) = 0.\\nProve or disprove: There exists a real scalar λ &gt; 0 with s1 (v, w) = λs2 (v, w) for all v, w ∈ V.\\n12.5 Show that the maps defined in Example 12.4 are norms on the corresponding vector spaces.\\n12.6 Show that n m\\n\u0004\\n\u0004\\nA 1 = max\\n|ai j | and A ∞ = max\\n|ai j |\\n1≤ j≤m\\n\\n1≤i≤n i=1 j=1 for all A = [ai j ] ∈ K n,m , where K = R or K = C (cp.\\n(6) in Example 12.4).\\n12.7 Sketch for the matrix A from (6) in Example 12.4 and p ∈ {1, 2, ∞}, the sets\\n{Av | v ∈ R2,1 , v p = 1 } ⊂ R2,1 .\\n12.8 Let V be a Euclidean or unitary vector space and let · be the norm induced by a scalar product on V. Show that · satisfies the parallelogram identity v+w for all v, w ∈ V.\\n\\n2\\n\\n+ v−w\\n\\n2\\n\\n= 2( v\\n\\n2\\n\\n+ w 2)</td>\n",
       "      <td>186.0</td>\n",
       "      <td>186</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.3 The Vector Product in mathbbR3,1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>euclidean unitary vector space cp definition write lemma v w v w ϕ v w ϕ v w sin ϕ geometric interpretation equation following norm vector product v w equal area parallelogram spanned v interpretation illustrated following figure exercise let v finite dimensional real complex vector space show exists scalar product show map defined example scalar product corresponding vector space let arbitrary scalar product show exists matrix rn n w av v w let v finite dimensional c-vector space let scalar product v following property v w v satisfy v w also v w prove disprove exists real scalar λ v w v w v w show map defined example norm corresponding vector space show n max j max j ai j k n k r k c cp example sketch matrix example p set av v v p let v euclidean unitary vector space let norm induced scalar product show satisfies parallelogram identity v w v w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>12.3 The Vector Product in R3,1\\n\\n185\\n\\n12.9 Let V be a K -vector space (K = R or K = C) with the scalar product \u0002·, ·\u0003 and the induced norm · .\\nShow that v, w ∈ V are orthogonal with respect to \u0002·, ·\u0003 if and only if v + λw = v − λw for all λ ∈ K .\\n12.10 Does there exist a scalar product \u0002·, ·\u0003 on Cn,1 , such that the 1-norm of Cn,1\\n(cp.\\n(5) in Example 12.4) is the induced norm by this scalar product?\\n12.11 Show that the inequality n\\n\u0003\u0004 i=1\\n\\n12.12\\n\\n12.13\\n\\n12.14\\n\\n12.15\\n\\nαi βi\\n\\n\u00052\\n\\n≤ n\\n\u0004\\n\\n(γi αi )2 · i=1 n \u0003\\n\u0004\\nβi \u00052 i=1\\n\\nγi holds for arbitrary real numbers α1 , . . . , αn , β1 , . . . , βn and positive real numbers γ1 , . . . , γn .\\nLet V be a finite dimensional Euclidean or unitary vector space with the scalar product \u0002·, ·\u0003.\\nLet f : V → V be a map with \u0002 f (v), f (w)\u0003 = \u0002v, w\u0003 for all v, w ∈ V. Show that f is an isomorphism.\\nLet V be a unitary vector space and suppose that f ∈ L(V, V) satisfies\\n\u0002 f (v), v\u0003 = 0 for all v ∈ V. Prove or disprove that f = 0.\\nDoes the same statement also hold for Euclidean vector spaces?\\nLet D = diag(d1 , . . . , dn ) ∈ Rn,n with d1 , . . . , dn &gt; 0.\\nShow that \u0002v, w\u0003 = w T Dv is a scalar product on Rn,1 .\\nAnalyze which properties of a scalar product are violated if at least one of the di is zero, or when all di are nonzero but have different signs.\\nOrthonormalize the following basis of the vector space C2,2 with respect to the scalar product \u0002A, B\u0003 = trace(B H A):\\n&amp;\u0006\\n\\n\u0007 \u0006 \u0007 \u0006 \u0007 \u0006 \u0007'\\n10\\n10\\n11\\n11\\n,\\n,\\n,\\n.\\n00\\n01\\n01\\n11\\n\\n12.16 Let Q ∈ Rn,n be an orthogonal or let Q ∈ Cn,n be a unitary matrix.\\nWhat are the possible values of det(Q)?\\n12.17 Let u ∈ Rn,1 \\ {0} and let\\nH (u) = In − 2\\n\\n1 uT u uu T ∈ Rn,n .\\n\\nShow that the n columns of H (u) form an orthonormal basis of Rn,1 with respect to the standard scalar product.\\n(Matrices of this form are called Householder matrices.9 We will study them in more detail in Example 18.15.)\\n12.18 Prove Lemma 12.21.\\n\\n9 Alston\\n\\nScott Householder (1904–1993), pioneer of Numerical Linear Algebra.</td>\n",
       "      <td>187.0</td>\n",
       "      <td>187</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.3 The Vector Product in mathbbR3,1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vector product let v k space k r k c scalar product induced norm show v w v orthogonal respect v λw v λw λ k exist scalar product cp example induced norm scalar product show inequality n αi βi n γi αi n βi γi hold arbitrary real number αn βn positive real number γn let v finite dimensional euclidean unitary vector space scalar product let f v v map f v f w v w show f isomorphism let v unitary vector space suppose f l v v satisfies f v v prove disprove f statement also hold euclidean vector space let diag dn rn n dn show w dv scalar product analyze property scalar product violated least one di zero di nonzero different sign orthonormalize following basis vector space respect scalar product trace b h let q rn n orthogonal let q cn n unitary matrix possible value det q let u let h u ut u uu rn n show n column h u form orthonormal basis respect standard scalar product matrix form called householder study detail example prove lemma alston scott householder pioneer numerical linear algebra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>186\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\n⎡\\n\\n12.19 Let\\n[v1 , v2 , v3 ] =\\n\\n√1\\n⎢ √21\\n⎣− 2\\n\\n0\\n\\n⎤\\n0 √12\\n⎥\\n0 √12 ⎦ ∈ R3,3 .\\n0 0\\n\\nAnalyze whether the vectors v1 , v2 , v3 are orthonormal with respect to the standard scalar product and compute the orthogonal complement of span{v1 , v2 , v3 }.\\n12.20 Let V be a Euclidean or unitary vector space with the scalar product \u0002·, ·\u0003, let u 1 , . . . , u k ∈ V and let U = span{u 1 , . . . , u k }.\\nShow that for v ∈ V we have v ∈ U ⊥ if and only if \u0002v, u j \u0003 = 0 for j = 1, . . . , k.\\n12.21 In the unitary vector space C4,1 with the standard scalar product let v1 =\\n[−1, i, 0, 1]T and v2 = [i, 0, 2, 0]T be given.\\nDetermine an orthonormal basis of span{v1 , v2 }⊥ .\\n12.22 Prove Lemma 12.24.</td>\n",
       "      <td>188.0</td>\n",
       "      <td>188</td>\n",
       "      <td>12 Euclidean and Unitary Vector Spaces</td>\n",
       "      <td>12.3 The Vector Product in mathbbR3,1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>euclidean unitary vector space let analyze whether vector orthonormal respect standard scalar product compute orthogonal complement span let v euclidean unitary vector space scalar product let u u k v let u span u u k show v v v u u j j unitary vector space standard scalar product let given determine orthonormal basis span prove lemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Chapter 13\\n\\nAdjoints of Linear Maps\\n\\nIn this chapter we introduce adjoints of linear maps.\\nIn some sense these represent generalizations of the (Hermitian) transposes of a matrices.\\nA matrix is symmetric\\n(or Hermitian) if it is equal to its (Hermitian) transpose.\\nIn an analogous way, an endomorphism is selfadjoint if it is equal to its adjoint endomorphism.\\nThe sets of symmetric (or Hermitian) matrices and of selfadjoint endomorphisms form certain vector spaces which will play a key role in our proof of the Fundamental Theorem of\\nAlgebra in Chap.\\n15.\\nSpecial properties of selfadjoint endomorphisms will be studied in Chap.\\n18.\\n\\n13.1 Basic Definitions and Properties\\nIn Chap.\\n12 we have considered Euclidean and unitary vector spaces, and hence vector spaces over the fields R and C. Now let V and W be vector spaces over a general field K , and let β be a bilinear form on V × W.\\nFor every fixed vector v ∈ V, the map\\nβv : W → K , w \u0004→ β(v, w), is a linear form on W. Thus, we can assign to every v ∈ V a vector βv ∈ W ∗ , which defines the map\\n(13.1)\\nβ (1) : V → W ∗ , v \u0004→ βv .\\nAnalogously, we define the map\\nβ (2) : W → V ∗ , w \u0004→ βw ,\\n\\n(13.2) where βw : V → K is defined by v \u0004→ β(v, w) for every w ∈ W.\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_13\\n\\n187</td>\n",
       "      <td>189.0</td>\n",
       "      <td>189</td>\n",
       "      <td>13 Adjoints of Linear Maps</td>\n",
       "      <td>13.1 Basic Definitions and Properties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter adjoints linear map chapter introduce adjoints linear map sense represent generalization hermitian transpose matrix matrix symmetric hermitian equal hermitian transpose analogous way endomorphism selfadjoint equal adjoint endomorphism set symmetric hermitian matrix selfadjoint endomorphisms form certain vector space play key role proof fundamental theorem algebra chap special property selfadjoint endomorphisms studied chap basic definition property chap considered euclidean unitary vector space hence vector space field r let v w vector space general field k let β bilinear form v every fixed vector v v map βv w k w β v w linear form thus assign every v v vector βv w defines map β v w v βv analogously define map β w v w βw βw v k defined v β v w every w springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>188\\n\\n13 Adjoints of Linear Maps\\n\\nLemma 13.1 The maps β (1) and β (2) defined in (13.1) and (13.2), respectively, are linear, i.e., β (1) ∈ L(V, W ∗ ) and β (2) ∈ L(W, V ∗ ).\\nIf dim(V) = dim(W) ∈ N and\\nβ is non-degenerate (cp.\\nDefinition 11.9), then β (1) and β (2) are bijective and thus isomorphisms.\\nProof We prove the assertion only for the map β (1) ; the proof for β (2) is analogous.\\nWe first show the linearity.\\nLet v1 , v2 ∈ V and λ1 , λ2 ∈ K .\\nFor every w ∈ W we then have\\nβ (1) (λ1 v1 + λ2 v2 )(w) = β(λ1 v1 + λ2 v2 , w)\\n= λ1 β(v1 , w) + λ2 β(v2 , w)\\n= λ1 β (1) (v1 )(w) + λ2 β (1) (v2 )(w)\\n\u0003\\n\u0002\\n= λ1 β (1) (v1 ) + λ2 β (1) (v2 ) (w), and hence β (1) (λ1 v1 +λ2 v2 ) = λ1 β (1) (v1 )+λ2 β (1) (v2 ).\\nTherefore, β (1) ∈ L(V, W ∗ ).\\nLet now dim(V) = dim(W) ∈ N and let β be non-degenerate.\\nWe show that β (1) ∈\\nL(V, W ∗ ) is injective.\\nBy (5) in Lemma 10.7, this holds if and only if ker(β (1) ) = {0}.\\nIf v ∈ ker(β (1) ), then β (1) (v) = βv = 0 ∈ W ∗ , and thus\\nβv (w) = β(v, w) = 0 for all w ∈ W.\\nSince β is non-degenerate, we have v = 0.\\nFinally, dim(V) = dim(W) and dim(W)\\n= dim(W ∗ ) imply that dim(V) = dim(W ∗ ) so that β (1) is bijective (cp.\\nCorollary 10.11).\\n\u0007\\n\u0006\\nWe next discuss the existence of the adjoint map.\\nTheorem 13.2 If V and W are K -vector spaces with dim(V) = dim(W) ∈ N and\\nβ is a non-degenerate bilinear form on V × W, then the following assertions hold:\\n(1) For every f ∈ L(V, V) there exists a uniquely determined g ∈ L(W, W) with\\nβ( f (v), w) = β(v, g(w)) for all v ∈ V and w ∈ W.\\nThe map g is called the right adjoint of f with respect to β.\\n(2) For every h ∈ L(W, W) there exists a uniquely determined k ∈ L(V, V) with\\nβ(v, h(w)) = β(k(v), w) for all v ∈ V and w ∈ W.\\nThe map k is called the left adjoint of h with respect to β.\\nProof We only show (1); the proof of (2) is analogous.\\nLet V ∗ be the dual space of V, let f ∗ ∈ L(V ∗ , V ∗ ) be the dual map of f , and let β (2) ∈ L(W, V ∗ ) be as in (13.2).\\nSince β is non-degenerate, β (2) is bijective by\\nLemma 13.1.\\nDefine g := (β (2) )−1 ◦ f ∗ ◦ β (2) ∈ L(W, W).</td>\n",
       "      <td>190.0</td>\n",
       "      <td>190</td>\n",
       "      <td>13 Adjoints of Linear Maps</td>\n",
       "      <td>13.1 Basic Definitions and Properties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adjoints linear map lemma map β β defined respectively linear β l v w β l w v dim v dim w n β non-degenerate cp definition β β bijective thus isomorphism proof prove assertion map β proof β analogous first show linearity let v k every w w β w β w β w β w β w β w β β w hence β β β therefore β l v w let dim v dim w n let β non-degenerate show β l v w injective lemma hold ker β v ker β β v βv w thus βv w β v w w since β non-degenerate v finally dim v dim w dim w dim w imply dim v dim w β bijective cp corollary next discus existence adjoint map theorem v w k space dim v dim w n β non-degenerate bilinear form v w following assertion hold every f l v v exists uniquely determined g l w w β f v w β v g w v v w map g called right adjoint f respect β every h l w w exists uniquely determined k l v v β v h w β k v w v v w map k called left adjoint h respect β proof show proof analogous let v dual space v let f l v v dual map f let β l w v since β non-degenerate β bijective lemma define g β f β l w w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>13.1 Basic Definitions and Properties\\n\\n189\\n\\nThen, for all v ∈ V and w ∈ W,\\nβ(v, g(w)) = β(v, ((β (2) )−1 ◦ f ∗ ◦ β (2) )(w))\\n\u0002\\n\u0003\\n= β (2) ((β (2) )−1 ◦ f ∗ ◦ β (2) )(w) (v)\\n\u0002\\n\u0003\\n= β (2) (β (2) )−1 ( f ∗ (β (2) (w))) (v)\\n\u0003\\n\u0002\\n= β (2) ◦ (β (2) )−1 ◦ β (2) (w) ◦ f (v)\\n= β (2) (w)( f (v))\\n= β( f (v), w).\\n(Recall that the dual map satisfies f ∗ (β (2) (w)) = β (2) (w) ◦ f .)\\nIt remains to show the uniqueness of g.\\nLet \u0004 g ∈ L(W, W) with β(v, \u0004 g (w)) =\\nβ( f (v), w) for all v ∈ V and w ∈ W. Then β(v, \u0004 g (w)) = β(v, g(w)), and hence\\nβ(v, (\u0004 g − g)(w)) = 0 for all v ∈ V and w ∈ W.\\nSince β is non-degenerate in the second variable, we have (\u0004 g − g)(w) = 0 for all w ∈ W, so that g = \u0004 g.\\n\u0007\\n\u0006\\nExample 13.3 Let V = W = K n,1 and β(v, w) = w T Bv with a matrix B ∈\\nG L n (K ), so that β is non-degenerate (cp.\\n(1) in Example 11.10).\\nWe consider the linear map f : V → V, v \u0004→ Fv, with a matrix F ∈ K n,n , and the linear map h : W → W, w \u0004→ H w, with a matrix H ∈ K n,n .\\nThen\\nβv : W → K , w \u0004→ w T (Bv),\\nβ (1) : V → W ∗ , v \u0004→ (Bv)T ,\\nβ (2) : W → V ∗ , w \u0004→ w T B, where we have identified the isomorphic vector spaces W ∗ and K 1,n , respectively\\nV ∗ and K 1,n , with each other.\\nIf g ∈ L(W, W) is the right adjoint of f with respect to β, then\\nβ( f (v), w) = w T B f (v) = w T B Fv = β(v, g(w)) = g(w)T Bv for all v ∈ V and w ∈ W. If we represent the linear map g via the multiplication with a matrix G ∈ K n,n , i.e., g(w) = Gw, then w T B Fv = w T G T Bv for all v, w ∈ K n,1 .\\nHence B F = G T B. Since B is invertible, the unique right adjoint is given by G = (B F B −1 )T = B −T F T B T .\\nAnalogously, for the left adjoint k ∈ L(V, V) of h with respect to β we obtain the equation\\nβ(v, h(w)) = (h(w))T Bv = w T H T Bv = β(k(v), w) = w T Bk(v) for all v ∈ V and w ∈ W. With k(v) = Lv for a matrix L ∈ K n,n , we obtain\\nH T B = B L and hence L = B −1 H T B.</td>\n",
       "      <td>191.0</td>\n",
       "      <td>191</td>\n",
       "      <td>13 Adjoints of Linear Maps</td>\n",
       "      <td>13.1 Basic Definitions and Properties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic definition property v v w w β v g w β v β f β w β β f β w v β β f β w v β β β w f v β w f v β f v w recall dual map satisfies f β w β w f remains show uniqueness let g l w w β v g w β f v w v v w β v g w β v g w hence β v g g w v v w since β non-degenerate second variable g g w w w g example let v w k β v w w bv matrix b g l n k β non-degenerate cp example consider linear map f v v v fv matrix f k n n linear map h w w w h w matrix h k n n βv w k w w bv β v w v bv β w v w w b identified isomorphic vector space w k n respectively v k n g l w w right adjoint f respect β β f v w w b f v w b fv β v g w g w bv v v w represent linear map g via multiplication matrix g k n n g w gw w b fv w g bv v w k hence b f g b since b invertible unique right adjoint given g b f b b f b analogously left adjoint k l v v h respect β obtain equation β v h w h w bv w h bv β k v w w bk v v v w k v lv matrix l k n n obtain h b b l hence l b h b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>190\\n\\n13 Adjoints of Linear Maps\\n\\nIf V is finite dimensional and β is a non-degenerate bilinear form on V, then by\\nTheorem 13.2 every f ∈ L(V, V) has a unique right adjoint g and a unique left adjoint k, such that\\nβ( f (v), w) = β(v, g(w)) and β(v, f (w)) = β(k(v), w)\\n\\n(13.3) for all v, w ∈ V. If β is symmetric, i.e., if β(v, w) = β(w, v) holds for all v, w ∈ V, then (13.3) yields\\nβ(v, g(w)) = β( f (v), w) = β(w, f (v)) = β(k(w), v) = β(v, k(w)).\\nTherefore, β(v, (g − k)(w)) = 0 for all v, w ∈ V, and hence g = k, since β is non-degenerate.\\nThus, we have proved the following result.\\nCorollary 13.4 If β is a symmetric and non-degenerate bilinear form on a finite dimensional K -vector space V, then for every f ∈ L(V, V) there exists a unique g ∈ L(V, V) with\\nβ( f (v), w) = β(v, g(w)) and β(v, f (w)) = β(g(v), w) for all v, w ∈ V.\\nBy definition, a scalar product on a Euclidean vector space is a symmetric and nondegenerate bilinear form (cp.\\nDefinition 12.1).\\nThis leads to the following corollary.\\nCorollary 13.5 If V is a finite dimensional Euclidean vector space with the scalar product ·, · , then for every f ∈ L(V, V) there exists a unique f ad ∈ L(V, V) with f (v), w = v, f ad (w) and v, f (w) = f ad (v), w\\n\\n(13.4) for all v, w ∈ V. The map f ad is called the adjoint of f (with respect to ·, · ).\\nIn order to determine whether a given map g ∈ L(V, V) is the unique adjoint of f ∈ L(V, V), only one of the two conditions in (13.4) have to be verified: If for f, g ∈ L(V, V) the equation f (v), w = v, g(w) holds for all v, w ∈ V, then also v, f (w) = f (w), v = w, g(v) = g(v), w for all v, w ∈ V, where we have used the symmetry of the scalar product.\\nSimilarly, if v, f (w) = g(v), w holds for all v, w ∈ V, then also f (v), w = v, g(w) for all v, w ∈ V.</td>\n",
       "      <td>192.0</td>\n",
       "      <td>192</td>\n",
       "      <td>13 Adjoints of Linear Maps</td>\n",
       "      <td>13.1 Basic Definitions and Properties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adjoints linear map v finite dimensional β non-degenerate bilinear form v theorem every f l v v unique right adjoint g unique left adjoint k β f v w β v g w β v f w β k v w v w β symmetric β v w β w v hold v w v yield β v g w β f v w β w f v β k w v β v k w therefore β v g k w v w v hence g k since β non-degenerate thus proved following result corollary β symmetric non-degenerate bilinear form finite dimensional k space v every f l v v exists unique g l v v β f v w β v g w β v f w β g v w v w definition scalar product euclidean vector space symmetric nondegenerate bilinear form cp definition lead following corollary corollary v finite dimensional euclidean vector space scalar product every f l v v exists unique f ad l v v f v w v f ad w v f w f ad v w v w map f ad called adjoint f respect order determine whether given map g l v v unique adjoint f l v v one two condition verified f g l v v equation f v w v g w hold v w v also v f w f w v w g v g v w v w v used symmetry scalar product similarly v f w g v w hold v w v also f v w v g w v w v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>13.1 Basic Definitions and Properties\\n\\n191\\n\\nExample 13.6 Consider the Euclidean vector space R3,1 with the scalar product\\n⎡\\n\\n⎤\\n100 v, w = w T Dv, where D = ⎣0 2 0⎦ ,\\n001 and the linear map\\n⎡ f : R3,1\\n\\n⎤\\n122\\n→ R3,1 , v \u0004→ Fv, where F = ⎣1 0 1⎦ .\\n200\\n\\nFor all v, w ∈ R3,1 we then have f (v), w = w T D Fv = w T D F D −1 Dv = (D −T F T D T w)T Dv = v, f ad (w) ,\\n⎡ and thus f ad : R3,1\\n\\n⎤\\n122\\n→ R3,1 , v \u0004→ D −1 F T Dv = ⎣1 0 0⎦ v,\\n220 where we have used that D is symmetric.\\nWe now show that uniquely determined adjoint maps also exist in the unitary case.\\nHowever, we cannot conclude this directly from Corollary 13.4, since a scalar product on a C-vector space is not a symmetric bilinear form, but a Hermitian sesquilinear form.\\nIn order to show the existence of the adjoint map in the unitary case we construct it explicitly.\\nThis construction works also in the Euclidean case.\\nLet V be a unitary vector space with the scalar product ·, · and let {u 1 , . . . , u n } be an orthonormal basis of V. For a given f ∈ L(V, V) we define the map n v, f (u i ) u i .\\ng : V → V, v \u0004→ i=1\\n\\nIf v, w ∈ V and λ, μ ∈ C, then n g(λv + μw) = n\\n\\nλv + μw, f (u i ) u i = i=1\\n\\n= λg(v) + μg(w), i=1\\n\\n\u0002\\n\u0003\\nλ v, f (u i ) u i + μ v, f (u i ) u i</td>\n",
       "      <td>193.0</td>\n",
       "      <td>193</td>\n",
       "      <td>13 Adjoints of Linear Maps</td>\n",
       "      <td>13.1 Basic Definitions and Properties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic definition property example consider euclidean vector space scalar product v w w dv linear map f v fv f v w f v w w fv w f dv f w dv v f ad w thus f ad v f dv v used symmetric show uniquely determined adjoint map also exist unitary case however conclude directly corollary since scalar product c-vector space symmetric bilinear form hermitian sesquilinear form order show existence adjoint map unitary case construct explicitly construction work also euclidean case let v unitary vector space scalar product let u u n orthonormal basis given f l v v define map n v f u u g v v v v w v λ μ c n g λv μw n λv μw f u u λg v μg w λ v f u u μ v f u u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>192\\n\\n13 Adjoints of Linear Maps and hence g ∈ L(V, V).\\nLet now v = n\\n\\nλi u i ∈ V and w ∈ V, then n i=1 n w, f (u j ) u j =\\n\\nλi u i , v, g(w) = n i=1 j=1 n\\n\\nλi w, f (u i ) = i=1\\n\\nλi f (u i ), w i=1\\n\\n= f (v), w .\\nFurthermore, v, f (w) = f (w), v = w, g(v) = g(v), w for all v, w ∈ V. If \u0004 g ∈ L(V, V) satisfies f (v), w = v, \u0004 g (w) for all v, w ∈ V, then g = \u0004 g , since the scalar product is positive definite.\\nWe can therefore formulate the following result analogously to Corollary 13.5.\\nCorollary 13.7 If V is a finite dimensional unitary vector space with the scalar product ·, · , then for every f ∈ L(V, V) there exists a unique f ad ∈ L(V, V) with f (v), w = v, f ad (w) and v, f (w) = f ad (v), w\\n\\n(13.5) for all v, w ∈ V. The map f ad is called the adjoint of f (with respect to ·, · ).\\nAs in the Euclidean case, again the validity of one of the two equations in (13.5) for all v, w ∈ V implies the validity of the other for all v, w ∈ V.\\nExample 13.8 Consider the unitary vector space C3,1 with the scalar product\\n⎡\\n\\n⎤\\n100 v, w = w H Dv, where D = ⎣0 2 0⎦ ,\\n001 and the linear map\\n⎡ f : C3,1\\n\\n⎤\\n1 2i 2\\n→ C3,1 , v \u0004→ Fv, where F = ⎣ i 0 −i ⎦ .\\n2 0 3i\\n\\nFor all v, w ∈ C3,1 we then have f (v), w = w H D Fv = w H D F D −1 Dv = (D −H F H D H w) H Dv\\n= v, f ad (w) , and thus\\n⎡ f ad : C3,1\\n\\n⎤\\n1 −2i 2\\n→ C3,1 , v \u0004→ D −1 F H Dv = ⎣ −i 0 0 ⎦ v,\\n2 2i −3i</td>\n",
       "      <td>194.0</td>\n",
       "      <td>194</td>\n",
       "      <td>13 Adjoints of Linear Maps</td>\n",
       "      <td>13.1 Basic Definitions and Properties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adjoints linear map hence g l v v let v n λi u v w v n n w f u j u j λi u v g w n n λi w f u λi f u w f v w furthermore v f w f w v w g v g v w v w g l v v satisfies f v w v g w v w v g g since scalar product positive definite therefore formulate following result analogously corollary corollary v finite dimensional unitary vector space scalar product every f l v v exists unique f ad l v v f v w v f ad w v f w f ad v w v w map f ad called adjoint f respect euclidean case validity one two equation v w v implies validity v w example consider unitary vector space scalar product v w w h dv linear map f v fv f v w f v w w h fv w h f dv f h h w h dv v f ad w thus f ad v f h dv v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>13.1 Basic Definitions and Properties\\n\\n193 where we have used that D is real and symmetric.\\nWe next investigate the properties of the adjoint map.\\nLemma 13.9 Let V be a finite dimensional Euclidean or unitary vector space.\\n(1) If f 1 , f 2 ∈ L(V, V) and λ1 , λ2 ∈ K (where K = R in the Euclidean and K = C in the unitary case), then\\n(λ1 f 1 + λ2 f 2 )ad = λ1 f 1ad + λ2 f 2ad .\\nIn the Euclidean case the map f \u0004→ f ad is therefore linear, and in the unity case semilinear.\\n(2) We have (IdV )ad = IdV .\\n(3) For every f ∈ L(V, V) we have ( f ad )ad = f .\\n(4) If f 1 , f 2 ∈ L(V, V), then ( f 2 ◦ f 1 )ad = f 1ad ◦ f 2ad .\\nProof\\n(1) If v, w ∈ V and λ1 , λ2 ∈ K , then\\n(λ1 f 1 + λ2 f 2 )(v), w = λ1 f 1 (v), w + λ2 f 2 (v), w\\n\u000e\\n\u000e\\n= λ1 v, f 1ad (w) + λ2 v, f 2ad (w)\\n= v, λ1 f 1ad (w) + λ2 f 2ad (w)\\n\u000f\\n\u0010\\n= v, λ1 f 1ad + λ2 f 2ad (w) , and thus (λ1 f 1 + λ2 f 2 )ad = λ1 f 1ad + λ2 f 2ad .\\n(2) For all v, w ∈ V we have IdV (v), w = v, w = v, IdV (w) , and thus\\n(IdV )ad = IdV .\\n(3) For all v, w ∈ V we have f ad (v), w = v, f (w) , and thus ( f ad )ad = f .\\n(4) For all v, w ∈ V we have\\n\u0002\\n\u000e\\n\u0003\u000e\\n( f 2 ◦ f 1 )(v), w = f 2 ( f 1 (v)), w = f 1 (v), f 2ad (w) = v, f 1ad f 2ad (w)\\n\u0002\\n\u0003\\n\u000e\\n= v, f 1ad ◦ f 2ad (w) , and thus ( f 2 ◦ f 1 )ad = f 1ad ◦ f 2ad .\\n\\n\u0007\\n\u0006\\n\\nThe following result shows relations between the image and kernel of an endomorphism and of its adjoint.\\nTheorem 13.10 If V is a finite dimensional Euclidean or unitary vector space and f ∈ L(V, V), then the following assertions hold:</td>\n",
       "      <td>195.0</td>\n",
       "      <td>195</td>\n",
       "      <td>13 Adjoints of Linear Maps</td>\n",
       "      <td>13.1 Basic Definitions and Properties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic definition property used real symmetric next investigate property adjoint map lemma let v finite dimensional euclidean unitary vector space f f l v v k k r euclidean k c unitary case f f ad f f euclidean case map f f ad therefore linear unity case semilinear idv ad idv every f l v v f ad ad f f f l v v f f ad f f proof v w v k f f v w f v w f v w v f w v f w v f w f w v f f w thus f f ad f f v w v idv v w v w v idv w thus idv ad idv v w v f ad v w v f w thus f ad ad f v w v f f v w f f v w f v f w v f f w v f f w thus f f ad f f following result show relation image kernel endomorphism adjoint theorem v finite dimensional euclidean unitary vector space f l v v following assertion hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>194\\n\\n13 Adjoints of Linear Maps\\n\\n(1) ker( f ad ) = im( f )⊥ .\\n(2) ker( f ) = im( f ad )⊥ .\\nProof\\n(1) If w ∈ ker( f ad ), then f ad (w) = 0 and\\n0 = v, f ad (w) = f (v), w for all v ∈ V, hence w ∈ im( f )⊥ .\\nIf, on the other hand, w ∈ im( f )⊥ , then\\n0 = f (v), w = v, f ad (w) for all v ∈ V. Since ·, · is non-degenerate, we have f ad (w) = 0 and, hence, w ∈ ker( f ad ).\\n\u0007\\n\u0006\\n(2) Using ( f ad )ad = f and (1) we get ker( f ) = ker(( f ad )ad ) = im( f ad )⊥ .\\nExample 13.11 Consider the unitary vector space C3,1 with the standard scalar product and the linear map f : C3,1\\n\\n⎡\\n⎤\\n1 i i\\n→ C3,1 , v \u0004→ Fv, with F = ⎣ i 0 0⎦ .\\n100\\n⎡\\n\\nThen f ad : C3,1 → C3,1 , v \u0004→ F H v, with F H\\n\\n⎤\\n1 −i 1\\n= ⎣ −i 0 0 ⎦ .\\n−i 0 0\\n\\nThe matrices F and F H have rank 2.\\nTherefore, dim(ker( f )) = dim(ker( f ad )) = 1.\\nA simple calculation shows that\\n⎧⎡\\n⎧⎡ ⎤⎫\\n⎤⎫\\n0 ⎬\\n⎨\\n⎨ 0 ⎬ ker( f ) = span ⎣ 1 ⎦ and ker( f ad ) = span ⎣1⎦ .\\n⎩\\n⎭\\n⎩\\n⎭\\n−1 i\\nThe dimension formula for linear maps implies that dim(im( f )) = dim(im( f ad )) = 2.\\nFrom the matrices F and F H we can see that\\n⎧⎡ ⎤ ⎡ ⎤⎫\\n⎧⎡ ⎤ ⎡ ⎤⎫\\n1 ⎬\\n1 ⎬\\n⎨ 1\\n⎨ 1 im( f ) = span ⎣ i ⎦ , ⎣0⎦ and im( f ad ) = span ⎣ −i ⎦ , ⎣0⎦ .\\n⎩\\n⎭\\n⎩\\n⎭\\n1\\n0\\n−i\\n0\\nThe equations ker( f ad ) = im( f )⊥ and ker( f ) = im( f ad )⊥ can be verified by direct computation.</td>\n",
       "      <td>196.0</td>\n",
       "      <td>196</td>\n",
       "      <td>13 Adjoints of Linear Maps</td>\n",
       "      <td>13.1 Basic Definitions and Properties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adjoints linear map ker f ad im f ker f im f ad proof w ker f ad f ad w v f ad w f v w v v hence w im f hand w im f f v w v f ad w v since non-degenerate f ad w hence w ker f ad using f ad ad f get ker f ker f ad ad im f ad example consider unitary vector space standard scalar product linear map f v fv f f ad v f h v f h matrix f f h rank therefore dim ker f dim ker f ad simple calculation show ker f span ker f ad span dimension formula linear map implies dim im f dim im f ad matrix f f h see im f span im f ad span equation ker f ad im f ker f im f ad verified direct computation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>13.2 Adjoint Endomorphisms and Matrices\\n\\n195\\n\\n13.2 Adjoint Endomorphisms and Matrices\\nWe now study the relation between the matrix representations of an endomorphism and its adjoint.\\nLet V be a finite dimensional unitary vector space with the scalar product ·, · and let f ∈ L(V, V).\\nFor an orthonormal basis B = {u 1 , . . . , u n } of V let [ f ] B,B = [ai j ] ∈ Cn,n , i.e., n f (u j ) = ak j u k , j = 1, . . . , n, k=1 and hence n ak j u k , u i = ai j , i, j = 1, . . . , n.\\nf (u j ), u i = k=1\\n\\nIf [ f ad ] B,B = [bi j ] ∈ Cn,n , i.e., n f ad (u j ) = bk j u k , j = 1, . . . , n, k=1 then bi j = f ad (u j ), u i = u j , f (u i ) = f (u i ), u j = a ji .\\nThus, [ f ad ] B,B = ([ f ] B,B ) H .\\nThe same holds for a finite dimensional Euclidean vector space, but then we can omit the complex conjugation.\\nTherefore, we have shown the following result.\\nTheorem 13.12 If V is a finite dimensional Euclidean or unitary vector space with the orthonormal basis B and f ∈ L(V, V), then\\n[ f ad ] B,B = ([ f ] B,B ) H .\\n(In the Euclidean case ([ f ] B,B ) H = ([ f ] B,B )T .)\\nAn important special class are the selfadjoint endomorphisms.\\nDefinition 13.13 Let V be a finite dimensional Euclidean or unitary vector space.\\nAn endomorphism f ∈ L(V, V) is called selfadjoint when f = f ad .\\nTrivial examples of selfadjoint endomorphism in L(V, V) are f = 0 and IdV .\\nCorollary 13.14\\n(1) If V is a finite dimensional Euclidean vector space, f ∈ L(V, V) is selfadjoint and B is an orthonormal basis of V, then [ f ] B,B is a symmetric matrix.</td>\n",
       "      <td>197.0</td>\n",
       "      <td>197</td>\n",
       "      <td>13 Adjoints of Linear Maps</td>\n",
       "      <td>13.2 Adjoint Endomorphisms and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adjoint endomorphisms matrix adjoint endomorphisms matrix study relation matrix representation endomorphism adjoint let v finite dimensional unitary vector space scalar product let f l v v orthonormal basis b u u n v let f b b ai j cn n n f u j ak j u k j n hence n ak j u k u ai j j f u j u f ad b b bi j cn n n f ad u j bk j u k j n bi j f ad u j u u j f u f u u j ji thus f ad b b f b b h hold finite dimensional euclidean vector space omit complex conjugation therefore shown following result theorem v finite dimensional euclidean unitary vector space orthonormal basis b f l v v f ad b b f b b h euclidean case f b b h f b b important special class selfadjoint endomorphisms definition let v finite dimensional euclidean unitary vector space endomorphism f l v v called selfadjoint f f ad trivial example selfadjoint endomorphism l v v f idv corollary v finite dimensional euclidean vector space f l v v selfadjoint b orthonormal basis v f b b symmetric matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>196\\n\\n13 Adjoints of Linear Maps\\n\\n(2) If V is a finite dimensional unitary vector space, f ∈ L(V, V) is selfadjoint and\\nB is an orthonormal basis of V, then [ f ] B,B is an Hermitian matrix.\\nThe selfadjoint endomorphisms again form a vector space.\\nHowever, one has to be careful to use the appropriate field over which this vector space is defined.\\nIn particular, the set of selfadjoint endomorphisms on a unitary vector space V does not form a C-vector space.\\nIf f = f ad ∈ L(V, V) \\ {0}, then (i f )ad = −i f ad = −i f = i f (cp.\\n(1) in Lemma 13.9).\\nSimilarly, the Hermitian matrices in Cn,n do not form a C-vector space.\\nIf A = A H ∈ Cn,n \\ {0} is Hermitian, then (i A) H = −i A H =\\n−i A = i A.\\nLemma 13.15\\n(1) If V is an n-dimensional Euclidean vector space, then the set of selfadjoint endomorphisms { f ∈ L(V, V) | f = f ad } forms an R-vector space of dimension n(n + 1)/2.\\n(2) If V is an n-dimensional unitary vector space, then the set of selfadjoint endomorphisms { f ∈ L(V, V) | f = f ad } forms an R-vector space of dimension n2.\\nProof Exercise.\\n\u0007\\n\u0006\\nA matrix A ∈ Cn,n with A = A T is called complex symmetric.\\nUnlike the Hermitian matrices, the complex symmetric matrices form a C-vector space.\\nLemma 13.16 The set of complex symmetric matrices in Cn,n forms a C-vector space of dimension n(n + 1)/2.\\nProof Exercise.\\n\u0007\\n\u0006\\nLemmas 13.15 and 13.16 will be used in Chap.\\n15 in our proof of the Fundamental\\nTheorem of Algebra.\\nExercises\\n13.1.\\nLet β(v, w) = w T Bv with B = diag(1, −1) be defined for v, w ∈ R2,1 .\\nConsider the linear maps f : R2,1 → R2,1 , v \u0004→ Fv, and h : R2,1 → R2,1 , w \u0004→ H w, where\\n\u0017 \u0018\\n\u0017 \u0018\\n12\\n10\\nF=\\n∈ R2,2 , H =\\n∈ R2,2 .\\n01\\n11\\nDetermine βv , β (1) and β (2) as in (13.1)–(13.2) as well as the right adjoint of f and the left adjoint of h with respect to β.\\n13.2.\\nLet (V, ·, · V ) and (W, ·, · W ) be two finite dimensional Euclidean vector spaces and let f ∈ L(V, W).\\nShow that there exists a unique g ∈\\nL(W, V) with f (v), w W = v, g(w) V for all v ∈ V and w ∈ W.\\n13.3.\\nLet v, w = w T Bv for all v, w ∈ R2,1 with\\n\u0017\\nB=\\n\\n\u0018\\n21\\n∈ R2,2 .\\n11</td>\n",
       "      <td>198.0</td>\n",
       "      <td>198</td>\n",
       "      <td>13 Adjoints of Linear Maps</td>\n",
       "      <td>13.2 Adjoint Endomorphisms and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adjoints linear map v finite dimensional unitary vector space f l v v selfadjoint b orthonormal basis v f b b hermitian matrix selfadjoint endomorphisms form vector space however one careful use appropriate field vector space defined particular set selfadjoint endomorphisms unitary vector space v form c-vector space f f ad l v v f ad f ad f f cp lemma similarly hermitian matrix cn n form c-vector space h cn n hermitian h h lemma v n-dimensional euclidean vector space set selfadjoint endomorphisms f l v v f f ad form r-vector space dimension n n v n-dimensional unitary vector space set selfadjoint endomorphisms f l v v f f ad form r-vector space dimension proof exercise matrix cn n called complex symmetric unlike hermitian matrix complex symmetric matrix form c-vector space lemma set complex symmetric matrix cn n form c-vector space dimension n n proof exercise lemma used chap proof fundamental theorem algebra exercise let β v w w bv b diag defined v w consider linear map f v fv h w h w h determine βv β β well right adjoint f left adjoint h respect β let v v w w two finite dimensional euclidean vector space let f l v w show exists unique g l w v f v w w v g w v v v w let v w w bv v w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>13.2 Adjoint Endomorphisms and Matrices\\n\\n197\\n\\n(a) Show that v, w = w T Bv is a scalar product on R2,1 .\\n(b) Using this scalar product, determine the adjoint map f ad of f : R2,1 →\\nR2,1 , v \u0004→ Fv, with F ∈ R2,2 .\\n(c) Investigate which properties F needs to satisfy so that f is selfadjoint.\\n13.4.\\nLet n ≥ 2 and f : Rn,1 → Rn,1 , [x1 , . . . , xn ]T \u0004→ [0, x1 , . . . , xn−1 ]T .\\n\\n13.5.\\n13.6.\\n\\n13.7.\\n\\n13.8.\\n\\n13.9.\\n13.10.\\n\\n13.11.\\n\\n13.12.\\n\\nDetermine the adjoint f ad of f with respect to the standard scalar product of\\nRn,1 .\\nLet V be a finite dimensional Euclidean or unitary vector space and let f ∈\\nL(V, V).\\nShow that ker( f ad ◦ f ) = ker( f ) and im( f ad ◦ f ) = im( f ad ).\\nLet V be a finite dimensional Euclidean or unitary vector space, let U ⊆ V be a subspace and let f ∈ L(V, V) with f (U) ⊆ U. Show that then f ad (U ⊥ ) ⊆\\nU ⊥.\\nLet V be a finite dimensional Euclidean or unitary vector space, let f ∈\\nL(V, V) and v ∈ V. Show that v ∈ im( f ) if and only if v ∈ ker( f ad )⊥ .\\n“Matrix version”: For A ∈ Cn,n and b ∈ Cn,1 the linear system of equations\\nAx = b has a solution if and only if b ∈ L (A H , 0)⊥ .\\nLet V be a finite dimensional Euclidean or unitary vector space and let f, g ∈\\nL(V, V) be selfadjoint.\\nShow that f ◦ g is selfadjoint if and only if f and g commute, i.e., f ◦ g = g ◦ f .\\nLet V be a finite dimensional unitary vector space and let f ∈ L(V, V).\\nShow that f is selfadjoint if and only if f (v), v ∈ R holds for all v ∈ V.\\nLet V be a finite dimensional Euclidean or unitary vector space and let f ∈\\nL(V, V) be a projection, i.e., f satisfies f 2 = f .\\nShow that f is selfadjoint if and only if ker( f ) ⊥ im( f ), i.e., v, w = 0 holds for all v ∈ ker( f ) and w ∈ im( f ).\\nLet V be a finite dimensional Euclidean or unitary vector space and let f, g ∈\\nL(V, V).\\nShow that if g ad ◦ f = 0 ∈ L(V, V), then v, w = 0 holds for all v ∈ im( f ) and w ∈ im(g).\\nFor two polynomials p, q ∈ R[t]≤n let\\n\u0019 1 p(t)q(t) dt.\\np, q :=\\n−1\\n\\n(a) Show that this defines a scalar product on R[t]≤n .\\n(b) Consider the map n f : R[t]≤n → R[t]≤n , p= n\\n\\nαi t i \u0004→ i=0 iαi t i−1 , i=1 and determine f ad , ker( f ad ), im( f ), ker( f ad )⊥ and im( f )⊥ .\\n13.13.\\nProve Lemma 13.15.\\n13.14.\\nProve Lemma 13.16.</td>\n",
       "      <td>199.0</td>\n",
       "      <td>199</td>\n",
       "      <td>13 Adjoints of Linear Maps</td>\n",
       "      <td>13.2 Adjoint Endomorphisms and Matrices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adjoint endomorphisms matrix show v w w bv scalar product b using scalar product determine adjoint map f ad f v fv f c investigate property f need satisfy f selfadjoint let n f xn determine adjoint f ad f respect standard scalar product let v finite dimensional euclidean unitary vector space let f l v v show ker f ad f ker f im f ad f im f ad let v finite dimensional euclidean unitary vector space let u v subspace let f l v v f u u show f ad u u let v finite dimensional euclidean unitary vector space let f l v v v show v im f v ker f ad matrix version cn n b linear system equation ax b solution b l h let v finite dimensional euclidean unitary vector space let f g l v v selfadjoint show f g selfadjoint f g commute f g g f let v finite dimensional unitary vector space let f l v v show f selfadjoint f v v r hold v let v finite dimensional euclidean unitary vector space let f l v v projection f satisfies f f show f selfadjoint ker f im f v w hold v ker f w im f let v finite dimensional euclidean unitary vector space let f g l v v show g ad f l v v v w hold v im f w im g two polynomial p q r let p q dt p q show defines scalar product r b consider map n f r r n αi iαi determine f ad ker f ad im f ker f ad im f prove lemma prove lemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Chapter 14\\n\\nEigenvalues of Endomorphisms\\n\\nIn previous chapters we have already studied eigenvalues and eigenvectors of matrices.\\nIn this chapter we generalize these concepts to endomorphisms, and we investigate when endomorphisms on finite dimensional vector spaces can be represented by diagonal matrices or (upper) triangular matrices.\\nFrom such representations we easily can read off important information about the endomorphism, in particular its eigenvalues.\\n\\n14.1 Basic Definitions and Properties\\nWe first consider an arbitrary vector space and then concentrate on the finite dimensional case.\\nDefinition 14.1 Let V be a K -vector space and f ∈ L(V, V).\\nIf λ ∈ K and v ∈\\nV \\ {0} satisfy f (v) = λv, then λ is called an eigenvalue of f , and v is called an eigenvector of f corresponding to λ.\\nBy definition, v = 0 cannot be an eigenvector, but an eigenvalue λ = 0 may occur\\n(cp. the example following Definition 8.7).\\nThe equation f (v) = λv can be written as\\n0 = λv − f (v) = (λIdV − f )(v).\\nHence, λ ∈ K is an eigenvalue of f if and only if ker(λIdV − f ) \u0003= {0}.\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_14\\n\\n199</td>\n",
       "      <td>200.0</td>\n",
       "      <td>&lt;FEFF&gt;</td>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.1 Basic Definitions and Properties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter eigenvalue endomorphisms previous chapter already studied eigenvalue eigenvectors matrix chapter generalize concept endomorphisms investigate endomorphisms finite dimensional vector space represented diagonal matrix upper triangular matrix representation easily read important information endomorphism particular eigenvalue basic definition property first consider arbitrary vector space concentrate finite dimensional case definition let v k space f l v v λ k v v satisfy f v λv λ called eigenvalue f v called eigenvector f corresponding λ definition v eigenvector eigenvalue λ may occur cp example following definition equation f v λv written λv f v λidv f v hence λ k eigenvalue f ker λidv f springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>200\\n\\n14 Eigenvalues of Endomorphisms\\n\\nWe already know that the kernel of an endomorphism on V forms a subspace of V\\n(cp.\\nLemma 10.7).\\nThis holds, in particular, for ker(λIdV − f ).\\nDefinition 14.2 If V is a K -vector space and λ ∈ K is an eigenvalue of f ∈ L(V, V), then the subspace\\nV f (λ) := ker(λIdV − f ) is called the eigenspace of f corresponding to λ and g(λ, f ) := dim(V f (λ)) is called the geometric multiplicity of the eigenvalue λ.\\nBy definition, the eigenspace V f (λ) is spanned by all eigenvectors of f corresponding to the eigenvalue λ.\\nIf V f (λ) is finite dimensional, then g(λ, f ) = dim(V f (λ)) is equal to the maximal number of linearly independent eigenvectors of f corresponding to λ.\\nDefinition 14.3 Let V be a K -vector space, let U ⊆ V be a subspace, and let f ∈ L(V, V).\\nIf f (U) ⊆ U, i.e., if f (u) ∈ U holds for all u ∈ U, then U is called an f -invariant subspace of V.\\nAn important example of f -invariant subspaces are the eigenspaces of f .\\nLemma 14.4 If V is a K -vector space and λ ∈ K is an eigenvalue of f ∈ L(V, V), then V f (λ) is an f -invariant subspace of V.\\nProof For every v ∈ V f (λ) we have f (v) = λv ∈ V f (λ).\\n\\n\u0006\\n\u0005\\n\\nWe now consider finite dimensional vector spaces and discuss the relationship between the eigenvalues of f and the eigenvalues of a matrix representation of f with respect to a given basis.\\nLemma 14.5 If V is a finite dimensional K -vector space and f ∈ L(V, V), then the following statements are equivalent:\\n(1) λ ∈ K is an eigenvalue of f .\\n(2) λ ∈ K is an eigenvalue of the matrix [ f ] B,B for every basis B of V.\\nProof Let λ ∈ K be an eigenvalue of f and let B = {v1 , . . . , vn } be an arbitrary basis of V. If v ∈ V is an eigenvector of f corresponding to the eigenvalue λ, then f (v) = λv and\u0002there exist (unique) coordinates μ1 , . . . , μn ∈ K , not all equal to zero, with v = nj=1 μ j v j .\\nUsing (10.4) we obtain\\n⎡\\n\\n⎤\\n⎡ ⎤\\nμ1\\nμ1\\n⎢ .. ⎥\\n⎢ .. ⎥\\n[ f ] B,B ⎣ . ⎦ = \u0002 B ( f (v)) = \u0002 B (λv) = λ\u0002 B (v) = λ ⎣ . ⎦ ,\\nμn\\n\\nμn</td>\n",
       "      <td>201.0</td>\n",
       "      <td>201</td>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.1 Basic Definitions and Properties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eigenvalue endomorphisms already know kernel endomorphism v form subspace v cp lemma hold particular ker λidv f definition v k space λ k eigenvalue f l v v subspace v f λ ker λidv f called eigenspace f corresponding λ g λ f dim v f λ called geometric multiplicity eigenvalue λ definition eigenspace v f λ spanned eigenvectors f corresponding eigenvalue λ v f λ finite dimensional g λ f dim v f λ equal maximal number linearly independent eigenvectors f corresponding λ definition let v k space let u v subspace let f l v v f u u f u u hold u u u called f subspace important example f subspace eigenspaces f lemma v k space λ k eigenvalue f l v v v f λ f subspace proof every v v f λ f v λv v f λ consider finite dimensional vector space discus relationship eigenvalue f eigenvalue matrix representation f respect given basis lemma v finite dimensional k space f l v v following statement equivalent λ k eigenvalue f λ k eigenvalue matrix f b b every basis b proof let λ k eigenvalue f let b vn arbitrary basis v v eigenvector f corresponding eigenvalue λ f v λv exist unique coordinate μn k equal zero v μ j v j using obtain f b b b f v b λv b v λ μn μn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>14.1 Basic Definitions and Properties\\n\\n201 and thus λ is an eigenvalue of [ f ] B,B .\\nIf, on the other hand, [ f ] B,B [μ1 , . . . , μn ]T = λ[μ1 , . . . , μn ]T with [μ1 , . . . ,\\n\u0003= 0 for a given (arbitrary) basis B = {v1 , . . . , vn } of V, then we set\\nμn ]T \u0002 v := nj=1 μ j v j .\\nThen v \u0003= 0 and\\n⎤\\n⎡ ⎤\\nμ1\\nμ1\\n⎢ ⎥\\n⎢ ⎥ f (v) =\\nμ j f (v j ) = ( f (v1 ), . . . , f (vn )) ⎣ ... ⎦ = (v1 , . . . , vn )[ f ] B,B ⎣ ... ⎦ j=1\\nμn\\nμn\\n⎛ ⎡ ⎤⎞\\nμ1\\n⎜ ⎢ .. ⎥⎟\\n= (v1 , . . . , vn ) ⎝λ ⎣ . ⎦⎠ = λv,\\n⎡ n\\n\\nμn i.e., λ is an eigenvalue of f .\\n\\n\u0006\\n\u0005\\n\\nLemma 14.5 implies that the eigenvalues of f are the roots of the characteristic polynomial of the matrix [ f ] B,B (cp.\\nTheorem 8.8).\\nThis, however, does not hold\\n\u0012 in general for a matrix representation of the form [ f ] B, \u0012\\nB , where B and B are two different bases of V. In general, the two matrices\\n[ f ] B, \u0012\\nB = [IdV ] B, \u0012\\nB [ f ] B,B and [ f ] B,B do not have the same eigenvalues.\\nExample 14.6 Consider the vector space R2,1 with the bases\\nB=\\n\\n\u0013\u0014 \u0015 \u0014 \u0015\u0016\\n0\\n1\\n,\\n,\\n1\\n0\\n\\n\u0012\\nB=\\n\\n\u0013\u0014\\n\\n\u0015 \u0014 \u0015\u0016\\n1\\n1\\n.\\n,\\n1\\n−1\\n\\nThen the endomorphism\\n\u0014 f : R2,1 → R2,1 , v \b→ Fv, where F =\\n\\n\u0015\\n01\\n,\\n10 has the matrix representations\\n\u0015\\n\u0014\\n\u0015\\n1 −1 1\\n01\\n.\\n, [ f ] B, \u0012\\n=\\nB =\\n11\\n10\\n2\\n\u0014\\n\\n[ f ] B,B\\n\\nWe have det(t I2 − [ f ] B,B ) = t 2 − 1, and thus f has the eigenvalues −1 and 1.\\nOn\\n1\\n2 the other hand, the characteristic polynomial of [ f ] B, \u0012\\nB is t − 2 , so that this matrix\\n√\\n√ has the eigenvalues −1/ 2 and 1/ 2.\\nFor two different bases B and \u0012\\nB of V the matrices [ f ] B,B and [ f ] \u0012\\nB, \u0012\\nB are similar\\n(cp. the discussion following Corollary 10.20).\\nIn Theorem 8.12 we have shown that</td>\n",
       "      <td>202.0</td>\n",
       "      <td>202</td>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.1 Basic Definitions and Properties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic definition property thus λ eigenvalue f b b hand f b b μn λ μn given arbitrary basis b vn v set μn v μ j v j v f v μ j f v j f f vn vn f b b μn μn vn λv n μn λ eigenvalue f lemma implies eigenvalue f root characteristic polynomial matrix f b b cp theorem however hold general matrix representation form f b b b b two different base general two matrix f b b idv b b f b b f b b eigenvalue example consider vector space base endomorphism f v fv f matrix representation f b b f b b det f b b thus f eigenvalue hand characteristic polynomial f b b matrix eigenvalue two different base b b v matrix f b b f b b similar cp discussion following corollary theorem shown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>202\\n\\n14 Eigenvalues of Endomorphisms similar matrices have the same characteristic polynomial.\\nThis justifies the following definition.\\nDefinition 14.7 If n ∈ N, V is an n-dimensional K -vector space with the basis B, and f ∈ L(V, V), then\\nP f := det(t In − [ f ] B,B ) ∈ K [t] is called the characteristic polynomial of f .\\nThe characteristic polynomial P f is always a monic polynomial with deg(P f ) = n = dim(V).\\nAs we have discussed before, P f is independent of the choice of the basis of V. A scalar λ ∈ K is an eigenvalue of f if and only if λ is a root of P f , i.e., P f (λ) = 0.\\nAs shown in Example 8.9, in real vector spaces with dimensions at least two, there exist endomorphisms that do not have eigenvalues.\\nIf λ is a root of P f , then P f = (t − λ) · q for a monic polynomial q ∈ K [t], i.e., the linear factor t − λ divides the polynomial P f ; we will show this formally in\\nCorollary 15.5 below.\\nIf also q(λ) = 0, then q = (t − λ) · \u0012 q for a monic polynomial q .\\nWe can continue until P f = (t − λ)d · g for a\\n\u0012 q ∈ K [t], and thus P f = (t − λ)2 · \u0012 g ∈ K [t] with g(λ) \u0003= 0.\\nThis leads to the following definition.\\nDefinition 14.8 Let V be a finite dimensional K -vector space, and let f ∈ L(V, V) have the eigenvalue λ ∈ K .\\nIf the characteristic polynomial of f has the form\\nP f = (t − λ)d · g for some g ∈ K [t] with g(λ) \u0003= 0, then d is called the algebraic multiplicity of the eigenvalue λ of f .\\nIt is denoted by a(λ, f ).\\nIf λ1 , . . . , λk are the pairwise distinct eigenvalues of f with corresponding algebraic multiplicities a(λ1 , f ), . . . , a(λk , f ), and if dim(V) = n, then a(λ1 , f ) + . . . + a(λk , f ) ≤ n, since deg(P f ) = dim(V) = n.\\nExample 14.9 The endomorphism f : R4,1 → R4,1 , v \b→ Fv with\\n⎡\\n\\n1\\n⎢0\\n⎢\\nF =⎣\\n0\\n0\\n\\n⎤\\n2 34\\n1 2 3⎥\\n⎥ ∈ R4,4 ,\\n0 0 1⎦\\n0 −1 0 has the characteristic polynomial P f = (t −1)2 (t 2 +1).\\nThe only real root of P f is 1, and a(λ1 , f ) = 2 &lt; 4 = dim(R4,1 ).</td>\n",
       "      <td>203.0</td>\n",
       "      <td>203</td>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.1 Basic Definitions and Properties</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eigenvalue endomorphisms similar matrix characteristic polynomial justifies following definition definition n n v n-dimensional k space basis b f l v v p f det f b b k called characteristic polynomial f characteristic polynomial p f always monic polynomial deg p f n dim v discussed p f independent choice basis scalar λ k eigenvalue f λ root p f p f λ shown example real vector space dimension least two exist endomorphisms eigenvalue λ root p f p f λ q monic polynomial q k linear factor λ divide polynomial p f show formally corollary also q λ q λ q monic polynomial q continue p f λ g q k thus p f λ g k g λ lead following definition definition let v finite dimensional k space let f l v v eigenvalue λ k characteristic polynomial f form p f λ g g k g λ called algebraic multiplicity eigenvalue λ f denoted λ f λk pairwise distinct eigenvalue f corresponding algebraic multiplicity f λk f dim v n f λk f n since deg p f dim v example endomorphism f v fv f characteristic polynomial p f real root p f f dim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>14.1 Basic Definitions and Properties\\n\\n203\\n\\nLemma 14.10 If V is a finite dimensional K -vector space and f ∈ L(V, V), then g(λ, f ) ≤ a(λ, f ) for every eigenvalue λ of f .\\nProof Let λ ∈ K be an eigenvalue of f with geometric multiplicity m = g(λ, f ).\\nThen there exist m linear independent eigenvectors v1 , . . . , vm ∈ V of f corresponding to the eigenvalue λ.\\nIf m = dim(V), then these m eigenvectors form a basis B of V. If m &lt; dim(V) = n, then we can extend the m eigenvectors to a basis\\nB = {v1 , . . . , vm , vm+1 , . . . , vn } of V.\\nWe have f (v j ) = λv j for j = 1, . . . , m and, therefore,\\n\u0014\\n[ f ] B,B =\\n\\nλIm Z 1\\n0 Z2\\n\\n\u0015 for two matrices Z 1 ∈ K m,n−m and Z 2 ∈ K n−m,n−m .\\nUsing (1) in Lemma 7.10 we obtain\\nP f = det(t In − [ f ] B,B ) = (t − λ)m · det(t In−m − Z 2 ), which implies a(λ, f ) ≥ m = g(λ, f ).\\n\\n\u0006\\n\u0005\\n\\nIn the following we will try to find a basis of V, so that the eigenvalues of a given endomorphism f can be read off easily from its matrix representation.\\nThe easiest forms of matrices in this sense are diagonal and triangular matrices, since their eigenvalues are just their diagonal entries.\\n\\n14.2 Diagonalizability\\nIn this section we will analyze when for a given endomorphism has a diagonal matrix representation.\\nWe formally define this property as follows.\\nDefinition 14.11 Let V be a finite dimensional K -vector space.\\nAn endomorphism f ∈ L(V, V) is called diagonalizable, if there exists a basis B of V, such that [ f ] B,B is a diagonal matrix.\\nAccordingly, a matrix A ∈ K n,n is diagonalizable when there exists a matrix\\nS ∈ G L n (K ) with A = S DS −1 for a diagonal matrix D ∈ K n,n .\\nIn order to analyze the diagonalizablility, we begin with a sufficient condition for the linear independence of eigenvectors.\\nThis condition also holds when V is infinite dimensional.\\nLemma 14.12 Let V be a K -vector space and f ∈ L(V, V).\\nIf λ1 , . . . , λk ∈ K , k ≥ 2, are pairwise distinct eigenvalues of f with corresponding eigenvectors v1 , . . . , vk ∈ V, then v1 , . . . , vk are linearly independent.</td>\n",
       "      <td>204.0</td>\n",
       "      <td>204</td>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.2 Diagonalizability</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic definition property lemma v finite dimensional k space f l v v g λ f λ f every eigenvalue λ f proof let λ k eigenvalue f geometric multiplicity g λ f exist linear independent eigenvectors vm v f corresponding eigenvalue λ dim v eigenvectors form basis b dim v n extend eigenvectors basis b vm vn f v j λv j j therefore f b b λim z two matrix z k z k using lemma obtain p f det f b b λ det z implies λ f g λ f following try find basis v eigenvalue given endomorphism f read easily matrix representation easiest form matrix sense diagonal triangular matrix since eigenvalue diagonal entry diagonalizability section analyze given endomorphism diagonal matrix representation formally define property follows definition let v finite dimensional k space endomorphism f l v v called diagonalizable exists basis b v f b b diagonal matrix accordingly matrix k n n diagonalizable exists matrix g l n k d diagonal matrix k n n order analyze diagonalizablility begin sufficient condition linear independence eigenvectors condition also hold v infinite dimensional lemma let v k space f l v v λk k k pairwise distinct eigenvalue f corresponding eigenvectors vk v vk linearly independent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>204\\n\\n14 Eigenvalues of Endomorphisms\\n\\nProof We prove the assertion by induction on k.\\nLet k = 2 and let v1 , v2 be eigenvectors of f corresponding to the eigenvalues λ1 \u0003= λ2 .\\nLet μ1 , μ2 ∈ K with\\nμ1 v1 + μ2 v2 = 0.\\nApplying f on both sides of this equation as well as multiplying the equation with λ2 yields the two equations\\nμ1 λ1 v1 + μ2 λ2 v2 = 0,\\nμ1 λ2 v1 + μ2 λ2 v2 = 0.\\nSubtracting the second equation from the first, we get μ1 (λ1 − λ2 )v1 = 0.\\nSince\\nλ1 \u0003= λ2 and v1 \u0003= 0, we have μ1 = 0.\\nThen from μ1 v1 + μ2 v2 = 0 we also obtain\\nμ2 = 0, since v2 \u0003= 0.\\nThus, v1 and v2 are linearly independent.\\nThe proof of the inductive step is analogous.\\nWe assume that the assertion holds for some k ≥ 2.\\nLet λ1 , . . . , λk+1 be pairwise distinct eigenvalues of f with corresponding eigenvectors v1 , . . . , vk+1 , and let μ1 , . . . , μk+1 ∈ K satisfy\\nμ1 v1 + . . . + μk vk + μk+1 vk+1 = 0.\\nApplying f to this equation yields\\nμ1 λ1 v1 + . . . + μk λk vk + μk+1 λk+1 vk+1 = 0, while a multiplication with λk+1 gives\\nμ1 λk+1 v1 + . . . + μk λk+1 vk + μk+1 λk+1 vk+1 = 0.\\nSubtracting this equation from the previous one we get\\nμ1 (λ1 − λk+1 )v1 + . . . + μk (λk − λk+1 )vk = 0.\\nSince λ1 , . . . , λk+1 are pairwise distinct and v1 , . . . , vk are linearly independent by the induction hypothesis, we obtain μ1 = · · · = μk = 0.\\nBut then μk+1 vk+1 = 0\\n\u0006\\n\u0005 implies that also μk+1 = 0, so that v1 , . . . , vk+1 are linearly independent.\\nUsing this result we next show that the sum of eigenspaces corresponding to pairwise distinct eigenvalues is direct (cp.\\nTheorem 9.31).\\nLemma 14.13 Let V be a K -vector space and f ∈ L(V, V).\\nIf λ1 , . . . , λk ∈ K , k ≥ 2, are pairwise distinct eigenvalues of f , then the corresponding eigenspaces satisfy k\\n\\nV f (λi ) ∩\\n\\nV f (λ j ) = {0} j=1 j\u0003 =i for all i = 1, . . . , k.</td>\n",
       "      <td>205.0</td>\n",
       "      <td>205</td>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.2 Diagonalizability</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eigenvalue endomorphisms proof prove assertion induction let k let eigenvectors f corresponding eigenvalue let k applying f side equation well multiplying equation yield two equation subtracting second equation first get since also obtain since thus linearly independent proof inductive step analogous assume assertion hold k let pairwise distinct eigenvalue f corresponding eigenvectors let k satisfy μk vk applying f equation yield μk λk vk multiplication give μk vk subtracting equation previous one get μk λk vk since pairwise distinct vk linearly independent induction hypothesis obtain μk implies also linearly independent using result next show sum eigenspaces corresponding pairwise distinct eigenvalue direct cp theorem lemma let v k space f l v v λk k k pairwise distinct eigenvalue f corresponding eigenspaces satisfy k v f λi v f λ j k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>14.2 Diagonalizability\\n\\n205\\n\\nProof Let i be fixed and let k v ∈ V f (λi ) ∩\\n\\nV f (λ j ).\\nj=1 j\u0003 =i\\n\\n\u0002\\nIn j\u0003=i v j for some v j ∈ V f (λ j ), j \u0003 = i.\\nThen −v +\\n\u0002 particular, we have v = v\\n=\\n0, and the linear independence of eigenvectors corresponding to pairwise j\u0003=i j distinct eigenvalues (cp.\\nLemma 14.12) implies v = 0.\\n\u0006\\n\u0005\\nThe following theorem gives necessary and sufficient conditions for the diagonalizability of an endomorphism on a finite dimensional vector space.\\nTheorem 14.14 If V is a finite dimensional K -vector space and f ∈ L(V, V), then the following statements are equivalent:\\n(1) f is diagonalizable.\\n(2) There exists a basis of V consisting of eigenvectors of f .\\n(3) The characteristic polynomial P f decomposes into n = dim(V) linear factors over K , i.e.,\\nP f = (t − λ1 ) · . . . · (t − λn ) with the eigenvalues λ1 , . . . , λn ∈ K of f , and for every eigenvalue λ j we have g(λ j , f ) = a(λ j , f ).\\nProof\\n(1) ⇔ (2): If f ∈ L(V, V) is diagonalizable, then there exists a basis B =\\n{v1 , . . . , vn } of V and scalars λ1 , . . . , λn ∈ K with\\n\\n[ f ] B,B\\n\\n⎡\\nλ1\\n⎢ ..\\n=⎣\\n.\\n\\n⎤\\n⎥\\n⎦,\\n\\n(14.1)\\n\\nλn and hence f (v j ) = λ j v j , j = 1, . . . , n.\\nThe scalars λ1 , . . . , λn are thus eigenvalues of f , and the corresponding eigenvectors are v1 , . . . , vn .\\nIf, on the other hand, there exists a basis B = {v1 , . . . , vn } of V consisting of eigenvectors of f , then f (v j ) = λ j v j , j = 1, . . . , n, for scalars λ1 , . . . , λn ∈ K\\n(the corresponding eigenvalues), and hence [ f ] B,B has the form (14.1).\\n(2) ⇒ (3): Let B = {v1 , . . . , vn } be a basis of V consisting of eigenvectors of f , and let λ1 , . . . , λn ∈ K be the corresponding eigenvalues.\\nThen [ f ] B,B has the form (14.1) and hence\\nP f = (t − λ1 ) · . . . · (t − λn ), so that P f decomposes into linear factors over K .</td>\n",
       "      <td>206.0</td>\n",
       "      <td>206</td>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.2 Diagonalizability</td>\n",
       "      <td>NaN</td>\n",
       "      <td>diagonalizability proof let fixed let k v v f λi v f λ j v j v j v f λ j j particular v v linear independence eigenvectors corresponding pairwise j distinct eigenvalue cp lemma implies v following theorem give necessary sufficient condition diagonalizability endomorphism finite dimensional vector space theorem v finite dimensional k space f l v v following statement equivalent f diagonalizable exists basis v consisting eigenvectors f characteristic polynomial p f decomposes n dim v linear factor k p f λn eigenvalue λn k f every eigenvalue λ j g λ j f λ j f proof f l v v diagonalizable exists basis b vn v scalar λn k f b b λn hence f v j λ j v j j scalar λn thus eigenvalue f corresponding eigenvectors vn hand exists basis b vn v consisting eigenvectors f f v j λ j v j j n scalar λn k corresponding eigenvalue hence f b b form let b vn basis v consisting eigenvectors f let λn k corresponding eigenvalue f b b form hence p f λn p f decomposes linear factor k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>206\\n\\n14 Eigenvalues of Endomorphisms\\n\\nWe still have to show that g(λ j , f ) = a(λ j , f ) for every eigenvalue λ j .\\nThe eigenvalue λ j has the algebraic multiplicity m j := a(λ j , f ) if and only if λ j occurs m j times on the diagonal of the (diagonal) matrix [ f ] B,B .\\nThis holds if and only if exactly m j vectors of the basis B are eigenvectors of f corresponding to the eigenvalue λ j .\\nEach of these m j linearly independent vectors is a element of the eigenspace V f (λ j ) and, hence, dim(V f (λ j )) = g(λ j , f ) ≥ m j = a(λ j , f ).\\nFrom Lemma 14.10 we know that g(λ j , f ) ≤ a(λ j , f ), and thus g(λ j , f ) = a(λ j , f ).\\nλk be the pairwise distinct eigenvalues of f with correspond(3) ⇒ (2): Let \u0012\\nλ1 , . . . , \u0012\\nλ j , f ), j = 1, . . . , k, ing geometric and algebraic multiplicities g(\u0012\\nλ j , f ) and a(\u0012 respectively.\\nSince P f decomposes into linear factors, we have k a(\u0012\\nλ j , f ) = n = dim(V).\\nj=1\\n\\nNow g(\u0012\\nλ j , f ) = a(\u0012\\nλ j , f ), j = 1, . . . , k, implies that k g(\u0012\\nλ j , f ) = n = dim(V).\\nj=1\\n\\nBy Lemma 14.13 we obtain (cp. also Theorem 9.31)\\nV f (\u0012\\nλ1 ) ⊕ . . . ⊕ V f (\u0012\\nλk ) = V.\\nλ j ), j = 1, . . . , k, then we\\nIf we select bases of the respective eigenspaces V f (\u0012 get a basis of V that consists of eigenvectors of f .\\n\u0006\\n\u0005\\nTheorem 14.14 and Lemma 14.12 imply an important sufficient condition for diagonalizability.\\nCorollary 14.15 If V is an n-dimensional K -vector space and f ∈ L(V, V) has n pairwise distinct eigenvalues, then f is diagonalizable.\\nThe condition of having n = dim(V) pairwise distinct eigenvalues is, however, not necessary for the diagonalizability of an endomorphism.\\nA simple counterexample is the identity IdV , which has the n-fold eigenvalue 1, while [IdV ] B,B = In holds for every basis B of V. On the other hand, there exist endomorphisms with multiple eigenvalues that are not diagonalizable.</td>\n",
       "      <td>207.0</td>\n",
       "      <td>207</td>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.2 Diagonalizability</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eigenvalue endomorphisms still show g λ j f λ j f every eigenvalue λ j eigenvalue λ j algebraic multiplicity j λ j f λ j occurs j time diagonal diagonal matrix f b b hold exactly j vector basis b eigenvectors f corresponding eigenvalue λ j j linearly independent vector element eigenspace v f λ j hence dim v f λ j g λ j f j λ j f lemma know g λ j f λ j f thus g λ j f λ j f λk pairwise distinct eigenvalue f correspond let λ j f j k ing geometric algebraic multiplicity g λ j f respectively since p f decomposes linear factor k λ j f n dim v g λ j f λ j f j k implies k g λ j f n dim v lemma obtain cp also theorem v f v f λk λ j j k select base respective eigenspaces v f get basis v consists eigenvectors f theorem lemma imply important sufficient condition diagonalizability corollary v n-dimensional k space f l v v n pairwise distinct eigenvalue f diagonalizable condition n dim v pairwise distinct eigenvalue however necessary diagonalizability endomorphism simple counterexample identity idv n-fold eigenvalue idv b b hold every basis b hand exist endomorphisms multiple eigenvalue diagonalizable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>14.2 Diagonalizability\\n\\n207\\n\\nExample 14.16 The endomorphism\\n\u0014 f :R\\n\\n2,1\\n\\n\u0015\\n11\\n→ R , v \b→ Fv with F =\\n,\\n01\\n2,1 has the characteristic polynomial (t − 1)2 and thus only has the eigenvalue 1.\\nWe have ker(V f (1)) = span{[1, 0]T } and thus g(1, f ) = 1 &lt; a(1, f ) = 2.\\nBy Theorem 14.14, f is not diagonalizable.\\n\\n14.3 Triangulation and Schur’s Theorem\\nIf the property g(λ j , f ) = a(λ j , f ) does not hold for every eigenvalue λ j of f , then f is not diagonalizable.\\nHowever, as long as the characteristic polynomial P f decomposes into linear factors, we can find a special basis B such that [ f ] B,B is a triangular matrix.\\nTheorem 14.17 If V is a finite dimensional K -vector space and f ∈ L(V, V), then the following statements are equivalent:\\n(1) The characteristic polynomial P f decomposes into linear factors over K .\\n(2) There exists a basis B of V such that [ f ] B,B is upper triangular, i.e., f can be triangulated.\\nProof\\n(2) ⇒ (1): If n = dim(V) and [ f ] B,B = [ri j ] ∈ K n,n is upper triangular, then\\nP f = (t − r11 ) · . . . · (t − rnn ).\\n(1) ⇒ (2): We show the assertion by induction on n = dim(V).\\nThe case n = 1 is trivial, since then [ f ] B,B ∈ K 1,1 .\\nSuppose that the assertion holds for an n ≥ 1, and let dim(V) = n + 1.\\nBy assumption,\\nP f = (t − λ1 ) · . . . · (t − λn+1 ), where λ1 , . . . , λn+1 ∈ K are the eigenvalues of f .\\nLet v1 ∈ V be an eigenvector corresponding to the eigenvalue λ1 .\\nWe extend this vector to a basis\\nB = {v1 , w2 , . . . , wn+1 } of V. With BW := {w2 , . . . , wn+1 } and W := span BW we have V = span{v1 } ⊕ W and\\n⎡\\n\\n[ f ] B,B\\n\\nλ1 a12\\n⎢ 0 a22\\n⎢\\n=⎢ .\\n..\\n⎣ ..\\n.\\n0 an+1,2\\n\\n⎤\\n· · · a1,n+1\\n. . . a2,n+1 ⎥\\n⎥\\n⎥.\\n..\\n..\\n⎦\\n.\\n.\\n· · · an+1,n+1</td>\n",
       "      <td>208.0</td>\n",
       "      <td>208</td>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.3 Triangulation and Schur's Theorem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>diagonalizability example endomorphism f r r v fv f characteristic polynomial thus eigenvalue ker v f span thus g f f theorem f diagonalizable triangulation schur theorem property g λ j f λ j f hold every eigenvalue λ j f f diagonalizable however long characteristic polynomial p f decomposes linear factor find special basis b f b b triangular matrix theorem v finite dimensional k space f l v v following statement equivalent characteristic polynomial p f decomposes linear factor k exists basis b v f b b upper triangular f triangulated proof n dim v f b b ri j k n n upper triangular p f rnn show assertion induction n dim v case n trivial since f b b k suppose assertion hold n let dim v n assumption p f k eigenvalue f let v eigenvector corresponding eigenvalue extend vector basis b bw w span bw v span w f b b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>208\\n\\n14 Eigenvalues of Endomorphisms\\n\\nWe define h ∈ L(W, span{v1 }) and g ∈ L(W, W) by n+1 h(w j ) := a1 j v1 and g(w j ) := a k j wk , j = 2, . . . , n + 1.\\nk=2\\n\\nThen f (w) = h(w) + g(w) for all w ∈ W, and\\n\u0014\\n[ f ] B,B =\\n\\n\u0015\\nλ1 [h] BW ,{v1 }\\n.\\n0 [g] BW ,BW\\n\\nConsequently,\\n(t − λ1 )Pg = P f = (t − λ1 ) · . . . · (t − λn+1 ), and hence Pg = (t − λ2 ) · . . . · (t − λn+1 ).\\nNow dim(W) = n and the characteristic polynomial of g ∈ L(W, W) decomposes into linear factors.\\nBy the w2 , . . . , w\\n\u0017n+1 } of W such that induction hypothesis there exists a basis \u0017\\nBW = {\u0017 upper triangular.\\nThus, for the basis\\nB\\n:=\\n{v\\n\u00172 , . . . , w\\n\u0017n+1 } the\\n[g] \u0017\\n1\\n1, w\\nBW\\nBW , \u0017\\n\u0006\\n\u0005 matrix [ f ] B1 ,B1 is upper triangular.\\nA “matrix version” of this theorem reads as follows: The characteristic polynomial\\nPA of A ∈ K n,n decomposes into linear factors over K if and only if A can be triangulated, i.e., there exists a matrix S ∈ G L n (K ) with A = S RS −1 for an upper triangular matrix R ∈ K n,n .\\nCorollary 14.18 Let V be a finite dimensional Euclidian or unitary vector space and f ∈ L(V, V).\\nIf P f decomposes over R (in the Euclidian case case) or C (in the unitary case) into linear factors, then there exists an orthonormal basis B of V, such that [ f ] B,B is upper triangular.\\nProof If P f decomposes into linear factors, then by Theorem 14.17 there exists a basis B1 of V, such that [ f ] B1 ,B1 is upper triangular.\\nApplying the Gram-Schmidt method to the basis B1 , we obtain an orthonormal basis B2 of V, such that [IdV ] B1 ,B2 is upper triangular (cp.\\nTheorem 12.11).\\nThen\\n[ f ] B2 ,B2 = [IdV ] B1 ,B2 [ f ] B1 ,B1 [IdV ] B2 ,B1 = [IdV ]−1\\nB2 ,B1 [ f ] B1 ,B1 [IdV ] B2 ,B1 .\\nThe invertible upper triangular matrices form a group with respect to the matrix multiplication (cp.\\nTheorem 4.13).\\nThus, all matrices in the product on the right\\n\u0006\\n\u0005 hand side are upper triangular, and hence [ f ] B2 ,B2 is upper triangular.\\nExample 14.19 Consider the Euclidian vector space R[t]≤1 with the scalar product\\n\u00181\\n\u0010 p, q\u0011 = 0 p(t)q(t) dt, and the endomorphism</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209</td>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.3 Triangulation and Schur's Theorem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eigenvalue endomorphisms define h l w span g l w w h w j j g w j k j wk j n f w h w g w w w f b b h bw g bw bw consequently pg p f hence pg dim w n characteristic polynomial g l w w decomposes linear factor w w induction hypothesis exists basis bw upper triangular thus basis b v w g w bw bw matrix f upper triangular matrix version theorem read follows characteristic polynomial pa k n n decomposes linear factor k triangulated exists matrix g l n k r upper triangular matrix r k n n corollary let v finite dimensional euclidian unitary vector space f l v v p f decomposes r euclidian case case c unitary case linear factor exists orthonormal basis b v f b b upper triangular proof p f decomposes linear factor theorem exists basis v f upper triangular applying gram-schmidt method basis obtain orthonormal basis v idv upper triangular cp theorem f idv f idv idv f idv invertible upper triangular matrix form group respect matrix multiplication cp theorem thus matrix product right hand side upper triangular hence f upper triangular example consider euclidian vector space r scalar product p p q dt endomorphism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>14.3 Triangulation and Schur’s Theorem\\n\\n209 f : R[t]≤1 → R[t]≤1 , α1 t + α0 \b→ 2α1 t + α0 .\\nWe have f (1) = 1 and f (t) = 2t, i.e., the polynomials 1 and t are eigenvectors of f corresponding to the (distinct) eigenvalues 1 and 2.\\nThus, \u0017\\nB = {1, t} is a basis\\n\u0017 is a diagonal matrix.\\nNote that\\nB is not an orthonormal basis, of R[t]≤1 , and [ f ] \u0017\\n\u0017\\nB, B since in particular \u00101, t\u0011 \u0003= 0.\\nSince P f decomposes into linear factors, Corollary 14.18 guarantees the existence of an orthonormal basis B for which [ f ] B,B is upper triangular.\\nIn the proof of the implication (1) ⇒ (2) of Theorem 14.17 one chooses any eigenvector of f , and then proceeds inductively in order to obtain the triangulation of f .\\nIn this example, let us use q1 = 1 as the first vector.\\nThis vector is an eigenvector of f with norm\\n1 corresponding to the eigenvalue 1.\\nIf q2 ∈ R[t]≤1 is a vector with norm 1 and\\n\u0010q1 , q2 \u0011 = 0, then B = {q1 , q2 } is an orthonormal basis for which [ f ] B,B is an upper triangular matrix.\\nWe construct the vector q2 by orthogonalizing t against q1 using the Gram-Schmidt method:\\n1\\n\u0017 q2 = t − \u0010t, q1 \u0011q1 = t − ,\\n2\\n\u0019\\n1 \u001a1/2\\n1\\n1\\n=√ ,\\n\u0012\u0017 q2 \u0012 = t − , t −\\n2\\n2\\n12\\n√\\n√\\n−1 q2 = \u0012\u0017 q2 \u0012 \u0017 q2 = 12t − 3.\\nThis leads to the triangulation\\n[ f ] B,B =\\n\\n\u0014 √ \u0015\\n1 3\\n∈ R2,2 .\\n0 2\\n\\n√\\nWe could also choose q1 = 3t, which is an eigenvector of f with norm 1 corresponding to the eigenvalue 2.\\nOrthogonalizing the vector 1 against q1 leads to the second basis vector q2 = −3t + 2.\\nWith the corresponding basis B1 we obtain the triangulation\\n\u0014\\n√ \u0015\\n2− 3\\n∈ R2,2 .\\n[ f ] B1 ,B1 =\\n0 1\\nThis example shows that in the triangulation of f the elements above the diagonal can be different for different orthonormal bases.\\nOnly the diagonal elements are (except for their order) uniquely determined, since they are the eigenvalues of f .\\nA more detailed statement about the uniqueness is given in Lemma 14.22.\\nIn the next chapter we will prove the Fundamental Theorem of Algebra, which states that every non-constant polynomial over C decomposes into linear factors.\\nThis result has the following corollary, which is known as Schur’s theorem.1\\n\\n1 Issai\\n\\nSchur (1875–1941).</td>\n",
       "      <td>210.0</td>\n",
       "      <td>210</td>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.3 Triangulation and Schur's Theorem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>triangulation schur theorem f r r f f polynomial eigenvectors f corresponding distinct eigenvalue thus b basis diagonal matrix note b orthonormal basis r f b b since particular since p f decomposes linear factor corollary guarantee existence orthonormal basis b f b b upper triangular proof implication theorem one chooses eigenvector f proceeds inductively order obtain triangulation f example let u use first vector vector eigenvector f norm corresponding eigenvalue r vector norm b orthonormal basis f b b upper triangular matrix construct vector orthogonalizing using gram-schmidt method lead triangulation f b b could also choose eigenvector f norm corresponding eigenvalue orthogonalizing vector lead second basis vector corresponding basis obtain triangulation f example show triangulation f element diagonal different different orthonormal base diagonal element except order uniquely determined since eigenvalue f detailed statement uniqueness given lemma next chapter prove fundamental theorem algebra state every non-constant polynomial c decomposes linear factor result following corollary known schur issai schur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>210\\n\\n14 Eigenvalues of Endomorphisms\\n\\nCorollary 14.20 If V is a finite dimensional unitary vector space, then every endomorphism on V can be unitarily triangulated, i.e., for each f ∈ L(V, V) there exists an orthonormal basis B of V, such that [ f ] B,B is upper triangular.\\nThe matrix [ f ] B,B is called a Schur form of f .\\nIf V is the unitary vector space Cn,1 with the standard scalar product, then we obtain the following “matrix version” of Corollary 14.20.\\nCorollary 14.21 If A ∈ Cn,n , then there exists a unitary matrix Q ∈ Cn,n with\\nA = Q R Q H for an upper triangular matrix R ∈ Cn,n .\\nThe matrix R is called a\\nSchur form of A.\\nThe following result shows that a Schur form of a matrix A ∈ Cn,n with n pairwise distinct eigenvalues is “almost unique”.\\nLemma 14.22 Let A ∈ Cn,n have n pairwise distinct eigenvalues, and let R1 , R2 ∈\\nCn,n be two Schur forms of A. If the diagonals of R1 and R2 are equal then R1 =\\nU R2 U H for a unitary diagonal matrix U .\\n\u0006\\n\u0005\\n\\nProof Exercise.\\n\\nA survey of the results on unitary similarity of matrices can be found in the article [Sha91].\\n\\nMATLAB-Minute.\\nConsider for n ≥ 2 the matrix\\n⎤\\n··· n\\n··· n + 1 ⎥\\n⎥\\n··· n + 2 ⎥\\n⎥ ∈ Cn,n .\\n.. ⎥\\n. ⎦\\n1 n + 1 n + 2 . . .\\n2n − 1\\n\\n⎡\\n1\\n⎢1\\n⎢\\n⎢\\nA = ⎢1\\n⎢ ..\\n⎣.\\n\\n2\\n3\\n4\\n..\\n.\\n\\n3\\n4\\n5\\n..\\n.\\n\\nCompute a Schur form of A using the command [U,R] = schur(A) for n =\\n2, 3, 4, . . .\\n10.\\nWhat are the eigenvalues of A?\\nFormulate a conjecture about the rank of A for general n.\\nCan you prove your conjecture?\\n\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n14.1.\\nLet V be a vector space and let f ∈ L(V, V) have the eigenvalue λ.\\nShow that im(λIdV − f ) is an f -invariant subspace.\\n14.2.\\nLet V be a finite dimensional vector space and let f ∈ L(V, V) be bijective.\\nShow that f and f −1 have the same invariant subspaces.</td>\n",
       "      <td>211.0</td>\n",
       "      <td>211</td>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.3 Triangulation and Schur's Theorem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eigenvalue endomorphisms corollary v finite dimensional unitary vector space every endomorphism v unitarily triangulated f l v v exists orthonormal basis b v f b b upper triangular matrix f b b called schur form f v unitary vector space standard scalar product obtain following matrix version corollary corollary cn n exists unitary matrix q cn n q r q h upper triangular matrix r cn n matrix r called schur form following result show schur form matrix cn n n pairwise distinct eigenvalue almost unique lemma let cn n n pairwise distinct eigenvalue let cn n two schur form diagonal equal u u h unitary diagonal matrix u proof exercise survey result unitary similarity matrix found article matlab-minute consider n matrix n n n cn n n n compute schur form using command u r schur n eigenvalue formulate conjecture rank general prove conjecture exercise following exercise k arbitrary field let v vector space let f l v v eigenvalue λ show im λidv f f subspace let v finite dimensional vector space let f l v v bijective show f f invariant subspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>14.3 Triangulation and Schur’s Theorem\\n\\n211\\n\\n14.3.\\nLet V be an n-dimensional K -vector space, let f ∈ L(V, V), and let U be an m-dimensional f -invariant subspace of V. Show that a basis B of V exists such that\\n\u0015\\n\u0014\\nA1 A2\\n[ f ] B,B =\\n0 A3 for some matrices A1 ∈ K m,m , A2 ∈ K m,n−m and A3 ∈ K n−m,n−m .\\n14.4.\\nLet K ∈ {R, C} and f : K 4,1 → K 4,1 , v \b→ Fv with\\n⎡\\n\\n1\\n⎢0\\nF =⎢\\n⎣0\\n0\\n\\n2 3\\n1 2\\n0 1\\n0 −1\\n\\n⎤\\n4\\n3⎥\\n⎥.\\n1⎦\\n0\\n\\nCompute P f and determine for K = R and K = C the eigenvalues of f with their algebraic and geometric multiplicities, as well as the associated eigenspaces.\\n14.5.\\nConsider the vector space R[t]≤n with the standard basis {1, t, . . . , t n } and the endomorphism n f : R[t]≤n → R[t]≤n , n\\n\\nαi t i \b→ i=0 i(i − 1)αi t i−2 = i=2 d2 p.\\ndt 2\\n\\nCompute P f , the eigenvalues of f with their algebraic and geometric multiplicities, and examine whether f is diagonalizable or not.\\nWhat changes if one considers as map the kth derivative (for k = 3, 4, . . . , n)?\\n14.6.\\nExamine whether the following matrices\\n\u0014\\nA=\\n\\n01\\n−1 0\\n\\n⎡\\n⎤\\n3\\n100\\n⎢0\\n3,3\\nB = ⎣ −1 2 0 ⎦ ∈ Q , C = ⎢\\n⎣2\\n−1 1 1\\n0\\n⎡\\n\\n\u0015\\n∈ Q2,2 ,\\n\\n1\\n2\\n2\\n0\\n\\n⎤\\n0 −2\\n0 0⎥\\n⎥ ∈ Q4,4\\n2 −4 ⎦\\n0 2 are diagonalizable.\\n14.7.\\nIs the set of all diagonalizable and invertible matrices a subgroup of G L n (K )?\\n14.8.\\nLet n ∈ N0 .\\nConsider the R-vector space R[t]≤n and the map f : R[t]≤n → R[t]≤n , p(t) \b→ p(t + 1) − p(t).\\n\\nShow that f is linear.\\nFor which n is f diagonalizable?\\n14.9.\\nLet V be an R-vector space with the basis {v1 , . . . , vn }.\\nExamine whether the following endomorphisms are diagonalizable or not:\\n(a) f (v j ) = v j + v j+1 , j = 1, . . . , n − 1, and f (vn ) = vn ,</td>\n",
       "      <td>212.0</td>\n",
       "      <td>212</td>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.3 Triangulation and Schur's Theorem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>triangulation schur theorem let v n-dimensional k space let f l v v let u m-dimensional f subspace show basis b v exists f b b matrix k k k let k r c f k k v fv f compute p f determine k r k c eigenvalue f algebraic geometric multiplicity well associated eigenspaces consider vector space r standard basis n endomorphism n f r r n αi αi dt compute p f eigenvalue f algebraic geometric multiplicity examine whether f diagonalizable change one considers map kth derivative k n examine whether following matrix b q c diagonalizable set diagonalizable invertible matrix subgroup g l n k let n consider r-vector space r map f r r p p p show f linear n f diagonalizable let v r-vector space basis vn examine whether following endomorphisms diagonalizable f v j v j v j n f vn vn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>212\\n\\n14 Eigenvalues of Endomorphisms\\n\\n(b) f (v j ) = jv j + v j+1 , j = 1, . . . , n − 1, and f (vn ) = nvn .\\n14.10.\\nLet V be a finite dimensional Euclidian vector space and let f ∈ L(V, V) with f + f ad = 0 ∈ L(V, V).\\nShow that f \u0003= 0 if and only if f is not diagonalizable.\\n14.11.\\nLet V be a C-vector space and let f ∈ L(V, V) with f 2 = −IdV .\\nDetermine all possible eigenvalues of f .\\n14.12.\\nLet V be a finite dimensional vector space and f ∈ L(V, V).\\nShow that\\nP f ( f ) = 0 ∈ L(V, V).\\n14.13.\\nLet V be a finite dimensional K -vector space, let f ∈ L(V, V) and p = (t − μ1 ) · . . . · (t − μm ) ∈ K [t]≤m .\\nShow that p( f ) is bijective if and only if μ1 , . . . , μm are not eigenvalues of f.\\n14.14.\\nDetermine conditions for the entries of the matrices\\n\u0014\\n\u0015\\nαβ\\nA=\\n∈ R2,2 ,\\nγ δ such that A is diagonalizable or can be triangulated.\\n14.15.\\nDetermine an endomorphism on R[t]≤3 that is not diagonalizable and that cannot be triangulated.\\n14.16.\\nLet V be a vector space with dim(V) = n.\\nShow that f ∈ L(V, V) can be triangulated if and only if there exist subspaces V0 , V1 , . . . , Vn of V with\\n(a) V j ⊂ V j+1 for j = 0, 1, . . . , n − 1,\\n(b) dim(V j ) = j for j = 0, 1, . . . , n, and\\n(c) V j is f -invariant for j = 0, 1, . . . , n.\\n14.17.\\nProve Lemma 14.22.</td>\n",
       "      <td>213.0</td>\n",
       "      <td>213</td>\n",
       "      <td>14 Eigenvalues of Endomorphisms</td>\n",
       "      <td>14.3 Triangulation and Schur's Theorem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eigenvalue endomorphisms b f v j jv j v j n f vn nvn let v finite dimensional euclidian vector space let f l v v f f ad l v v show f f diagonalizable let v c-vector space let f l v v f determine possible eigenvalue f let v finite dimensional vector space f l v v show p f f l v v let v finite dimensional k space let f l v v p μm k show p f bijective μm eigenvalue determine condition entry matrix αβ γ δ diagonalizable triangulated determine endomorphism r diagonalizable triangulated let v vector space dim v show f l v v triangulated exist subspace vn v v j v j n b dim v j j j n c v j f j prove lemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Chapter 15\\n\\nPolynomials and the Fundamental Theorem of Algebra\\n\\nIn this chapter we discuss polynomials in more detail.\\nWe consider the division of polynomials and derive classical results from polynomial algebra, including the factorization into irreducible factors.\\nWe also prove the Fundamental Theorem of\\nAlgebra, which states that every non-constant polynomial over the complex numbers has a least one complex root.\\nThis implies that every complex matrix and every endomorphism on a (finite dimensional) complex vector space has at least one eigenvalue.\\n\\n15.1 Polynomials\\nLet us recall some of the most important terms in the context of polynomials.\\nIf K is a field, then p = α0 + α1 t + . . . + αn t n with n ∈ N0 and α0 , α1 , . . . αn ∈ K is a polynomial over K in the variable t.\\nThe set K [t] of all these polynomials forms a commutative ring with unit (cp.\\nExample 3.17).\\nIf αn \u0003= 0, then deg( p) = n is called the degree of p.\\nIf αn = 1, then p is called monic.\\nIf p = 0, then deg( p) := −∞, and if deg( p) &lt; 1, then p is called constant.\\nLemma 15.1 For two polynomials p, q ∈ K [t] the following assertions hold:\\n(1) deg( p + q) ≤ max{deg( p), deg(q)}.\\n(2) deg( p · q) = deg( p) + deg(q).\\n\u0007\\n\u0006\\n\\nProof Exercise.\\nWe now introduce some concepts associated with the division of polynomials.\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_15\\n\\n213</td>\n",
       "      <td>214.0</td>\n",
       "      <td>214</td>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.1 Polynomials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter polynomial fundamental theorem algebra chapter discus polynomial detail consider division polynomial derive classical result polynomial algebra including factorization irreducible factor also prove fundamental theorem algebra state every non-constant polynomial complex number least one complex root implies every complex matrix every endomorphism finite dimensional complex vector space least one eigenvalue polynomial let u recall important term context polynomial k field p αn n n αn k polynomial k variable set k polynomial form commutative ring unit cp example αn deg p n called degree αn p called monic p deg p deg p p called constant lemma two polynomial p q k following assertion hold deg p q max deg p deg q deg p q deg p deg q proof exercise introduce concept associated division polynomial springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>214\\n\\n15 Polynomials and the Fundamental Theorem of Algebra\\n\\nDefinition 15.2 Let K be a field.\\n(1) If for two polynomials p, s ∈ K [t] there exists a polynomial q ∈ K [t] with p = s · q, then s is called a divisor of p and we write s| p (read this as “s divides p”).\\n(2) Two polynomials p, s ∈ K [t] are called coprime, if q| p and q|s for some q ∈ K [t] always imply that q is constant.\\n(3) A non-constant polynomial p ∈ K [t] is called irreducible (over K ), if p = s · q for two polynomials s, q ∈ K [t] implies that s or q are constant.\\nIf there exist two non-constant polynomials s, q ∈ K [t] with p = s · q, then p is called reducible (over K ).\\nNote that the property of irreducibility is only defined for polynomials of degree at least 1.\\nA polynomial of degree 1 is always irreducible.\\nWhether a polynomial of degree at least 2 is irreducible may depend on the underlying field.\\nExample 15.3 The polynomial 2 − t 2 ∈ Q[t] is irreducible, but the factorization\\n2 − t2 =\\n\\n\u0002√\\n\\n\u0003 \u0002√\\n\u0003\\n2−t · 2+t shows that 2 − t 2 ∈ R[t] is reducible.\\nThe polynomial 1 + t 2 ∈ R[t] is irreducible, but using the imaginary unit i we have\\n1 + t 2 = (−i + t) · (i + t), so that 1 + t 2 ∈ C[t] is reducible.\\nThe next result concerns the division with remainder of polynomials.\\nTheorem 15.4 If p ∈ K [t] and s ∈ K [t] \\ {0}, then there exist uniquely defined polynomials q, r ∈ K [t] with p = s · q + r and deg(r ) &lt; deg(s).\\n\\n(15.1)\\n\\nProof We show first the existence of polynomials q, r ∈ K [t] such that (15.1) holds.\\nIf deg(s) = 0, then s = s0 for an s0 ∈ K \\ {0} and (15.1) follows with q := s0−1 · p and r := 0, where deg(r ) &lt; deg(s).\\nWe now assume that deg(s) ≥ 1.\\nIf deg( p) &lt; deg(s), then we set q := 0 and r := p.\\nThen p = s · q + r with deg(r ) &lt; deg(s).\\nLet n := deg( p) ≥ m := deg(s) ≥ 1.\\nWe prove (15.1) by induction on n.\\nIf n = 1, then m = 1.\\nHence p = p1 · t + p0 with p1 \u0003= 0 and s = s1 · t + s0 with s1 \u0003= 0.\\nTherefore, p = s · q + r for q := p1 s1−1 , r := p0 − p1 s1−1 s0 , where deg(r ) &lt; deg(s).</td>\n",
       "      <td>215.0</td>\n",
       "      <td>215</td>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.1 Polynomials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polynomial fundamental theorem algebra definition let k field two polynomial p k exists polynomial q k p q called divisor p write p read divide p two polynomial p k called coprime p q k always imply q constant non-constant polynomial p k called irreducible k p q two polynomial q k implies q constant exist two non-constant polynomial q k p q p called reducible k note property irreducibility defined polynomial degree least polynomial degree always irreducible whether polynomial degree least irreducible may depend underlying field example polynomial q irreducible factorization show r reducible polynomial r irreducible using imaginary unit c reducible next result concern division remainder polynomial theorem p k k exist uniquely defined polynomial q r k p q r deg r deg proof show first existence polynomial q r k hold deg k follows q p r deg r deg assume deg deg p deg set q r p q r deg r deg let n deg p deg prove induction n hence p therefore p q r q r deg r deg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>15.1 Polynomials\\n\\n215\\n\\nSuppose that the assertion holds for an n ≥ 1.\\nLet two polynomials p and s with n + 1 = deg( p) ≥ deg(s) = m be given, and let pn+1 (\u0003= 0) and sm (\u0003= 0) be the highest coefficients of p and s.\\nIf h := p − pn+1 sm−1 s · t n+1−m ∈ K [t], then deg(h) &lt; deg( p) = n + 1.\\nBy the induction hypothesis there exist polynomials\\n\u0004 q , r ∈ K [t] with h = s ·\u0004 q + r and deg(r ) &lt; deg(s).\\nIt then follows that p = s · q + r with q := \u0004 q + pn+1 sm−1 t n+1−m , where deg(r ) &lt; deg(s).\\nIt remains to show the uniqueness.\\nSuppose that (15.1) holds and that there exist polynomials \u0005 q ,\u0005 r ∈ K [t] with p = s · \u0005 q +\u0005 r and deg(\u0005 r ) &lt; deg(s).\\nThen r −\u0005 r = s · (\u0005 q − q).\\nIf \u0005 r − r \u0003= 0, then \u0005 q − q \u0003= 0 and thus deg(r − \u0005 r ) = deg(s · (\u0005 q − q)) = deg(s) + deg(\u0005 q − q) ≥ deg(s).\\nOn the other hand, we also have deg(r − \u0005 r ) ≤ max{deg(r ), deg(\u0005 r )} &lt; deg(s).\\nThis is a contradiction, which shows that indeed r = \u0005 r and q = \u0005 q.\\n\\n\u0007\\n\u0006\\n\\nThis theorem has some important consequences for the roots of polynomials.\\nThe first of these is known as the Theorem of Ruffini.1\\nCorollary 15.5 If λ ∈ K is a root of p ∈ K [t], i.e., p(λ) = 0, then there exists a uniquely determined polynomial q ∈ K [t] with p = (t − λ) · q.\\nProof When we apply Theorem 15.4 to the polynomials p and s = t − λ \u0003= 0, then we get uniquely determined polynomials q and r with deg(r ) &lt; deg(s) = 1 and p = (t − λ) · q + r.\\nThe polynomial r is constant and evaluating it at λ gives\\n0 = p(λ) = (λ − λ) · q(λ) + r (λ) = r (λ),\\n1 Paolo\\n\\nRuffini (1765–1822).</td>\n",
       "      <td>216.0</td>\n",
       "      <td>216</td>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.1 Polynomials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polynomial suppose assertion hold n let two polynomial p n deg p deg given let sm highest coefficient p h p k deg h deg p n induction hypothesis exist polynomial q r k h q r deg r deg follows p q r q q deg r deg remains show uniqueness suppose hold exist polynomial q r k p q r deg r deg r r q q r r q q thus deg r r deg q q deg deg q q deg hand also deg r r max deg r deg r deg contradiction show indeed r r q q theorem important consequence root polynomial first known theorem corollary λ k root p k p λ exists uniquely determined polynomial q k p λ q proof apply theorem polynomial p λ get uniquely determined polynomial q r deg r deg p λ q polynomial r constant evaluating λ give p λ λ λ q λ r λ r λ paolo ruffini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>216\\n\\n15 Polynomials and the Fundamental Theorem of Algebra which yields r = 0 and p = (t − λ) · q.\\n\\n\u0007\\n\u0006\\n\\nIf a polynomial p ∈ K [t] has at least degree 2 and a root λ ∈ K , then the linear factor t − λ is a divisor of p and, in particular, p is reducible.\\nThe converse of this statement does not hold.\\nFor instance the polynomial 4−4t 2 +t 4 = (2−t 2 )·(2−t 2 ) ∈\\nQ[t] is reducible, but it does not have a root in Q.\\nCorollary 15.5 motivates the following definition.\\nDefinition 15.6 If λ ∈ K is a root of p ∈ K [t] \\ {0}, then its multiplicity is the uniquely determined nonnegative integer m, such that p = (t − λ)m · q for a polynomial q ∈ K [t] with q(λ) \u0003= 0.\\nRecursive application of Corollary 15.5 to a given polynomial p ∈ K [t] leads to the following result.\\nCorollary 15.7 If λ1 . . . , λk ∈ K are pairwise distinct roots of p ∈ K [t] \\ {0} with the corresponding multiplicities m 1 , . . . , m k , then there exists a unique polynomial q ∈ K [t] with p = (t − λ1 )m 1 · . . . · (t − λk )m k · q and q(λ j ) \u0003= 0 for j = 1, . . . , k.\\nIn particular, the sum of the multiplicities of all pairwise distinct roots of p is at most deg( p).\\nThe next result is known as the Lemma of Bézout.2\\nLemma 15.8 If p, s ∈ K [t] \\ {0} are coprime, then there exist polynomials q1 , q2 ∈\\nK [t] with p · q1 + s · q2 = 1.\\nProof We may assume without loss of generality that deg( p) ≥ deg(s) (≥ 0), and we proceed by induction on deg(s).\\nIf deg(s) = 0, then s = s0 for an s0 ∈ K \\ {0}, and thus p · q1 + s · q2 = 1 with q1 := 0, q2 := s0−1 .\\nSuppose that the assertion holds for all polynomials p, s ∈ K [t] \\ {0} with deg(s) = n for an n ≥ 0.\\nLet p, s ∈ K [t] \\ {0} with deg( p) ≥ deg(s) = n + 1 be given.\\nBy Theorem 15.4 there exist polynomials q and r with p = s · q + r and deg(r ) &lt; deg(s).\\nHere we have r \u0003= 0, since by assumption p and s are coprime.\\nSuppose that there exists a non-constant polynomial h ∈ K [t] that divides both s and r .\\nThen h also divides p, in contradiction to the assumption that p and s are coprime.\\nThus, the polynomials s and r are coprime.\\nSince deg(r ) &lt; deg(s), we can\\n2 Étienne\\n\\nBézout (1730–1783).</td>\n",
       "      <td>217.0</td>\n",
       "      <td>217</td>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.1 Polynomials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polynomial fundamental theorem algebra yield r p λ q polynomial p k least degree root λ k linear factor λ divisor p particular p reducible converse statement hold instance polynomial q reducible root corollary motivates following definition definition λ k root p k multiplicity uniquely determined nonnegative integer p λ q polynomial q k q λ recursive application corollary given polynomial p k lead following result corollary λk k pairwise distinct root p k corresponding multiplicity k exists unique polynomial q k p λk k q q λ j j particular sum multiplicity pairwise distinct root p deg p next result known lemma lemma p k coprime exist polynomial k p proof may assume without loss generality deg p deg proceed induction deg deg k thus p suppose assertion hold polynomial p k deg n n let p k deg p deg n given theorem exist polynomial q r p q r deg r deg r since assumption p coprime suppose exists non-constant polynomial h k divide r h also divide p contradiction assumption p coprime thus polynomial r coprime since deg r deg étienne bézout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>15.1 Polynomials\\n\\n217 apply the induction hypothesis to the polynomials s, r ∈ K [t] \\ {0}.\\nHence there q2 ∈ K [t] with exist polynomials \u0004 q1 , \u0004 q2 = 1.\\ns ·\u0004 q1 + r · \u0004\\nFrom r = p − s · q we then get q2 = p · \u0004 q2 + s · (\u0004 q1 − q · \u0004 q2 ),\\n1 = s ·\u0004 q1 + ( p − s · q) · \u0004\\n\u0007\\n\u0006 which completes the proof.\\nUsing the Lemma of Bézout we can easily prove the following result.\\n\\nLemma 15.9 If p ∈ K [t] is irreducible and a divisor of the product s · h of two polynomials s, h ∈ K [t], then p divides at least one of the factors, i.e., p|s or p|h.\\nProof If s = 0, then p|s, because every polynomial is a divisor of the zero polynomial.\\nIf s \u0003= 0 and p is not a divisor of s, then p and s are coprime, since p is irreducible.\\nBy Lemma 15.8 there exist polynomials q1 , q2 ∈ K [t] with p · q1 + s · q2 = 1, and hence h = h · 1 = (q1 · h) · p + q2 · (s · h).\\nThe polynomial p divides both terms on the right hand side, and thus also p|h.\\n\\n\u0007\\n\u0006\\n\\nBy recursive application of Lemma 15.9 we obtain the Euclidean theorem, which describes a prime factor decomposition in the ring of polynomials.\\nTheorem 15.10 Every polynomial p = α0 + α1 t + . . . + αn t n ∈ K [t] \\ {0} has a unique (up to the ordering of the factors) decomposition p = μ · p1 · . . . · pk with μ ∈ K and monic irreducible polynomials p1 , . . . , pk ∈ K [t].\\nProof If deg( p) = 0, and thus p = α0 , then the assertion holds with k = 0 and\\nμ = α0 .\\nLet deg( p) ≥ 1.\\nIf p is irreducible, then the assertion holds with p1 = μ−1 p and μ = αn .\\nIf p is reducible, then p = p1 · p2 for two non-constant polynomials p1 and p2 .\\nThese are either irreducible, or we can decompose them further.\\nEvery multiplicative decomposition of p that is obtained in this way has at most deg( p) = n non-constant factors.\\nSuppose that p = μ · p1 · . . . · pk = β · q 1 · . . . · q \u0002\\n\\n(15.2) for some k, \u0002, where 1 ≤ \u0002 ≤ k ≤ n, μ, β ∈ K , as well as monic irreducible polynomials p1 , . . . , pk , q1 , . . . , q\u0002 ∈ K [t].\\nThen p1 | p and hence p1 |q j for some j.\\nSince the polynomials p1 and q j are irreducible, we must have p1 = q j .</td>\n",
       "      <td>218.0</td>\n",
       "      <td>218</td>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.1 Polynomials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polynomial apply induction hypothesis polynomial r k hence k exist polynomial r r p q get p q p q completes proof using lemma bézout easily prove following result lemma p k irreducible divisor product h two polynomial h k p divide least one factor proof every polynomial divisor zero polynomial p divisor p coprime since p irreducible lemma exist polynomial k p hence h h h p h polynomial p divide term right hand side thus also recursive application lemma obtain euclidean theorem describes prime factor decomposition ring polynomial theorem every polynomial p αn n k unique ordering factor decomposition p μ pk μ k monic irreducible polynomial pk k proof deg p thus p assertion hold k μ let deg p p irreducible assertion hold p μ αn p reducible p two non-constant polynomial either irreducible decompose every multiplicative decomposition p obtained way deg p n non-constant factor suppose p μ pk β q q k k n μ β k well monic irreducible polynomial pk k p hence j j since polynomial q j irreducible must q j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>218\\n\\n15 Polynomials and the Fundamental Theorem of Algebra\\n\\nWe may assume without loss of generality that j = 1 and cancel the polynomial p1 = q1 in the identity (15.2), which gives\\nμ · p2 · . . . · pk = β · q 2 · . . . · q \u0002 .\\nProceeding analogously for the polynomials p2 , . . . , pk , we finally obtain k = \u0002,\\n\u0006\\n\u0007\\nμ = β and p j = q j for j = 1, . . . , k.\\n\\n15.2 The Fundamental Theorem of Algebra\\nWe have seen above that the existence of roots of a polynomial depends on the field over which it is considered.\\nThe field C is special in this sense, since here the\\nFundamental Theorem of Algebra3 guarantees that every non-constant polynomial has a root.\\nIn order to use this theorem in our context, we first present an equivalent formulation in the language of Linear Algebra.\\nTheorem 15.11 The following statements are equivalent:\\n(1) Every non-constant polynomial p ∈ C[t] has a root in C.\\n(2) If V \u0003= {0} is a finite dimensional C-vector space, then every endomorphism f ∈ L(V, V) has an eigenvector.\\nProof\\n(1) ⇒ (2): If V \u0003= {0} and f ∈ L(V, V), then the characteristic polynomial P f ∈\\nC[t] is non-constant, since deg(P f ) = dim(V) &gt; 0.\\nThus, P f has a root in C, which is an eigenvalue of f , so that f indeed has an eigenvector.\\n(2) ⇒ (1): Let p = α0 + α1 t + . . . + αn t n ∈ C[t] be a non-constant polynomial with αn \u0003= 0.\\nThe roots of p are equal to the roots of the monic polynomial p , then PA = \u0005 p (cp.\\n\u0005 p := αn−1 p.\\nLet A ∈ Cn,n be the companion matrix of \u0005\\nLemma 8.4).\\nIf V is an n-dimensional C-vector space and B is an arbitrary basis of V, then there exists a uniquely determined f ∈ L(V, V) with [ f ] B,B = A (cp.\\nTheorem 10.16).\\nBy assumption, f has an eigenvector and hence also an eigenvalue,\\n\u0007\\n\u0006 so that \u0005 p = PA has a root.\\nThe Fundamental Theorem of Algebra cannot be proven without tools from Analysis.\\nIn particular, one needs that polynomials are continuous.\\nWe will use the following standard result, which is based on the continuity of polynomials.\\nLemma 15.12 Every polynomial p ∈ R[t] with odd degree has a (real) root.\\n3 Numerous proofs of this important result exist.\\nCarl Friedrich Gauß (1777–1855) alone gave four different proofs, starting with the one in his dissertation from 1799, which contained however a gap.\\nThe history of this result is described in detail in the book [Ebb91].</td>\n",
       "      <td>219.0</td>\n",
       "      <td>219</td>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.2 The Fundamental Theorem of Algebra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polynomial fundamental theorem algebra may assume without loss generality j cancel polynomial identity give μ pk β q q proceeding analogously polynomial pk finally obtain k μ β p j q j j fundamental theorem algebra seen existence root polynomial depends field considered field c special sense since fundamental theorem guarantee every non-constant polynomial root order use theorem context first present equivalent formulation language linear algebra theorem following statement equivalent every non-constant polynomial p c root v finite dimensional c-vector space every endomorphism f l v v eigenvector proof v f l v v characteristic polynomial p f c non-constant since deg p f dim v thus p f root c eigenvalue f f indeed eigenvector let p αn n c non-constant polynomial αn root p equal root monic polynomial p pa p cp p let cn n companion matrix lemma v n-dimensional c-vector space b arbitrary basis v exists uniquely determined f l v v f b b cp theorem assumption f eigenvector hence also eigenvalue p pa root fundamental theorem algebra proven without tool analysis particular one need polynomial continuous use following standard result based continuity polynomial lemma every polynomial p r odd degree real root numerous proof important result exist carl friedrich gauß alone gave four different proof starting one dissertation contained however gap history result described detail book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>15.2 The Fundamental Theorem of Algebra\\n\\n219\\n\\nProof Let the highest coefficient of p be positive.\\nThen lim p(t) = +∞, t→∞ lim p(t) = −∞.\\nt→−∞\\n\\nSince the real function p(t) is continuous, the Intermediate Value Theorem from\\nAnalysis implies the existence of a root of p.\\nThe argument in the case of a negative leading coefficient is analogous.\\n\u0007\\n\u0006\\nOur proof of the Fundamental Theorem of Algebra below follows the presentation in the article [Der03].\\nThe proof is by induction on the dimension of V. However, we do not use the usual consecutive order, i.e., dim(V) = 1, 2, 3, . . . , but an order that is based on the sets\\nM j := {2m · \u0002 | 0 ≤ m ≤ j − 1, \u0002 odd} ⊂ N, j = 1, 2, 3, . . . .\\n\\nFor instance,\\nM1 = {\u0002 | \u0002 odd} = {1, 3, 5, 7, . . . },\\n\\nM2 = M1 ∪ {2, 6, 10, 14, . . . }.\\n\\nLemma 15.13\\n(1) If V is an R-vector space and if dim(V) is odd, i.e., dim(V) ∈ M1 , then every f ∈ L(V, V) has an eigenvector.\\n(2) Let K be a field and j ∈ N. If for every K -vector space V with dim(V) ∈ M j every f ∈ L(V, V) has an eigenvector, then two commuting f 1 , f 2 ∈ L(V, V) have a common eigenvector.\\nThat is, if f 1 ◦ f 2 = f 2 ◦ f 1 , then there exists a vector v ∈ V \\ {0} and two scalars λ1 , λ2 ∈ K with f 1 (v) = λ1 v and f 2 (v) = λ2 v.\\n(3) If V is an R-vector space and if dim(V) is odd, then two commuting f 1 , f 2 ∈\\nL(V, V) have a common eigenvector.\\nProof\\n(1) For every f ∈ L(V, V) the degree of P f ∈ R[t] is odd.\\nHence Lemma 15.12 implies that P f has a root, and therefore f has an eigenvector.\\n(2) We proceed by induction on dim(V), where dim(V) runs through the elements of\\nM j in increasing order.\\nThe set M j is a proper subset of N consisting of natural numbers that are not divisible by 2 j and, in particular, 1 is the smallest element of M j .\\nIf dim(V) = 1 ∈ M j , then by assumption two arbitrary f 1 , f 2 ∈ L(V, V) each have an eigenvector, i.e., f 1 (v1 ) = λ1 v1 , f 2 (v2 ) = λ2 v2 .\\n\\nSince dim(V) = 1, we have v1 = αv2 for an α ∈ K \\ {0}.\\nThus, f 2 (v1 ) = f 2 (αv2 ) = α f 2 (v2 ) = λ2 (αv2 ) = λ2 v1 ,</td>\n",
       "      <td>220.0</td>\n",
       "      <td>220</td>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.2 The Fundamental Theorem of Algebra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fundamental theorem algebra proof let highest coefficient p positive lim p lim p since real function p continuous intermediate value theorem analysis implies existence root argument case negative leading coefficient analogous proof fundamental theorem algebra follows presentation article proof induction dimension however use usual consecutive order dim v order based set j j odd n j instance odd lemma v r-vector space dim v odd dim v every f l v v eigenvector let k field j every k space v dim v j every f l v v eigenvector two commuting f f l v v common eigenvector f f f f exists vector v v two scalar k f v v f v v r-vector space dim v odd two commuting f f l v v common eigenvector proof every f l v v degree p f r odd hence lemma implies p f root therefore f eigenvector proceed induction dim v dim v run element j increasing order set j proper subset n consisting natural number divisible j particular smallest element j dim v j assumption two arbitrary f f l v v eigenvector f f since dim v α k thus f f α f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>220\\n\\n15 Polynomials and the Fundamental Theorem of Algebra i.e., v1 is a common eigenvector of f 1 and f 2 .\\nLet now dim(V) ∈ M j , and let the assertion be proven for all K -vector spaces whose dimensions is an element of M j that is smaller than dim(V).\\nLet f 1 , f 2 ∈\\nL(V, V) with f 1 ◦ f 2 = f 2 ◦ f 1 .\\nBy assumption, f 1 has an eigenvector v1 with corresponding eigenvalue λ1 .\\nLet\\nU := im(λ1 IdV − f 1 ),\\n\\nW := V f1 (λ1 ) = ker(λ1 IdV − f 1 ).\\n\\nThe subspaces U and W of V are f 1 -invariant, i.e., f 1 (U) ⊆ U and f 1 (W) ⊆ W.\\nFor the space W we have shown this in Lemma 14.4 and for the space U this can be easily shown as well (cp.\\nExercise 14.1).\\nThe subspaces U and W are also f 2 -invariant:\\nIf u ∈ U, then u = (λ1 IdV − f 1 )(v) for a v ∈ V. Since f 1 and f 2 commute, we have f 2 (u) = ( f 2 ◦ (λ1 IdV − f 1 ))(v) = ((λ1 IdV − f 1 ) ◦ f 2 )(v)\\n= (λ1 IdV − f 1 )( f 2 (v)) ∈ U.\\nIf w ∈ W, then\\n(λ1 IdV − f 1 )( f 2 (w)) = ((λ1 IdV − f 1 ) ◦ f 2 )(w) = ( f 2 ◦ (λ1 IdV − f 1 ))(w)\\n= f 2 ((λ1 IdV − f 1 )(w)) = f 2 (0) = 0, hence f 2 (w) ∈ W.\\nWe have dim(V) = dim(U) + dim(W) and since dim(V) is not divisible by 2 j , either dim(U) or dim(W) is not divisible by 2 j .\\nHence either dim(U) ∈ M j or dim(W) ∈ M j .\\nIf the corresponding subspace is a proper subspace of V, then its dimension is an element of M j that is smaller than dim(V).\\nBy the induction hypothesis then f 1 and f 2 have a common eigenvector in this subspace.\\nThus, f 1 and f 2 have a common eigenvector in V.\\nIf the corresponding subspace is equal to V, then this must be the subspace W, since dim(W) ≥ 1.\\nBut if V = W, then every vector in V \\ {0} is an eigenvector of f 1 .\\nBy assumption also f 2 has an eigenvector, so that there exists at least one common eigenvector of f 1 and f 2 .\\n(3) By (1) it follows that the assumption of (2) holds for K = R and j = 1, which means that (3) holds as well.\\n\u0007\\n\u0006\\nWe will now prove the Fundamental Theorem of Algebra in the formulation (2) of Theorem 15.11.\\nTheorem 15.14 If V \u0003= {0} is a finite dimensional C-vector space, then every f ∈\\nL(V, V) has an eigenvector.</td>\n",
       "      <td>221.0</td>\n",
       "      <td>221</td>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.2 The Fundamental Theorem of Algebra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polynomial fundamental theorem algebra common eigenvector f f let dim v j let assertion proven k space whose dimension element j smaller dim v let f f l v v f f f f assumption f eigenvector corresponding eigenvalue let u im idv f w v ker idv f subspace u w v f f u u f w space w shown lemma space u easily shown well cp exercise subspace u w also f u u u idv f v v since f f commute f u f idv f v idv f f v idv f f v u w w idv f f w idv f f w f idv f w f idv f w f hence f w dim v dim u dim w since dim v divisible j either dim u dim w divisible j hence either dim u j dim w j corresponding subspace proper subspace v dimension element j smaller dim v induction hypothesis f f common eigenvector subspace thus f f common eigenvector corresponding subspace equal v must subspace w since dim w v w every vector v eigenvector f assumption also f eigenvector exists least one common eigenvector f f follows assumption hold k r j mean hold well prove fundamental theorem algebra formulation theorem theorem v finite dimensional c-vector space every f l v v eigenvector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>15.2 The Fundamental Theorem of Algebra\\n\\n221\\n\\nProof We prove the assertion by induction on j = 1, 2, 3, . . . and dim(V) ∈ M j .\\nWe start with j = 1 and thus by showing the assertion for all C-vector spaces of odd dimension.\\nLet V be an arbitrary C-vector space with n := dim(V) ∈ M1 .\\nLet f ∈ L(V, V) and consider an arbitrary scalar product on V (such a scalar product always exists; cp.\\nExercise 12.1), as well as the set of self-adjoint maps with respect to this scalar product,\\nH := {g ∈ L(V, V) | g = g ad }.\\nBy Lemma 13.15 the set H forms an R-vector space of dimension n 2 .\\nIf we define h 1 , h 2 ∈ L(H, H) by h 1 (g) :=\\n\\n1\\n1\\n( f ◦ g + g ◦ f ad ), h 2 (g) := ( f ◦ g − g ◦ f ad )\\n2\\n2i for all g ∈ H, then h 1 ◦ h 2 = h 2 ◦ h 1 (cp.\\nExercise 15.8).\\nSince n is odd, also n 2 is odd.\\nBy (3) in Lemma 15.13, h 1 and h 2 have a common eigenvector.\\nHence, there exists a \u0004 g ∈ H \\ {0} with g ) = λ1\u0004 g , h 2 (\u0004 g ) = λ2 \u0004 g for some λ1 , λ2 ∈ R.\\nh 1 (\u0004\\nWe have (h 1 + ih 2 )(g) = f ◦ g for all g ∈ H and therefore, in particular, g ) = (λ1 + iλ2 )\u0004 g.\\nf ◦\u0004 g = (h 1 + ih 2 )(\u0004\\nSince \u0004 g \u0003= 0, there exists a v ∈ V with \u0004 g (v) \u0003= 0.\\nThen g (v)), f (\u0004 g (v)) = (λ1 + iλ2 ) (\u0004 which shows that \u0004 g (v) ∈ V is an eigenvector of f , so that the proof for j = 1 is complete.\\nAssume now that for some j ≥ 1 and every C-vector space V with dim(V) ∈ M j , every f ∈ L(V, V) has an eigenvector.\\nThen (2) in Lemma 15.13 implies that every two commuting f 1 , f 2 ∈ L(V, V) have a common eigenvector.\\nWe have to show that for every C-vector space V with dim(V) ∈ M j+1 , every f ∈ L(V, V) has an eigenvector.\\nSince\\nM j+1 = M j ∪ {2 j q | q odd}, we only have to prove this for C-vector spaces V with n := dim(V) = 2 j q for odd q.\\nLet V be such a vector space and let f ∈ L(V, V) be given.\\nWe choose an arbitrary basis of V and denote the matrix representation of f with respect to this basis by\\nA ∈ Cn,n .\\nLet</td>\n",
       "      <td>222.0</td>\n",
       "      <td>222</td>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.2 The Fundamental Theorem of Algebra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fundamental theorem algebra proof prove assertion induction j dim v j start j thus showing assertion c-vector space odd dimension let v arbitrary c-vector space n dim v let f l v v consider arbitrary scalar product v scalar product always exists cp exercise well set self-adjoint map respect scalar product h g l v v g g ad lemma set h form r-vector space dimension n define h h l h h h g f g g f ad h g f g g f ad g h h h h h cp exercise since n odd also n odd lemma h h common eigenvector hence exists g h g g h g g h h ih g f g g h therefore particular g f g h ih since g exists v v g v g v f g v show g v v eigenvector f proof j complete assume j every c-vector space v dim v j every f l v v eigenvector lemma implies every two commuting f f l v v common eigenvector show every c-vector space v dim v every f l v v eigenvector since j j q q odd prove c-vector space v n dim v j q odd q let v vector space let f l v v given choose arbitrary basis v denote matrix representation f respect basis cn n let</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>222\\n\\n15 Polynomials and the Fundamental Theorem of Algebra\\n\\nS := {B ∈ Cn,n | B = B T } be the set of complex symmetric n × n matrices.\\nIf we define h 1 , h 2 ∈ L(S, S) by h 1 (B) := AB + B A T , h 2 (B) := AB A T for all B ∈ S, then h 1 ◦ h 2 = h 2 ◦ h 1 (cp.\\nExercise 15.9).\\nBy Lemma 13.16 the set\\nS forms a C-vector space of dimension n(n + 1)/2.\\nWe have n = 2 j q for an odd natural number q.\\nThus, n(n + 1)\\n2 j q (2 j q + 1)\\n=\\n= 2 j−1 q · (2 j q + 1) ∈ M j .\\n2\\n2\\nBy the induction hypothesis, the commuting endomorphisms h 1 and h 2 have a common eigenvector.\\nHence there exists a \u0004\\nB ∈ S \\ {0} with\\nB) = λ1 \u0004\\nB) = λ2 \u0004\\nB, h 2 ( \u0004\\nB for some λ1 , λ2 ∈ C.\\nh1( \u0004\\nIn particular, we have λ1 \u0004\\nB = A\u0004\\nB+\u0004\\nB A T .\\nMultiplying this equation from the left with A yields\\nB = A2 \u0004\\nB) = A2 \u0004\\nB + A\u0004\\nB A T = A2 \u0004\\nB + h2( \u0004\\nB + λ2 \u0004\\nB,\\nλ1 A \u0004 so that\\n\\n\u0002\\n\\n\u0003\\nB = 0.\\nA 2 − λ1 A + λ2 I n \u0004\\n\\nWe now factorize t 2 − λ1 t + λ2 = (t − α)(t − β) with\\nα=\\n\\nλ1 +\\n\\n\u0006\\nλ21 − 4λ2\\n2\\n\\n, β=\\n\\nλ1 −\\n\\n\u0006\\nλ21 − 4λ2\\n2\\n\\n, where we have used that every complex number has a square root.\\nThen\\n(A − αIn )(A − β In ) \u0004\\nB = 0.\\nBv \u0003= 0.\\nIf (A − β In ) \u0004\\nBv = 0, then \u0004\\nBv is\\nSince \u0004\\nB \u0003= 0, there exists a v ∈ Cn,1 with \u0004\\nBv \u0003= 0, then an eigenvector of A corresponding to the eigenvalue β.\\nIf (A − β In ) \u0004\\nBv is an eigenvector of A corresponding to the eigenvalue α.\\nSince A has\\n(A − β In ) \u0004 an eigenvector, also f has an eigenvector.\\n\u0007\\n\u0006</td>\n",
       "      <td>223.0</td>\n",
       "      <td>223</td>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.2 The Fundamental Theorem of Algebra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polynomial fundamental theorem algebra b cn n b b set complex symmetric n n matrix define h h l h b ab b h b ab b h h h h cp exercise lemma set form c-vector space dimension n n n j q odd natural number q thus n n j q j q q j q j induction hypothesis commuting endomorphisms h h common eigenvector hence exists b b b b h b particular b b multiplying equation left yield b b b b b b b b n factorize α β used every complex number square root αin β b bv β bv bv since b exists v bv eigenvector corresponding eigenvalue β β bv eigenvector corresponding eigenvalue α since β eigenvector also f eigenvector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>15.2 The Fundamental Theorem of Algebra\\n\\n223\\n\\nMATLAB-Minute.\\nCompute the eigenvalues of the matrix\\n⎡\\n\\n1\\n⎢1\\n⎢\\nA=⎢\\n⎢2\\n⎣5\\n4\\n\\n2\\n2\\n3\\n1\\n2\\n\\n3\\n4\\n4\\n4\\n3\\n\\n4\\n3\\n1\\n2\\n1\\n\\n⎤\\n5\\n5⎥\\n⎥\\n5,5\\n5⎥\\n⎥∈R\\n⎦\\n3\\n5 using the command eig(A).\\nBy definition a real matrix A can only have real eigenvalues.\\nThe reason for the occurrence of complex eigenvalues is that MATLAB interprets every matrix as a complex matrix.\\nThis means that within MATLAB every matrix can be unitarily triangulated, since every complex polynomial (of degree at least 1) decomposes into linear factors.\\nAs a direct corollary of the Fundamental Theorem of Algebra and (2) in\\nLemma 15.13 we have the following result.\\nCorollary 15.15 If V \u0003= {0} is a finite dimensional C-vector space, then two commuting f 1 , f 2 ∈ L(V, V) have a common eigenvector.\\nExample 15.16 The two complex 2 × 2 matrices\\n\u000e\\n\u000e i 1\\n2i 1\\nA= and B =\\n1 i\\n1 2i commute.\\nThe eigenvalues of A are ±1 + i and those of B are ±2 + i.\\nHence A and B do not have a common eigenvalue, while [1, 1]T and [−1, 1]T are common eigenvectors of A and B.\\nUsing Corollary 15.15, Schur’s theorem (Corollary 14.20) can be generalized as follows.\\nTheorem 15.17 If V \u0003= {0} is a finite dimensional unitary vector space and f 1 , f 2 ∈\\nL(V, V) commute, then f 1 and f 2 can be simultaneously unitarily triangulated, i.e., there exists an orthonormal basis B of V, such that [ f 1 ] B,B and [ f 2 ] B,B are both upper triangular.\\nProof Exercise.\\n\\n\u0007\\n\u0006</td>\n",
       "      <td>224.0</td>\n",
       "      <td>224</td>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.2 The Fundamental Theorem of Algebra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fundamental theorem algebra matlab-minute compute eigenvalue matrix using command eig definition real matrix real eigenvalue reason occurrence complex eigenvalue matlab interprets every matrix complex matrix mean within matlab every matrix unitarily triangulated since every complex polynomial degree least decomposes linear factor direct corollary fundamental theorem algebra lemma following result corollary v finite dimensional c-vector space two commuting f f l v v common eigenvector example two complex matrix b commute eigenvalue b hence b common eigenvalue common eigenvectors b using corollary schur theorem corollary generalized follows theorem v finite dimensional unitary vector space f f l v v commute f f simultaneously unitarily triangulated exists orthonormal basis b v f b b f b b upper triangular proof exercise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>224\\n\\n15 Polynomials and the Fundamental Theorem of Algebra\\n\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n15.1.\\nProve Lemma 15.1.\\n15.2.\\nShow the following assertions for p1 , p2 , p3 ∈ K [t]:\\n(a)\\n(b)\\n(c)\\n(d) p1 |( p1 p2 ).\\np1 | p2 and p2 | p3 imply that p1 | p3 .\\np1 | p2 and p1 | p3 imply that p1 |( p2 + p3 ).\\nIf p1 | p2 and p2 | p1 , then there exists a c ∈ K \\ {0} with p1 = cp2 .\\n\\n15.3.\\nExamine whether the following polynomials are irreducible: p1 = t 3 − t 2 + t − 1 ∈ Q[t], p4 = t 3 − t 2 + t − 1 ∈ R[t], p2 = t 3 − t 2 + t − 1 ∈ C[t], p5 = 4t 3 − 4t 2 − t + 1 ∈ Q[t], p3 = 4t 3 − 4t 2 − t + 1 ∈ R[t], p6 = t 3 − 4t 2 − t + 1 ∈ C[t].\\n\\nDetermine the decompositions into irreducible factors.\\n15.4.\\nDecompose the polynomials p1 = t 2 − 2, p2 = t 2 + 2, p3 = t 4 − 1 and p4 = t 2 + t + 1 into irreducible factors over the fields K = Q, K = R and\\nK = C.\\n15.5.\\nShow the following assertions for p ∈ K [t]:\\n(a) If deg( p) = 1, then p is irreducible.\\n(b) If deg( p) ≥ 2 and p has a root, then p is not irreducible.\\n(c) If deg( p) ∈ {2, 3}, then p is irreducible if and only if p does not have a root.\\n15.6.\\nLet A ∈ G L n (C), n ≥ 2, and let adj(A) ∈ Cn,n be the adjunct of A. Show that there exist n − 1 matrices A j ∈ Cn,n with det(−A j ) = det(A), j =\\n1, . . . , n − 1, and n−1\\n\u000f\\nAj.\\nadj(A) = j=1\\n\\n(Hint: Use PA to construct a polynomial p ∈ C[t]≤n−1 with adj(A) = p(A) and express p as product of linear factors.)\\n15.7.\\nShow that two polynomials p, q ∈ C[t] \\ {0} have a common root if and only if there exist polynomials r1 , r2 ∈ C[t] with 0 ≤ deg(r1 ) &lt; deg( p) such that\\n0 ≤ deg(r2 ) &lt; deg(q) and p · r2 + q · r1 = 0.\\n15.8.\\nLet V be a finite dimensional unitary vector space, f ∈ L(V, V), H = {g ∈\\nL(V, V) | g = g ad } and let h 1 : H → L(V, V), g \u0010→\\n\\n1\\n( f ◦ g + g ◦ f ad ),\\n2</td>\n",
       "      <td>225.0</td>\n",
       "      <td>225</td>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.2 The Fundamental Theorem of Algebra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polynomial fundamental theorem algebra exercise following exercise k arbitrary field prove lemma show following assertion k b c imply imply exists c k examine whether following polynomial irreducible q r c q r c determine decomposition irreducible factor decompose polynomial irreducible factor field k q k r k show following assertion p k deg p p irreducible b deg p p root p irreducible c deg p p irreducible p root let g l n c n let adj cn n adjunct show exist n matrix j cn n det j det j n aj adj hint use pa construct polynomial p c adj p express p product linear factor show two polynomial p q c common root exist polynomial c deg deg p deg deg q p q let v finite dimensional unitary vector space f l v v h g l v v g g ad let h h l v v g f g g f ad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>15.2 The Fundamental Theorem of Algebra\\n\\n225 h 2 : H → L(V, V), g \u0010→\\n\\n1\\n( f ◦ g − g ◦ f ad ).\\n2i\\n\\nShow that h 1 , h 2 ∈ L(H, H) and h 1 ◦ h 2 = h 2 ◦ h 1 .\\n15.9.\\nLet A ∈ Cn,n , S = {B ∈ Cn,n | B = B T } and let h 1 : S → Cn,n ,\\n\\nB \u0010→ AB + B A T , h 2 : S → Cn,n ,\\n\\nB \u0010→ AB A T .\\n\\nShow that h 1 , h 2 ∈ L(S, S) and h 1 ◦ h 2 = h 2 ◦ h 1 .\\n15.10.\\nLet V be a C-vector space, f ∈ L(V, V) and let U \u0003= {0} be a finite dimensional f -invariant subspace of V. Show that U contains at least one eigenvector of f .\\n15.11.\\nLet V \u0003= {0} be a K -vector space and let f ∈ L(V, V).\\nShow the following statements:\\n(a) If K = C, then there exists an f -invariant subspace U of V with dim(U) = 1.\\n(b) If K = R, then there exists an f -invariant subspace U of V with dim(U) ∈\\n{1, 2}.\\n15.12.\\nProve Theorem 15.17.\\n15.13.\\nConstruct an example showing that the condition f ◦ g = g ◦ f in Theorem 15.17 is sufficient but not necessary for the simultaneous unitary triangulation of f and g.\\n15.14.\\nLet A ∈ K n,n be a diagonal matrix with pairwise distinct diagonal entries and B ∈ K n,n with AB = B A. Show that in this case B is a diagonal matrix.\\nWhat can you say about B, when the diagonal entries of A are not all pairwise distinct?\\n15.15.\\nShow that the matrices\\n\u000e\\n\u000e\\n01\\n−1 1\\n, B=\\nA=\\n10\\n1 −1 commute and determine a unitary matrix Q such that Q H AQ and Q H B Q are upper triangular.\\n15.16.\\nShow the following statements for p ∈ K [t]:\\n(a) For all A ∈ K n,n and S ∈ G L n (K ) we have p(S AS −1 ) = Sp(A)S −1 .\\n(b) For all A, B, C ∈ K n,n with AB = C A we have Ap(B) = p(C)A.\\n(c) If K = C and A ∈ Cn,n , then there exists a unitary matrix Q, such that\\nQ H AQ and Q H p(A)Q are upper triangular.\\n15.17.\\nLet V be a finite dimensional unitary vector space.\\nLet f ∈ L(V, V) be normal, i.e., f satisfies f ◦ f ad = f ad ◦ f .</td>\n",
       "      <td>226.0</td>\n",
       "      <td>226</td>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.2 The Fundamental Theorem of Algebra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fundamental theorem algebra h h l v v g f g g f ad show h h l h h h h h h let cn n b cn n b b let h cn n b ab b h cn n b ab show h h l h h h h let v c-vector space f l v v let u finite dimensional f subspace show u contains least one eigenvector f let v k space let f l v v show following statement k c exists f subspace u v dim u b k r exists f subspace u v dim u prove theorem construct example showing condition f g g f theorem sufficient necessary simultaneous unitary triangulation f let k n n diagonal matrix pairwise distinct diagonal entry b k n n ab b show case b diagonal matrix say b diagonal entry pairwise distinct show matrix commute determine unitary matrix q q h aq q h b q upper triangular show following statement p k k n n g l n k p sp b b c k n n ab c ap b p c c k c cn n exists unitary matrix q q h aq q h p q upper triangular let v finite dimensional unitary vector space let f l v v normal f satisfies f f ad f ad f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>226\\n\\n15 Polynomials and the Fundamental Theorem of Algebra\\n\\n(a) Show that if λ ∈ C is an eigenvalue of f , then V f (λ)⊥ is an f -invariant subspace.\\n(b) Show (using (a)) that f is diagonalizable.\\n(Hint: Show by induction on dim(V), that V is the direct sum of the eigenspaces of f .)\\n(c) Show (using(a) or (b)), that f is even unitarily diagonalizable, i.e., there exists an orthonormal basis B of V such that [ f ] B,B is a diagonal matrix.\\n(d) Let g ∈ L(V, V) be unitarily diagonalizable.\\nShow that g is normal.\\n(This shows that an endomorphism on a finite dimensional unitary vector space is normal if and only if it is unitarily diagonalizable.\\nWe will give a different proof of this result in Theorem 18.2.)\\n15.18.\\nLet V be a finite dimensional K -vector space, f ∈ L(V, V) and V = U1 ⊕U2 , where U1 , U2 are f -invariant subspaces of V. Let, furthermore, f j := f |U j ∈\\nL(U j , U j ), j = 1, 2.\\n(a) For every v ∈ V there exist unique u 1 ∈ U1 and u 2 ∈ U2 with v = u 1 +u 2 .\\nShow that then also f (v) = f (u 1 ) + f (u 2 ) = f 1 (u 1 ) + f 2 (u 2 ).\\n(We write this as f = f 1 ⊕ f 2 and call f the direct sum of f 1 and f 2 with respect to the decomposition V = U1 ⊕ U2 .)\\n(b) Show that rank( f ) = rank( f 1 ) + rank( f 2 ) and P f = P f1 · P f2 .\\n(c) Show that a(λ, f ) = a(λ, f 1 ) + a(λ, f 2 ) for all λ ∈ K .\\n(Here we set a(λ, h) = 0, if λ is not an eigenvalue of h ∈ L(V, V).)\\n(d) Show that g(λ, f ) = g(λ, f 1 ) + g(λ, f 2 ) for all λ ∈ K .\\n(Here we set g(λ, h) = dim(ker(λIdV −h)) even if λ is not an eigenvalue of h ∈ L(V, V).)\\n(e) Show that p( f ) = p( f 1 ) ⊕ p( f 2 ) for all p ∈ K [t].</td>\n",
       "      <td>227.0</td>\n",
       "      <td>227</td>\n",
       "      <td>15 Polynomials and the Fundamental Theorem of Algebra</td>\n",
       "      <td>15.2 The Fundamental Theorem of Algebra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polynomial fundamental theorem algebra show λ c eigenvalue f v f λ f subspace b show using f diagonalizable hint show induction dim v v direct sum eigenspaces f c show using b f even unitarily diagonalizable exists orthonormal basis b v f b b diagonal matrix let g l v v unitarily diagonalizable show g normal show endomorphism finite dimensional unitary vector space normal unitarily diagonalizable give different proof result theorem let v finite dimensional k space f l v v v f subspace let furthermore f j f j l u j u j j every v v exist unique u u v u show also f v f u f u f u f u write f f f call f direct sum f f respect decomposition v b show rank f rank f rank f p f p p c show λ f λ f λ f λ k set λ h λ eigenvalue h l v v show g λ f g λ f g λ f λ k set g λ h dim ker λidv even λ eigenvalue h l v v e show p f p f p f p k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Chapter 16\\n\\nCyclic Subspaces, Duality and the Jordan\\nCanonical Form\\n\\nIn this chapter we use the duality theory to analyze the properties of an endomorphism f on a finite dimensional vector space V in detail.\\nWe are particularly interested in the algebraic and geometric multiplicities of the eigenvalues of f and the characterization of the corresponding eigenspaces.\\nOur strategy in this analysis is to decompose the vector space V into a direct sum of f -invariant subspaces so that, with appropriately chosen bases, the essential properties of f will be obvious from its matrix representation.\\nThe matrix representation that we derive is called the Jordan canonical form of f .\\nBecause of its great importance there have been many different derivations of this form using different mathematical tools.\\nOur approach using duality theory is based on an article by Vlastimil Pták (1925–1999) from 1956 [Pta56].\\n\\n16.1 Cyclic f -invariant Subspaces and Duality\\nLet V be a finite dimensional K -vector space.\\nIf f ∈ L(V, V) and v0 ∈ V \\ {0}, then there exists a uniquely defined smallest number m ∈ N, such that the vectors v0 , f (v0 ), . . . , f m−1 (v0 ) are linearly independent and the vectors v0 , f (v0 ), . . . , f m−1 (v0 ), f m (v0 ) are linearly dependent.\\nObviously m ≤ dim(V), since at most dim(V) vectors of V can be linearly independent.\\nThe number m is called the grade of v0 with respect to f .\\nWe denote this grade by m( f, v0 ).\\nThe vector v0 = 0 is linearly dependent, and thus its grade is 0 (with respect to any f ).\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_16\\n\\n227</td>\n",
       "      <td>228.0</td>\n",
       "      <td>228</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.1 Cyclic f-invariant Subspaces and Duality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter cyclic subspace duality jordan canonical form chapter use duality theory analyze property endomorphism f finite dimensional vector space v detail particularly interested algebraic geometric multiplicity eigenvalue f characterization corresponding eigenspaces strategy analysis decompose vector space v direct sum f subspace appropriately chosen base essential property f obvious matrix representation matrix representation derive called jordan canonical form f great importance many different derivation form using different mathematical tool approach using duality theory based article vlastimil pták cyclic f subspace duality let v finite dimensional k space f l v v v exists uniquely defined smallest number n vector f f linearly independent vector f f f linearly dependent obviously dim v since dim v vector v linearly independent number called grade respect f denote grade f vector linearly dependent thus grade respect f springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>228\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\nFor v0 \u0004= 0 we have m( f, v0 ) = 1 if and only if the vectors v0 , f (v0 ) are linearly dependent.\\nThis holds if and only if v0 is an eigenvector of f .\\nIf v0 \u0004= 0 is not an eigenvector of f , then m( f, v0 ) ≥ 2.\\nFor every j ∈ N we define the subspace\\nK j ( f, v0 ) := span{v0 , f (v0 ), . . . , f j−1\\n\\n(v0 )} ⊆ V.\\n\\nThe space K j ( f, v0 ) is called the jth Krylov subspace1 of f and v0 .\\nLemma 16.1 If V is a finite dimensional K -vector space, f ∈ L(V, V), and v0 ∈ V, then the following assertions hold:\\n(1) If m = m( f, v0 ), then Km ( f, v0 ) is an f -invariant subspace of V, and span{v0 } = K1 ( f, v0 ) ⊂ K2 ( f, v0 ) ⊂ · · · ⊂ Km ( f, v0 ) = Km+ j ( f, v0 ) for all j ∈ N.\\n(2) If m = m( f, v0 ) and U ⊆ V is an f -invariant subspace that contains the vector v0 , then Km ( f, v0 ) ⊆ U. Thus, among all f -invariant subspaces of V that contain the vector v0 , the Krylov subspace Km ( f, v0 ) is the one of smallest dimension.\\n(3) If f m−1 (v0 ) \u0004= 0 and f m (v0 ) = 0 for an m ∈ N, then dim(K j ( f, v0 )) = j for j = 1, . . . , m.\\nProof\\n(1) Exercise.\\n(2) The assertion is trivial if v0 = 0.\\nThus, let v0 \u0004= 0 with m = d( f, v0 ) ≥ 1 and let U ⊆ V be an f -invariant subspace that contains v0 .\\nThen U also contains the vectors f (v0 ), . . . , f m−1 (v0 ), so that Km ( f, v0 ) ⊆ U and, in particular, dim(U) ≥ m = dim(Km ( f, v0 ).\\n(3) Let γ0 , . . . , γm−1 ∈ K with\\n0 = γ0 v0 + . . . + γm−1 f m−1 (v0 ).\\nIf we apply f m−1 to both sides, then 0 = γ0 f m−1 (v0 ) and thus γ0 = 0, since f m−1 (v0 ) \u0004= 0.\\nIf m &gt; 1, then we apply inductively f m−k for k = 2, . . . , m and obtain γ1 = · · · = γm−1 = 0.\\nThus, the vectors v0 , . . . , f m−1 (v0 ) are linearly\\n\b independent, which implies that dim(K j ( f, v0 )) = j for j = 1, . . . , m.\\nThe vectors v0 , f (v0 ), . . . , f m−1 (v0 ) form, by construction, a basis of the Krylov subspace Km ( f, v0 ).\\nThe application of f to a vector f k (v0 ) of this basis yields the next basis vector f k+1 (v0 ), k = 0, 1, . . . , m − 2, and the application of f to the last vector f m−1 (v0 ) yields a linear combination of all basis vectors, since f m (v0 ) ∈ Km ( f, v0 ).\\nDue to this special structure, the subspace Km ( f, v0 ) is called a cyclic f -invariant subspace.\\n1 Aleksey\\n\\nNikolaevich Krylov (1863–1945).</td>\n",
       "      <td>229.0</td>\n",
       "      <td>229</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.1 Cyclic f-invariant Subspaces and Duality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic subspace duality jordan canonical form f vector f linearly dependent hold eigenvector f eigenvector f f every j n define subspace k j f span f f space k j f called jth krylov f lemma v finite dimensional k space f l v v v following assertion hold f km f f subspace v span f f km f j f j f u v f subspace contains vector km f u thus among f subspace v contain vector krylov subspace km f one smallest dimension f f n dim k j f j j proof exercise assertion trivial thus let f let u v f subspace contains u also contains vector f f km f u particular dim u dim km f let k f apply f side f thus since f apply inductively f k obtain thus vector f linearly independent implies dim k j f j j vector f f form construction basis krylov subspace km f application f vector f k basis yield next basis vector f k application f last vector f yield linear combination basis vector since f km f due special structure subspace km f called cyclic f subspace aleksey nikolaevich krylov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>16.1 Cyclic f -invariant Subspaces and Duality\\n\\n229\\n\\nDefinition 16.2 Let V \u0004= {0} be a K -vector space.\\nAn endomorphism f ∈ L(V, V) is called nilpotent, if f m = 0 holds for an m ∈ N. If at the same time f m−1 \u0004= 0, then f is called nilpotent of index m.\\nThe zero map f = 0 is the only nilpotent endomorphism of index m = 1.\\nIf\\nV = {0}, then the zero map is the only endomorphism on V. This map is nilpotent of index m = 1, where in this case we omit the requirement f m−1 = f 0 \u0004= 0.\\nIf f is nilpotent of index m and v \u0004= 0 is any vector with f m−1 (v) \u0004= 0, then f ( f m−1 )(v) = f m (v) = 0 = 0 · f m−1 (v).\\nHence f m−1 (v) is an eigenvector of f corresponding to the eigenvalue 0.\\nOur construction in Sect.\\n16.2 will show that 0 is the only eigenvalue of a nilpotent endomorphism (also cp.\\nExercise 8.3).\\nLemma 16.3 If V \u0004= {0} is a K -vector space and if f ∈ L(V, V) is nilpotent of index m, then m ≤ dim(V).\\nProof If f is nilpotent of index m, then there exists a v0 ∈ V with f m−1 (v0 ) \u0004= 0 and f m (v0 ) = 0.\\nBy (3) in Lemma 16.1 the m vectors v0 , . . . , f m−1 (v0 ) are linearly independent, which implies that m ≤ dim(V).\\n\b\\nExample 16.4 In the vector space K 3,1 the endomorphism f : K 3,1\\n\\n⎡ ⎤\\n⎡ ⎤\\nν1\\n0\\n→ K 3,1 , ⎣ν2 ⎦ → ⎣ν1 ⎦ ,\\nν3\\nν2 is nilpotent of index 3, since f \u0004= 0, f 2 \u0004= 0 and f 3 = 0.\\nIf U is an f -invariant subspace of V, then f |U ∈ L(U, U), where f |U : U → U, u → f (u), is the restriction of f to the subspace U (cp.\\nDefinition 2.12).\\nTheorem 16.5 Let V be a finite dimensional K -vector space and f ∈ L(V, V).\\nThen there exist f -invariant subspaces U1 ⊆ V and U2 ⊆ V with V = U1 ⊕ U2 , such that f |U1 ∈ L(U1 , U1 ) is bijective and f |U2 ∈ L(U2 , U2 ) is nilpotent.\\nProof If v ∈ ker( f ), then f 2 (v) = f ( f (v)) = f (0) = 0.\\nThus, v ∈ ker( f 2 ) and therefore ker( f ) ⊆ ker( f 2 ).\\nProceeding inductively we see that\\n{0} ⊆ ker( f ) ⊆ ker( f 2 ) ⊆ ker( f 3 ) ⊆ · · · .\\nSince V is finite dimensional, there exists a smallest number m ∈ N0 with ker( f m ) = ker( f m+ j ) for all j ∈ N. For this number m let\\nU1 := im( f m ), U2 := ker( f m ).</td>\n",
       "      <td>230.0</td>\n",
       "      <td>230</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.1 Cyclic f-invariant Subspaces and Duality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic f subspace duality definition let v k space endomorphism f l v v called nilpotent f hold time f f called nilpotent index zero map f nilpotent endomorphism index v zero map endomorphism map nilpotent index case omit requirement f f f nilpotent index v vector f v f f v f v f v hence f v eigenvector f corresponding eigenvalue construction sect show eigenvalue nilpotent endomorphism also cp exercise lemma v k space f l v v nilpotent index dim v proof f nilpotent index exists v f f lemma vector f linearly independent implies dim v example vector space k endomorphism f k k nilpotent index since f f f u f subspace v f l u u f u u u f u restriction f subspace u cp definition theorem let v finite dimensional k space f l v v exist f subspace v v v f l bijective f l nilpotent proof v ker f f v f f v f thus v ker f therefore ker f ker f proceeding inductively see ker f ker f ker f since v finite dimensional exists smallest number ker f ker f j j number let im f ker f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>230\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\n(If f is bijective, then m = 0, U1 = V and U2 = {0}.)\\nWe now show that the spaces\\nU1 and U2 satisfy the assertion.\\nFirst observe that U1 and U2 are both f -invariant: If v ∈ U1 , then v = f m (w) for some w ∈ V, and therefore f (v) = f ( f m (w)) = f m ( f (w)) ∈ U1 .\\nIf v ∈ U2 , then f m ( f (v)) = f ( f m (v)) = f (0) = 0, and therefore f (v) ∈ U2 .\\nWe have U1 + U2 ⊆ V. An application of the dimension formula for linear maps\\n(cp.\\nTheorem 10.9) to f m gives dim(V) = dim(U1 ) + dim(U2 ).\\nIf v ∈ U1 ∩ U2 , then v = f m (w) for some w ∈ V (since v ∈ U1 ) and hence\\n0 = f m (v) = f m ( f m (w)) = f 2m (w).\\nThe first equation holds since v ∈ U2 .\\nBy the definition of m we have ker( f m ) = ker( f 2m ), which implies f m (w) = 0, and therefore v = f m (w) = 0.\\nFrom U1 ∩U2 =\\n{0} we obtain V = U1 ⊕ U2 .\\nLet now v ∈ ker( f |U1 ) ⊆ U1 be given.\\nSince v ∈ U1 , there exists a vector w ∈ V with v = f m (w), which implies 0 = f (v) = f ( f m (w)) = f m+1 (w).\\nBy the definition of m we have ker( f m ) = ker( f m+1 ), thus w ∈ ker( f m ), and therefore v = f m (w) = 0.\\nThis implies that ker( f |U1 ) = {0}, i.e., f |U1 is injective and thus also bijective (cp.\\nCorollary 10.11).\\nIf, on the other hand, v ∈ U2 , then by definition 0 = f m (v) = ( f |U2 )m (v), and\\n\b thus ( f |U2 )m is the zero map in L(U2 , U2 ), so that f |U2 is nilpotent.\\nFor the further development we recall some terms and results from Chap.\\n11.\\nLet\\nV be a finite dimensional K -vector space and let V ∗ be the dual space of V. If U ⊆ V and W ⊆ V ∗ are two subspaces and if the bilinear form\\nβ : U × W → K , (v, h) → h(v),\\n\\n(16.1) is non-degenerate, then U, W is called a dual pair with respect to β.\\nThis requires that dim(U) = dim(W).\\nFor f ∈ L(U, U) the dual map f ∗ ∈ L(U ∗ , U ∗ ) is defined by f ∗ : U ∗ → U ∗ , h → h ◦ f.\\nFor all v ∈ U and h ∈ U ∗ we have ( f ∗ (h))(v) = h( f (v)).\\nFurthermore, ( f k )∗ =\\n( f ∗ )k for all k ∈ N0 .\\nThe set\\nU 0 := {h ∈ V ∗ | h(u) = 0 for all u ∈ U} is called the annihilator of U. This set is a subspace of V ∗ (cp.\\nExercise 11.5).\\nAnalogously, the set\\nW 0 := {v ∈ V | h(v) = 0 for all h ∈ W} is called the annihilator of W. This set is a subspace of V.</td>\n",
       "      <td>231.0</td>\n",
       "      <td>231</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.1 Cyclic f-invariant Subspaces and Duality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic subspace duality jordan canonical form f bijective v show space satisfy assertion first observe f v v f w w v therefore f v f f w f f w v f f v f f v f therefore f v application dimension formula linear map cp theorem f give dim v dim dim v v f w w v since v hence f v f f w f w first equation hold since v definition ker f ker f implies f w therefore v f w obtain v let v ker f given since v exists vector w v v f w implies f v f f w f w definition ker f ker f thus w ker f therefore v f w implies ker f f injective thus also bijective cp corollary hand v definition f v f v thus f zero map l f nilpotent development recall term result chap let v finite dimensional k space let v dual space u v w v two subspace bilinear form β u w k v h h v non-degenerate u w called dual pair respect β requires dim u dim w f l u u dual map f l u u defined f u u h h v u h u f h v h f v furthermore f k f k k set u h v h u u u called annihilator u set subspace v cp exercise analogously set w v v h v h w called annihilator set subspace v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>16.1 Cyclic f -invariant Subspaces and Duality\\n\\n231\\n\\nLemma 16.6 Let V be a finite dimensional K -vector space, f ∈ L(V, V), V ∗ the dual space of V, f ∗ ∈ L(V ∗ , V ∗ ) the dual map of f , and let U ⊆ V and W ⊆ V ∗ be two subspaces.\\nThen the following assertions hold:\\n(1) dim(V) = dim(W) + dim(W 0 ) = dim(U) + dim(U 0 ).\\n(2) If f is nilpotent of index m, then f ∗ is nilpotent of index m.\\n(3) If W ⊆ V ∗ is an f ∗ -invariant subspace, then W 0 ⊆ V is an f -invariant subspace.\\n(4) If U, W are a dual pair with respect to the bilinear form defined in (16.1), then\\nV = U ⊕ W 0.\\nProof\\n(1) Exercise.\\n(2) For all v ∈ V we have f m (v) = 0 and hence,\\n0 = h( f m (v)) = (( f m )∗ (h))(v) = (( f ∗ )m (h))(v) for every h ∈ V ∗ and v ∈ V, so that f ∗ is nilpotent of index at most m.\\nIf ( f ∗ )m−1 = 0, then ( f ∗ )m−1 (h) = 0 for all h ∈ V ∗ , and therefore 0 =\\n(( f ∗ )m−1 (h))(v) = h( f m−1 (v)) for all v ∈ V. This implies that f m−1 = 0, in contradiction to the assumption that f is nilpotent of index m.\\nThus, f ∗ is nilpotent of index m.\\n(3) Let w ∈ W 0 .\\nFor every h ∈ W, we have f ∗ (h) ∈ W, and thus 0 = f ∗ (h)(w) = h( f (w)).\\nHence f (w) ∈ W 0 .\\n(4) If u ∈ U ∩ W 0 , then h(u) = 0 for all h ∈ W, since u ∈ W 0 .\\nSince U, W is a dual pair with respect to the bilinear form defined in (16.1), we have u = 0.\\nMoreover, dim(U) = dim(W) and using (1) we obtain dim(V) = dim(W) + dim(W 0 ) = dim(U) + dim(W 0 ).\\nFrom U ∩ W 0 = {0} we obtain V = U ⊕ W 0 .\\n\\n\b\\n\\nExample 16.7 We consider the vector space V = R2,1 with the canonical basis\\nB = {e1 , e2 }.\\nFor the subspaces\\n\u0006\u0007 \b\\n0\\n⊂ V,\\nU = span\\n1\\nW = h ∈ V ∗ [h] B,{1} = [α, α] for an α ∈ R ⊂ V ∗ , we have\\nU 0 = h ∈ V ∗ [h] B,{1} = [α, 0] for an α ∈ R ⊂ V ∗ ,\\n\u0006\u0007\\n\b\\n1\\n0\\nW = span\\n⊂ V.\\n−1</td>\n",
       "      <td>232.0</td>\n",
       "      <td>232</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.1 Cyclic f-invariant Subspaces and Duality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic f subspace duality lemma let v finite dimensional k space f l v v v dual space v f l v v dual map f let u v w v two subspace following assertion hold dim v dim w dim w dim u dim u f nilpotent index f nilpotent index w v f subspace w v f subspace u w dual pair respect bilinear form defined v u w proof exercise v v f v hence h f v f h v f h v every h v v v f nilpotent index f f h h v therefore f h v h f v v implies f contradiction assumption f nilpotent index thus f nilpotent index let w w every h w f h w thus f h w h f w hence f w w u u w h u h w since u w since u w dual pair respect bilinear form defined u moreover dim u dim w using obtain dim v dim w dim w dim u dim w u w obtain v u w example consider vector space v canonical basis b subspace v u span w h v h b α α α r v u h v h b α α r v w span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>232\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\nIn this example, we easily see that dim(V) = dim(W) + dim(W 0 ) = dim(U) + dim(U 0 ), and that U, W form a dual pair with respect to the bilinear form defined in\\n(16.1) with K = R. Moreover, V = U ⊕ W 0 .\\nThe following theorem presents, for a given nilpotent f , a decomposition of V into f -invariant subspaces.\\nThe idea of the decomposition is to construct a dual pair of subspaces U ⊆ V and W ⊆ V ∗ , where U is f -invariant and W is f ∗ -invariant.\\nBy (3) in Lemma 16.6 then W 0 is f -invariant and with (4) in Lemma 16.6 it follows that V = U ⊕ W 0 .\\nTheorem 16.8 Let V be a finite dimensional K -vector space and let f ∈ L(V, V) be nilpotent of index m.\\nLet v0 ∈ V satisfy f m−1 (v0 ) \u0004= 0 and let h 0 ∈ V ∗ satisfy h 0 ( f m−1 (v0 )) \u0004= 0.\\nThen m( f, v0 ) = m( f ∗ , h 0 ) = m, and the f - and f ∗ -invariant subspaces Km ( f, v0 )\\n⊆ V and Km ( f ∗ , h 0 ) ⊆ V ∗ , respectively, are a dual pair with respect to the bilinear form defined in (16.1).\\nFurthermore,\\nV = Km ( f, v0 ) ⊕ (Km ( f ∗ , h 0 ))0 , where (Km ( f ∗ , h 0 ))0 is an f -invariant subspace of V.\\nProof Let v0 ∈ V be a vector with f m−1 (v0 ) \u0004= 0.\\nSince f m (v0 ) = 0, the space\\nKm ( f, v0 ) is an m-dimensional f -invariant subspace of V (cp.\\n(3) in Lemma 16.1).\\nLet h 0 ∈ V ∗ be a vector with\\n0 \u0004= h 0 ( f m−1 (v0 )) = (( f ∗ )m−1 (h 0 ))(v0 ).\\nThen, in particular, 0 \u0004= ( f ∗ )m−1 (h 0 ) ∈ L(V ∗ , V ∗ ).\\nSince f is nilpotent of index m, also f ∗ is nilpotent of index m (cp.\\n(2) in Lemma 16.6), so that\\n( f ∗ )m (h 0 ) = 0 ∈ L(V ∗ , V ∗ ).\\nTherefore, Km ( f ∗ , h 0 ) is an m-dimensional f ∗ -invariant subspace of V ∗ (cp.\\n(3) in\\nLemma 16.1).\\nIt remains to show that Km ( f, v0 ), Km ( f ∗ , h 0 ) are a dual pair.\\nLet m−1 v1 =\\n\\nγ j f j (v0 ) ∈ Km ( f, v0 ) j=0 be a vector with h(v1 ) = β(v1 , h) = 0 for all h ∈ Km ( f ∗ , h 0 ).\\nWe show inductively that then γ0 = · · · = γm−1 = 0, and thus v1 = 0.</td>\n",
       "      <td>233.0</td>\n",
       "      <td>233</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.1 Cyclic f-invariant Subspaces and Duality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic subspace duality jordan canonical form example easily see dim v dim w dim w dim u dim u u w form dual pair respect bilinear form defined k moreover v u w following theorem present given nilpotent f decomposition v f subspace idea decomposition construct dual pair subspace u v w v u f w f lemma w f lemma follows v u w theorem let v finite dimensional k space let f l v v nilpotent index let v satisfy f let h v satisfy h f f f h f f subspace km f v km f h v respectively dual pair respect bilinear form defined furthermore v km f km f h km f h f subspace proof let v vector f since f space km f m-dimensional f subspace v cp lemma let h v vector h f f h particular f h l v v since f nilpotent index also f nilpotent index cp lemma f h l v v therefore km f h m-dimensional f subspace v cp lemma remains show km f km f h dual pair let γ j f j km f vector h β h h km f h show inductively thus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>16.1 Cyclic f -invariant Subspaces and Duality\\n\\n233\\n\\nUsing ( f ∗ )m−1 (h 0 ) ∈ Km ( f ∗ , h 0 ) our assumption on the vector v1 yields m−1\\n∗ m−1\\n\\n0 = (( f )\\n\\n(h 0 ))(v1 ) = h 0 ( f m−1\\n\\n(v1 )) =\\n\\nγ j h 0 ( f m−1+ j (v0 )) j=0\\n\\n= γ0 h 0 ( f m−1 (v0 )).\\nThe last equation holds, since f m−1+ j (v0 ) = 0 for j = 1, . . . , m − 1 (because f m = 0).\\nFrom h 0 ( f m−1 (v0 )) \u0004= 0 we obtain γ0 = 0.\\nSuppose now that γ0 = · · · = γk−1 = 0 for a k, 1 ≤ k ≤ m − 2.\\nUsing\\n( f ∗ )m−1−k (h 0 ) ∈ Km ( f ∗ , h 0 ) our assumption on the vector v1 yields m−1\\n∗ m−1−k\\n\\n0 = (( f )\\n\\n(h 0 ))(v1 ) = h 0 ( f m−1−k\\n\\n(v1 )) =\\n\\nγ j h 0 ( f m−1+ j−k (v0 )) j=0\\n\\n= γk h 0 ( f m−1 (v0 )).\\nThe last equation holds, since γ j = 0 for j = 0, . . . , k − 1 and f m−1+ j−k (v0 ) = 0 for j = k + 1, . . . , m − 1.\\nWe have v1 = 0 as asserted, and therefore the bilinear form defined in (16.1) for the spaces Km ( f, v0 ), Km ( f ∗ , h 0 ) is non-degenerate in the first variable.\\nAnalogously, the bilinear form is non-degenerate in the second variable, and hence\\nKm ( f, v0 ), Km ( f ∗ , h 0 ) are a dual pair.\\nUsing (4) in Lemma 16.6 we now have V = Km ( f, v0 ) ⊕ (Km ( f ∗ , h 0 ))0 , where the space (Km ( f ∗ , h 0 ))0 , is by (3) in Lemma 16.6 an f -invariant subspace of V. \b\\n\\n16.2 The Jordan Canonical Form\\nLet V be a finite dimensional K -vector space and f ∈ L(V, V).\\nIf there exists a basis B of V consisting of eigenvectors of f , then [ f ] B,B is a diagonal matrix, i.e., f is diagonalizable.\\nA necessary and sufficient condition for this is that the characteristic polynomial P f decomposes into linear factors over K and that in addition g( f, λ j ) = a( f, λ j ) for every eigenvalue λ j (cp.\\nTheorem 14.14).\\nIf P f decomposes into linear factors but g( f, λ j ) &lt; a( f, λ j ) holds for at least one eigenvalue λ j , then f is not diagonalizable but can still be triangulated, i.e., there exists a basis B of V, such that [ f ] B,B is an upper triangular matrix\\n(cp.\\nTheorem 14.17).\\nFrom this triangular matrix we can read off the algebraic, but usually not the geometric multiplicities of the eigenvalues.\\nThe goal of the following construction is to determine a basis B of V, so that [ f ] B,B is upper triangular and in addition to the algebraic also reveals the geometric multiplicities of the eigenvalues.\\nUnder the assumption that P f decomposes into linear factors over K , we will construct a basis B of V for which [ f ] B,B is a block diagonal matrix of the form</td>\n",
       "      <td>234.0</td>\n",
       "      <td>234</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.2 The Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic f subspace duality using f h km f h assumption vector yield f h h f γ j h f j h f last equation hold since f j j f h f obtain suppose k k using f h km f h assumption vector yield f h h f γ j h f γk h f last equation hold since γ j j k f j k asserted therefore bilinear form defined space km f km f h non-degenerate first variable analogously bilinear form non-degenerate second variable hence km f km f h dual pair using lemma v km f km f h space km f h lemma f subspace jordan canonical form let v finite dimensional k space f l v v exists basis b v consisting eigenvectors f f b b diagonal matrix f diagonalizable necessary sufficient condition characteristic polynomial p f decomposes linear factor k addition g f λ j f λ j every eigenvalue λ j cp theorem p f decomposes linear factor g f λ j f λ j hold least one eigenvalue λ j f diagonalizable still triangulated exists basis b v f b b upper triangular matrix cp theorem triangular matrix read algebraic usually geometric multiplicity eigenvalue goal following construction determine basis b v f b b upper triangular addition algebraic also reveals geometric multiplicity eigenvalue assumption p f decomposes linear factor k construct basis b v f b b block diagonal matrix form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>234\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\n⎡\\n[ f ] B,B = ⎣\\n\\nJd1 (λ1 )\\n\\n⎤\\n...\\n\\n⎦\\nJdm (λm ), where each diagonal block has the form\\n⎡\\n⎤\\nλj 1\\n⎢ ... ... ⎥\\n⎢\\n⎥\\nJd j (λ j ) := ⎢\\n∈ K d j ,d j\\n... ⎥\\n⎣\\n1⎦\\nλj\\n\\n(16.2) for some λ j ∈ K and d j ∈ N, j = 1, . . . , m.\\nA matrix of the form (16.2) is called a\\nJordan block of size d j corresponding to the eigenvalue λ j .\\nIn the following construction we first do not assume that P f decomposes into linear factors.\\nWe only assume the existence of a single eigenvalue λ1 ∈ K of f .\\nUsing this eigenvalue, we define the endomorphism g := f − λ1 IdV ∈ L(V, V).\\nBy Theorem 16.5 there exist g-invariant subspaces U ⊆ V and W ⊆ V with\\nV = U ⊕ W, such that g1 := g|U is nilpotent and g|W is bijective.\\nThen U \u0004= {0}, since otherwise W = V and g|W = g|V = g would be bijective, which contradicts the assumption that λ1 is an eigenvalue of f .\\nLet g1 be nilpotent of index d1 .\\nThen by construction 1 ≤ d1 ≤ dim(U).\\nLet w1 ∈ U be a vector with g1d1 −1 (w1 ) \u0004= 0.\\nSince g1d1 (w1 ) = 0, the vector g1d1 −1 (w1 ) is a eigenvector of g1 corresponding to the eigenvalue 0.\\nBy (3) in Lemma 16.1, the d1 vectors w1 , g1 (w1 ), . . . , g1d1 −1 (w1 ) are linearly independent and U1 := Kd1 (g1 , w1 ) is a d1 -dimensional g1 -invariant subspace of U.\\nConsider the basis\\n\u0011\\n\u0010\\nB1 := g1d1 −1 (w1 ), . . . , g1 (w1 ), w1 of U1 .\\nThen the matrix representation g1 |U1 with respect to the basis B1 is given by\\n[g1 |U1 ] B1 ,B1 = Jd1 (0) ∈ K d1 ,d1 .</td>\n",
       "      <td>235.0</td>\n",
       "      <td>235</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.2 The Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic subspace duality jordan canonical form f b b jdm λm diagonal block form λj jd j λ j k j j λj λ j k j n j matrix form called jordan block size j corresponding eigenvalue λ j following construction first assume p f decomposes linear factor assume existence single eigenvalue k f using eigenvalue define endomorphism g f idv l v v theorem exist g-invariant subspace u v w v v u w nilpotent bijective u since otherwise w v g would bijective contradicts assumption eigenvalue f let nilpotent index construction dim u let u vector since vector eigenvector corresponding eigenvalue lemma vector linearly independent subspace u consider basis matrix representation respect basis given k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>16.2 The Jordan Canonical Form\\n\\n235\\n\\nThis shows, in particular, that the characteristic polynomial of g1 |U1 is given by the monomial t d1 , and hence 0 is the only eigenvalue of g1 |U1 .\\nMoreover, by construction\\n[g1 |U1 ] B1 ,B1 = [g|U1 ] B1 ,B1 .\\nIf d1 = dim(U), then our construction is complete for the moment.\\nIf, on the other hand, d1 &lt; dim(U), then applying Theorem 16.8 to g1 ∈ L(U, U) shows that there\\n\u0012 \u0004= {0} with U = U1 ⊕ U,\\n\u0012 and we consider exists a g1 -invariant subspace U g2 := g1 |U\u0012.\\nThis map is nilpotent of index d2 , where 1 ≤ d2 ≤ d1 .\\nWe now carry out the same construction as before:\\n\u0012 with g d2 −1 (w2 ) \u0004= 0.\\nThen g d2 −1 (w2 ) is an\\nWe determine a vector w2 ∈ U\\n2\\n2 eigenvector of g2 , U2 := Kd2 (g2 , w2 ) is a d2 -dimensional g2 -invariant subspace of\\n\u0012 ⊂ U and for the basis\\nU\\n\u0011\\n\u0010\\nB2 := g2d2 −1 (w2 ), . . . , g2 (w2 ), w2 of U2 we have\\n\\n[g2 |U2 ] B2 ,B2 = Jd2 (0) ∈ K d2 ,d2 , where again [g2 |U2 ] B2 ,B2 = [g|U2 ] B2 ,B2 by construction.\\nAfter k ≤ dim(U) steps this procedure terminates.\\nWe then have found a decomposition of U of the form\\nU = Kd1 (g1 , w1 ) ⊕ . . . ⊕ Kdk (gk , wk ) = Kd1 (g, w1 ) ⊕ . . . ⊕ Kdk (g, wk ).\\nIn the second equation we have used that Kd j (g j , w j ) = Kd j (g, wk ) for j = 1, . . . , k.\\nIf we combine the constructed bases B1 , . . . , Bk to a basis B of U, then\\n[g|U ] B,B\\n\\n⎡\\n[g|U1 ] B1 ,B1\\n...\\n⎣\\n=\\n\\n⎤\\n\\n⎡\\n\\n⎦ = ⎣\\n\\nJd1 (0)\\n\\n[g|Uk ] Bk ,Bk\\n\\n⎤\\n...\\n\\n⎦.\\nJdk (0)\\n\\nThus, the nilpotent endomorphism g1 = g|U has the characteristic polynomial t d1 +...+dk , and its only eigenvalue is 0.\\nWe now transfer these results to f = g + λ1 IdV .\\nEvery g-invariant subspace is f -invariant and one observes easily that\\nKd j ( f, w j ) = Kd j (g, w j ), j = 1, . . . , k</td>\n",
       "      <td>236.0</td>\n",
       "      <td>236</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.2 The Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jordan canonical form show particular characteristic polynomial given monomial hence eigenvalue moreover construction dim u construction complete moment hand dim u applying theorem l u u show u u consider exists subspace u map nilpotent index carry construction g g determine vector u eigenvector subspace u basis u k construction k dim u step procedure terminates found decomposition u form u kdk gk wk g kdk g wk second equation used kd j g j w j kd j g wk j combine constructed base bk basis b u b b bk bk jdk thus nilpotent endomorphism characteristic polynomial eigenvalue transfer result f g idv every g-invariant subspace f one observes easily kd j f w j kd j g w j j k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>236\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\n(cp.\\nExercise 16.3).\\nHence, it follows that\\nU = Kd1 ( f, w1 ) ⊕ . . . ⊕ Kdk ( f, wk ).\\nFor every j = 1, . . . , k and 0 ≤ \u0002 ≤ d j − 1 we have\\n\u0014\\n\u0013\\n\u0014\\n\u0013 f g \u0002 (w j ) = g g \u0002 (w j ) + λ1 g \u0002 (w j ) = λ1 g \u0002 (w j ) + g \u0002+1 (w j ),\\n\\n(16.3) where g d j (w j ) = 0.\\nThe matrix representation of f |U with respect to the basis B of\\nU is therefore given by\\n[ f |U ] B,B\\n\\n⎡\\n[ f |U1 ] B1 ,B1\\n...\\n=⎣\\n\\n⎤\\n\\n⎡\\n\\n⎦=⎣\\n\\nJd1 (λ1 )\\n\\n[ f |Uk ] Bk ,Bk\\n\\n⎤\\n...\\n\\n⎦.\\n\\n(16.4)\\n\\nJdk (λ1 )\\n\\nThe map g|W = f |W − λ1 IdW is bijective by construction, i.e., λ1 is not an eigenvalue of f |W .\\nTherefore, a( f, λ1 ) = dim(U) = d1 + . . . + dk .\\nIn order to determine g( f, λ1 ), let v ∈ U be an arbitrary vector.\\nThen there exist scalars α j,\u0002 ∈ K with d j −1 k\\n\\nα j,\u0002 g \u0002 (w j ).\\nv= j=1 \u0002=0\\n\\nUsing (16.3) we obtain k d j −1 f (v) = k d j −1 k d j −1\\n\u0015\\n\u0016\\nα j,\u0002 f g \u0002 (w j ) =\\nα j,\u0002 λ1 g \u0002 (w j ) +\\nα j,\u0002 g \u0002+1 (w j ) j=1 \u0002=0 j=1 \u0002=0 k d j −2\\n\\n= λ1 v + j=1 \u0002=0\\n\\nα j,\u0002 g \u0002+1 (w j ).\\nj=1 \u0002=0\\n\\nThe vectors in the last sum are linearly independent.\\nHence, f (v) = λ1 v if and only if α j,\u0002 = 0 for j = 1, . . . , k and \u0002 = 0, 1, . . . , d j − 2.\\nThis shows that every eigenvector of f corresponding to the eigenvalue λ1 has the form k\\n\\nα j g d j −1 (w j ), v= j=1 where at least one α j is nonzero, so that we have\\nV f (λ1 ) = span{g d1 −1 (w1 ), . . . , g dk −1 (wk )}.</td>\n",
       "      <td>237.0</td>\n",
       "      <td>237</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.2 The Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic subspace duality jordan canonical form cp exercise hence follows u f kdk f wk every j k j f g w j g g w j g w j g w j g w j g j w j matrix representation f respect basis b u therefore given f b b f f bk bk jdk map f idw bijective construction eigenvalue f therefore f dim u dk order determine g f let v u arbitrary vector exist scalar α j k j k α j g w j using obtain k j f v k j k j α j f g w j α j g w j α j g w j k j v α j g w j vector last sum linearly independent hence f v v α j j k j show every eigenvector f corresponding eigenvalue form k α j g j w j least one α j nonzero v f span g g dk wk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>16.2 The Jordan Canonical Form\\n\\n237\\n\\nSince g d1 −1 (w1 ), . . . , g dk −1 (wk ) are linearly independent, it follows that g( f, λ1 ) = k.\\nThe geometric multiplicity of the eigenvalue λ1 therefore is equal to the number of\\nJordan blocks corresponding to the eigenvalue λ1 in the matrix representation (16.4).\\nFurthermore, we observe that in every subspace Kd j ( f, w j ), the endomorphism f has exactly one (linear independent) eigenvector corresponding to the eigenvalue λ1 .\\nWe summarize these results in the following theorem.\\nTheorem 16.9 Let V be a finite dimensional K -vector space and let f ∈ L(V, V).\\nIf λ1 ∈ K is an eigenvalue of f , then the following assertions hold:\\n(1) There exist f -invariant subspaces {0} \u0004= U ⊆ V and W ⊂ V with V = U ⊕ W.\\nThe map f |U − λ1 IdU is nilpotent and the map f |W − λ1 IdW is bijective.\\nIn particular, λ1 is not an eigenvalue of f |W .\\n(2) The subspace U from (1) can be written as\\nU = Kd1 ( f, w1 ) ⊕ . . . ⊕ Kdk ( f, wk ) for some vectors w1 , . . . , wk ∈ U, where Kd j ( f, w j ) is a d j -dimensional f invariant subspace of V, j = 1, . . . , k.\\nThis is called a cyclic decomposition of U.\\n(3) There exists a basis B of U with\\n⎡\\n[ f |U ] B,B = ⎣\\n\\nJd1 (λ1 )\\n\\n⎤\\n...\\n\\n⎦.\\nJdk (λ1 )\\n\\n(4) We have a( f, λ1 ) = d1 + . . . + dk and g( f, λ1 ) = k.\\nIf f has a further eigenvalue λ2 \u0004= λ1 , then it is an eigenvalue of the restriction f |W ∈ L(W, W) and we can apply Theorem 16.9 to f |W .\\nThe vector space W then is the direct sum of the form W = X ⊕ Y, where f |X − λ2 IdX is nilpotent and f |Y − λ2 IdY is bijective.\\nThe space X has a cyclic decomposition analogous to (2) in Theorem 16.9, and there exists a matrix representation of f |X analogous to (3).\\nThis construction can be carried out for all eigenvalues of f .\\nIf the characteristic polynomial P f decomposes into linear factors over K , then we finally obtain a cyclic decomposition of the entire space V, which gives the following theorem.\\nTheorem 16.10 Let V be a finite dimensional K -vector space and let f ∈ L(V, V).\\nIf the characteristic polynomial P f decomposes into linear factors over K , then there exists a basis B of V, such that\\n⎡\\n[ f ] B,B = ⎣\\n\\nJd1 (λ1 )\\n\\n⎤\\n...\\n\\n⎦,\\n\\n(16.5)\\n\\nJdm (λm ) where λ1 , . . . , λm ∈ K are the (not necessarily pairwise distinct) eigenvalues of f .\\nFor every eigenvalue λ j of f then a( f, λ j ) is equal to the sum of the sizes of all</td>\n",
       "      <td>238.0</td>\n",
       "      <td>238</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.2 The Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jordan canonical form since g g dk wk linearly independent follows g f geometric multiplicity eigenvalue therefore equal number jordan block corresponding eigenvalue matrix representation furthermore observe every subspace kd j f w j endomorphism f exactly one linear independent eigenvector corresponding eigenvalue summarize result following theorem theorem let v finite dimensional k space let f l v v k eigenvalue f following assertion hold exist f subspace u v w v v u map f idu nilpotent map f idw bijective particular eigenvalue f subspace u written u f kdk f wk vector wk u kd j f w j j f invariant subspace v j called cyclic decomposition u exists basis b u f b b jdk f dk g f f eigenvalue eigenvalue restriction f l w w apply theorem f vector space w direct sum form w x f idx nilpotent f idy bijective space x cyclic decomposition analogous theorem exists matrix representation f analogous construction carried eigenvalue f characteristic polynomial p f decomposes linear factor k finally obtain cyclic decomposition entire space v give following theorem theorem let v finite dimensional k space let f l v v characteristic polynomial p f decomposes linear factor k exists basis b v f b b jdm λm λm k necessarily pairwise distinct eigenvalue f every eigenvalue λ j f f λ j equal sum size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>238\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\nJordan blocks corresponding to λ j in (16.5), and g( f, λ j ) is equal to the number of Jordan blocks corresponding to λ j in (16.5).\\nA matrix representation of the form\\n(16.5) is called a Jordan canonical form2 of f .\\nFrom Theorem 14.14 we know that f ∈ L(V, V) is diagonalizable if and only if P f decomposes into linear factors over K and g( f, λ j ) = a( f, λ j ) holds for every eigenvalue λ j of f .\\nIf P f decomposes into linear factors, then the Jordan canonical form (16.5) shows that g( f, λ j ) = a( f, λ j ) if and only if every Jordan block corresponding to λ j is of size 1.\\nThe Fundamental Theorem of Algebra yields the following corollary of Theorem 16.10.\\nCorollary 16.11 If V is a finite dimensional C-vector space, then every f ∈ L(V, V) has a Jordan canonical form.\\nThe following uniqueness result justifies the name canonical form.\\nTheorem 16.12 Let V be a finite dimensional K -vector space.\\nIf f ∈ L(V, V) has a Jordan canonical form, then it is unique up to the order of the Jordan blocks on the diagonal.\\nProof Let dim(V) = n and let B1 , B2 be two bases of V with\\n⎡\\nA1 = [ f ] B1 ,B1 = ⎣ as well as\\n\\nJd1 (λ1 )\\n\\n⎡\\nA2 = [ f ] B2 ,B2 = ⎣\\n\\nJc1 (μ1 )\\n\\n⎤\\n...\\n\\n⎦ ∈ K n,n ,\\nJdm (λm )\\n⎤\\n\\n...\\n\\n⎦ ∈ K n,n .\\nJck (μk )\\n\\nFor a given eigenvalue λ j , 1 ≤ j ≤ m, we define\\n\u0014\\n\u0013 rs(1) (λ j ) := rank (A1 − λ j In )s , s = 0, 1, 2, . . . .\\nThen\\n\\n(1)\\n(λ j ) − rs(1) (λ j ), s = 1, 2, . . . , ds(1) (λ j ) := rs−1 is equal to the number of Jordan blocks J\u0002 (λ j ) ∈ K \u0002,\u0002 on the diagonal of A1 with\\n\u0002 ≥ s.\\nThe number of Jordan blocks corresponding to the eigenvalue λ j with exact size s therefore is given by\\n(1)\\n(1)\\n(1)\\n(λ j ) = rs−1\\n(λ j ) − 2rs(1) (λ j ) + rs+1\\n(λ j ) ds(1) (λ j ) − ds+1\\n\\n2 Marie\\n\\n(16.6)\\n\\nEnnemond Camille Jordan (1838–1922) derived this form 1870.\\nTwo years earlier, Karl\\nWeierstraß (1815–1897) proved a result that implies the Jordan canonical form.</td>\n",
       "      <td>239.0</td>\n",
       "      <td>239</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.2 The Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic subspace duality jordan canonical form jordan block corresponding λ j g f λ j equal number jordan block corresponding λ j matrix representation form called jordan canonical f theorem know f l v v diagonalizable p f decomposes linear factor k g f λ j f λ j hold every eigenvalue λ j f p f decomposes linear factor jordan canonical form show g f λ j f λ j every jordan block corresponding λ j size fundamental theorem algebra yield following corollary theorem corollary v finite dimensional c-vector space every f l v v jordan canonical form following uniqueness result justifies name canonical form theorem let v finite dimensional k space f l v v jordan canonical form unique order jordan block diagonal proof let dim v n let two base v f well f k n n jdm λm k n n jck μk given eigenvalue λ j j define r λ j rank λ j λ j r λ j d λ j equal number jordan block λ j k diagonal number jordan block corresponding eigenvalue λ j exact size therefore given λ j λ j λ j λ j d λ j marie ennemond camille jordan derived form two year earlier karl weierstraß proved result implies jordan canonical form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>16.2 The Jordan Canonical Form\\n\\n239\\n\\n(cp.\\nExample 16.13).\\nThe matrices A1 and A2 are similar and, therefore, have the same eigenvalues, i.e.,\\n{λ1 , . . . , λm } = {μ1 , . . . , μk }.\\nFurthermore, rank\\n\\n\u0013\u0013\\n\\nA1 − αIn\\n\\n\u0014m \u0014\\n\\n= rank\\n\\n\u0013\u0013\\n\\nA2 − αIn\\n\\n\u0014m \u0014 for all α ∈ K and m ∈ N0 .\\nIn particular, for every λ j there exists μi ∈ {μ1 , . . . , μk } with μi = λ j and for this μi and the matrix A2 we get rs(2) (μi ) := rank\\n\\n\u0013\u0013\\n\\nA2 − μi In\\n\\n\u0014s \u0014\\n\\n= rs(1) (λ j ), s = 0, 1, 2, . . . .\\n\\nNow (16.6) shows that the matrix A2 has, up to reordering, the same Jordan blocks\\n\b on the diagonal as the matrix A1 .\\nExample 16.13 This example illustrates the construction in the proof of Theorem 16.12.\\nIf\\n⎤\\n⎡\\n11\\n⎡\\n⎤\\n⎥\\n⎢ 1\\nJ2 (1)\\n⎥\\n⎢\\n⎥ ∈ R5,5 ,\\n⎢\\n⎣\\n⎦\\n1\\nJ1 (1)\\nA=\\n(16.7)\\n=⎢\\n⎥\\n⎣\\nJ2 (0)\\n0 1⎦\\n0 then (A − 1 · I5 )0 = I5 ,\\n⎡\\n\\n01\\n⎢ 0\\n⎢\\n0\\nA − 1 · I5 = ⎢\\n⎢\\n⎣\\n−1\\n\\n⎤\\n\\n⎡\\n\\n⎤\\n00\\n⎢ 0\\n⎥\\n⎥\\n⎢\\n⎥\\n⎥\\n⎥ , (A − 1 · I5 )2 = ⎢\\n⎥,\\n0\\n⎢\\n⎥\\n⎥\\n⎣\\n⎦\\n1\\n1 −2 ⎦\\n−1\\n1 and we get r0 (1) = 5, r1 (1) = 3, rs (1) = 2, s ≥ 2, d1 (1) = 2, d2 (1) = 1, ds (1) = 0, s ≥ 3, d1 (1) − d2 (1) = 1, d2 (1) − d3 (1) = 1, ds (1) − ds+1 (1) = 0, s ≥ 3.</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.2 The Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jordan canonical form cp example matrix similar therefore eigenvalue λm μk furthermore rank αin rank αin α k particular every λ j exists μi μk μi λ j μi matrix get r μi rank μi r λ j show matrix reordering jordan block diagonal matrix example example illustrates construction proof theorem get r d d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>240\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\nWe now consider the powers of a Jordan block Jd (λ) ∈ K d,d .\\nSince Id and Jd (0) commute,\\n\u0017 \u0018 k\\nλk− j (Jd (0)) j = j j=0 k\\n\\n(Jd (λ))k = (λId + Jd (0))k = k j=0 p ( j) (λ)\\n(Jd (0)) j , j!\\nfor every k ∈ N0 , where p ( j) is the jth derivative of the polynomial p = t k with respect to t, p (0) = (t k )(0) = t k , p ( j) = (t k )( j) = k(k −1)·. . .·(k − j +1) t k− j , j = 1, . . . , k.\\n\\nWe can now easily show the following result.\\nLemma 16.14 If p ∈ K [t] is a polynomial of degree k ≥ 0, then k p (Jd (λ)) = j=0 p ( j) (λ)\\n(Jd (0)) j .\\nj!\\n\\n(16.8)\\n\b\\n\\nProof Exercise.\\n\\nConsidered as a linear map from K d,1 to K d,1 , the matrix Jd (0) represents an\\n“upshift”, since\\n⎤ ⎡ ⎤\\nα2\\nα1\\n⎢α2 ⎥ ⎢ ... ⎥\\n⎥ ⎢ ⎥\\nJd (0) ⎢\\n⎣ ... ⎦ = ⎣α ⎦ for all d\\n⎡\\n\\nαd\\n\\nClearly,\\n\\n0\\n\\n⎡\\n\\n⎤\\nα1\\n⎢α2 ⎥\\n⎢ . ⎥ ∈ K d,1 .\\n⎣ .. ⎦\\nαd\\n\\n(Jd (0))\u0002 \u0004= 0, \u0002 = 0, 1, . . . , d − 1, (Jd (0))d = 0, and hence the linear map Jd (0) is nilpotent of index d.\\nThe sum on the right hand side of (16.8) therefore has at most d terms, even when deg( p) &gt; d.\\nMoreover, the right hand side of (16.8) shows that p (Jd (λ)) is an upper triangular matrix with constant entries on its diagonals.\\nA matrix with constant diagonals is called a Toeplitz matrix.3 In particular, on the main diagonal we have the entry p(λ).\\nFrom (16.8) we see that p(Jd (λ)) = 0 holds if and only if p(λ) = p \u0010 (λ) = · · · = p (d−1) (λ) = 0.\\nThus we have shown the following result.\\n\\n3 Otto\\n\\nToeplitz (1881–1940).</td>\n",
       "      <td>241.0</td>\n",
       "      <td>241</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.2 The Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic subspace duality jordan canonical form consider power jordan block jd λ k since id jd commute k j jd j j k jd λ k λid jd k k p j λ jd j j every k p j jth derivative polynomial p k respect p k k p j k j k k k j j j easily show following result lemma p k polynomial degree k k p jd λ p j λ jd j j proof exercise considered linear map k k matrix jd represents upshift since jd αd clearly k αd jd jd hence linear map jd nilpotent index sum right hand side therefore term even deg p moreover right hand side show p jd λ upper triangular matrix constant entry diagonal matrix constant diagonal called toeplitz particular main diagonal entry p λ see p jd λ hold p λ p λ p λ thus shown following result otto toeplitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>16.2 The Jordan Canonical Form\\n\\n241\\n\\nLemma 16.15 Let p ∈ K [t] be a polynomial and Jd (λ) ∈ K d,d be a Jordan block.\\n(1) The matrix p(Jd (λ)) is invertible if and only if λ is not a root of p.\\n(2) We have p(Jd (λ)) = 0 ∈ K d,d if and only if λ is a d-fold root of p, i.e., if the linear factor (t − λ)d is a divisor of p.\\nLet V be a finite dimensional K -vector space and let f ∈ L(V, V), where we do not assume that P f decomposes into linear factors.\\nFrom the Cayley-Hamilton theorem (Theorem 8.6) we know that P f ( f ) = 0 ∈ L(V, V), i.e., there exists a monic polynomial of degree at most dim(V), which annihilates the endomorphism f .\\nLet p1 , p2 ∈ K [t] be two monic polynomials of smallest possible degree with p1 ( f ) = p2 ( f ) = 0.\\nThen ( p1 − p2 )( f ) = 0, and since p1 and p2 are monic, p1 − p2 ∈ K [t] is a polynomial with deg( p1 − p2 ) &lt; deg( p1 ) = deg( p2 ).\\nThe minimality assumption on deg( p1 ) and deg( p2 ) implies that p1 − p2 = 0, i.e., p1 = p2 .\\nThus, for every f ∈ L(V, V) there exists a uniquely determined monic polynomial of minimal degree which annihilates f .\\nThis justifies the following definition.\\nDefinition 16.16 If V is finite dimensional K -vector space and f ∈ L(V, V), then the uniquely determined monic polynomial of minimal degree that annihilates f is called the minimal polynomial of f .\\nWe denote this polynomial by M f .\\nBy construction we always have deg(M f ) ≤ deg(P f ) = dim(V).\\nLemma 16.17 If V is a finite dimensional K -vector space and f ∈ L(V, V), then the minimal polynomial M f divides every polynomial that annihilates f and is, in particular, a divisor of the characteristic polynomial P f .\\nProof For p = 0 we have p( f ) = 0 and M f divides p.\\nIf p ∈ K [t] \\ {0} is a polynomial with p( f ) = 0, then deg(M f ) ≤ deg( p).\\nUsing division with remainder\\n(cp.\\nTheorem 15.4), there exist uniquely determined polynomials q, r ∈ K [t] with p = q · M f + r and deg(r ) &lt; deg(M f ).\\nThus,\\n0 = p( f ) = q( f )M f ( f ) + r ( f ) = r ( f ).\\nThe minimality of deg(M f ) implies that r = 0, and hence M f divides p.\\n\\n\b\\n\\nIf P f decomposes into linear factors, then we can explicitly construct M f using the Jordan canonical form of f .\\nLemma 16.18 Let V be a finite dimensional K -vector space.\\nIf f ∈ L(V, V) has a\\nJordan canonical form with pairwise distinct eigenvalues \u0012\\nλ1 , . . . , \u0012\\nλk and if d\u00121 , . . . , d\u0012k are the respective maximal sizes of the corresponding Jordan blocks, then\\nMf = k\\n\u0019 j=1\\n\\n\u0012\\n\\n(t − \u0012\\nλ j )d j .</td>\n",
       "      <td>242.0</td>\n",
       "      <td>242</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.2 The Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jordan canonical form lemma let p k polynomial jd λ k jordan block matrix p jd λ invertible λ root p jd λ k λ d-fold root p linear factor λ divisor let v finite dimensional k space let f l v v assume p f decomposes linear factor cayley-hamilton theorem theorem know p f f l v v exists monic polynomial degree dim v annihilates endomorphism f let k two monic polynomial smallest possible degree f f f since monic k polynomial deg deg deg minimality assumption deg deg implies thus every f l v v exists uniquely determined monic polynomial minimal degree annihilates f justifies following definition definition v finite dimensional k space f l v v uniquely determined monic polynomial minimal degree annihilates f called minimal polynomial f denote polynomial f construction always deg f deg p f dim v lemma v finite dimensional k space f l v v minimal polynomial f divide every polynomial annihilates f particular divisor characteristic polynomial p f proof p p f f divide p k polynomial p f deg f deg p using division remainder cp theorem exist uniquely determined polynomial q r k p q f r deg r deg f thus p f q f f f r f r f minimality deg f implies r hence f divide p f decomposes linear factor explicitly construct f using jordan canonical form f lemma let v finite dimensional k space f l v v jordan canonical form pairwise distinct eigenvalue λk respective maximal size corresponding jordan block mf k λ j j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>242\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\nProof We know from Lemma 16.17 that M f is a divisor of P f .\\nTherefore,\\nMf = k\\n\u0019\\n\\n(t − \u0012\\nλ j )\u0002 j j=1 for some exponents \u00021 , . . . , \u0002k .\\nIf\\n⎡\\nA=⎣\\n\\nJd1 (λ1 )\\n\\n⎤\\n...\\n\\n⎦\\nJdm (λm ) is a Jordan canonical form of f , then M f ( f ) = 0 ∈ L(V, V) is equivalent to\\nM f (A) = 0 ∈ K n,n , where n = dim(V).\\nWe have M f (A) = 0 if and only if\\nM f (Jd j (λ j )) = 0 for j = 1, . . . , m.\\nFor this it is necessary and sufficient that\\nM f (Jd\u0012j (\u0012\\nλ j )) = 0 for j = 1, . . . , k.\\nBy Lemma 16.15 this holds if and only if every\\n\u0012 of the linear factors (t − \u0012\\nλ j )d j , j = 1, . . . , k, is a divisor of M f .\\nTherefore, M f has the desired form.\\n\b\\nExample 16.19 If f is an endomorphism with the Jordan canonical form A in (16.7), then\\nP f = (t − 1)3 t 2 , M f = (t − 1)2 t 2 and\\n\\n⎡\\n\\n⎤\\n00\\n⎢ 0\\n⎥\\n⎢\\n⎥\\n2 2\\n⎢\\n⎥\\n0\\nM f (A) = (A − 1 · I5 ) A = ⎢\\n⎥\\n⎣\\n1 −2 ⎦\\n1\\n\\n⎡\\n\\n12\\n⎢ 1\\n⎢\\n⎢\\n1\\n⎢\\n⎣\\n0\\n\\n⎤\\n⎥\\n⎥\\n⎥,\\n⎥\\n0⎦\\n0 which shows that M f (A) = 0 ∈ R5,5 and M f ( f ) = 0 ∈ L(V, V).\\nThe Jordan canonical form is of great importance in theoretical Linear Algebra.\\nIn practical applications, however, where usually matrices over K = R or K = C are considered, it is not so relevant, since there is no numerically stable method for computing the Jordan canonical form of a general matrix in finite precision arithmetic.\\nThe reason for the lack of such a method is that the entries of the Jordan canonical form do not depend continuously on the entries of the given matrix.\\nExample 16.20 Consider the matrix\\n\b\\nε1\\n, ε ∈ R.\\nA(ε) =\\n00\\n\u0007</td>\n",
       "      <td>243.0</td>\n",
       "      <td>243</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.2 The Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic subspace duality jordan canonical form proof know lemma f divisor p f therefore mf k λ j j exponent jdm λm jordan canonical form f f f l v v equivalent f k n n n dim v f f jd j λ j j necessary sufficient f λ j j lemma hold every linear factor λ j j j k divisor f therefore f desired form example f endomorphism jordan canonical form p f f f show f f f l v v jordan canonical form great importance theoretical linear algebra practical application however usually matrix k r k c considered relevant since numerically stable method computing jordan canonical form general matrix finite precision arithmetic reason lack method entry jordan canonical form depend continuously entry given matrix example consider matrix ε ε</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>16.2 The Jordan Canonical Form\\n\\n243\\n\\nFor every given ε \u0004= 0, the matrix A(ε) has the two distinct eigenvalues ε and 0, and hence the diagonal matrix\\n\u0007 \b\\nε0\\nJ (ε) =\\n00 is a Jordan canonical form of A(ε).\\nHowever, for ε → 0, we obtain\\nA(ε) →\\n\\n\u0007 \b\\n01\\n,\\n00\\n\\nJ (ε) →\\n\\n\u0007 \b\\n00\\n.\\n00\\n\\nThus, J (ε) does not converge to the Jordan canonical form of A(0) for ε → 0.\\nA similar example is given by the matrices in Exercise 8.5: While A(0) is a\\nJordan block of size n corresponding to the eigenvalue 1, for every ε \u0004= 0 we obtain a diagonalizable matrix A(ε) ∈ Cn,n with n pairwise distinct eigenvalues.\\n\\nMATLAB-Minute.\\nLet\\nA = T −1\\n\\n\u0007\\n\\n\b\\n10\\nT ∈ C2,2 ,\\n11 where T ∈ C2,2 is a random matrix constructed with the command T= rand(2).\\nConstruct several such matrices and always compute the eigenvalues using the command eig(A).\\nDisplay the eigenvalues in format long.\\nOne observes that the two eigenvalues are real or complex conjugates, and that they always have an error starting from the 8th digit after the decimal point, i.e., an error on the order of 10−8 .\\nThis does not happen by chance, but is due to the behavior of the eigenvalues under perturbations, which arise from rounding errors in the computer.\\n\\n16.3 Computation of the Jordan Canonical Form\\nWe now derive a method for the computation of the Jordan canonical form of an endomorphism f on a finite dimensional K -vector space V. We assume that P f decomposes into linear factors over K , and that the roots of P f , i.e., the eigenvalues of f , are known.\\nThe construction follows the important steps in the existence proof of the Jordan canonical form in Sect.\\n16.2.\\nSuppose that λ is an eigenvalue of f and that f has a corresponding Jordan block of size s.\\nThen there exist s linearly independent vectors t1 , . . . , ts with [ f ] \u001a\\nB, \u001a\\nB = Js (λ)</td>\n",
       "      <td>244.0</td>\n",
       "      <td>244</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.3 Computation of the Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jordan canonical form every given ε matrix ε two distinct eigenvalue ε hence diagonal matrix j ε jordan canonical form ε however ε obtain ε j ε thus j ε converge jordan canonical form ε similar example given matrix exercise jordan block size n corresponding eigenvalue every ε obtain diagonalizable matrix ε cn n n pairwise distinct eigenvalue matlab-minute let random matrix constructed command rand construct several matrix always compute eigenvalue using command eig display eigenvalue format long one observes two eigenvalue real complex conjugate always error starting digit decimal point error order happen chance due behavior eigenvalue perturbation arise rounding error computer computation jordan canonical form derive method computation jordan canonical form endomorphism f finite dimensional k space assume p f decomposes linear factor k root p f eigenvalue f known construction follows important step existence proof jordan canonical form sect suppose λ eigenvalue f f corresponding jordan block size exist linearly independent vector t f b b j λ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>244\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form for \u001a\\nB = {t1 , . . . , ts }.\\nWith t0 := 0 and writing Id instead of IdV for simplicity of notation, we then have\\n( f − λId)(t1 ) = t0 ,\\n( f − λId)(t2 ) = t1 ,\\n..\\n.\\n( f − λId)(ts ) = ts−1 , hence ts− j = ( f − λId) j (ts ) for j = 0, 1, . . . , s.\\nThe vectors ts , ts−1 , . . . , t1 form a sequence as the one we have constructed in the context of the Krylov subspaces, and span{ts , ts−1 , . . . , t1 } = Ks ( f − λId, ts ).\\nThe reverse sequence t1 , t 2 , . . . , t s is called a Jordan chain of f corresponding to the eigenvalue λ.\\nThe vector t1 is an eigenvector of f corresponding to λ.\\nFor the vector t2 we then have ( f −λId)(t2 ) \u0004= 0 and\\n( f − λId)2 (t2 ) = ( f − λId)(t1 ) = 0.\\nHence t2 ∈ ker(( f − λId)2 ) \\ ker( f − λId), and in general t j ∈ ker(( f − λId) j ) \\ ker(( f − λId) j−1 ), j = 1, . . . , s.\\n\\nThis motivates the following definition.\\nDefinition 16.21 Let V be a finite dimensional K -vector space, let f ∈ L(V, V) have the eigenvalue λ ∈ K , and let k ∈ N. A vector v ∈ V with v ∈ ker(( f − λId)k ) \\ ker(( f − λId)k−1 ) is called a principal vector of level k of f corresponding to the eigenvalue λ.\\nPrincipal vectors of level one are eigenvectors.\\nPrincipal vectors of higher levels can be considered generalizations of eigenvectors, and they are therefore sometimes called generalized eigenvectors.\\nFor the computation of the Jordan canonical form of f , we thus need to know the number and lengths of the Jordan chains corresponding to the different eigenvalues of f .\\nThese correspond to the number and sizes of the Jordan blocks of f .\\nIf F is a matrix representation of f with respect to an arbitrary basis, then (cp. the proof of\\nTheorem 16.12)</td>\n",
       "      <td>245.0</td>\n",
       "      <td>245</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.3 Computation of the Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic subspace duality jordan canonical form b t writing id instead idv simplicity notation f λid f λid f λid t hence j f λid j t j vector t form sequence one constructed context krylov subspace span t k f λid t reverse sequence called jordan chain f corresponding eigenvalue λ vector eigenvector f corresponding λ vector f f λid f λid hence ker f λid ker f λid general j ker f λid j ker f λid j motivates following definition definition let v finite dimensional k space let f l v v eigenvalue λ k let k vector v v v ker f λid k ker f λid called principal vector level k f corresponding eigenvalue λ principal vector level one eigenvectors principal vector higher level considered generalization eigenvectors therefore sometimes called generalized eigenvectors computation jordan canonical form f thus need know number length jordan chain corresponding different eigenvalue f correspond number size jordan block f f matrix representation f respect arbitrary basis cp proof theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>16.3 Computation of the Jordan Canonical Form\\n\\n245 ds (λ) := rank((F − λI )s−1 ) − rank((F − λI )s )\\n= dim(im(( f − λId)s−1 )) − dim(im(( f − λId)s ))\\n= dim(V) − dim(ker(( f − λId)s−1 )) − (dim(V) − dim(ker(( f − λId)s )))\\n= dim(ker(( f − λId)s )) − dim(ker(( f − λId)s−1 )) is the number of Jordan blocks corresponding to λ of size at least s.\\nThis implies, in particular, that ds (λ) ≥ ds+1 (λ) ≥ 0, s = 1, 2, . . . , and ds (λ) − ds+1 (λ) is the number of Jordan blocks of exact size s corresponding to λ.\\nThere exists a smallest number m ∈ N with\\n{0} = ker(( f − λId)0 ) ⊂ ker(( f − λId)1 ) ⊂ · · · ⊂ ker(( f − λId)m ) = ker(( f − λId)m+1 ).\\n\\nHence ds (λ) = 0 for all s ≥ m + 1, so that there is no Jordan block corresponding to λ of size m + 1 or larger.\\nIn order to compute the Jordan canonical form, we therefore proceed as follows:\\n(1) Determine the eigenvalues of f .\\n(2) For every eigenvalue λ of f carry out the following steps:\\n(a) Determine the smallest number m ∈ N with ker(( f −λId)0 ) ⊂ ker(( f −λId)1 ) ⊂ · · · ⊂ ker(( f −λId)m ) = ker(( f −λId)m+1 ).\\n\\nThen dim(ker(( f − λId)m )) = a(λ, f ).\\n(b) For s = 1, . . . , m determine ds (λ) = dim(ker(( f − λId)s )) − dim(ker(( f − λId)s−1 )) &gt; 0.\\nIf s ≥ m + 1, then ds (λ) = 0, and d1 (λ) = dim(ker( f − λId)) = g(λ, f ) is the number of Jordan blocks corresponding to λ.\\n(c) To simplify notation, we write ds := ds (λ) and determine the Jordan chains as follows:\\n(i) Since dm − dm+1 = dm , there exist dm Jordan blocks of size m.\\nFor each of these blocks we determine a Jordan chain of dm principal vectors of level m, i.e., vectors t1,m , t2,m , . . . , tdm ,m ∈ ker(( f − λId)m ) \\ ker(( f − λId)m−1 )</td>\n",
       "      <td>246.0</td>\n",
       "      <td>246</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.3 Computation of the Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>computation jordan canonical form d λ rank f λi rank f λi dim im f λid dim im f λid dim v dim ker f λid dim v dim ker f λid dim ker f λid dim ker f λid number jordan block corresponding λ size least implies particular d λ λ d λ λ number jordan block exact size corresponding λ exists smallest number n ker f λid ker f λid ker f λid ker f λid hence d λ jordan block corresponding λ size larger order compute jordan canonical form therefore proceed follows determine eigenvalue f every eigenvalue λ f carry following step determine smallest number n ker f ker f ker f ker f dim ker f λid λ f b determine d λ dim ker f λid dim ker f λid d λ λ dim ker f λid g λ f number jordan block corresponding λ c simplify notation write d d λ determine jordan chain follows since dm dm exist dm jordan block size block determine jordan chain dm principal vector level vector tdm ker f λid ker f λid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>246\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form with the following property: dm\\n\u001b\\nIf α1 , . . . , αdm ∈ K with\\nαi ti,m ∈ ker(( f − λId)m−1 ), then α1 = i=1\\n\\n· · · = αdm = 0.\\nHere the first index in ti, j indicates the number of the chain, and the second indicates the level of the principal vector (from ker(( f − λId) j ) and not ker(( f − λId) j−1 )).\\n(ii) For j = m, m − 1, . . . , 2 we proceed as follows:\\nWhen we have determined d j principal vectors of level j, say t1, j , t2, j ,\\n. . . , td j , j , we apply f − λId to each of these vectors, hence ti, j−1 := ( f − λId)(ti, j ), 1 ≤ i ≤ d j , in order to determine the principal vectors of level j − 1.\\ndj\\n\u001b\\nαi ti, j−1 ∈ ker(( f − λId) j−2 ), then\\nIf α1 , . . . , αd j ∈ K with i=1\\n\\n⎛ dj\\n\\n0 = ( f − λId) j−2 ⎝ dj\\n\u001b\\n\\n⎛\\n\\n⎞ dj\\n\\nαi ti, j−1 ⎠ = ( f − λId) j−1 ⎝ i=1 and thus\\n\\n⎞\\n\\nαi ti, j ⎠ , i=1\\n\\nαi ti, j ∈ ker(( f − λId) j−1 ) giving α1 = · · · = αd j = 0.\\ni=1 d j−1 &gt; d j , then there exist d j − d j−1 Jordan blocks of size j − 1.\\nFor\\nIf these we need the Jordan chains of length j − 1.\\nThus we extend the already computed t1, j−1 , t2, j−1 , . . . , td j , j−1 ∈ ker(( f − λId) j−1 ) \\ ker(( f − λId) j−2 ) to d j−1 principal vectors of level ( j − 1) (but only if d j−1 &gt; d j ) via t1, j−1 , t2, j−1 , . . . , td j−1 , j−1 ∈ ker(( f − λId) j−1 ) \\ ker(( f − λId) j−2 ), where the following must hold: If α1 , . . . , αd j−1 ∈ K with d\u001b j−1\\n\\nαi ti, j−1 ∈ i=1 ker(( f − λId) j−2 ), then α1 = · · · = αd j−1 = 0.\\nAfter completing the step for j = 2, we have obtained (linearly independent) vectors t1,1 , t2,1 , . . . , td1 ,1 ∈ ker( f − λId).\\nSince dim(ker( f − λId)) = d1 , we have found a basis of ker( f − λId).\\nIn this way we have determined d1 different Jordan chains that we combine as follows:\\nTλ := t1,1 , t1,2 , . . . , t1,m ; t2,1 , t2,2 , . . . , t2,∗ ; . . . ; td1 ,1 , . . . , td1 ,∗ .</td>\n",
       "      <td>247.0</td>\n",
       "      <td>247</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.3 Computation of the Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic subspace duality jordan canonical form following property dm αdm k αi ti ker f λid αdm first index ti j indicates number chain second indicates level principal vector ker f λid j ker f λid ii j proceed follows determined j principal vector level j say j j td j j apply f λid vector hence ti f λid ti j j order determine principal vector level j dj αi ti ker f λid αd j k dj f λid dj dj αi ti f λid thus αi ti j αi ti j ker f λid giving αd j j exist j jordan block size j need jordan chain length j thus extend already computed td j ker f λid ker f λid principal vector level j j via td ker f λid ker f λid following must hold αd k αi ti ker f λid αd completing step j obtained linearly independent vector ker f λid since dim ker f λid found basis ker f λid way determined different jordan chain combine follows tλ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>16.3 Computation of the Jordan Canonical Form\\n\\n247\\n\\nEach chain begins with an eigenvector, followed by principal vectors of increasing levels.\\nHere we use the convention that the chains are ordered decreasingly according to their length.\\n(3) Jordan chains are linearly independent, if their first vectors (the eigenvectors) are linearly independent.\\n(Show this as an exercise.)\\nThus, if λ1 , . . . , λ\u0002 are the pairwise distinct eigenvalues of f , then\\nT = Tλ1 , . . . , Tλ\u0002 is a basis, for which [ f ]T,T is in Jordan canonical form.\\nExample 16.22 We interpret the matrix\\n⎡\\n\\n50\\n⎢ 01\\n⎢\\nF =⎢\\n⎢ −1 0\\n⎣ 00\\n00\\n\\n10\\n00\\n30\\n01\\n00\\n\\n⎤\\n0\\n0⎥\\n⎥\\n5,5\\n0⎥\\n⎥∈R\\n⎦\\n0\\n4 as endomorphism on R5,1 .\\n(1) The eigenvalues of F are the roots of PF = (t − 1)2 (t − 4)3 .\\nIn particular PF decomposes into linear factors and F has a Jordan canonical form.\\n(2) We now consider the different eigenvalues of F:\\n(a) For the eigenvalue λ1 = 1 we obtain\\n⎛⎡\\n\\n4\\n⎜⎢ 0\\n⎜⎢\\n⎢ ker(F − I ) = ker ⎜\\n⎜⎢ −1\\n⎝⎣ 0\\n0\\n\\n0\\n0\\n0\\n0\\n0\\n\\n10\\n00\\n20\\n00\\n00\\n\\n⎤⎞\\n0\\n⎟\\n0⎥\\n⎥⎟\\n⎥\\n0 ⎥⎟\\n⎟ = span{e2 , e4 }.\\n0 ⎦⎠\\n3\\n\\nHere dim(ker(F − I )) = 2 = a(1, F).\\nFor the eigenvalue λ2 = 4 we obtain\\n⎤⎞\\n1 0 1 00\\n⎜⎢ 0 −3 0 0 0 ⎥⎟\\n⎥⎟\\n⎜⎢\\n⎢\\n⎥⎟ ker(F − 4 I ) = ker ⎜\\n⎜⎢ −1 0 −1 0 0 ⎥⎟ = span{e1 − e3 , e5 },\\n⎝⎣ 0 0 0 −3 0 ⎦⎠\\n0 0 0 00\\n⎛⎡</td>\n",
       "      <td>248.0</td>\n",
       "      <td>248</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.3 Computation of the Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>computation jordan canonical form chain begin eigenvector followed principal vector increasing level use convention chain ordered decreasingly according length jordan chain linearly independent first vector eigenvectors linearly independent show exercise thus pairwise distinct eigenvalue f basis f jordan canonical form example interpret matrix f endomorphism eigenvalue f root pf particular pf decomposes linear factor f jordan canonical form consider different eigenvalue f eigenvalue obtain ker f ker span dim ker f f eigenvalue obtain ker f ker span</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>248\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\n⎛⎡\\n\\n0\\n⎜⎢ 0\\n⎜⎢\\n⎢ ker((F − 4 I )2 ) = ker ⎜\\n⎜⎢ 0\\n⎝⎣ 0\\n0\\n\\n00\\n90\\n00\\n00\\n00\\n\\n0\\n0\\n0\\n9\\n0\\n\\n⎤⎞\\n0\\n⎟\\n0⎥\\n⎥⎟\\n⎥\\n0 ⎥⎟\\n⎟ = span{e1 , e3 , e5 }.\\n0 ⎦⎠\\n0\\n\\nHere dim(ker((F − 4 I )2 )) = 3 = a(4, F).\\n(b) For λ1 = 1 we have d1 (1) = dim(ker(F − I )) = 2.\\nFor λ2 = 4 we have d1 (4) = dim(ker(F − 4 I )) = 2 and d2 (4) = dim(ker((F − 4 I )2 )) − dim(ker(F − 4 I )) = 3 − 2 = 1.\\n(c) Computation of the Jordan chains:\\n• For λ1 = 1 we have m = 1.\\nAs principal vectors of level one we choose t1,1 = e2 and t2,1 = e4 .\\nThese form a basis of ker(F − I ): If α1 , α2 ∈ R with α1 e2 + α2 e4 = 0, then α1 = α2 = 0.\\nFor λ1 = 1 we are finished.\\n• For λ2 = 4 we have m = 2, and we choose a principal vector of level two, say t1,2 = e1 .\\nFor this vector we have: If α1 ∈ R with α1 e1 ∈ span{e1 − e3 , e5 }, then α1 = 0.\\nWe compute t1,1 := (F − 4 I )t1,2 = e1 − e3 .\\nSince d1 (4) = 2 &gt; 1 = d2 (4), we have to add to t1,1 another principal vector of level one, and we choose t2,1 = e5 .\\nSince the vectors are linearly independent, α1 t1,1 + α2 t2,1 ∈ ker((F − 4 I )0 ) = {0} implies that α1 =\\nα2 = 0.\\nIn this way we get\\n⎤\\n⎤\\n⎡\\n00\\n110\\n⎢1 0⎥\\n⎢ 0 0 0⎥\\n⎥\\n⎥\\n⎢\\n⎢\\n⎥\\n⎥\\n⎢\\nTλ1 = ⎢ 0 0 ⎥ and Tλ2 = ⎢\\n⎢ −1 0 0 ⎥ .\\n⎣0 1⎦\\n⎣ 0 0 0⎦\\n00\\n001\\n⎡\\n\\n(3) The coordinate transformation matrix is T = [Tλ1 Tλ2 ], and the Jordan canonical form of F is\\n⎤\\n⎡\\n⎤\\n⎡\\n1\\n01 000\\n⎢0 0 0 1 0⎥\\n⎥\\n⎢ 1\\n⎥\\n⎢\\n⎥\\n⎢\\n−1\\n−1\\n⎥\\n⎥\\n⎢\\n4 1 ⎥ = T F T, where T = ⎢\\n⎢ 0 0 −1 0 0 ⎥ .\\n⎢\\n⎣\\n⎦\\n⎣\\n1 0 1 0 0⎦\\n4\\n00 001\\n4</td>\n",
       "      <td>249.0</td>\n",
       "      <td>249</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.3 Computation of the Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic subspace duality jordan canonical form ker f ker span dim ker f f b dim ker f dim ker f dim ker f dim ker f c computation jordan chain principal vector level one choose form basis ker f r finished choose principal vector level two say vector r span compute f since add another principal vector level one choose since vector linearly independent ker f implies way get coordinate transformation matrix jordan canonical form f f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>16.3 Computation of the Jordan Canonical Form\\n\\n249\\n\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n16.1.\\nProve Lemma 16.1 (1).\\n16.2.\\nProve Lemma 16.6 (1).\\n16.3.\\nLet V be a K -vector space, f ∈ L(V, V) and λ ∈ K .\\nProve or disprove: A subspace U ⊆ V is f -invariant, if it is ( f − λIdV )-invariant.\\n16.4.\\nLet V be a finite dimensional K -vector space, f ∈ L(V, V), v ∈ V and\\nλ ∈ K .\\nShow that K j ( f, v) = K j ( f − λIdV , v) for all j ∈ N. Conclude that the grade of v with respect to f is equal to the grade of v with respect to f − λIdV .\\n16.5.\\nProve Lemma 16.14.\\n16.6.\\nLet V be a finite dimensional Euclidean or unitary vector space and let f ∈\\nL(V, V) be selfadjoint and nilpotent.\\nShow that then f = 0.\\n16.7.\\nLet V \u0004= {0} be a finite dimensional K -vector space, let f ∈ L(V, V) be nilpotent of index m and suppose that P f decomposes into linear factors.\\nShow the following assertions:\\n(a) P f = t n with n = dim(V).\\n(b) M f = t m .\\n(c) There exists a vector v ∈ V of grade m with respect f .\\n(d) For every λ ∈ K we have M f −λIdV = (t + λ)m .\\n16.8.\\nLet V be a finite dimensional K -vector space and f ∈ L(V, V).\\nShow the following assertions:\\n(a) ker( f j ) ⊆ ker( f j+1 ) for all j ≥ 0 and there exists an m ≥ 0 with ker( f m ) = ker( f m+1 ).\\nFor this m we have ker( f m ) = ker( f m+ j ) for all j ≥ 1.\\n(b) im( f j ) ⊇ im( f j+1 ) for all j ≥ 0 and there exists an \u0002 ≥ 0 with im( f \u0002 ) = im( f \u0002+1 ).\\nFor this \u0002 we have im( f \u0002 ) = im( f \u0002+ j ) for all j ≥ 1.\\n(c) If m, \u0002 ≥ 0 are minimal with ker( f m ) = ker( f m+1 ) and im( f \u0002 ) = im( f \u0002+1 ), then m = \u0002.\\n(Theorem 16.5 now implies that V = ker( f m ) ⊕ im( f m ) is a decomposition of V into f -invariant subspaces.)\\n16.9.\\nLet V be a finite dimensional K -vector space and let f ∈ L(V, V) be a projection (cp.\\nExercise 13.10).\\nShow the following assertions:\\n(a) v ∈ im( f ) implies that f (v) = v.\\n(b) V = im( f ) ⊕ ker( f ).\\n(c) There exists a basis B of V with\\n\u0007\\n[ f ] B,B =\\n\\n\b\\n\\nIk\\n0n−k\\n\\n, where k = dim(im( f )) and n = dim(V).\\nIn particular, P f = (t −1)k t n−k and λ ∈ {0, 1} for every eigenvalue λ of f .</td>\n",
       "      <td>250.0</td>\n",
       "      <td>250</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.3 Computation of the Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>computation jordan canonical form exercise following exercise k arbitrary field prove lemma prove lemma let v k space f l v v λ k prove disprove subspace u v f f λidv let v finite dimensional k space f l v v v v λ k show k j f v k j f λidv v j conclude grade v respect f equal grade v respect f λidv prove lemma let v finite dimensional euclidean unitary vector space let f l v v selfadjoint nilpotent show f let v finite dimensional k space let f l v v nilpotent index suppose p f decomposes linear factor show following assertion p f n n dim v b f c exists vector v v grade respect f every λ k f λ let v finite dimensional k space f l v v show following assertion ker f j ker f j exists ker f ker f ker f ker f j j b im f j im f j exists im f im f im f im f j j c minimal ker f ker f im f im f theorem implies v ker f im f decomposition v f subspace let v finite dimensional k space let f l v v projection cp exercise show following assertion v im f implies f v b v im f ker f c exists basis b v f b b ik k dim im f n dim v particular p f k λ every eigenvalue λ f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>250\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\n(d) The map g = IdV − f is a projection with ker(g) = im( f ) and im(g) = ker( f ).\\n16.10.\\nLet V be a finite dimensional K -vector space and let U, W ⊆ V be two subspaces with V = U ⊕ W. Show that there exists a uniquely determined projection f ∈ L(V, V) with im( f ) = U and ker( f ) = W.\\n16.11.\\nDetermine the Jordan canonical form of the matrices\\n⎡\\n⎤\\n⎡\\n⎤\\n2 10 00\\n1 −1 0 0\\n⎢ −1 1 1 0 0 ⎥\\n⎢\\n⎥\\n⎢ 1 −1 0 0 ⎥\\n4,4\\n5,5\\n⎥\\n⎢\\n⎥\\nA=⎣\\n∈R , B=⎢\\n⎢ −1 0 3 0 0 ⎥ ∈ R\\n3 0 3 −3 ⎦\\n⎣ −1 −1 0 1 1 ⎦\\n4 −1 3 −3\\n−2 −1 1 −1 3 using the method presented in Sect.\\n16.3.\\nDetermine also the minimal polynomial.\\n16.12.\\nDetermine the Jordan canonical form and the minimal polynomial of the linear map f : C≤3 [t] → C≤3 [t], α0 + α1 t + α2 t 2 + α3 t 3 → α1 + α2 t + α3 t 3 .\\n16.13.\\nDetermine (up to the order of blocks) all matrices J in Jordan canonical form with PJ = (t + 1)3 (t − 1)3 and M J = (t + 1)2 (t − 1)2 .\\n16.14.\\nLet V \u0004= {0} be a finite dimensional K -vector space, f ∈ L(V, V), and suppose that P f decomposes into linear factors.\\nShow the following assertions:\\n(a) P f = M f holds if and only if g(λ, f ) = 1 for all eigenvalues λ of f .\\n(b) f is diagonalizable if and only if M f has only simple roots, i.e., roots with multiplicity one.\\n(c) A root of λ ∈ K of M f is simple if and only if ker( f − λIdV ) = ker(( f − λIdV )2 ).\\n16.15.\\nLet V be a K -vector space of dimension 2 or 3 and let f ∈ L(V, V) with P f decomposing into linear factors.\\nShow that the Jordan canonical form of f is uniquely determined by P f and M f .\\nWhy does this not hold any longer if dim(V) ≥ 4?\\n16.16.\\nLet A ∈ K n,n be a matrix for which the characteristic polynomial decomposes into linear factors.\\nShow that there exists a diagonalizable matrix D and a nilpotent matrix N with A = D + N and D N = N D.\\n16.17.\\nLet A ∈ K n,n be a matrix that has a Jordan canonical form.\\nWe define\\n⎤\\n⎡\\nλ\\n⎤\\n⎡\\n1\\n⎢\\n#\\n\"\\n. . .\\n1⎥\\n⎥\\n⎢\\nInR := δi,n+1− j = ⎣ . . . ⎦ , JnR (λ) := ⎢\\n⎥ ∈ K n,n .\\n⎣ ... ... ⎦\\n1\\nλ 1</td>\n",
       "      <td>251.0</td>\n",
       "      <td>251</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.3 Computation of the Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cyclic subspace duality jordan canonical form map g idv f projection ker g im f im g ker f let v finite dimensional k space let u w v two subspace v u show exists uniquely determined projection f l v v im f u ker f determine jordan canonical form matrix r using method presented sect determine also minimal polynomial determine jordan canonical form minimal polynomial linear map f determine order block matrix j jordan canonical form pj j let v finite dimensional k space f l v v suppose p f decomposes linear factor show following assertion p f f hold g λ f eigenvalue λ f b f diagonalizable f simple root root multiplicity one c root λ k f simple ker f λidv ker f λidv let v k space dimension let f l v v p f decomposing linear factor show jordan canonical form f uniquely determined p f f hold longer dim v let k n n matrix characteristic polynomial decomposes linear factor show exists diagonalizable matrix nilpotent matrix n n n n let k n n matrix jordan canonical form define λ inr δi j jnr λ k n n λ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>16.3 Computation of the Jordan Canonical Form\\n\\nShow the following assertions:\\n(a)\\n(b)\\n(c)\\n(d)\\n\\nInR Jn (λ)InR = Jn (λ)T .\\nA and A T are similar.\\nJn (λ) = InR JnR (λ).\\nA can be written as a product of two symmetric matrices.\\n\\n16.18.\\nDetermine for the matrix\\n⎡\\n\\n⎤\\n511\\nA = ⎣0 5 1⎦ ∈ R3,3\\n004 two symmetric matrices S1 , S2 ∈ R3,3 with A = S1 S2 .\\n\\n251</td>\n",
       "      <td>252.0</td>\n",
       "      <td>252</td>\n",
       "      <td>16 Cyclic Subspaces, Duality and the Jordan Canonical Form</td>\n",
       "      <td>16.3 Computation of the Jordan Canonical Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>computation jordan canonical form show following assertion b c inr jn λ inr jn λ similar jn λ inr jnr λ written product two symmetric matrix determine matrix two symmetric matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>Chapter 17\\n\\nMatrix Functions and Systems of Differential Equations\\n\\nIn this chapter we give an introduction to the area of matrix functions.\\nWe first define general matrix functions and derive their most important properties.\\nUsing the examples of network analysis and chemical reactions, we illustrate how matrix functions arise naturally in applications.\\nThe network analysis example involves the exponential function of matrices, and we study the properties of this important function in detail.\\nThe analysis of chemical reaction kinetics leads to a system of ordinary differential equations, whose solution again is based on the matrix exponential function.\\n\\n17.1 Matrix Functions and the Matrix Exponential\\nFunction\\nIn the following we will study functions that yield for a given n × n matrix again an n × n matrix.\\nA possible definition of such a function is given by the entrywise application one\\n\u0002\\n\u0003 could define for\\n\u0002 \u0003 of scalar functions to the matrix.\\nFor instance,\\nA = ai j ∈ Cn,n the function sin(A) by sin(A) := sin(ai j ) .\\nHowever, such a definition\\n\u0004 \u0005is not compatible with the matrix multiplication, since in general already\\n2\\nA \u0003= ai2j .\\nThe following definition of the primary matrix function from [Hig08, Definition 1.1–1.2] will turn out to be consistent with the matrix multiplication.\\nSince this definition is based on the Jordan canonical form, we assume for simplicity that\\nA ∈ Cn,n .\\nOur considerations also apply to square matrices over R, as long as they have a Jordan canonical form.\\nDefinition 17.1 Let A ∈ Cn,n have the Jordan canonical form\\nJ = diag(Jd1 (λ1 ), . . . , Jdm (λm )) = S −1 AS,\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_17\\n\\n253</td>\n",
       "      <td>253.0</td>\n",
       "      <td>&lt;FEFF&gt;</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.1 Matrix Functions and the Matrix Exponential Function</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter matrix function system differential equation chapter give introduction area matrix function first define general matrix function derive important property using example network analysis chemical reaction illustrate matrix function arise naturally application network analysis example involves exponential function matrix study property important function detail analysis chemical reaction kinetics lead system ordinary differential equation whose solution based matrix exponential function matrix function matrix exponential function following study function yield given n n matrix n n matrix possible definition function given entrywise application one could define scalar function matrix instance ai j cn n function sin sin sin ai j however definition compatible matrix multiplication since general already following definition primary matrix function definition turn consistent matrix multiplication since definition based jordan canonical form assume simplicity cn n consideration also apply square matrix r long jordan canonical form definition let cn n jordan canonical form j diag jdm λm springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>254\\n\\n17 Matrix Functions and Systems of Differential Equations and let \u0002 ⊂ C be such that {λ1 , . . . , λm } ⊆ \u0002.\\nA function f : \u0002 → C is said to be defined on the spectrum of A, if the values f ( j) (λi ) for i = 1, . . . , m and j = 0, 1 . . . , di − 1\\n\\n(17.1) exist.\\nHere f ( j) (λi ), j = 1, . . . , di − 1, is the jth derivative of the function f (λ) with respect to λ evaluated at λi .\\nIf λi ∈ R, then this is the real derivative, and for\\nλi ∈ C \\ R it is the complex derivative.\\nMoreover, we assume that equal eigenvalues that occur in different Jordan blocks are mapped to the same values in (17.1).\\nIf f is defined on the spectrum of A then the primary matrix function f (A) is defined by f (A) := S f (J )S −1 where f (J ) := diag( f (Jd1 (λ1 )), . . . , f (Jdm (λm ))) (17.2) and\\n⎡\\n⎢\\n⎢\\n⎢\\n⎢\\n\u0007\\n\u0006 f Jdi (λi ) := ⎢\\n⎢\\n⎢\\n⎢\\n⎣\\n\\n⎤\\n(di −1)\\n(λi )\\n. . . f (di −1)!\\n⎥\\n..\\n.\\n⎥\\n⎥ f (λi ) f \u0007 (λi ) . .\\n.\\n⎥\\n..\\n..\\nf \u0007\u0007 (λi ) ⎥\\n⎥ for i = 1, . . . , m.\\n(17.3)\\n.\\n.\\n2!\\n⎥\\n⎥\\n..\\n. f \u0007 (λi ) ⎦ f (λi ) f (λi ) f \u0007 (λi ) f \u0007\u0007 (λi )\\n2!\\n\\nNote that for the definition of f (A) in (17.2)–(17.3) only the existence of the values in (17.1) is required.\\n√\\nExample 17.2 Let √\\nA = I2 ∈ C2,2 and let f (z)\\n√ = z (the square root function).\\nIf we set f (1) = 1 = +1, then f (A) = A = I2 by Definition\\n√ 17.1.\\nIf we\\n1 = −1, then choose the other branch of the square root function, i.e., f\\n(1)\\n=\\n√ f (A) = A = −I2 .\\nThe matrices I2 and −I2 are primary square roots of A = I2 .\\nTaking different branches of a function for different Jordan blocks corresponding to the same eigenvalue is incompatible with Definition 17.1.\\nFor instance, the matrices\\n\u000e\\nX1 =\\n\\n1 0\\n0 −1\\n\\n\u000e\\n\\n\u000f and X 2 =\\n\\n−1 0\\n01\\n\\n\u000f are incompatible with Definition 17.1, despite the fact that X 12 = I2 and X 22 = I2 .\\nAll solutions X ∈ Cn,n of the matrix equation X 2 = A are called square roots of the matrix A ∈ Cn,n .\\nBut as Example 17.2 shows, some of these may not be primary square roots according to Definition 17.1.\\nIn the following, by f (A) we will always mean a primary matrix function according to Definition 17.1, and will usually omit the term “primary”.\\nIn (16.8) we have shown that for each polynomial p ∈ C[t] of degree k ≥ 0 we have</td>\n",
       "      <td>254.0</td>\n",
       "      <td>254</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.1 Matrix Functions and the Matrix Exponential Function</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix function system differential equation let c λm function f c said defined spectrum value f j λi j di exist f j λi j di jth derivative function f λ respect λ evaluated λi λi r real derivative λi c r complex derivative moreover assume equal eigenvalue occur different jordan block mapped value f defined spectrum primary matrix function f defined f f j f j diag f f jdm λm f jdi λi di λi f di f λi f λi f λi f λi f λi f λi f λi f λi note definition f existence value required example let let f z z square root function set f f definition choose branch square root function f f matrix primary square root taking different branch function different jordan block corresponding eigenvalue incompatible definition instance matrix x incompatible definition despite fact x x solution x cn n matrix equation x called square root matrix cn n example show may primary square root according definition following f always mean primary matrix function according definition usually omit term primary shown polynomial p c degree k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>17.1 Matrix Functions and the Matrix Exponential Function p(Jdi (λi )) =\\n\\n255 k\\n\u0010 p ( j) (λi )\\n(Jdi (0)) j .\\nj!\\nj=0\\n\\n(17.4)\\n\\nA simple comparison shows that this formula agrees with (17.3) for f = p.\\nThis means that the computation of p(Jdi (λi )) with (17.4) leads to the same result as the definition of p(Jdi (λi )) by (17.3).\\nMore generally, the following result holds.\\nLemma 17.3 Let A ∈ Cn,n and p = αk t k + . . . + α1 t + α0 ∈ C[t].\\nThen (17.2)–\\n(17.3) with f = p yields a matrix function f (A) that satisfies f (A) = αk Ak + . . . +\\nα1 A + α0 In .\\nProof Exercise.\\nIf we consider, in particular, the polynomial f = t 2 in (17.2)–(17.3), then the resulting f (A) is equal to the product A ∗ A. This shows that the definition of the primary matrix function f (A) is consistent with the matrix multiplication.\\nThe following theorem, which is of great practical and theoretical importance, shows that the matrix f (A) can always be written as a polynomial in A.\\nTheorem 17.4 Let A ∈ Cn,n have the minimal polynomial M A , and let f (A) be as in Definition 17.1.\\nThen there exists a uniquely determined polynomial p ∈ C[t] of degree at most deg(M A ) − 1 with f (A) = p(A).\\nIn particular, A f (A) = f (A)A, f (A T ) = f (A)T as well as f (V AV −1 ) = V f (A)V −1 for all V ∈ G L n (C).\\nProof We will not present the proof here since it requires advanced results from interpolation theory.\\nDetails can be found in [Hig08, Chap.\\n1].\\nUsing Theorem 17.4 we can show that the primary matrix function f (A) in\\nDefinition 17.1 is independent of the choice of the Jordan canonical form of A. We already know from Theorem 16.12, that the Jordan canonical form of A is unique up to the order of the Jordan blocks.\\nIf\\nJ = diag(Jd1 (λ1 ), . . . , Jdm (λm )) = S −1 AS,\\nλ1 ), . . . , Jd\u0011 (\u0011\\nλm )) = \u0011\\nS −1 A\u0011\\nS\\nJ\u0011 = diag(Jd\u0011 (\u0011\\n1 m are two Jordan canonical forms of A, then J\u0011 = P T J P for a permutation matrix\\nP ∈ Rn,n , where the matrices J and J\u0011 are the same up to the order of diagonal blocks.\\nHence f (J ) = diag( f (Jd1 (λ1 )), . . . , f (Jdm (λm )))\\n\u0006\\n\u0007\\n= P P T diag( f (Jd1 (λ1 )), . . . , f (Jdm (λm )))P P T\\n\u0006\\n\u0007\\n= P diag( f (Jd\u0011 (\u0011\\nλ1 )), . . . , f (Jd\u0011 (\u0011\\nλm ))) P T\\n1\\n\\n= P f ( J\u0011)P T .\\nm</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.1 Matrix Functions and the Matrix Exponential Function</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix function matrix exponential function p jdi λi k p j λi jdi j j simple comparison show formula agrees f mean computation p jdi λi lead result definition p jdi λi generally following result hold lemma let cn n p αk k c f p yield matrix function f satisfies f αk ak proof exercise consider particular polynomial f resulting f equal product show definition primary matrix function f consistent matrix multiplication following theorem great practical theoretical importance show matrix f always written polynomial theorem let cn n minimal polynomial let f definition exists uniquely determined polynomial p c degree deg f p particular f f f f well f v av v f v v g l n c proof present proof since requires advanced result interpolation theory detail found chap using theorem show primary matrix function f definition independent choice jordan canonical form already know theorem jordan canonical form unique order jordan block j diag jdm λm λm diag two jordan canonical form p j p permutation matrix p rn n matrix j order diagonal block hence f j diag f f jdm λm p p diag f f jdm λm p p p diag f f λm p p f p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>256\\n\\n17 Matrix Functions and Systems of Differential Equations\\n\\nTheorem 17.4 applied to the matrix J yields the existence of a polynomial p with f (J ) = p(J ).\\nThus, we get f (A) = S f (J )S −1 = Sp(J )S −1 = p(A) = p(\u0011\\nS J\u0011\u0011\\nS −1 ) = \u0011\\nS P T p(J )P \u0011\\nS −1 = \u0011\\nS P T f (J )P \u0011\\nS −1\\n=\u0011\\nS f ( J\u0011)\u0011\\nS −1 .\\n\\nLet us now consider the exponential function f (z) = e z that is infinitely often complex differentiable throughout C. In particular, e z is defined (in the sense of\\nDefinition 17.1) on the spectrum of every given matrix\\nA = Sdiag(Jd1 (λ1 ), . . . , Jdm (λm ))S −1 ∈ Cn,n .\\nIf t ∈ C is arbitrary (but fixed), then the derivatives of the function et z with respect to the variable z are given by d j tz e = t j et z , dz j j = 0, 1, 2, . . . .\\n\\nWe will use the notation exp(M) instead of e M for the exponential function of a matrix\\nM. For every Jordan block Jd (λ) of A we then have, by (17.3) with f (z) = e z ,\\n⎡\\n2\\n1 t t2! . . .\\n⎢\\n⎢ 1 t ...\\n⎢\\n⎢\\n.. ..\\nexp(t Jd (λ)) = etλ ⎢\\n. .\\n⎢\\n⎢\\n..\\n⎣\\n.\\nt d−1 ⎤\\n(d−1)!\\n\\n.. ⎥\\n. ⎥ d−1\\n⎥\\n\u0010\\n1\\n⎥ tλ\\n(t Jd (0))k , t2 ⎥ = e\\n⎥ k!\\n2!\\nk=0\\n⎥ t ⎦\\n1\\n\\n(17.5) and the matrix exponential function exp(t A) is given by exp(t A) = Sdiag(exp(t Jd1 (λ1 )), . . . , exp(t Jdm (λm )))S −1 .\\n\\n(17.6)\\n\\nThe parameter t will be used in the next section in the context of linear differential equations.\\nIn Analysis it is shown that for every z ∈ C the function e z can be represented by the absolutely convergent series ez =\\n\\n∞\\n\u0010 zj\\n.\\nj!\\nj=0\\n\\nUsing this series and the equation (Jd (0))\u0003 = 0 for all \u0003 ≥ d, we obtain</td>\n",
       "      <td>256.0</td>\n",
       "      <td>256</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.1 Matrix Functions and the Matrix Exponential Function</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix function system differential equation theorem applied matrix j yield existence polynomial p f j p j thus get f f j sp j p p p p j p p f j p f let u consider exponential function f z e z infinitely often complex differentiable throughout particular e z defined sense definition spectrum every given matrix sdiag jdm λm cn n c arbitrary fixed derivative function et z respect variable z given j tz e j et z dz j j use notation exp instead e exponential function matrix every jordan block jd λ f z e z exp jd λ etλ tλ jd k e k matrix exponential function exp given exp sdiag exp exp jdm λm parameter used next section context linear differential equation analysis shown every z c function e z represented absolutely convergent series ez zj j using series equation jd obtain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>17.1 Matrix Functions and the Matrix Exponential Function\\n\\n257\\n\\n⎛\\n⎞ \u0016\\n\u0017\\n∞\\n∞ d−1 j\\n\u0010\\n\u0010\\n\u0010\\n1\\n(tλ)\\n1\\n\u0003\\n\u0003 tλ\\n⎝\\n⎠ exp(t Jd (λ)) = e\\n·\\n(t Jd (0)) =\\n(t Jd (0))\\n\u0003!\\nj!\\n\u0003!\\nj=0\\n\u0003=0\\n\u0003=0\\n\u0016 j\\n\u0017\\n∞\\n\u0010\\n\u0010 (tλ) j−\u0003 1\\n\u0003\\n=\\n· (t Jd (0))\\n( j − \u0003)! \u0003!\\nj=0\\n\u0003=0\\n\u0017\\n\u0016 j \u0018 \u0019\\n∞\\n\u0010 tj \u0010 j j−\u0003\\n=\\nλi (Jd (0))\u0003\\n\u0003 j!\\nj=0\\n\u0003=0\\n∞\\n\u0010 tj\\n=\\n(λId + Jd (0)) j j!\\nj=0\\n\\n=\\n\\n∞\\n\u0010\\n1\\n(t Jd (λ)) j .\\nj!\\nj=0\\n\\n(17.7)\\n\\nIn this derivation we have used the absolute convergence of the exponential series and the finiteness of the series with the matrix Jd (0).\\nThis allows the application of the Cauchy product formula1 for absolutely convergent series, which is also proven in Analysis.\\nLemma 17.5 If A ∈ Cn,n , t ∈ C and exp(t A) is the matrix exponential function in\\n(17.5)–(17.6), then\\n∞\\n\u0010\\n1\\n(t A) j .\\nexp(t A) = j!\\nj=0\\nProof In (17.7) we have shown this already for Jordan blocks.\\nThe assertion then follows from\\n⎛\\n⎞\\n∞\\n∞\\n\u0010\\n\u0010\\n1\\n1\\n(t S J S −1 ) j = S ⎝\\n(t J ) j ⎠ S −1 j!\\nj!\\nj=0 j=0 and the representation (17.6) of the matrix exponential function.\\nWe immediately see from Lemma 17.5 that for a matrix A ∈ Rn,n and every real t the matrix exponential function exp(t A) is a real matrix.\\nThe following result presents further important properties of the matrix exponential function.\\nLemma 17.6 If the two matrices A, B ∈ Cn,n commute, then exp(A + B) = exp(A) exp(B).\\nFor every matrix A ∈ Cn,n we have exp(A) ∈ G L n (C) with\\n(exp(A))−1 = exp(−A).\\n\\n1 Augustin\\n\\nLouis Cauchy (1789–1857).</td>\n",
       "      <td>257.0</td>\n",
       "      <td>257</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.1 Matrix Functions and the Matrix Exponential Function</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix function matrix exponential function j tλ tλ exp jd λ e jd jd j j tλ jd j j tj j λi jd j tj λid jd j j jd λ j j derivation used absolute convergence exponential series finiteness series matrix jd allows application cauchy product absolutely convergent series also proven analysis lemma cn n c exp matrix exponential function j exp j proof shown already jordan block assertion follows j j j j j j representation matrix exponential function immediately see lemma matrix rn n every real matrix exponential function exp real matrix following result present important property matrix exponential function lemma two matrix b cn n commute exp b exp exp b every matrix cn n exp g l n c exp exp augustin louis cauchy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>258\\n\\n17 Matrix Functions and Systems of Differential Equations\\n\\nProof If A and B commute, then the Cauchy product formula yields\\n⎛\\n\\n⎞\u0016\\n\u0017\\n\u0016 j\\n\u0017\\n∞\\n∞\\n∞\\n\u0010\\n\u0010\\n\u0010\\n\u0010 1\\n1\\n1\\n1 j\\n\u0003\\n\u0003 j−\u0003 exp(A) exp(B) = ⎝\\nA ⎠\\nB =\\nA\\nB j!\\n\u0003!\\n\u0003!\\n( j − \u0003)!\\nj=0 j=0 \u0003=0\\n\u0003=0\\n\u0016\\n\u0017 j \u0018 \u0019\\n∞\\n∞\\n\u0010\\n\u0010\\n1 \u0010 j\\n1\\n\u0003 j−\u0003\\nA B\\n=\\n=\\n(A + B) j\\n\u0003 j!\\nj!\\nj=0 j=0\\n\u0003=0\\n= exp(A + B).\\nHere we have used the binomial formula for commuting matrices (cp.\\nExercise 4.10).\\nSince A and −A commute, we have exp(A) exp(−A) = exp(A − A) = exp(0) =\\n\\n∞\\n\u0010\\n1 j\\n0 = In , j!\\nj=0 and hence exp(A) ∈ G L n (C) with (exp(A))−1 = exp(−A).\\nFor non-commuting matrices the statements in Lemma 17.6 in general do not hold\\n(cp.\\nExercise 17.9).\\n\\nMATLAB-Minute.\\nCompute the matrix exponential function exp(A) for the matrix\\n⎤\\n1 −1 3 4 5\\n⎢ −1 −2 4 3 5 ⎥\\n⎥\\n⎢\\n5,5\\n⎥\\nA=⎢\\n⎢ 2 0 −3 1 5 ⎥ ∈ R\\n⎣ 3 0 0 −2 −3 ⎦\\n4 0 0 −3 −5\\n⎡ using the command E1=expm(A).\\n(Look at help expm.)\\nAlso compute the diagonalization of A using the command [S,D]=eig(A), and form the matrix exponential function exp(A) as E2=S∗expm(D)/S.\\nCompare the matrices E1 and E2 and compute the relative error norm(E1E2)/norm(E2).\\n(Look at help norm.)\\n\\nExample 17.7 Let A = [ai j ] ∈ Cn,n be a symmetric matrix with aii = 0 and ai j ∈\\n{0, 1} for all i, j = 1, . . . , n.\\nWe identify the matrix A with a graph G A = (V A , E A ) consisting of a set of n vertices V A = {1, . . . , n} and a set of edges E A ⊆ V A × V A .\\nFor i = 1, . . . , n the row i of A is identified with the vertex i ∈ E A , and every entry ai j = 1 is identified with an edge (i, j) ∈ E A .\\nDue to the symmetry of A, we have ai j = 1 if and only if a ji = 1.\\nWe therefore consider in the following the elements</td>\n",
       "      <td>258.0</td>\n",
       "      <td>258</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.1 Matrix Functions and the Matrix Exponential Function</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix function system differential equation proof b commute cauchy product formula yield j j exp exp b b b j j j j b b j j j exp b used binomial formula commuting matrix cp exercise since commute exp exp exp exp j j hence exp g l n c exp exp non-commuting matrix statement lemma general hold cp exercise matlab-minute compute matrix exponential function exp matrix r using command look help expm also compute diagonalization using command form matrix exponential function exp compare matrix compute relative error norm look help norm example let ai j cn n symmetric matrix aii ai j j identify matrix graph g v e consisting set n vertex v n set edge e v v n row identified vertex e every entry ai j identified edge j e due symmetry ai j ji therefore consider following element</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>17.1 Matrix Functions and the Matrix Exponential Function\\n\\n259 of E A as unordered pairs, i.e., (i, j) = ( j, i).\\nThe following example illustrates this identification:\\n⎡\\n⎤\\n01110\\n⎢1 0 0 1 1⎥\\n⎢\\n⎥\\n⎥\\nA=⎢\\n⎢1 0 0 0 1⎥\\n⎣1 1 0 0 0⎦\\n01100 is identified with G A = (V A , E A ), where\\nE A = {1, 2, 3, 4, 5}, V A = {(1, 2), (1, 3), (1, 4), (2, 4), (2, 5), (3, 5)}, and the graph G A can be displayed as follows:\\n\\nA path of length m from the vertex k1 to the vertex km+1 is an ordered list of vertices k1 , k2 , . . . , km+1 , where (ki , ki+1 ) ∈ V A for i = 1, . . . , m.\\nIf k1 = km+1 , then this is a closed path of length m.\\nIn the above example, paths from 1 to 4 are given by 1, 2, 4 and 1, 2, 5, 3, 1, 2, 4; these have the lengths 2 and 6, respectively.\\nIn the mathematical field of Graph Theory one usually assumes that the vertices in a path are pairwise distinct.\\nOur deviation from this convention is motivated by the following interpretation of a matrix A and its powers:\\nAn entry ai j = 1 in the matrix A means that there exists a path of length 1 from vertex i to vertex j, i.e., the vertices i and j are adjacent.\\nIf ai j = 0, then no such path exists.\\nThe matrix A is therefore called the adjacency matrix of the graph G A .\\nIf we square the adjacency matrix, then the entry in the (i, j) position is given by\\n(A2 )i j = n\\n\u0010 ai\u0003 a\u0003j .\\n\\n\u0003=1\\n\\nIn the sum on the right hand side, we obtain for a given \u0003 a 1 if and only if (i, \u0003) ∈ E A and (\u0003, j) ∈ E A .\\nThe sum on the right had side therefore is equal to the number of vertices that are adjacent to both i and j.\\nHence the (i, j) entry of A2 is equal to the number of pairwise distinct paths from i to j (i \u0003= j), or the pairwise distinct closed paths from i to i of length 2 in G A .\\nMore generally, one can show the following (cp.\\nExercise 17.10):\\nLet A = [ai j ] ∈ Cn,n be a symmetric adjacency matrix, i.e., A = A T with aii = 0 and ai j ∈ {0, 1} for all i, j = 1, . . . , n, and let G A be the graph identified with A.\\nThen for each m ∈ N the (i, j) entry of Am is equal to the number of pairwise distinct paths from i to j (i \u0003= j) or the pairwise distinct closed paths from i to i of length m in G A .</td>\n",
       "      <td>259.0</td>\n",
       "      <td>259</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.1 Matrix Functions and the Matrix Exponential Function</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix function matrix exponential function e unordered pair j j following example illustrates identification identified g v e e v graph g displayed follows path length vertex vertex ordered list vertex ki v closed path length example path given length respectively mathematical field graph theory one usually assumes vertex path pairwise distinct deviation convention motivated following interpretation matrix power entry ai j matrix mean exists path length vertex vertex j vertex j adjacent ai j path exists matrix therefore called adjacency matrix graph g square adjacency matrix entry j position given j n sum right hand side obtain given e j e sum right side therefore equal number vertex adjacent j hence j entry equal number pairwise distinct path j j pairwise distinct closed path length g generally one show following cp exercise let ai j cn n symmetric adjacency matrix aii ai j j n let g graph identified n j entry equal number pairwise distinct path j j pairwise distinct closed path length g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>260\\n\\n17 Matrix Functions and Systems of Differential Equations\\n\\nFor the above matrix A we obtain\\n⎡\\n⎡\\n⎤\\n31012\\n2\\n⎢1 3 2 1 0⎥\\n⎢6\\n⎢\\n⎢\\n⎥\\n3\\n⎢\\n⎥\\nA2 = ⎢\\n⎢ 0 2 2 1 0 ⎥ and A = ⎢ 5\\n⎣1 1 1 2 1⎦\\n⎣4\\n20012\\n1\\n\\n6\\n2\\n1\\n4\\n5\\n\\n5\\n1\\n0\\n2\\n4\\n\\n4\\n4\\n2\\n2\\n2\\n\\n⎤\\n1\\n5⎥\\n⎥\\n4⎥\\n⎥.\\n2⎦\\n0\\n\\nThe 3 pairwise distinct closed paths of length 2 from 1 to 1 are\\n1, 2, 1, 1, 3, 1, 1, 4, 1 and the 4 pairwise distinct paths of length 3 from 1 to 4 are\\n1, 2, 1, 4, 1, 3, 1, 4, 1, 4, 1, 4, 1, 4, 2, 4.\\nNumerous real world applications involve networks that can be modeled mathematically using graphs.\\nExamples include social, biological, telecommunication or airline networks.\\nThe properties of such networks are studied in the interdisciplinary area of Network Science.\\nAn important task is to identify participants in the network that are central in the sense that their functionality has a significant impact on the entire network.\\nIf the network has been modeled by a graph, then we can study the centrality of the vertices.\\nFor example, a vertex can be considered central if it is connected to a large part of the graph via many short closed paths.\\nLonger connections are usually less important, and thus paths should be scaled down according to their length.\\nIf we use the scaling factor 1/m! for a path of length m, then for the vertex i in the graph G A with the adjacency matrix A we obtain a centrality measure of the form\\n\u0018\\n\u0019\\n1\\n1 2\\n1 3\\nA + A + A + ...\\n.\\n1!\\n2!\\n3!\\nii\\nThe relative ordering of the vertices according to this formula is not changed when we add the constant 1.\\nWe then obtain the centrality of the vertex i as\\n\u0018\\n\\n1\\n1\\nI + A + A2 + A3 + . . .\\n2\\n3!\\n\\n\u0019\\n= (exp(A))ii .\\nii\\n\\nAnother important quantity is the so-called communicability between the vertices i and j for i \u0003= j, which is given by the weighted sum of the pairwise distinct paths from i to j, i.e., by\\n\u0018\\nI + A+\\n\\n1 2\\n1\\nA + A3 + . . .\\n2\\n3!\\n\\n\u0019\\n= (exp(A))i j .\\nij\\n\\nFor the above matrix A the MATLAB function expm yields</td>\n",
       "      <td>260.0</td>\n",
       "      <td>260</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.1 Matrix Functions and the Matrix Exponential Function</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix function system differential equation matrix obtain pairwise distinct closed path length pairwise distinct path length numerous real world application involve network modeled mathematically using graph example include social biological telecommunication airline network property network studied interdisciplinary area network science important task identify participant network central sense functionality significant impact entire network network modeled graph study centrality vertex example vertex considered central connected large part graph via many short closed path longer connection usually le important thus path scaled according length use scaling factor path length vertex graph g adjacency matrix obtain centrality measure form ii relative ordering vertex according formula changed add constant obtain centrality vertex exp ii ii another important quantity so-called communicability vertex j j given weighted sum pairwise distinct path j exp j ij matrix matlab function expm yield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>17.1 Matrix Functions and the Matrix Exponential Function\\n\\n⎡\\n\\n3.7630\\n⎢ 3.1953\\n⎢ exp(A) = ⎢\\n⎢ 2.2500\\n⎣ 2.7927\\n1.8176\\n\\n3.1953\\n3.7630\\n1.8176\\n2.7927\\n2.2500\\n\\n2.2500\\n1.8176\\n2.4881\\n1.2749\\n1.9204\\n\\n2.7927\\n2.7927\\n1.2749\\n2.8907\\n1.2749\\n\\n261\\n\\n⎤\\n1.8176\\n2.2500 ⎥\\n⎥\\n1.9204 ⎥\\n⎥.\\n1.2749 ⎦\\n2.4881\\n\\nThe vertices 1 and 2 have the largest centrality, followed by 4, 3 and 5.\\nIf we would define the centrality of a vertex as the number of adjacent vertices, then in this example we could not distinguish between the vertices 3, 4 and 5.\\nThe largest communicability in this example exists between the vertices 1 and 2.\\nFurther information concerning the analysis of networks using adjacency matrices and matrix functions can be found in the article [EstH10].\\n\\n17.2 Systems of Linear Ordinary Differential Equations\\nA differential equation describes a relationship between a desired function and its derivatives.\\nSuch equations are used in all areas of science and engineering for modeling physical phenomena.\\nOrdinary differential equations involve a function of one variable and its derivatives, while partial differential equations involve functions of several variables and their partial derivatives.\\nIn this section we focus on ordinary differential equations of first order, i.e., those in which only the function and its first derivative occur.\\nA simple example for the modeling with ordinary differential equations of first order is the increase or decrease of a biological population, such as bacteria in a petri dish.\\nLet y = y(t) be the size of the population at time t.\\nIf there is enough food and if the external conditions (e.g. temperature or pressure) are constant, then the population grows with a (real) rate k &gt; 0, that is proportional to the current number of individuals.\\nThis can be described by the equation ẏ := d y = ky.\\ndt\\n\\n(17.8)\\n\\nClearly, one can also take k &lt; 0, and then the population shrinks.\\nWe are then looking for a function y : D ⊂ R → R that satisfies (17.8).\\nThe general solution of (17.8) is given by the exponential function y = cetk , where c ∈ R is an arbitrary constant.\\nFor a unique solution of (17.8) we need to know the size of the population at a given initial time t0 .\\nIn this way we obtain the initial value problem ẏ = ky, y(t0 ) = y0 ,</td>\n",
       "      <td>261.0</td>\n",
       "      <td>261</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.2 Systems of Linear Ordinary Differential Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix function matrix exponential function exp vertex largest centrality followed would define centrality vertex number adjacent vertex example could distinguish vertex largest communicability example exists vertex information concerning analysis network using adjacency matrix matrix function found article system linear ordinary differential equation differential equation describes relationship desired function derivative equation used area science engineering modeling physical phenomenon ordinary differential equation involve function one variable derivative partial differential equation involve function several variable partial derivative section focus ordinary differential equation first order function first derivative occur simple example modeling ordinary differential equation first order increase decrease biological population bacteria petri dish let size population time enough food external condition temperature pressure constant population grows real rate k proportional current number individual described equation dt clearly one also take k population shrink looking function r r satisfies general solution given exponential function cetk c r arbitrary constant unique solution need know size population given initial time way obtain initial value problem ky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>262\\n\\n17 Matrix Functions and Systems of Differential Equations which, as we will show below, is solved uniquely by the function y = e(t−t0 )k y0 .\\nExample 17.8 In a chemical reaction certain initial substances (called educts or reactants) are transformed into other substances (called products).\\nReactions can be distinguished concerning their order.\\nHere we only discuss reactions of first order, where the reaction rate is determined by only one educt.\\nIn reactions of second and higher order one typically obtains nonlinear differential equations, which are beyond our focus in this chapter.\\nIf, for example, the educt A1 is transformed into the product A2 with the rate\\n−k1 &lt; 0, then we write this reaction symbolically as\\nA1 k1\\n\\n/ A2 , and we model it mathematically by the ordinary differential equation ẏ1 = −k1 y1 .\\nHere the value y1 (t) is the concentration of the substance A1 at time t.\\nFor the concentration of the product A2 , which grows with the rate k1 &gt; 0, we have the corresponding equation ẏ2 = k1 y1 .\\nIt may happen that a reaction of first order develops in both directions.\\nIf A1 transforms into A2 with the rate −k1 , and A2 transforms into A1 with the rate −k2 , i.e., k1\\n/\\nA1 o\\nA2 , k2 then we can model this reaction mathematically by the system of linear ordinary differential equations ẏ1 = −k1 y1 + k2 y2 , ẏ2 = k1 y1 − k2 y2 .\\nCombining the functions y1 and y2 in a vector valued function y = [y1 , y2 ]T , we can write this system as\\n\u000f\\n−k1 k2\\n.\\nẏ = Ay, where A = k1 −k2\\n\u000e</td>\n",
       "      <td>262.0</td>\n",
       "      <td>262</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.2 Systems of Linear Ordinary Differential Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix function system differential equation show solved uniquely function e k example chemical reaction certain initial substance called educts reactant transformed substance called product reaction distinguished concerning order discus reaction first order reaction rate determined one educt reaction second higher order one typically obtains nonlinear differential equation beyond focus chapter example educt transformed product rate write reaction symbolically model mathematically ordinary differential equation value concentration substance time concentration product grows rate corresponding equation may happen reaction first order develops direction transforms rate transforms rate model reaction mathematically system linear ordinary differential equation combining function vector valued function write system ay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>17.2 Systems of Linear Ordinary Differential Equations\\n\\n263\\n\\nThe derivative of the function y(t) is always considered entrywise,\\n\u000e \u000f ẏ ẏ = 1 .\\nẏ2\\nReactions can also have several steps.\\nFor example, a reaction of the form\\nA1 k1\\n\\n/ A2 o k2\\n\\n/\\n\\nA3 k4\\n\\n/ A4 k3 leads to the differential equations ẏ1 = −k1 y1 , ẏ2 = k1 y1 − k2 y2 + k3 y3 , ẏ3 = k2 y2 − (k3 + k4 )y3 , ẏ4 = k4 y3 , and thus to the system\\n⎤\\n⎡\\n0\\n0\\n−k1 0\\n⎢ k1 −k2 k3\\n0⎥\\n⎥ ẏ = Ay, where A = ⎢\\n⎣ 0 k2 −(k3 + k4 ) 0⎦ .\\n0\\n0\\n0 k4\\nThe sum of the entries in each column of A is equal to zero, since for every decrease in a substance with a certain rate other substances increase with the same rate.\\nIn summary, a chemical reaction of first order leads to a system of linear ordinary differential equations of first order that can be written as ẏ = Ay with a (real) square matrix A.\\nWe now derive the general theory for systems of linear (real or complex) ordinary differential equations of first order of the form ẏ = Ay + g, t ∈ [0, a].\\n\\n(17.9)\\n\\nHere A ∈ K n,n is a given matrix, a is a given positive real number, g : [0, a] → K n,1 is a given function, y : [0, a] → K n,1 is the desired solution, and we assume that\\nK = R or K = C. If g(t) = 0 ∈ K n,1 for all t ∈ [0, a], then the system (17.9) is called homogeneous, otherwise it is called non-homogeneous.\\nFor a given system of the form (17.9), the system ẏ = Ay, t ∈ [0, a],\\n(17.10) is called the associated homogeneous system.</td>\n",
       "      <td>263.0</td>\n",
       "      <td>263</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.2 Systems of Linear Ordinary Differential Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>system linear ordinary differential equation derivative function always considered entrywise reaction also several step example reaction form lead differential equation thus system ay sum entry column equal zero since every decrease substance certain rate substance increase rate summary chemical reaction first order lead system linear ordinary differential equation first order written ay real square matrix derive general theory system linear real complex ordinary differential equation first order form ay g k n n given matrix given positive real number g k given function k desired solution assume k r k g k system called homogeneous otherwise called non-homogeneous given system form system ay called associated homogeneous system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>264\\n\\n17 Matrix Functions and Systems of Differential Equations\\n\\nLemma 17.9 The solutions of the homogeneous system (17.10) form a subspace of the (infinite dimensional) K -vector space of the continuously differentiable functions from the interval [0, a] to K n,1 .\\nProof We will show the required properties according to Lemma 9.5.\\nThe function w = 0 is continuously differentiable on [0, a] and solves the homogeneous system\\n(17.10).\\nThus, the solution set of this system is not empty.\\nIf w1 , w2 : [0, a] → K n,1 are continuously differentiable solutions and if α1 , α2 ∈ K , then w = α1 w1 + α2 w2 is continuously differentiable on [0, a], and ẇ = α1 ẇ1 + α2 ẇ2 = α1 Aw1 + α2 Aw2 = Aw, i.e., the function w is a solution of the homogeneous system.\\nThe following characterization of the solutions of the non-homogeneous system\\n(17.9) is analogous to the characterization of the solution set of a non-homogeneous linear system of equations in Lemma 6.2 (also cp.\\n(8) in Lemma 10.7 ).\\nLemma 17.10 If w1 : [0, a] → K n,1 is a solution of the non-homogeneous system\\n(17.9), then every other solution y can be written as y = w1 + w2 , where w2 is a solution of the associated homogeneous system (17.10).\\nProof If w1 and y are solutions of (17.9), then ẏ − ẇ1 = (Ay + g) − (Aw1 + g) =\\nA(y − w1 ).\\nThe difference w2 := y − w1 thus is a solution of the associated homogeneous system and y = w1 + w2 .\\nIn order to describe the solutions of systems of ordinary differential equations, we consider for a given matrix A ∈ K n,n the matrix exponential function exp(t A) from\\nLemma 17.5 or (17.5)–(17.6), where we now consider t ∈ [0, a] as real variable.\\nThe power series of the matrix exponential function in Lemma 17.5 converges, and it can be differentiated termwise with respect to the variable t, where again the derivative of a matrix with respect to the variable t is considered entrywise.\\nThis yields d d exp(t A) = dt dt\\n\\n\u0018\\n\\n1\\n1\\nI + (t A) + (t A)2 + (t A)3 + . . .\\n2\\n6\\n1\\n= A + t A2 + t 2 A3 + . . .\\n2\\n= A exp(t A).\\n\\n\u0019\\n\\nThe same result is obtained by the entrywise differentiation of the matrix exp(t A) in\\n(17.5)–(17.6) with respect to t.\\nWith</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.2 Systems of Linear Ordinary Differential Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix function system differential equation lemma solution homogeneous system form subspace infinite dimensional k space continuously differentiable function interval k proof show required property according lemma function w continuously differentiable solves homogeneous system thus solution set system empty k continuously differentiable solution k w continuously differentiable aw function w solution homogeneous system following characterization solution non-homogeneous system analogous characterization solution set non-homogeneous linear system equation lemma also cp lemma lemma k solution non-homogeneous system every solution written solution associated homogeneous system proof solution ay g g difference thus solution associated homogeneous system order describe solution system ordinary differential equation consider given matrix k n n matrix exponential function exp lemma consider real variable power series matrix exponential function lemma converges differentiated termwise respect variable derivative matrix respect variable considered entrywise yield exp dt dt exp result obtained entrywise differentiation matrix exp respect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>17.2 Systems of Linear Ordinary Differential Equations\\n\\n⎡\\n2\\n1 t t2! . . .\\n⎢\\n⎢ 1 t ...\\n⎢\\n⎢\\n.. ..\\nM(t) := ⎢\\n. .\\n⎢\\n⎢\\n..\\n⎣\\n.\\n\\n265 t d−1 ⎤\\n(d−1)!\\n\\n.. ⎥\\n. ⎥\\n⎥\\n⎥ t2 ⎥\\n2! ⎥\\n⎥ t ⎦\\n1 we obtain\\n\u0007 d d \u0006 tλ exp(t Jd (λ)) = e M(t) dt dt\\n= λetλ M(t) + etλ Ṁ(t)\\n= λetλ M(t) + etλ Jd (0)M(t)\\n= (λId + Jd (0)) etλ M(t)\\n= Jd (λ) exp(t Jd (λ)), which also gives d dt exp(t A) = A exp(t A).\\n\\nTheorem 17.11\\n(1) The unique solution of the homogeneous differential equation system (17.10) for a given initial condition y(0) = y0 ∈ K n,1 is given by the function y = exp(t A)y0 .\\n(2) The set of all solutions of the homogeneous differential equation system (17.10) forms an n-dimensional K -vector space with the basis {exp(t A)e1 , . . . , exp(t A)en }.\\nProof\\n(1) If y = exp(t A)y0 , then\\n\u0019\\n\u0018 d d\\n(exp(t A)y0 ) = exp(t A) y0 = (A exp(t A))y0 dt dt\\n= A(exp(t A)y0 ) = Ay, ẏ = and y(0) = exp(0)y0 = In y0 = y0 .\\nHence y is a solution of (17.10) that satisfies the initial condition.\\nIf w is another such solution and u := exp(−t A)w, then d\\n(exp(−t A)w) = −A exp(−t A)w + exp(−t A)ẇ dt\\n= exp(−t A) (ẇ − Aw) = 0 ∈ K n,1 , u̇ = which shows that the function u has constant entries.\\nIn particular, we then have u = u(0) = w(0) = y0 = y(0) and w = exp(t A)y0 , where we have used that exp(−t A) = (exp(t A))−1 (cp.\\nLemma 17.6).</td>\n",
       "      <td>265.0</td>\n",
       "      <td>265</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.2 Systems of Linear Ordinary Differential Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>system linear ordinary differential equation obtain tλ exp jd λ e dt dt λetλ etλ λetλ etλ jd λid jd etλ jd λ exp jd λ also give dt exp exp theorem unique solution homogeneous differential equation system given initial condition k given function exp set solution homogeneous differential equation system form n-dimensional k space basis exp exp en proof exp exp exp exp dt dt exp ay exp hence solution satisfies initial condition w another solution u exp w exp w exp w exp dt exp aw k show function u constant entry particular u u w w exp used exp exp cp lemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>266\\n\\n17 Matrix Functions and Systems of Differential Equations\\n\\n(2) Each of the functions exp(t A)e j , . . . , exp(t A)en : [0, a] → K n,1 , j = 1, . . . , n, solves the homogeneous system ẏ = Ay.\\nSince the matrix exp(t A) ∈ K n,n is invertible for every t ∈ [0, a] (cp.\\nLemma 17.6), these functions are linearly independent.\\nIf \u0011 y is an arbitrary solution of ẏ = Ay, then \u0011 y(0) = y0 for some y0 ∈ K n,1 .\\nBy\\n(1) then \u0011 y is the unique solution of the initial value problem with y(0) = y0 , so y is a linear combination of the functions that \u0011 y = exp(t A)y0 .\\nAs a consequence, \u0011 exp(t A)e1 , . . . , exp(t A)en .\\nTo describe the solution of the non-homogeneous system (17.9), we need the integral of functions of the form\\n⎤ w1\\n⎢ ⎥ w = ⎣ ... ⎦ : [0, a] → K n,1 .\\n⎡ wn\\n\\nFor every fixed t ∈ [0, a] we define\\n\u001a\\n0\\n\\n⎡\u001b t t\\n\\n⎢ w(s)ds := ⎣\\n\\n0\\n\\n\u001bt\\n0\\n\\n⎤ w1 (s)ds\\n⎥\\n..\\nn,1\\n⎦ ∈ K ,\\n.\\nwn (s)ds i.e., we apply the integral entrywise to the function w.\\nBy this definition we have d dt\\n\\n\u0018\u001a t\\n\\n\u0019 w(s)ds\\n\\n= w(t)\\n\\n0 for all t ∈ [0, a].\\nWe can now determine an explicit solution formula for systems of linear differential equations based on the so-called Duhamel integral.2\\nTheorem 17.12 The unique solution of the non-homogeneous differential equation system (17.9) with the initial condition y(0) = y0 ∈ K n,1 is given by\\n\u001a y = exp(t A)y0 + exp(t A) t exp(−s A)g(s)ds.\\n\\n(17.11)\\n\\n0\\n\\nProof The derivative of the function y defined in (17.11) is\\n\u0018\\n\u0019\\n\u001a t exp(t A) exp(−s A)g(s)ds\\n0\\n\u001a t exp(−s A)g(s)ds + exp(t A) exp(−t A)g\\n= A exp(t A)y0 + A exp(t A) d d ẏ =\\n(exp(t A)y0 ) + dt dt\\n\\n0\\n2 Jean-Marie\\n\\nConstant Duhamel (1797–1872).</td>\n",
       "      <td>266.0</td>\n",
       "      <td>266</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.2 Systems of Linear Ordinary Differential Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix function system differential equation function exp e j exp en k j n solves homogeneous system ay since matrix exp k n n invertible every cp lemma function linearly independent arbitrary solution ay k unique solution initial value problem linear combination function exp consequence exp exp en describe solution non-homogeneous system need integral function form w k wn every fixed define w d d k wn d apply integral entrywise function definition dt w d w determine explicit solution formula system linear differential equation based so-called duhamel theorem unique solution non-homogeneous differential equation system initial condition k given exp exp exp g d proof derivative function defined exp exp g d exp g d exp exp g exp exp exp dt dt jean-marie constant duhamel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>17.2 Systems of Linear Ordinary Differential Equations\\n\\n\u001a t\\n\\n= A exp(t A)y0 + A exp(t A)\\n\\n267 exp(−s A)g(s)ds + g\\n\\n0\\n\\n= Ay + g.\\nFurthermore, we have\\n\u001a\\n\\n0 y(0) = exp(0)y0 + exp(0) exp(−s A)g(s)ds = y0 ,\\n\\n0 so that y also satisfies the initial condition.\\nLet now \u0011 y be another solution of (17.9) that satisfies the initial condition.\\nBy\\nLemma 17.10 we then have \u0011 y = y + w, where w solves the homogeneous system\\n(17.10).\\nTherefore, w = exp(t A)c for some c ∈ K n,1 (cp.\\n(2) in Theorem 17.11).\\ny = y.\\nFor t = 0 we obtain y0 = y0 + c, where c = 0 and hence \u0011\\nIn the above theorems we have shown that for the explicit solution of systems of linear ordinary differential equations of first order, we have to compute the matrix exponential function.\\nWhile we have introduced this function using the Jordan canonical form of the given matrix, numerical computations based on the Jordan canonical form are not advisable (cp.\\nExample 16.20).\\nBecause of its significant practical relevance, numerous different algorithms for computing the matrix exponential function have been proposed.\\nBut, as shown in the article [MolV03], no existing algorithm is completely satisfactory.\\nExample 17.13 The example from circuit simulation presented in Sect.\\n1.5 lead to the system of ordinary differential equations\\nR\\n1\\n1 d\\nI = − I − VC + VS , dt\\nL\\nL\\nL d\\n1\\nVC = − I.\\ndt\\nC\\nUsing (17.11) and the initial values I (0) = I 0 and VC (0) = VC0 , we obtain the solution\\n\u000e \u000f\\n\u0018 \u000e\\n\u000f\u0019 \u000e 0 \u000f\\nI\\n−R/L −1/L\\nI\\n= exp t\\nVC0\\nVC\\n−1/C 0\\n\u0018\\n\u000e\\n\u000f\\n\u000f\u0019 \u000e\\n\u001a t\\n−R/L −1/L\\nVS (s)\\n+ exp (t − s) ds.\\n0\\n−1/C 0\\n0\\nExample 17.14 Let us also consider an example from Mechanics.\\nA weight with mass m &gt; 0 is attached to a spring with the spring constant μ &gt; 0.\\nLet x0 &gt; 0 be the distance of the weight from its equilibrium position, as illustrated in the following figure:</td>\n",
       "      <td>267.0</td>\n",
       "      <td>267</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.2 Systems of Linear Ordinary Differential Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>system linear ordinary differential equation exp exp exp g d g ay furthermore exp exp exp g d also satisfies initial condition let another solution satisfies initial condition lemma w w solves homogeneous system therefore w exp c c k cp theorem obtain c c hence theorem shown explicit solution system linear ordinary differential equation first order compute matrix exponential function introduced function using jordan canonical form given matrix numerical computation based jordan canonical form advisable cp example significant practical relevance numerous different algorithm computing matrix exponential function proposed shown article existing algorithm completely satisfactory example example circuit simulation presented sect lead system ordinary differential equation r vc v dt l l l vc dt c using initial value vc obtain solution exp vc v exp d example let u also consider example mechanic weight mass attached spring spring constant μ let distance weight equilibrium position illustrated following figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>268\\n\\n17 Matrix Functions and Systems of Differential Equations\\n\\nWe want to determine the position x(t) of the weight at time t ≥ 0, where x(0) = x0 .\\nThe extension of the spring is described by Hooke’s law.3 The corresponding ordinary differential equation of second order is ẍ =\\n\\nμ d2 x = − x,\\n2 dt m with initial conditions x(0) = x0 and ẋ(0) = v0 , where v0 &gt; 0 is the initial velocity of the weight.\\nWe can write this differential equation of second order for x as a system of first order by introducing the velocity v as new variable.\\nThe velocity is given by the derivative of the position with respect to time, i.e., v = ẋ, and thus for the acceleration we have v̇ = ẍ, which yields the system\\n\u000e ẏ = Ay, where A =\\n\\n0 1\\n− mμ 0\\n\\n\u000f and y =\\n\\n\u000e \u000f x\\n.\\nv\\n\\nThe initial condition then is y(0) = y0 = [x0 , v0 ]T .\\nBy Theorem 17.11, the unique solution of this homogeneous initial value problem is given by the function y = exp(t A)y0 .\\nWe consider A as an element of C2,2 .\\nThe eigenvalues of A\u001c",
       "are the two complex (non-real) numbers λ1 = iρ and λ2 = −iρ =\\nλ1 , where ρ := mμ .\\nCorresponding eigenvectors are s1 =\\n\\n\u000f\\n\u000e \u000f\\n\u000e\\n1\\n1\\n∈ C2,1 , s2 =\\n∈ C2,1 iρ\\n−iρ and thus exp(t A)y0 = S\\n\\n3 Sir\\n\\n\u000f\\n\u000e itρ\\n\u000e\\n\u000f\\n0 e\\n1 1\\n−1\\nS y\\n,\\nS\\n=\\n∈ C2,2 .\\n0\\n0 e−itρ iρ −iρ\\n\\nRobert Hooke (1635–1703).</td>\n",
       "      <td>268.0</td>\n",
       "      <td>268</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.2 Systems of Linear Ordinary Differential Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>matrix function system differential equation want determine position x weight time x extension spring described hooke corresponding ordinary differential equation second order μ x x dt initial condition x initial velocity weight write differential equation second order x system first order introducing velocity v new variable velocity given derivative position respect time v thus acceleration yield system ay mμ x v initial condition theorem unique solution homogeneous initial value problem given function exp consider element eigenvalue two complex non-real number iρ ρ mμ corresponding eigenvectors iρ thus exp sir itρ e iρ robert hooke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>17.2 Systems of Linear Ordinary Differential Equations\\n\\n269\\n\\nExercises\\n\\n\u0004 \u0005\\n17.1 Construct a matrix A = [ai j ] ∈ C2,2 with A3 \u0003= ai3j .\\n17.2 Determine all solutions X ∈ C2,2 of the matrix equation X 2 = I2 , and classify which of these solutions are primary square roots of I2 .\\n17.3 Determine a matrix X ∈ C2,2 with real entries and X 2 = −I2 .\\n17.4 Prove Lemma 17.3.\\n17.5 Prove the following assertions for A ∈ Cn,n :\\n(a) det(exp(A)) = exp(trace(A)).\\n(b) If A H = −A, then exp(A) is unitary.\\n(c) If A2 = I , then exp(A) = 21 (e + 1e )I + 21 (e − 1e )A.\\n17.6 Let A = S diag(Jd1 (λ1 ), . . . , Jdm (λm )) S −1 ∈ Cn,n with rank(A) = n.\\nDetermine the primary matrix function f (A) for f (z) = z −1 .\\nDoes this function also exist if rank(A) &lt; n?\\n17.7 Let log : {z = r eiϕ | r &gt; 0, −π &lt; ϕ &lt; π} → C, r eiϕ \u000e→ ln(r ) + iϕ, be the principle branch of the complex logarithm (where ln denotes the real natural logarithm).\\nShow that this function is defined on the spectrum of\\n\u000e\\n\\n\u000f\\n01\\nA=\\n∈ C2,2 ,\\n−1 0 and compute log(A) as well as exp(log(A)).\\n17.8 Compute\\n\u0018\u000e exp\\n\\n01\\n−1 0\\n\\n\u000f\u0019\\n\\n\u0018\u000e\\n, exp\\n\\n−1 1\\n−1 −3\\n\\n\u000f\u0019\\n\\n⎛⎡\\n⎤⎞\\nπ1 1\\n, sin ⎝⎣ 0 π 1 ⎦⎠ .\\n0 0π\\n\\n17.9 Construct two matrices A, B ∈ C2,2 with exp(A + B) \u0003= exp(A) exp(B).\\n17.10 Prove the assertion on the entries of Ad in Example 17.7.\\n17.11 Let\\n⎡\\n⎤\\n511\\nA = ⎣0 5 1⎦ ∈ R3,3 .\\n004\\nCompute exp(t A) for t ∈ R and solve the homogeneous system of differential equations ẏ = Ay with the initial condition y(0) = [1, 1, 1]T .\\n17.12 Compute the matrix exp(t A) from Example 17.14 explicitly and thus show that exp(t A) ∈ R2,2 (for t ∈ R), despite the fact that the eigenvalues and eigenvectors of A are not real.</td>\n",
       "      <td>269.0</td>\n",
       "      <td>269</td>\n",
       "      <td>17 Matrix Functions and Systems  of Differential Equations</td>\n",
       "      <td>17.2 Systems of Linear Ordinary Differential Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>system linear ordinary differential equation exercise construct matrix ai j determine solution x matrix equation x classify solution primary square root determine matrix x real entry x prove lemma prove following assertion cn n det exp exp trace b h exp unitary c exp e e let diag jdm λm cn n rank determine primary matrix function f f z z function also exist rank n let log z r eiϕ r ϕ π c r eiϕ ln r iϕ principle branch complex logarithm ln denotes real natural logarithm show function defined spectrum compute log well exp log compute exp exp sin π construct two matrix b exp b exp exp b prove assertion entry ad example let compute exp r solve homogeneous system differential equation ay initial condition compute matrix exp example explicitly thus show exp r despite fact eigenvalue eigenvectors real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Chapter 18\\n\\nSpecial Classes of Endomorphisms\\n\\nIn this chapter we discuss some classes of endomorphisms (or square matrices) whose eigenvalues and eigenvectors have special properties.\\nSuch properties only exist under further assumptions, and in this chapter our assumptions concern the relationship between the given endomorphism and its adjoint endomorphism.\\nThus, we focus on Euclidean or unitary vector spaces.\\nThis leads to the classes of normal, orthogonal, unitary and selfadjoint endomorphisms.\\nEach of these classes has a natural counterpart in the set of square (real or complex) matrices.\\n\\n18.1 Normal Endomorphisms\\nWe start with the definition of a normal1 endomorphism or matrix.\\nDefinition 18.1 Let V be a finite dimensional Euclidean or unitary vector space.\\nAn endomorphism f ∈ L(V, V) is called normal if f ◦ f ad = f ad ◦ f .\\nA matrix A ∈ Rn,n or A ∈ Cn,n is called normal if A T A = A A T or A H A = A A H , respectively.\\nFor all z ∈ C we have zz = |z|2 = zz.\\nThe property of normality can therefore be interpreted as a generalization of this property of complex numbers.\\nWe will first study the properties of normal endomorphisms on a finite dimensional unitary vector space V. Recall the following results:\\n(1) If B is an orthonormal basis of V and if f ∈ L(V, V), then ([ f ] B,B ) H = [ f ad ] B,B\\n(cp.\\nTheorem 13.12).\\n(2) Every f ∈ L(V, V) can be unitarily triangulated (cp.\\nCorollary 14.20, Schur’s theorem).\\nThis does not hold in general in the Euclidean case, since not every real polynomial decomposes into linear factors over R.\\n1 This term was introduced by Otto Toeplitz (1881–1940) in 1918 in the context of bilinear forms.\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_18\\n\\n271</td>\n",
       "      <td>270.0</td>\n",
       "      <td>&lt;FEFF&gt;</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.1 Normal Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter special class endomorphisms chapter discus class endomorphisms square matrix whose eigenvalue eigenvectors special property property exist assumption chapter assumption concern relationship given endomorphism adjoint endomorphism thus focus euclidean unitary vector space lead class normal orthogonal unitary selfadjoint endomorphisms class natural counterpart set square real complex matrix normal endomorphisms start definition endomorphism matrix definition let v finite dimensional euclidean unitary vector space endomorphism f l v v called normal f f ad f ad f matrix rn n cn n called normal h h respectively z c zz zz property normality therefore interpreted generalization property complex number first study property normal endomorphisms finite dimensional unitary vector space recall following result b orthonormal basis v f l v v f b b h f ad b b cp theorem every f l v v unitarily triangulated cp corollary schur theorem hold general euclidean case since every real polynomial decomposes linear factor term introduced otto toeplitz context bilinear form springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>272\\n\\n18 Special Classes of Endomorphisms\\n\\nUsing these results we obtain the following characterization of normal endomorphisms on a unitary vector space.\\nTheorem 18.2 If V is a finite dimensional unitary vector space, then f ∈ L(V, V) is normal if and only if there exists an orthonormal basis B of V such that [ f ] B,B is a diagonal matrix, i.e., f is unitarily diagonalizable.\\nProof Let f ∈ L(V, V) be normal and let B be an orthonormal basis of V such that R := [ f ] B,B is an upper triangular matrix.\\nThen R H = [ f ad ] B,B , and from f ◦ f ad = f ad ◦ f we obtain\\nR R H = [ f ◦ f ad ] B,B = [ f ad ◦ f ] B,B = R H R.\\nWe now show by induction on n = dim(V) that R is diagonal.\\nThis is obvious for n = 1.\\nLet the assertion hold for an n ≥ 1, and let R ∈ Cn+1,n+1 be upper triangular with\\nR R H = R H R. We write R as\\n\u0003\\n\u0002\\nR1 r 1\\n,\\nR=\\n0 α1 where R1 ∈ Cn,n is upper triangular, r1 ∈ Cn,1 , and α1 ∈ C. Then\\n\u0002\\n\\nR1 R1H + r1r1H α1r1\\nα1r1H\\n|α1 |2\\n\\n\u0003\\n\\n\u0002\\n= RRH = RH R =\\n\\n\u0003\\nR1H r1\\nR1H R1\\n.\\nr1H R1 r1H r1 + |α1 |2\\n\\nFrom |α1 |2 = r1H r1 + |α1 |2 we obtain r1H r1 = 0, hence r1 = 0 and R1 R1H = R1H R1 .\\nBy the induction hypothesis, R1 ∈ Cn,n is diagonal, and therefore\\n\u0002\\nR=\\n\\nR1 0\\n0 α1\\n\\n\u0003 is diagonal as well.\\nConversely, suppose that there exists orthonormal basis B of V such that [ f ] B,B is diagonal.\\nThen [ f ad ] B,B = ([ f ] B,B ) H is diagonal and, since diagonal matrices commute, we have\\n[ f ◦ f ad ] B,B = [ f ] B,B [ f ad ] B,B = [ f ad ] B,B [ f ] B,B = [ f ad ◦ f ] B,B , which implies f ◦ f ad = f ad ◦ f , and hence f is normal.\\n\\n\u0006\\n\u0005\\n\\nThe application of this theorem to the unitary vector space V = Cn,1 with the standard scalar product and a matrix A ∈ Cn,n viewed as element of L(V, V) yields the following “matrix version”.\\nCorollary 18.3 A matrix A ∈ Cn,n is normal if and only if there exists an orthonormal basis of Cn,1 consisting of eigenvectors of A, i.e., A is unitarily diagonalizable.</td>\n",
       "      <td>271.0</td>\n",
       "      <td>271</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.1 Normal Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>special class endomorphisms using result obtain following characterization normal endomorphisms unitary vector space theorem v finite dimensional unitary vector space f l v v normal exists orthonormal basis b v f b b diagonal matrix f unitarily diagonalizable proof let f l v v normal let b orthonormal basis v r f b b upper triangular matrix r h f ad b b f f ad f ad f obtain r r h f f ad b b f ad f b b r h show induction n dim v r diagonal obvious n let assertion hold n let r upper triangular r r h r h write r r cn n upper triangular rrh rh r obtain hence induction hypothesis cn n diagonal therefore diagonal well conversely suppose exists orthonormal basis b v f b b diagonal f ad b b f b b h diagonal since diagonal matrix commute f f ad b b f b b f ad b b f ad b b f b b f ad f b b implies f f ad f ad f hence f normal application theorem unitary vector space v standard scalar product matrix cn n viewed element l v v yield following matrix version corollary matrix cn n normal exists orthonormal basis consisting eigenvectors unitarily diagonalizable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>18.1 Normal Endomorphisms\\n\\n273\\n\\nThe following theorem presents another characterization of normal endomorphisms on a unitary vector space.\\nTheorem 18.4 If V is a finite dimensional unitary vector space, then f ∈ L(V, V) is normal if and only if there exists a polynomial p ∈ C[t] with p( f ) = f ad .\\nProof If p( f ) = f ad for a polynomial p ∈ C[t], then f ◦ f ad = f ◦ p( f ) = p( f ) ◦ f = f ad ◦ f, and hence f is normal.\\nConversely, if f is normal, then there exists an orthonormal basis B of V, such that [ f ] B,B = diag(λ1 , . . . , λn ).\\nFurthermore,\\n\u0004\\n\u0005\\n[ f ad ] B,B = ([ f ] B,B ) H = diag λ1 , . . . , λn .\\nLet p ∈ C[t] be a polynomial with p(λ j ) = λ j for j = 1, . . . , n.\\nSuch a polynomial can be explicitly constructed using the Lagrange basis of C[t]≤n−1 (cp.\\nExercise 10.12).\\nThen\\n\u0005\\n\u0004\\n\u0004\\n\u0005\\n[ f ad ] B,B = diag λ1 , . . . , λn = diag p(λ1 ), . . . , p(λn ) = p(diag(λ1 , . . . , λn ))\\n\u0004\\n\u0005\\n= p [ f ] B,B = [ p( f )] B,B , and hence also f ad = p( f ).\\n\\n\u0006\\n\u0005\\n\\nSeveral other characterizations of normal endomorphisms on a finite dimensional unitary vector space and of normal matrices A ∈ Cn,n can be found in the article [HorJ12] (see also Exercise 18.8).\\nWe now consider the Euclidean case, where we focus on real square matrices.\\nAll the results can be formulated analogously for normal endomorphisms on a finite dimensional Euclidean vector space.\\nLet A ∈ Rn,n be normal, i.e., A T A = A A T .\\nThen A also satisfies A H A = A A H and when A is considered as an element of Cn,n , it is unitarily diagonalizable, i.e.,\\nA = S DS H holds for a unitary matrix S ∈ Cn,n and a diagonal matrix D ∈ Cn,n .\\nDespite the fact that A has real entries, neither S nor D will be real in general, since\\nA as an element of Rn,n may not be diagonalizable.\\nFor instance,\\n\u0002\\nA=\\n\\n\u0003\\n12\\n∈ R2,2\\n−2 1 is a normal matrix that is not diagonalizable (over R).\\nConsidered as element of C2,2 , it has the eigenvalues 1 + 2i and 1 − 2i and it is unitarily diagonalizable.\\nTo discuss the case of real normal matrices in more detail, we first prove a “real version” of Schur’s theorem.</td>\n",
       "      <td>272.0</td>\n",
       "      <td>272</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.1 Normal Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal endomorphisms following theorem present another characterization normal endomorphisms unitary vector space theorem v finite dimensional unitary vector space f l v v normal exists polynomial p c p f f ad proof p f f ad polynomial p c f f ad f p f p f f f ad f hence f normal conversely f normal exists orthonormal basis b v f b b diag λn furthermore f ad b b f b b h diag λn let p c polynomial p λ j λ j j polynomial explicitly constructed using lagrange basis c cp exercise f ad b b diag λn diag p p λn p diag λn p f b b p f b b hence also f ad p f several characterization normal endomorphisms finite dimensional unitary vector space normal matrix cn n found article see also exercise consider euclidean case focus real square matrix result formulated analogously normal endomorphisms finite dimensional euclidean vector space let rn n normal also satisfies h h considered element cn n unitarily diagonalizable d h hold unitary matrix cn n diagonal matrix cn n despite fact real entry neither real general since element rn n may diagonalizable instance normal matrix diagonalizable r considered element eigenvalue unitarily diagonalizable discus case real normal matrix detail first prove real version schur theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>274\\n\\n18 Special Classes of Endomorphisms\\n\\nTheorem 18.5 For every matrix A ∈ Rn,n there exists an orthogonal matrix\\nU ∈ Rn,n with\\n⎤\\n⎡\\nR11 . . .\\nR1m\\n⎢\\n. ⎥ n,n\\n..\\nU T AU = R = ⎣\\n. .. ⎦ ∈ R ,\\nRmm where for every j = 1, . . . , m either R j j ∈ R1,1 or\\nRjj =\\n\\n( j)\\n\\n( j) r1 r2\\n( j)\\n2,2 with r3 \b= 0.\\n( j) ( j) ∈ R r3 r4\\n\\nIn the second case R j j has, considered as complex matrix, a pair of complex conjugate eigenvalues of the form α j ± iβ j with α j ∈ R and β j ∈ R \\ {0}.\\nThe matrix R is called a real Schur form of A.\\nProof We proceed via induction on n.\\nFor n = 1 we have A = [a11 ] = R and\\nU = [1].\\nSuppose that the assertion holds for some n ≥ 1 and let A ∈ Rn+1,n+1 be given.\\nWe consider A as an element of Cn+1,n+1 .\\nThen A has an eigenvalue λ = α+iβ ∈ C,\\nα, β ∈ R, corresponding to the eigenvector v = x + iy ∈ Cn+1,1 , x, y ∈ Rn+1,1 , and we have Av = λv.\\nDividing this equation into its real and imaginary parts, we obtain the two real equations\\nAx = αx − β y and Ay = βx + αy.\\n\\n(18.1)\\n\\nWe have two cases:\\nCase 1: β = 0.\\nThen the two equations in (18.1) are Ax = αx and Ay =\\nαy.\\nThus at least one of the real vectors x or y is an eigenvector corresponding to the real eigenvalue α of A. Without loss of generality we assume that this is the vector x and that x 2 = 1.\\nWe extend x by the vectors w2 , . . . , wn+1 to an orthonormal basis of Rn+1,1 with respect to the standard scalar product.\\nThe matrix\\nU1 := [x, w2 , . . . , wn+1 ] ∈ Rn+1,n+1 then is orthogonal and satisfies\\n\u0002\\nU1T AU1 =\\n\\nα \u0002\\n0 A1\\n\\n\u0003 for a matrix A1 ∈ Rn,n .\\nBy the induction hypothesis there exists an orthogonal matrix\\nU2 ∈ Rn,n such that R1 := U2T A1 U2 has the desired form.\\nThe matrix\\n\u0002\\nU := U1 is orthogonal and satisfies\\n\\n1 0\\n0 U2\\n\\n\u0003</td>\n",
       "      <td>273.0</td>\n",
       "      <td>273</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.1 Normal Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>special class endomorphisms theorem every matrix rn n exists orthogonal matrix u rn n n n u au r r rmm every j either r j j rjj j j j j j r second case r j j considered complex matrix pair complex conjugate eigenvalue form α j iβ j α j r β j r matrix r called real schur form proof proceed via induction n r u suppose assertion hold n let given consider element eigenvalue λ c α β r corresponding eigenvector v x iy x av λv dividing equation real imaginary part obtain two real equation ax αx β ay βx αy two case case β two equation ax αx ay αy thus least one real vector x eigenvector corresponding real eigenvalue α without loss generality assume vector x x extend x vector orthonormal basis respect standard scalar product matrix x orthogonal satisfies α matrix rn n induction hypothesis exists orthogonal matrix rn n desired form matrix u orthogonal satisfies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>18.1 Normal Endomorphisms\\n\\n\u0002\\nU T AU =\\n\\n275\\n\\n1 0\\n0 U2T\\n\\n\u0003\\n\\n\u0002\\nU1T AU1\\n\\n1 0\\n0 U2\\n\\n\u0003\\n\\n\u0002\\n=\\n\\nα \u0002\\n0 R1\\n\\n\u0003\\n=: R, where R has the desired form.\\nCase 2: β \b= 0.\\nWe first show that x, y are linearly independent.\\nIf x = 0, then using β \b= 0 in the first equation in (18.1) implies that also y = 0.\\nThis is not possible, since the eigenvector v = x + iy must be nonzero.\\nThus, x \b= 0, and using β \b= 0 in the second equation in (18.1) implies that also y \b= 0.\\nIf x, y ∈ Rn,1 \\ {0} are linearly dependent, then there exists a μ ∈ R \\ {0} with x = μy.\\nThe two equations in (18.1) then can be written as\\nAx = (α − βμ)x and Ax =\\n\\n1\\n(β + αμ)x,\\nμ which implies that β(1 + μ2 ) = 0.\\nSince 1 + μ2 \b= 0 for all μ ∈ R, this implies\\nβ = 0, which contradicts the assumption that β \b= 0.\\nConsequently, x, y are linearly independent.\\nWe can combine the two equations in (18.1) to the system\\n\u0002\\n\u0003\\nαβ\\nA[x, y] = [x, y]\\n,\\n−β α where rank([x, y]) = 2.\\nApplying the Gram-Schmidt method with respect to the standard scalar product of Rn+1,1 to the matrix [x, y] ∈ Rn+1,2 yields\\n[x, y] = [q1 , q2 ]\\n\\n\u0003\\n\u0002 r11 r12\\n=: Q R1 ,\\n0 r22 with Q T Q = I2 and R1 ∈ G L 2 (R).\\nIt then follows that\\nAQ = A[x, y]R1−1 = [x, y]\\n\\n\u0002\\n\\n\u0002\\n\u0003\\n\u0003\\nαβ\\nαβ\\nR1−1 .\\nR1−1 = Q R1\\n−β α\\n−β α\\n\u0002\\n\\nThe real matrix\\nR2 := R1\\n\\n\u0003\\nαβ\\nR1−1\\n−β α has, considered as element of C2,2 , the pair of complex conjugate eigenvalues α ± iβ with β \b= 0.\\nIn particular, the (2, 1)-entry of R2 is nonzero, since otherwise R2 would have two real eigenvalues.\\nWe again extend q1 , q2 by vectors w3 , . . . , wn+1 to an orthonormal basis of Rn+1,1 with respect to the standard scalar product.\\n(For n = 1 the list w3 , . . . , wn+1 is empty.)\\nThen U1 := [Q, w3 , . . . , wn+1 ] ∈ Rn+1,n+1 is orthogonal and we have\\n\u000f\\n\u000f\\n\u000e\\n\u000e\\nU1T AU1 = U1T AQ, A[w3 , . . . , wn+1 ] = U1T Q R2 , A[w3 , . . . , wn+1 ] =\\n\\n\u0002\\n\\nR2 \u0002\\n0 A1\\n\\n\u0003</td>\n",
       "      <td>274.0</td>\n",
       "      <td>274</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.1 Normal Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal endomorphisms u au α r r desired form case β first show x linearly independent x using β first equation implies also possible since eigenvector v x iy must nonzero thus x using β second equation implies also x linearly dependent exists μ r x μy two equation written ax α βμ x ax β αμ x μ implies β since μ r implies β contradicts assumption β consequently x linearly independent combine two equation system αβ x x α rank x applying gram-schmidt method respect standard scalar product matrix x yield x q q q g l r follows aq x x αβ αβ q α α real matrix αβ α considered element pair complex conjugate eigenvalue α iβ β particular nonzero since otherwise would two real eigenvalue extend vector orthonormal basis respect standard scalar product n list empty q orthogonal aq q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>276\\n\\n18 Special Classes of Endomorphisms for a matrix A1 ∈ Rn−1,n−1 .\\nAnalogously to the first case, an application of the induction hypothesis to this matrix yields the desired matrices R and U .\\n\u0005\\n\u0006\\nTheorem 18.5 implies the following result for real normal matrices.\\nCorollary 18.6 A matrix A ∈ Rn,n is normal if and only if there exists an orthogonal matrix U ∈ Rn,n with\\nU T AU = diag(R1 , . . . , Rm ), where, for every j = 1, . . . , m either R j ∈ R1,1 or\\n\u0002\\nRj =\\n\\nαj βj\\n−β j α j\\n\\n\u0003\\n∈ R2,2 with β j \b= 0.\\n\\nIn the second case the matrix R j has, considered as complex matrix, a pair of complex conjugate eigenvalues of the form α j ± iβ j .\\n\u0006\\n\u0005\\n\\nProof Exercise.\\nExample 18.7 The matrix\\n√ ⎤\\n√\\n⎡\\n0 2− 2\\n1 ⎣ √\\nA=\\n−√2 1\\n1 ⎦ ∈ R3,3\\n2\\n2 1\\n1 has, considered as a complex matrix, the eigenvalues 1, i, −i.\\nIt is therefore neither diagonalizable nor can it be triangulated over R. For the orthogonal matrix\\n⎤\\n0 2 √0\\n√\\n1\\nU = ⎣ −√2 0 √2 ⎦ ∈ R3,3\\n2\\n20 2\\n⎡ the transformed matrix\\n\\n⎡\\n\\n⎤\\n010\\nU T AU = ⎣ −1 0 0 ⎦\\n001 is in real Schur form.\\n\\n18.2 Orthogonal and Unitary Endomorphisms\\nIn this section we extend the concept of orthogonal and unitary matrices to endomorphisms.</td>\n",
       "      <td>275.0</td>\n",
       "      <td>275</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.2 Orthogonal and Unitary Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>special class endomorphisms matrix analogously first case application induction hypothesis matrix yield desired matrix r u theorem implies following result real normal matrix corollary matrix rn n normal exists orthogonal matrix u rn n u au diag rm every j either r j rj αj βj j α j β j second case matrix r j considered complex matrix pair complex conjugate eigenvalue form α j iβ j proof exercise example matrix considered complex matrix eigenvalue therefore neither diagonalizable triangulated orthogonal matrix u transformed matrix u au real schur form orthogonal unitary endomorphisms section extend concept orthogonal unitary matrix endomorphisms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>18.2 Orthogonal and Unitary Endomorphisms\\n\\n277\\n\\nDefinition 18.8 Let V be a finite dimensional Euclidean or unitary vector space.\\nAn endomorphism f ∈ L(V, V) is called orthogonal or unitary, respectively, if f ad ◦ f = IdV .\\nIf f ad ◦ f = IdV , then f ad ◦ f is bijective and hence f is injective (cp.\\nExercise 2.7).\\nCorollary 10.11 implies that f is bijective.\\nHence f ad is the unique inverse of f , and we also have f ◦ f ad = IdV (cp. our remarks following Definition 2.21).\\nNote that an orthogonal or unitary endomorphism f is normal, and therefore all results from the previous section also apply to f .\\nLemma 18.9 Let V be a finite dimensional Euclidean or unitary vector space and let f ∈ L(V, V) be orthogonal or unitary, respectively.\\nIf B is an orthonormal basis of V, then [ f ] B,B is an orthogonal or unitary matrix, respectively.\\nProof Let dim(V) = n.\\nFor every orthonormal basis B of V we have\\nIn = [IdV ] B,B = [ f ad ◦ f ] B,B = [ f ad ] B,B [ f ] B,B = ([ f ] B,B ) H [ f ] B,B , and thus [ f ] B,B is orthogonal or unitary, respectively.\\n(In the Euclidean case\\n\u0006\\n\u0005\\n([ f ] B,B ) H = ([ f ] B,B )T .)\\nIn the following theorem we show that an orthogonal or unitary endomorphism is characterized by the fact that it does not change the scalar product of arbitrary vectors.\\nLemma 18.10 Let V be a finite dimensional Euclidean or unitary vector space with the scalar product ·, · .\\nThen f ∈ L(V, V) is orthogonal or unitary, respectively, if and only if f (v), f (w) = v, w for all v, w ∈ V.\\nProof If f is orthogonal or unitary and if v, w ∈ V, then\\n\u0010\\n\u0011 v, w = IdV (v), w = ( f ad ◦ f )(v), w = f (v), f (w) .\\nOn the other hand, suppose that v, w = f (v), f (w) for all v, w ∈ V. Then\\n\u0011\\n\u0010\\n0 = v, w − f (v), f (w) = v, w − v, ( f ad ◦ f )(w)\\n\u0010\\n\u0011\\n= v, (IdV − f ad ◦ f )(w) .\\nSince the scalar product is non-degenerate and v can be chosen arbitrarily, we have\\n\u0006\\n\u0005\\n(IdV − f ad ◦ f )(w) = 0 for all w ∈ V, and hence IdV = f ad ◦ f .\\nWe have the following corollary (cp.\\nLemma 12.13).\\nCorollary 18.11 If V is a finite dimensional Euclidean or unitary vector space with the scalar product ·, · , f ∈ L(V, V) is orthogonal or unitary, respectively, and\\n· = ·, · 1/2 is the norm induced by the scalar product, then f (v) = v for all v ∈ V.</td>\n",
       "      <td>276.0</td>\n",
       "      <td>276</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.2 Orthogonal and Unitary Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>orthogonal unitary endomorphisms definition let v finite dimensional euclidean unitary vector space endomorphism f l v v called orthogonal unitary respectively f ad f idv f ad f idv f ad f bijective hence f injective cp exercise corollary implies f bijective hence f ad unique inverse f also f f ad idv cp remark following definition note orthogonal unitary endomorphism f normal therefore result previous section also apply f lemma let v finite dimensional euclidean unitary vector space let f l v v orthogonal unitary respectively b orthonormal basis v f b b orthogonal unitary matrix respectively proof let dim v every orthonormal basis b v idv b b f ad f b b f ad b b f b b f b b h f b b thus f b b orthogonal unitary respectively euclidean case f b b h f b b following theorem show orthogonal unitary endomorphism characterized fact change scalar product arbitrary vector lemma let v finite dimensional euclidean unitary vector space scalar product f l v v orthogonal unitary respectively f v f w v w v w proof f orthogonal unitary v w v v w idv v w f ad f v w f v f w hand suppose v w f v f w v w v w f v f w v w v f ad f w v idv f ad f w since scalar product non-degenerate v chosen arbitrarily idv f ad f w w v hence idv f ad f following corollary cp lemma corollary v finite dimensional euclidean unitary vector space scalar product f l v v orthogonal unitary respectively norm induced scalar product f v v v v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>278\\n\\n18 Special Classes of Endomorphisms\\n\\nFor the vector space V = Cn,1 with the standard scalar product and induced norm v 2 = (v H v)1/2 as well as a unitary matrix A ∈ Cn,n , we have Av 2 = v 2 for all v ∈ Cn,1 .\\nThus,\\nAv 2\\nA 2 = sup\\n=1 v 2 v∈Cn,1 \\{0}\\n(cp.\\n(6) in Example 12.4).\\nThis holds analogously for orthogonal matrices A ∈ Rn,n .\\nWe now study the eigenvalues and eigenvectors of orthogonal and unitary endomorphisms.\\nLemma 18.12 Let V be a finite dimensional Euclidean or unitary vector space and let f ∈ L(V, V) be orthogonal or unitary, respectively.\\nIf λ is an eigenvalue of f , then |λ| = 1.\\nProof Let ·, · be the scalar product on V. If f (v) = λv with v \b= 0, then v, v = IdV (v), v = ( f ad ◦ f )(v), v = f (v), f (v) = λv, λv = |λ|2 v, v , and v, v \b= 0 implies that |λ| = 1.\\n\\n\u0006\\n\u0005\\n\\nThe statement of Lemma 18.12 holds, in particular, for unitary and orthogonal matrices.\\nHowever, one should keep in mind that an orthogonal matrix (or an orthogonal endomorphism) may not have an eigenvalue.\\nFor example, the orthogonal matrix\\n\u0002\\nA=\\n\\n\u0003\\n0 −1\\n∈ R2,2\\n1 0 has the characteristic polynomial PA = t 2 + 1, which has no real roots.\\nIf considered as an element of C2,2 , the matrix A has the eigenvalues i and −i.\\nTheorem 18.13\\n(1) If A ∈ Cn,n is unitary, then there exists a unitary matrix U ∈ Cn,n with\\nU H AU = diag(λ1 , . . . , λn ) and |λ j | = 1 for j = 1, . . . , n.\\n(2) If A ∈ Rn,n is orthogonal, then there exists an orthogonal matrix U ∈ Rn,n with\\nU T AU = diag(R1 , . . . , Rm ), where for every j = 1, . . . , m either R j = [λ j ] ∈ R1,1 with λ j = ±1 or\\n\u0002 cj sj\\nRj =\\n−s j c j\\n\\n\u0003\\n∈ R2,2 with s j \b= 0 and c2j + s 2j = 1.</td>\n",
       "      <td>277.0</td>\n",
       "      <td>277</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.2 Orthogonal and Unitary Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>special class endomorphisms vector space v standard scalar product induced norm v v h v well unitary matrix cn n av v v thus av sup v cp example hold analogously orthogonal matrix rn n study eigenvalue eigenvectors orthogonal unitary endomorphisms lemma let v finite dimensional euclidean unitary vector space let f l v v orthogonal unitary respectively λ eigenvalue f proof let scalar product f v λv v v v idv v v f ad f v v f v f v λv λv v v v v implies statement lemma hold particular unitary orthogonal matrix however one keep mind orthogonal matrix orthogonal endomorphism may eigenvalue example orthogonal matrix characteristic polynomial pa real root considered element matrix eigenvalue theorem cn n unitary exists unitary matrix u cn n u h au diag λn j j rn n orthogonal exists orthogonal matrix u rn n u au diag rm every j either r j λ j λ j cj sj rj j c j j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>18.2 Orthogonal and Unitary Endomorphisms\\n\\n279\\n\\nProof\\n(1) A unitary matrix A ∈ Cn,n is normal and hence unitarily diagonalizable (cp.\\nCorollary 18.3).\\nBy Lemma 18.12, all eigenvalues of A have absolute value 1.\\n(2) An orthogonal matrix A is normal and hence by Corollary 18.6 there exists an orthogonal matrix U ∈ Rn,n with U T AU = diag(R1 , . . . , Rm ), where either\\nR j ∈ R1,1 or\\n\u0003\\n\u0002\\nαj βj\\n∈ R2,2\\nRj =\\n−β j α j with β j \b= 0.\\nIn the first case then R j = [λ j ] with |λ j | = 1 by Lemma 18.12.\\nSince A and U are orthogonal, also U T AU is orthogonal, and hence every diagonal block R j is orthogonal as well.\\nFrom R Tj R j = I2 we obtain α2j +β 2j = 1,\\n\u0006\\n\u0005 so that R j has the desired form.\\nWe now study two important classes of orthogonal matrices.\\nExample 18.14 Let i, j, n ∈ N with 1 ≤ i &lt; j ≤ n and let α ∈ R. We define\\n⎡\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\nRi j (α) := ⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎣\\n\\n⎤\\n\\n1\\n..\\n\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥ ←i\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥ ← j\\n⎥\\n⎥\\n⎥\\n⎥\\n⎦\\n\\n.\\n1\\n− sin(α) cos(α)\\n1\\n..\\n\\n.\\n1 sin(α) cos(α)\\n1\\n..\\n\\n.\\n1\\n\\n↑ i\\n\\n.\\n\\n↑ j\\n\\nThe matrix Ri j (α) = [ri j ] ∈ Rn,n is equal to the identity matrix In except for its entries rii = cos(α), ri j = − sin(α), r ji = sin(α), r j j = cos(α).\\nFor n = 2 we have the matrix\\n\u0002\\nR12 (α) =\\n\\n\u0003 cos(α) − sin(α)\\n, sin(α) cos(α)</td>\n",
       "      <td>278.0</td>\n",
       "      <td>278</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.2 Orthogonal and Unitary Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>orthogonal unitary endomorphisms proof unitary matrix cn n normal hence unitarily diagonalizable cp corollary lemma eigenvalue absolute value orthogonal matrix normal hence corollary exists orthogonal matrix u rn n u au diag rm either r j αj βj rj j α j β j first case r j λ j j lemma since u orthogonal also u au orthogonal hence every diagonal block r j orthogonal well r tj r j obtain r j desired form study two important class orthogonal matrix example let j n n j n let α define ri j α j sin α co α sin α co α j matrix ri j α ri j rn n equal identity matrix except entry rii co α ri j sin α r ji sin α r j j co α n matrix α co α sin α sin α co α</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>280\\n\\n18 Special Classes of Endomorphisms which satisfies\\n\u0002\\nR12 (α)T R12 (α) =\\n= cos(α) sin(α)\\n− sin(α) cos(α)\\n\\n\u0003\u0002 cos(α) − sin(α) sin(α) cos(α)\\n\\n\u0003\\n\\n\u0003\\n\u0002 2\\n0 cos (α) + sin2 (α)\\n0 cos2 (α) + sin2 (α)\\n\\n= I2 = R12 (α)R12 (α)T .\\nOne easily sees that each of the matrices Ri j (α) ∈ Rn,n is orthogonal.\\nThe multiplication of a vector v ∈ Rn,1 with the matrix Ri j (α) results in a (counterclockwise) rotation of v by the angle α in the (i, j)-coordinate plane.\\nIn Numerical Mathematics, the matrices Ri j (α) are called Givens rotations.2 This is illustrated in the figure below for the vector v = [1.0, 0.75]T ∈ R2,1 and the matrices R12 (π/2) and\\n), which represent rotations by 90 and 120 degrees, respectively.\\nR12 ( 2π\\n3\\n\\nExample 18.15 For u ∈ Rn,1 \\ {0} we define the Householder matrix\\nH (u) := In −\\n\\n2 uT u uu T ∈ Rn,n ,\\n\\n(18.2) and for u = 0 we set H (0) := In .\\nFor every u ∈ Rn,1 then H (u) is an orthogonal matrix (cp.\\nExercise 12.17).\\nThe multiplication of a vector v ∈ Rn,1 with the matrix\\nH (u) describes a reflection of v at the hyperplane\\n(span{u})⊥ =\\n\\n\u0012\\n\\n\u0013 y ∈ Rn,1 | u T y = 0 , i.e., the hyperplane of vectors that are orthogonal to u with respect to the standard scalar product.\\nThis is illustrated in the figure below for the vector v = [1.75, 0.5]T ∈\\nR2,1 and the Householder matrix\\n\u0002 \u0003\\n01\\nH (u) =\\n,\\n10 which corresponds to u = [−1, 1]T ∈ R2,1 .\\n2 Wallace\\n\\nGivens (1910–1993), pioneer of Numerical Linear Algebra.</td>\n",
       "      <td>279.0</td>\n",
       "      <td>279</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.2 Orthogonal and Unitary Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>special class endomorphisms satisfies α α co α sin α sin α co α co α sin α sin α co α co α α α α α α one easily see matrix ri j α rn n orthogonal multiplication vector v matrix ri j α result counterclockwise rotation v angle α j plane numerical mathematics matrix ri j α called given illustrated figure vector v matrix represent rotation degree respectively example u define householder matrix h u ut u uu rn n u set h every u h u orthogonal matrix cp exercise multiplication vector v matrix h u describes reflection v hyperplane span u u hyperplane vector orthogonal u respect standard scalar product illustrated figure vector v householder matrix h u corresponds u wallace given pioneer numerical linear algebra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>18.2 Orthogonal and Unitary Endomorphisms\\n\\n281\\n\\nMATLAB-Minute.\\nLet u = [5, 3, 1]T ∈ R3,1 .\\nApply the command norm(u) to compute the Euclidean norm of u and form the Householder matrix H=eye(3)(2/(u’∗u))∗(u∗u’).\\nCheck the orthogonality of H via the computation of norm(H’∗H-eye(3)).\\nForm the vector v=H∗u and compare the Euclidean norms of u and v.\\n\\n18.3 Selfadjoint Endomorphisms\\nWe have already studied selfadjoint endomorphisms f on a finite dimensional Euclidean or unitary vector space.\\nThe defining property for this class of endomorphisms is f = f ad (cp.\\nDefinition 13.13).\\nObviously, selfadjoint endomorphisms are normal and hence the results of\\nSect.\\n18.1 hold.\\nWe now strengthen some of these results.\\nLemma 18.16 For a finite dimensional Euclidean or unitary vector space V and f ∈ L(V, V), the following statements are equivalent:\\n(1) f is selfadjoint.\\n(2) For every orthonormal basis B of V we have [ f ] B,B = ([ f ] B,B ) H .\\n(3) There exists an orthonormal basis B of V with [ f ] B,B = ([ f ] B,B ) H .\\n(In the Euclidean case ([ f ] B,B ) H = ([ f ] B,B )T .)\\nProof In Corollary 13.14 we have already shown that (1) implies (2), and obviously (2) implies (3).\\nIf (3) holds, then [ f ] B,B = ([ f ] B,B ) H = [ f ad ] B,B (cp.\\nTheorem 13.12), and hence f = f ad , so that (1) holds.\\n\u0006\\n\u0005\\nWe have the following strong result on the diagonalizability of selfadjoint endomorphisms in both the Euclidean and the unitary case.\\nTheorem 18.17 If V is a finite dimensional Euclidean or unitary vector space and f ∈ L(V, V) is selfadjoint, then there exists an orthonormal basis B of V such that\\n[ f ] B,B is a real diagonal matrix.</td>\n",
       "      <td>280.0</td>\n",
       "      <td>280</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.3 Selfadjoint Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>orthogonal unitary endomorphisms matlab-minute let u apply command norm u compute euclidean norm u form householder matrix u check orthogonality h via computation norm h form vector compare euclidean norm u selfadjoint endomorphisms already studied selfadjoint endomorphisms f finite dimensional euclidean unitary vector space defining property class endomorphisms f f ad cp definition obviously selfadjoint endomorphisms normal hence result sect hold strengthen result lemma finite dimensional euclidean unitary vector space v f l v v following statement equivalent f selfadjoint every orthonormal basis b v f b b f b b h exists orthonormal basis b v f b b f b b h euclidean case f b b h f b b proof corollary already shown implies obviously implies hold f b b f b b h f ad b b cp theorem hence f f ad hold following strong result diagonalizability selfadjoint endomorphisms euclidean unitary case theorem v finite dimensional euclidean unitary vector space f l v v selfadjoint exists orthonormal basis b v f b b real diagonal matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>282\\n\\n18 Special Classes of Endomorphisms\\n\\nProof Consider first the unitary case.\\nIf f is selfadjoint, then f is normal and hence unitarily diagonalizable (cp.\\nTheorem 18.2).\\nLet B be an orthonormal basis of V so that [ f ] B,B is a diagonal matrix.\\nThen [ f ] B,B = [ f ad ] B,B = ([ f ] B,B ) H implies that the diagonal entries of [ f ] B,B , which are the eigenvalues of f , are real.\\nLet V be an n-dimensional Euclidean vector space.\\nIf \u0014\\nB = {v1 , . . . , vn } is an is symmetric and in particular normal.\\nBy Corolorthonormal basis of V, then [ f ] \u0014\\n\u0014\\nB, B lary 18.6, there exists an orthogonal matrix U = [u i j ] ∈ Rn,n with\\nU T [ f ]\u0014\\nB, \u0014\\nB U = diag(R1 , . . . , Rm ), where for j = 1, . . . , m either R j ∈ R1,1 or\\n\u0002\\n\\nαj βj\\nRj =\\n−β j α j\\n\\n\u0003\\n∈ R2,2 with β j \b= 0.\\n\\nSince U T [ f ] \u0014\\nB, \u0014\\nB U is symmetric, a 2 × 2 block R j with β j \b = 0 cannot occur.\\nThus,\\nU is a real diagonal matrix.\\nU T [ f ]\u0014\\nB, \u0014\\nB\\nWe define the basis B = {w1 , . . . , wn } of V by\\n(w1 , . . . , wn ) = (v1 , . . . , vn )U.\\nT\\n−1\\n= [IdV ] \u0014\\nThen, by construction, U = [IdV ] B, \u0014\\nB and hence U = U\\nB,B .\\nTherefore,\\nT\\nU\\n=\\n[ f\\n]\\n.\\nIf\\n·,\\n· is the scalar product on\\nV, then vi , v j = δi j ,\\nU [ f ]\u0014\\n\u0014\\nB,B\\nB, B i, j = 1, . . . , n.\\nWith U T U = In we get wi , w j = n\\n\u0015\u0016 k=1 u ki vk , n\\n\u0016 n n \u0016 n\\n\u0017 \u0016\\n\u0016 u \u0003j v\u0003 = u ki u \u0003j vk , v\u0003 = u ki u k j = δi j .\\n\\n\u0003=1 k=1 \u0003=1\\n\\nHence B is an orthonormal basis of V.\\nk=1\\n\\n\u0006\\n\u0005\\n\\nThis theorem has the following “matrix version”.\\nCorollary 18.18\\n(1) If A ∈ Rn,n is symmetric, then there exist an orthogonal matrix U ∈ Rn,n and a diagonal matrix D ∈ Rn,n with A = U DU T .\\n(2) If A ∈ Cn,n is Hermitian, then there exist a unitary matrix U ∈ Cn,n and a diagonal matrix D ∈ Rn,n with A = U DU H .\\nThe statement (1) in this corollary is known as the principal axes transformation.\\nWe will briefly discuss the background of this name from the theory of bilinear forms and their applications in geometry.\\nA symmetric matrix A = [ai j ] ∈ Rn,n defines a symmetric bilinear form on Rn,1 via</td>\n",
       "      <td>281.0</td>\n",
       "      <td>281</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.3 Selfadjoint Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>special class endomorphisms proof consider first unitary case f selfadjoint f normal hence unitarily diagonalizable cp theorem let b orthonormal basis v f b b diagonal matrix f b b f ad b b f b b h implies diagonal entry f b b eigenvalue f real let v n-dimensional euclidean vector space b vn symmetric particular normal corolorthonormal basis v f b b lary exists orthogonal matrix u u j rn n u f b b u diag rm j either r j αj βj rj j α j β j since u f b b u symmetric block r j β j occur thus u real diagonal matrix u f b b define basis b wn v wn vn idv construction u idv b b hence u u b b therefore u f scalar product v vi v j δi j u f b b b b j u u get wi w j n u ki vk n n n n u u ki u vk u ki u k j δi j hence b orthonormal basis theorem following matrix version corollary rn n symmetric exist orthogonal matrix u rn n diagonal matrix rn n u du cn n hermitian exist unitary matrix u cn n diagonal matrix rn n u du h statement corollary known principal ax transformation briefly discus background name theory bilinear form application geometry symmetric matrix ai j rn n defines symmetric bilinear form via</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>18.3 Selfadjoint Endomorphisms\\n\\n283\\n\\nβ A : Rn,1 × Rn,1 → R, (x, y) \u0012→ y T Ax = n n \u0016\\n\u0016 ai j xi y j .\\ni=1 j=1\\n\\nThe map q A : Rn,1 → R, x \u0012→ β A (x, x) = x T Ax, is called the quadratic form associated with this symmetric bilinear form.\\nSince A is symmetric, there exists an orthogonal matrix U = [u 1 , . . . , u n ] such that U T AU = D is a real diagonal matrix.\\nIf B1 = {e1 , . . . , en }, then [β A ] B1 ×B1 = A.\\nThe set B2 = {u 1 , . . . , u n } forms an orthonormal basis of Rn,1 with respect to the standard scalar product, and [u 1 , . . . , u n ] = [e1 , . . . , en ]U , hence U = [IdRn,1 ] B2 ,B1 .\\nFor the change of bases from of B1 to B2 we obtain\\n\u0005T\\n\u0004\\n[β A ] B2 ×B2 = [IdRn,1 ] B2 ,B1 [β A ] B1 ×B1 [IdRn,1 ] B2 ,B1 = U T AU = D\\n(cp.\\nTheorem 11.14).\\nThus, the real diagonal matrix D represents the bilinear form\\nβ A defined by A with respect to the basis B2 .\\nThe quadratic form q A associated with β A is also transformed to a simpler form by this change of bases, since analogously\\n⎡ q A (x) = x T Ax = x T U DU T x = y T Dy = n\\n\u0016\\n\\nλi yi2 = q D (y), i=1\\n\\n⎤ y1\\n⎢ ⎥ y = ⎣ ... ⎦ := U T x.\\nyn\\n\\nThus, the quadratic form q A is turned into a “sum of squares”, defined by the quadratic form q D .\\nThe principal axes transformation is given by the change of bases from the canonical basis of Rn,1 to the basis given by the pairwise orthonormal eigenvectors of A in\\nRn,1 .\\nThe n pairwise orthogonal subspaces span{u j }, j = 1, . . . , n, form the n principal axes.\\nThe geometric interpretation of this term is illustrated in the following example.\\nExample 18.19 For the symmetric matrix\\nA=\\n\u0002 we have\\nU T AU =\\n\\n\u0002 \u0003\\n41\\n∈ R2,2\\n12\\n\\n√\\n\u0003\\n0√\\n3+ 2\\n= D,\\n0\\n3− 2</td>\n",
       "      <td>282.0</td>\n",
       "      <td>282</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.3 Selfadjoint Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>selfadjoint endomorphisms β r x ax n n ai j xi j map q r x β x x x ax called quadratic form associated symmetric bilinear form since symmetric exists orthogonal matrix u u u n u au real diagonal matrix en β set u u n form orthonormal basis respect standard scalar product u u n en u hence u change base obtain β β u au cp theorem thus real diagonal matrix represents bilinear form β defined respect basis quadratic form q associated β also transformed simpler form change base since analogously q x x ax x u du x dy n λi q u yn thus quadratic form q turned sum square defined quadratic form q principal ax transformation given change base canonical basis basis given pairwise orthonormal eigenvectors n pairwise orthogonal subspace span u j j n form n principal ax geometric interpretation term illustrated following example example symmetric matrix u au</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>284\\n\\n18 Special Classes of Endomorphisms with the orthogonal matrix U = [u 1 , u 2 ] ∈ R2,2 and\\n\u0002 \u0003\\n\u0002\\n\u0003 c\\n−s\\n, u2 =\\n, where u1 = s c\\n√\\n1+ 2\\n1 c= \u0018\\n= 0.9239, s = \u0018\\n= 0.3827.\\n√\\n√\\n(1 + 2)2 + 1\\n(1 + 2)2 + 1\\n(The numbers here are rounded to the fourth significant digit.)\\nWith the associated quadratic form q A (x) = 4x12 + 2x1 x2 + 2x22 , we define the set\\nE A = {x ∈ R2,1 | q A (x) − 1 = 0}.\\nAs described above, the principal axes transformation consists in the transformation from the canonical coordinate system to a coordinate system given by an orthonormal basis of eigenvectors of A. If we carry out this transformation and replace q A by the quadratic form q D , we get the set\\n\u0012\\n\\nED = y ∈ R\\n\\n2,1\\n\\n\u0019\\n| q D (y) − 1 = 0 = [y1 , y2 ]T ∈ R2,1\\n\u0013 where β1 =\\n\\n\u001c",
       "\\n1\\n√ = 0.4760, β2 =\\n3+ 2\\n\\n\u001a 2\\n\u001b\\n\u001a y1 y22\\n\u001a\\n\u001a β2 + β2 − 1 = 0 ,\\n1\\n2\\n\\n1\\n√ = 0.7941.\\n3− 2\\n\\nThis set forms the ellipse centered at the origin of the two dimensional cartesian coordinate system (spanned by the canonical basis vectors e1 , e2 ) with axes of lengths\\nβ1 and β2 , which is illustrated on the left part of the following figure:\\n\\nThe elements x ∈ E A are given by x = U y for y ∈ E D .\\nThe orthogonal matrix\\n\u0002\\nU= c −s s c\\n\\n\u0003</td>\n",
       "      <td>283.0</td>\n",
       "      <td>283</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.3 Selfadjoint Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>special class endomorphisms orthogonal matrix u u u c c number rounded fourth significant digit associated quadratic form q x define set e x q x described principal ax transformation consists transformation canonical coordinate system coordinate system given orthonormal basis eigenvectors carry transformation replace q quadratic form q get set ed r q set form ellipse centered origin two dimensional cartesian coordinate system spanned canonical basis vector ax length illustrated left part following figure element x e given x u e orthogonal matrix c c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>18.3 Selfadjoint Endomorphisms\\n\\n285 is a Givens rotation that rotates the ellipse E D counterclockwise by the angle cos−1 (c) = 0.3926 (approximately 22.5 degrees).\\nHence E A is just a “rotated version” of E D .\\nThe right part of the figure above shows the ellipse E A in the cartesian coordinate system.\\nThe dashed lines indicate the respective spans of the vectors u 1 and u 2 , which are the eigenvectors of A and the principal axes of the ellipse E A .\\nLet A ∈ Rn,n be symmetric.\\nFor a given vector v ∈ Rn,1 and a scalar α ∈ R,\\nQ(x) = x T Ax + v T x + α, x ∈ Rn,1 is a quadratic function in n variables (the entries of the vector x).\\nThe set of zeros of this function, i.e., the set {x ∈ Rn,1 | Q(x) = 0}, is called a hypersurface of degree\\n2 or a quadric.\\nIn Example 18.19 we have already seen quadrics in the case n = 2 and with v = 0.\\nWe next give some further examples.\\nExample 18.20\\n(1) Let n = 3, A = I3 , v = [0, 0, 0]T and α = −1.\\nThe corresponding quadric\\n\u0012\\n\\n[x1 , x2 , x3 ]T ∈ R3,1 | x12 + x22 + x32 − 1 = 0\\n\\n\u0013 is the surface of the ball with radius 1 around the origin:\\n\\n\u0003\\n10\\n, v = [0, 2]T and α = 0.\\nThe corresponding quadric\\n00\\n\\n\u0002\\n(2) Let n = 2, A =\\n\\n\u0012\\n\u0013\\n[x1 , x2 ]T ∈ R2,1 | x12 + 2x2 = 0 is a parabola:</td>\n",
       "      <td>284.0</td>\n",
       "      <td>284</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.3 Selfadjoint Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>selfadjoint endomorphisms given rotation rotates ellipse e counterclockwise angle c approximately degree hence e rotated version e right part figure show ellipse e cartesian coordinate system dashed line indicate respective span vector u u eigenvectors principal ax ellipse e let rn n symmetric given vector v scalar α r q x x ax v x α x quadratic function n variable entry vector x set zero function set x q x called hypersurface degree quadric example already seen quadric case n v next give example example let n v α corresponding quadric surface ball radius around origin v α corresponding quadric let n parabola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>286\\n\\n18 Special Classes of Endomorphisms\\n\\n⎡\\n\\n⎤\\n100\\n(3) Let n = 3, A = ⎣0 0 0⎦, v = [0, 2, 0]T and α = 0.\\nThe corresponding quadric\\n000\\n\u0012\\n\\n[x1 , x2 , x3 ]T ∈ R3,1 | x12 + 2x2 = 0\\n\\n\u0013 is a parabolic cylinder:\\n\\nCorollary 18.18 motivates the following definition.\\nDefinition 18.21 If A ∈ Rn,n is symmetric or A ∈ Cn,n is Hermitian with n + positive, n − negative and n 0 zero eigenvalues (counted with their corresponding multiplicities), then the triple (n + , n − , n 0 ) is called the inertia of A.\\nLet us first consider, for simplicity, only the case of real symmetric matrices.\\nLemma 18.22 If A ∈ Rn,n symmetric has the inertia (n + , n − , n 0 ), then A and\\nS A = diag(In + , −In − , 0n 0 ) are congruent.\\nProof Let A ∈ Rn,n be symmetric and let A = U \u0004U T with an orthogonal matrix\\nU ∈ Rn,n and \u0004 = diag(λ1 , . . . , λn ) ∈ Rn,n .\\nIf A has the inertia (n + , n − , n 0 ), then we can assume without loss of generality that\\n⎡\\n\u0004n +\\n\u0004n −\\n\u0004=⎣\\n\\n⎤\\n⎦ = diag(\u0004n + , \u0004n − , 0n 0 ),\\n0n 0 where the diagonal matrices \u0004n + and \u0004n − contain the positive and negative eigenvalues of A, respectively, and 0n 0 ∈ Rn 0 ,n 0 .\\nWe have \u0004 = \u0005S A \u0005, where\\nS A := diag(In + , −In − , 0n 0 ) ∈ Rn,n ,\\n\u0005 := diag((\u0004n + )1/2 , (−\u0004n − )1/2 , In 0 ) ∈ G L n (R).</td>\n",
       "      <td>285.0</td>\n",
       "      <td>285</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.3 Selfadjoint Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>special class endomorphisms let n v α corresponding quadric parabolic cylinder corollary motivates following definition definition rn n symmetric cn n hermitian n positive n negative n zero eigenvalue counted corresponding multiplicity triple n n n called inertia let u first consider simplicity case real symmetric matrix lemma rn n symmetric inertia n n n diag congruent proof let rn n symmetric let u orthogonal matrix u rn n diag λn rn n inertia n n n assume without loss generality diag diagonal matrix contain positive negative eigenvalue respectively rn n diag rn n diag g l n r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>18.3 Selfadjoint Endomorphisms\\n\\n287\\n\\n√\\n√\\nHere (diag(μ1 , . . . , μm ))1/2 = diag( μ1 , . . . , μm ) and thus\\nA = U \u0004U T = U \u0005S A \u0005U T = (U \u0005)S A (U \u0005)T .\\n\\n\u0006\\n\u0005\\n\\nThis result will be used in the proof of Sylvester’s law of inertia.3\\nTheorem 18.23 The inertia of a symmetric matrix A ∈ Rn,n is invariant under congruence, i.e., for every matrix G ∈ G L n (R) the matrices A and G T AG have the same inertia.\\nProof The assertion is trivial for A = 0.\\nLet A \b= 0 have the inertia (n + , n − , n 0 ), then not both n + and n − can be equal to zero.\\nWe assume without loss of generality that n + &gt; 0.\\n(If n + = 0, then the following argument can be applied for n − &gt; 0.)\\nBy Lemma 18.22 there exist G 1 ∈ G L n (R) and S A = diag(In + , −In − , 0n 0 ) with\\nA = G 1T S A G 1 .\\nLet G 2 ∈ G L n (R) be arbitrary and set B := G 2T AG 2 .\\nThen B n − ,\u0014 n 0 ).\\nTherefore, B = G 3T S B G 3 for S B = is symmetric and has an inertia (\u0014 n + ,\u0014 n + and diag(I\u0014n + , −I\u0014n − , 0\u0014n 0 ) and a matrix G 3 ∈ G L n (R).\\nIf we show that n + = \u0014 n 0 , then also n − = \u0014 n−.\\nn0 = \u0014\\nWe have\\n\u0005T\\n\u0004 −1 \u0005T T\\n\u0004\\n−1\\nT\\nBG −1\\nG 3 S B G 3 G −1\\nA = G −1\\n2\\n2 = G2\\n2 = G 4 S B G 4 , G 4 := G 3 G 2 , n0.\\nand G 4 ∈ G L n (R) implies that rank(A) = rank(S B ) = rank(B), hence n 0 = \u0014\\nWe set\\nG −1\\n1 = [u 1 , . . . , u n + , v1 , . . . , vn − , w1 , . . . , wn 0 ] and u1, . . . , \u0014 u\u0014n + ,\u0014 v1 , . . . ,\u0014 v\u0014n − , w\\n\u00141 , . . . , w\\n\u0014n 0 ].\\nG −1\\n4 = [\u0014 v1 , . . . ,\u0014 v\u0014n − , w\\n\u00141 , . . . , w\\n\u0014n 0 }.\\nSince n + &gt;\\nLet V1 := span{u 1 , . . . , u n + } and V2 := span{\u0014\\n0, we have dim(V1 ) ≥ 1.\\nIf x ∈ V1 \\ {0}, then x= n+\\n\u0016\\n\\nT\\nα j u j = G −1\\n1 [α1 , . . . , αn + , 0, . . . , 0] j=1 for some α1 , . . . , αn + ∈ R that are not all zero.\\nThis implies x T Ax = n+\\n\u0016\\n\\nα2j &gt; 0.\\nj=1\\n\\n3 James\\n\\nJoseph Sylvester (1814–1897) proved this result for quadratic forms in 1852.\\nHe also coined the name law of inertia which according to him is “expressing the fact of the existence of an invariable number inseparably attached to such [bilinear] forms”.</td>\n",
       "      <td>286.0</td>\n",
       "      <td>286</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.3 Selfadjoint Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>selfadjoint endomorphisms diag μm diag μm thus u u u u result used proof sylvester law theorem inertia symmetric matrix rn n invariant congruence every matrix g g l n r matrix g ag inertia proof assertion trivial let inertia n n n n n equal zero assume without loss generality n n following argument applied n lemma exist g g l n r diag g g let g g l n r arbitrary set b g ag b n n therefore b g b g b symmetric inertia n n diag matrix g g l n r show n n also n bg g b g g g g b g g g g g g l n r implies rank rank b rank b hence n set g u u n vn wn w w g w w since n let span u u n span dim x α j u j g αn αn r zero implies x ax james joseph sylvester proved result quadratic form also coined name law inertia according expressing fact existence invariable number inseparably attached bilinear form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>288\\n\\n18 Special Classes of Endomorphisms\\n\\nIf, on the other hand, x ∈ V2 , then an analogous argument shows that x T Ax ≤ 0.\\nHence V1 ∩ V2 = {0}, and the dimension formula for subspaces (cp.\\nTheorem 9.29) yields dim(V ) + dim(V2 ) − dim(V1 ∩ V2 ) = dim(V1 + V2 ) ≤ dim(Rn,1 ) = n,\\n\u001d",
       " \u001e",
       "\u001f 1\\n\u001d",
       " \u001e",
       "\u001f\\n\u001d",
       "\\n\u001e",
       "\u001f\\n=n +\\n\\n=n−\u0014 n+\\n\\n=0 n + .\\nIf we repeat the same construction by interchanging the roles of and thus n + ≤ \u0014 n + , then \u0014 n + ≤ n + .\\nThus, n + = \u0014 n + and the proof is complete.\\n\u0006\\n\u0005 n + and \u0014\\nIn the following result we transfer Lemma 18.22 and Theorem 18.23 to complex\\nHermitian matrices.\\nTheorem 18.24 Let A ∈ Cn,n be Hermitian with the inertia (n + , n − , n 0 ).\\nThen there exists a matrix G ∈ G L n (C) with\\nA = G H diag(In + , In − , 0n 0 ) G.\\nMoreover, for every matrix G ∈ G L n (C) the matrices A and G H AG have the same inertia.\\nProof Exercise.\\n\\n\u0006\\n\u0005\\n\\nFinally, we discuss a special class of symmetric and Hermitian matrices.\\nDefinition 18.25 A real symmetric or complex Hermitian n × n matrix A is called\\n(1) positive semidefinite, if v H Av ≥ 0 for all v ∈ Rn,1 resp. v ∈ Cn,1 ,\\n(2) positive definite, if v H Av &gt; 0 for all v ∈ Rn,1 \\ {0} resp. v ∈ Cn,1 \\ {0}.\\nIf in (1) or (2) the reverse inequality holds, then the corresponding matrices are called negative semidefinite or negative definite, respectively.\\nFor selfadjoint endomorphisms we define analogously: If V is a finite dimensional\\nEuclidean or unitary vector space with the scalar product ·, · and if f ∈ L(V, V) is selfadjoint, then f is called positive semidefinite or positive definite, if f (v), v ≥ 0 for all v ∈ V resp. f (v), v &gt; 0 for all v ∈ V \\ {0}.\\nThe following theorem characterizes symmetric positive definite matrices; see\\nExercise 18.19 and Exercise 18.20 for the transfer of the results to positive semidefinite matrices resp. positive definite endomorphisms.\\nTheorem 18.26 If A ∈ Rn,n is symmetric, then the following statements are equivalent:\\n(1) A is positive definite.\\n(2) All eigenvalues of A are real and positive.\\n(3) There exists a lower triangular matrix L ∈ G L n (R) with A = L L T .</td>\n",
       "      <td>287.0</td>\n",
       "      <td>287</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.3 Selfadjoint Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>special class endomorphisms hand x analogous argument show x ax hence dimension formula subspace cp theorem yield dim v dim dim dim dim n n repeat construction interchanging role thus n n n n thus n n proof complete n following result transfer lemma theorem complex hermitian matrix theorem let cn n hermitian inertia n n n exists matrix g g l n c g h diag moreover every matrix g g l n c matrix g h ag inertia proof exercise finally discus special class symmetric hermitian matrix definition real symmetric complex hermitian n n matrix called positive semidefinite v h av v resp v positive definite v h av v resp v reverse inequality hold corresponding matrix called negative semidefinite negative definite respectively selfadjoint endomorphisms define analogously v finite dimensional euclidean unitary vector space scalar product f l v v selfadjoint f called positive semidefinite positive definite f v v v v resp f v v v v following theorem characterizes symmetric positive definite matrix see exercise exercise transfer result positive semidefinite matrix resp positive definite endomorphisms theorem rn n symmetric following statement equivalent positive definite eigenvalue real positive exists lower triangular matrix l g l n r l l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>18.3 Selfadjoint Endomorphisms\\n\\n289\\n\\nProof\\n(1) ⇒ (2): The symmetric matrix A is diagonalizable with real eigenvalues (cp.\\n(1) in Corollary 18.18).\\nIf λ is an eigenvalue with associated eigenvector v, i.e.,\\nAv = λv, then λv T v = v T Av &gt; 0 and v T v &gt; 0 implies that λ &gt; 0.\\n(2) ⇒ (1): Let A = U T diag(λ1 , . . . , λn ) U be a diagonalization A with an orthogonal matrix U ∈ Rn,n (cp.\\n(1) in Corollary 18.18) and λ j &gt; 0, j = 1, . . . , n.\\nLet v ∈ Rn,1 \\ {0} be arbitrary and let w := U v. Then w \b= 0 and v = U T w, so that v T Av = (U T w)T U T diag(λ1 , . . . , λn ) U (U T w) = w T diag(λ1 , . . . , λn ) w\\n= n\\n\u0016\\n\\nλ j w 2j &gt; 0.\\nj=1\\n\\n(3) ⇒ (1): If A = L L T with L ∈ G L n (R), then for every v ∈ Cn,1 \\ {0} we have v T Av = v T L L T v = L T v\\n\\n2\\n2\\n\\n&gt; 0, since L T is invertible.\\n(Note that here we do not need that L is lower triangular.)\\n(1) ⇒ (3): Let A = U T diag(λ1 , . . . , λn ) U be a diagonalization of A with an orthogonal matrix U ∈ Rn,n (cp.\\n(1) in Corollary 18.18).\\nSince A is positive definite, we know from (2) that λ j &gt; 0, j = 1, . . . , n.\\nWe set\\n!\\n!\\n\u00041/2 := diag( λ1 , . . . , λn ), and then have A = (U \u00041/2 )(\u00041/2 U T ) =: B T B. Let B = Q R be a Q Rdecomposition of the invertible matrix B (cp.\\nCorollary 12.12), where Q ∈ Rn,n is orthogonal and R ∈ Rn,n is an invertible upper triangular matrix.\\nThen A =\\n\u0006\\n\u0005\\nB T B = (Q R)T (Q R) = L L T , where L := R T .\\nOne easily sees that an analogous result holds for complex Hermitian matrices\\nA ∈ Cn,n .\\nIn this case in assertion (3) the lower triangular matrix is L ∈ G L n (C) with A = L L H .\\nThe factorization A = L L T in (3) is called a Cholesky factorization4 of A. It is special case of the LU -decomposition in Theorem 5.4.\\nIn fact, Theorem 18.26 shows that an LU -decomposition of a (real) symmetric positive definite matrix can be computed without row permutations.\\nIn order to compute the Cholesky factorization of the symmetric positive definite matrix A = [ai j ] ∈ Rn,n , we consider the equation\\n\\n4 André-Louis\\n\\nCholesky (1875–1918).</td>\n",
       "      <td>288.0</td>\n",
       "      <td>288</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.3 Selfadjoint Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>selfadjoint endomorphisms proof symmetric matrix diagonalizable real eigenvalue cp corollary λ eigenvalue associated eigenvector v av λv λv v v av v v implies λ let u diag λn u diagonalization orthogonal matrix u rn n cp corollary λ j j let v arbitrary let w u w v u w v av u w u diag λn u u w w diag λn w n λ j w l l l g l n r every v v av v l l v l v since l invertible note need l lower triangular let u diag λn u diagonalization orthogonal matrix u rn n cp corollary since positive definite know λ j j set diag λn u u b b let b q r q rdecomposition invertible matrix b cp corollary q rn n orthogonal r rn n invertible upper triangular matrix b b q r q r l l l r one easily see analogous result hold complex hermitian matrix cn n case assertion lower triangular matrix l g l n c l l h factorization l l called cholesky special case lu theorem fact theorem show lu real symmetric positive definite matrix computed without row permutation order compute cholesky factorization symmetric positive definite matrix ai j rn n consider equation cholesky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>290\\n\\n18 Special Classes of Endomorphisms\\n\\n⎤⎡\\n⎤\\n⎡ l11 · · · ln1 l11\\n⎥⎢\\n⎥\\n⎢\\nA = L L T = ⎣ ... . . . ⎦ ⎣ . . . ... ⎦ .\\nlnn ln1 · · · lnn\\nFor the first row of A we obtain\\n2\\n=⇒ l11 = a11 = l11 a1 j = l11l j1 =⇒ l j1\\n\\n√ a11 , a1 j\\n=\\n, l11\\n\\n(18.3) j = 2, . . . , n.\\n\\n(18.4)\\n\\nAnalogously, for the rows i = 2, . . . , n of A we obtain aii = i\\n\u0016 i−1\\n#1/2\\n\"\\n\u0016 li j li j =⇒ lii = aii − li2j\\n, j=1 ai j = n\\n\u0016\\n\\n(18.5) j=1 lik l jk = k=1 i\\n\u0016 lik l jk = k=1 i−1\\n\u0016 lik l jk + lii l ji k=1\\n\\n#\\n\u0016\\n1\" ai j − lik l jk , for j &gt; i.\\nlii k=1 i−1\\n\\n=⇒ l ji =\\n\\n(18.6)\\n\\nThe symmetric or Hermitian positive definite matrices are closely related to the positive definite bilinear forms on Euclidian or unitary vector spaces.\\nTheorem 18.27 If V is a finite dimensional Euclidian or unitary vector space and if β is a symmetric or Hermitian bilinear form on V, respectively, then the following statements are equivalent:\\n(1) β is positive definite, i.e., β(v, v) &gt; 0 for all v ∈ V \\ {0}.\\n(2) For every basis B of V the matrix representation [β] B×B is (symmetric or Hermitian) positive definite.\\n(3) There exists a basis B of V such that the matrix representation [β] B×B is (symmetric or Hermitian) positive definite.\\n\u0006\\n\u0005\\n\\nProof Exercise.\\nExercises\\n\\n18.1 Let A ∈ Rn,n be normal.\\nShow that α A for every α ∈ R, Ak for every k ∈ N0 , and p(A) for every p ∈ R[t] are normal.\\n18.2 Let A, B ∈ Rn,n be normal.\\nAre A + B and AB then normal as well?\\n18.3 Let A ∈ R2,2 be normal but not symmetric.\\nShow that then\\n\u0002\\n\\nαβ\\nA=\\n−β α\\n\\n\u0003</td>\n",
       "      <td>289.0</td>\n",
       "      <td>289</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.3 Selfadjoint Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>special class endomorphisms l l lnn lnn first row obtain j l j j analogously row n obtain aii li j li j lii aii ai j n lik l jk lik l jk lik l jk lii l ji ai j lik l jk j lii l ji symmetric hermitian positive definite matrix closely related positive definite bilinear form euclidian unitary vector space theorem v finite dimensional euclidian unitary vector space β symmetric hermitian bilinear form v respectively following statement equivalent β positive definite β v v v v every basis b v matrix representation β symmetric hermitian positive definite exists basis b v matrix representation β symmetric hermitian positive definite proof exercise exercise let rn n normal show α every α r ak every k p every p r normal let b rn n normal b ab normal well let normal symmetric show αβ α</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>18.3 Selfadjoint Endomorphisms\\n\\n291 for some α ∈ R and β ∈ R \\ {0}.\\n18.4 Prove Corollary 18.6 using Theorem 18.5.\\n18.5 Show that real skew-symmetric matrices (i. e., matrices with A = −A T ∈\\nRn,n ) and complex skew-Hermitian matrices (i. e., matrices with A = −A H ∈\\nCn,n ) are normal.\\n18.6 Let V be a finite dimensional unitary vector space and let f ∈ L(V, V) be normal.\\nShow the following assertions:\\n(a) If f = f 2 , then f is selfadjoint.\\n(b) If f 2 = f 3 , then f = f 2 .\\n(c) If f is nilpotent, then f = 0.\\n18.7 Let V be a finite dimensional real or complex vector space and let f ∈ L(V, V) be diagonalizable.\\nShow that there exists a scalar product on V such that f is normal with respect to this scalar products.\\n18.8 Let A ∈ Cn,n .\\nShow the following assertions:\\n(a) A is normal if and only if there exists a normal matrix B with n distinct eigenvalues that commutes with A.\\n(b) A is normal if and only if A + a I is normal for every a ∈ C.\\n(c) Let H (A) := 21 (A + A H ) be the Hermitian and S(A) := 21 (A − A H ) the skew-Hermitian part of A. Show that A = H (A)+S(A), H (A) H = H (A) and S(A) H = −S(A).\\nShow, furthermore, that A is normal if and only if\\nH (A) and S(A) commute.\\n18.9 Show that if A ∈ Cn,n is normal and if f (z) = az+b with ad − bc \b= 0 is cz+d defined on the spectrum of A, then f (A) = (a A + bI )(c A + d I )−1 .\\n(The map f (z) is called a Möbius transformation.5 Such transformations play an important role in Function Theory and in many other areas of Mathematics.)\\n18.10 Let V be a finite dimensional Euclidian or unitary vector space and let f ∈\\nL(V, V) be orthogonal or unitary, respectively.\\nShow that f −1 exists and is again orthogonal or unitary, respectively.\\n18.11 Let u ∈ Rn,1 and let the Householder matrix H (u) be defined as in (18.2).\\nShow the following assertions:\\n(a) For u \b= 0 the matrices H (u) and [−e1 , e2 , . . . , en ] are orthogonally similar, i.e., there exists an orthogonal matrix Q ∈ Rn,n with\\nQ T H (u)Q = [−e1 , e2 , . . . , en ].\\n(This implies that H (u) only has the eigenvalues 1 and −1 with the algebraic multiplicities n − 1 and 1, respectively.)\\n(b) Every orthogonal matrix A ∈ Rn,n can be written as product of n Householder matrices, i.e., there exist u 1 , . . . , u n ∈ Rn,1 with A = H (u 1 ) . . .\\nH (u n ).\\n5 August\\n\\nFerdinand Möbius (1790–1868).</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.3 Selfadjoint Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>selfadjoint endomorphisms α r β r prove corollary using theorem show real skew-symmetric matrix matrix rn n complex skew-hermitian matrix matrix h cn n normal let v finite dimensional unitary vector space let f l v v normal show following assertion f f f selfadjoint b f f f f c f nilpotent f let v finite dimensional real complex vector space let f l v v diagonalizable show exists scalar product v f normal respect scalar product let cn n show following assertion normal exists normal matrix b n distinct eigenvalue commute b normal normal every c let h h hermitian h skew-hermitian part show h h h h h show furthermore normal h commute show cn n normal f z ad bc defined spectrum f bi c map f z called möbius transformation play important role function theory many area mathematics let v finite dimensional euclidian unitary vector space let f l v v orthogonal unitary respectively show f exists orthogonal unitary respectively let u let householder matrix h u defined show following assertion u matrix h u en orthogonally similar exists orthogonal matrix q rn n q h u q en implies h u eigenvalue algebraic multiplicity n respectively b every orthogonal matrix rn n written product n householder matrix exist u u n h u h u n august ferdinand möbius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>292\\n\\n18 Special Classes of Endomorphisms\\n\\n18.12 Let v ∈ Rn,1 satisfy v T v = 1.\\nShow that there exists an orthogonal matrix\\nU ∈ Rn,n with U v = e1 .\\n18.13 Transfer the proofs of Lemma 18.22 and Theorem 18.23 to complex Hermitian matrices and thus show Theorem 18.24.\\n18.14 Determine for the symmetric matrix\\n\u0002\\n\u0003\\n10 6\\nA=\\n∈ R2,2\\n6 10 an orthogonal matrix U ∈ R2,2 such that U T AU is diagonal.\\nIs A positive\\n(semi-)definite?\\n18.15 Let K ∈ {R, C} and let {v1 , . . . , vn } be a basis of K n,1 .\\nProve or disprove: A matrix A = A H ∈ K n,n is positive definite if and only if v Hj Av j &gt; 0 for all j = 1, . . . , n.\\n18.16 Use Definition 18.25 to test whether the symmetric matrices\\n\u0002 \u0003 \u0002 \u0003 \u0002 \u0003\\n11\\n12\\n21\\n,\\n,\\n∈ R2,2\\n11\\n21\\n12 are positive (semi-)definite.\\nDetermine in all cases the inertia.\\n18.17 Let\\n\u0003\\n\u0002\\nA11 A12\\n∈ Rn,n\\nA=\\nT\\nA12\\nA22\\nT\\nT with A11 = A11\\n∈ G L m (R), A12 ∈ Rm,n−m and A22 = A22\\n∈ Rn−m,n−m .\\nThe\\n−1\\nT m,m is called the Schur complement 6 of matrix S := A22 − A12 A11 A12 ∈ R\\nA11 in A. Show that A is positive definite if A11 and S are positive definite.\\n(For the Schur complement, see also Exercise 4.17.)\\n18.18 Show that A ∈ Cn,n is Hermitian positive definite if and only if x, y = y H Ax defines a scalar product on Cn,1 .\\n18.19 Prove the following version of Theorem 18.26 for positive semidefinite matrices.\\n\\nIf A ∈ Rn,n is symmetric, then the following statements are equivalent:\\n(1) A is positive semidefinite.\\n(2) All eigenvalues of A are real and nonnegative.\\n(3) There exists an upper triangular matrix L ∈ Rn,n with A = L L T .\\n18.20 Let V be a finite dimensional Euclidian or unitary vector space and let f ∈\\nL(V, V) be selfadjoint.\\nShow that f is positive definite if and only if all eigenvalues of f are real and positive.\\n18.21 Let A ∈ Rn,n .\\nA matrix X ∈ Rn,n with X 2 = A is called a square root of A\\n(cp.\\nSect.\\n17.1).\\n6 Issai\\n\\nSchur (1875–1941).</td>\n",
       "      <td>291.0</td>\n",
       "      <td>291</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.3 Selfadjoint Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>special class endomorphisms let v satisfy v v show exists orthogonal matrix u rn n u v transfer proof lemma theorem complex hermitian matrix thus show theorem determine symmetric matrix orthogonal matrix u u au diagonal positive definite let k r c let vn basis k prove disprove matrix h k n n positive definite v hj av j j use definition test whether symmetric matrix positive definite determine case inertia let rn n g l r rm called schur complement matrix r show positive definite positive definite schur complement see also exercise show cn n hermitian positive definite x h ax defines scalar product prove following version theorem positive semidefinite matrix rn n symmetric following statement equivalent positive semidefinite eigenvalue real nonnegative exists upper triangular matrix l rn n l l let v finite dimensional euclidian unitary vector space let f l v v selfadjoint show f positive definite eigenvalue f real positive let rn n matrix x rn n x called square root cp sect issai schur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>18.3 Selfadjoint Endomorphisms\\n\\n293\\n\\n(a) Show that a symmetric positive definite matrix A ∈ Rn,n has a symmetric positive definite square root.\\n(b) Show that the matrix\\n⎡\\n\\n⎤\\n33\\n6\\n6\\nA = ⎣ 6 24 −12 ⎦\\n6 −12 24 is symmetric positive definite and compute a symmetric positive definite square root of A.\\n(c) Show that the matrix A = Jn (0), n ≥ 2, does not have a square root.\\n18.22 Show that the matrix\\n\\n⎡\\n⎤\\n210\\nA = ⎣1 2 1⎦ ∈ R3,3\\n012 is positive definite and compute a Cholesky factorization of A using (18.3)–\\n(18.6).\\n18.23 Let A, B ∈ Cn,n be Hermitian and let B be furthermore positive definite.\\nShow that the polynomial det(t B − A) ∈ C[t]≤n has exactly n real roots.\\n18.24 Prove Theorem 18.27.</td>\n",
       "      <td>292.0</td>\n",
       "      <td>292</td>\n",
       "      <td>18 Special Classes of Endomorphisms</td>\n",
       "      <td>18.3 Selfadjoint Endomorphisms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>selfadjoint endomorphisms show symmetric positive definite matrix rn n symmetric positive definite square root b show matrix symmetric positive definite compute symmetric positive definite square root c show matrix jn n square root show matrix positive definite compute cholesky factorization using let b cn n hermitian let b furthermore positive definite show polynomial det b c exactly n real root prove theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>Chapter 19\\n\\nThe Singular Value Decomposition\\n\\nThe matrix decomposition introduced in this chapter is very important in many practical applications, since it yields the best possible approximation (in a certain sense) of a given matrix by a matrix of low rank.\\nA low rank approximation can be considered a “compression” of the data represented by the given matrix.\\nWe illustrate this below with an example from image processing.\\nWe first prove the existence of the decomposition.\\nTheorem 19.1 Let A ∈ Cn,m with n ≥ m be given.\\nThen there exist unitary matrices\\nV ∈ Cn,n and W ∈ Cm,m such that\\n\u0003\\n\u0002\\n\u0002r 0r,m−r\\nH\\n∈ Rn,m , \u0002r = diag(σ1 , . . . , σr ), with \u0002 =\\nA = V \u0002W\\n0n−r,r 0n−r,m−r\\n(19.1) where σ1 ≥ σ2 ≥ · · · ≥ σr &gt; 0 and r = rank(A).\\nProof If A = 0, then we set V = In , \u0002 = 0 ∈ Cn,m , \u0002r = [ ], W = Im , and we are finished.\\nLet A \u0004= 0 and r := rank(A).\\nSince n ≥ m, we have 1 ≤ r ≤ m, and since\\nA H A ∈ Cm,m is Hermitian, there exists a unitary matrix W = [w1 , . . . , wm ] ∈ Cm,m with\\nW H (A H A)W = diag(λ1 , . . . , λm ) ∈ Rm,m\\n(cp.\\n(2) in Corollary 18.18).\\nWithout loss of generality we assume that λ1 ≥ λ2 ≥\\n· · · ≥ λm .\\nFor every j = 1, . . . , m then A H Aw j = λ j w j , and hence\\nλ j w Hj w j = w Hj A H Aw j = \u0006Aw j \u000622 ≥ 0, i.e., λ j ≥ 0 for j = 1, . . . , m.\\nThen rank(A H A) = rank(A) = r (to see this, modify the proof of Lemma 10.25 for the complex case).\\nTherefore, the matrix A H A has exactly r positive eigenvalues λ1 , . . . , λr and m − r times the eigenvalue 0.\\nWe then\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_19\\n\\n295</td>\n",
       "      <td>293.0</td>\n",
       "      <td>&lt;FEFF&gt;</td>\n",
       "      <td>19 The Singular Value Decomposition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter singular value decomposition matrix decomposition introduced chapter important many practical application since yield best possible approximation certain sense given matrix matrix low rank low rank approximation considered compression data represented given matrix illustrate example image processing first prove existence decomposition theorem let cn n given exist unitary matrix v cn n w cm h rn diag σr v r σr r rank proof set v cn w im finished let r rank since n r since h cm hermitian exists unitary matrix w wm cm w h h w diag λm rm cp corollary without loss generality assume λm every j h aw j λ j w j hence λ j w hj w j w hj h aw j j λ j j rank h rank r see modify proof lemma complex case therefore matrix h exactly r positive eigenvalue λr r time eigenvalue springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>296\\n\\n19 The Singular Value Decomposition\\n1/2 define σ j := λ j , j = 1, . . . , r , and have σ1 ≥ σ2 ≥ · · · ≥ σr .\\nLet \u0002r be as in\\n(19.1),\\nD :=\\n\\n\u0003\\n\u0002\\n\u0002r 0\\n∈ G L m (R),\\n0 Im−r\\n\\nX = [x1 , . . . , xm ] := AW D −1 ,\\n\\nVr := [x1 , . . . , xr ], and Z := [xr +1 , . . . , xm ].\\nThen\\n\u0002\\n\\n\u0003 \u0002 H\u0003\\n\u0003\\n\u0002\\nVr\\nVrH Vr VrH Z\\nIr 0\\nH\\n−1\\nH H\\n−1\\n=\\n,\\n[V\\n,\\nZ\\n]\\n=\\nX\\nX\\n=\\nD\\nW\\nA\\nAW\\nD\\n= r\\nZH\\nZ H Vr Z H Z\\n0 0 which implies, in particular, that Z = 0 and VrH Vr = Ir .\\nWe extend the vectors xr +1 , . . . , \u0004 xn } of Cn,1 with respect to x1 , . . . , xr to an orthonormal basis {x1 , . . . , xr , \u0004 the standard scalar product.\\nThen the matrix xr +1 , . . . , \u0004 xn ] ∈ Cn,n\\nV := [Vr , \u0004 is unitary.\\nFrom X = AW D −1 and X = [Vr , Z ] = [Vr , 0] we finally obtain\\n\b\\n\u0007\\nA = [Vr , 0]DW H and A = V \u0002W H with \u0002 as in (19.1).\\nAs the proof shows, Theorem 19.1 can be formulated analogously for real matrices\\nA ∈ Rn,m with n ≥ m.\\nIn this case the two matrices V and W are orthogonal.\\nIf n &lt; m we can apply the theorem to A H (resp.\\nA T in the real case).\\nDefinition 19.2 A decomposition of the form (19.1) is called a singular value decomposition or short SVD1 of the matrix A. The diagonal entries of the matrix\\n\u0002r are called singular values and the columns of V resp.\\nW are called left resp. right singular vectors of A.\\nFrom (19.1) we obtain the unitary diagonalizations of the matrices A H A and\\nA AH ,\\n\u0002 2 \u0003\\n\u0002 2 \u0003\\n\u0002r 0\\n\u0002r 0\\nH\\nH\\nH\\nA A=W and A A = V\\nW\\nV H.\\n0 0\\n0 0\\nThe singular values of A are therefore uniquely determined as the positive square roots of the positive eigenvalues of A H A or A A H .\\nThe unitary matrices V and W in the singular value decomposition, however, are (as the eigenvectors in general) not uniquely determined.\\n\\n1 In the development of this decomposition from special cases in the middle of the 19th century to its current general form many important players of the history of Linear Algebra played a role.\\nIn the historical notes concerning the singular value decomposition in [HorJ91] one finds contributions of Jordan (1873), Sylvester (1889/1890) and Schmidt (1907).\\nThe current form was shown in 1939 by Carl Henry Eckart (1902–1973) and Gale Young.</td>\n",
       "      <td>294.0</td>\n",
       "      <td>294</td>\n",
       "      <td>19 The Singular Value Decomposition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>singular value decomposition define σ j λ j j r σr let g l r x xm aw vr xr z xr xm vr vrh vr vrh z ir h h h v z x x w aw r zh z h vr z h z implies particular z vrh vr ir extend vector xr xn respect xr orthonormal basis xr standard scalar product matrix xr xn cn n v vr unitary x aw x vr z vr finally obtain vr dw h v h proof show theorem formulated analogously real matrix rn n case two matrix v w orthogonal n apply theorem h resp real case definition decomposition form called singular value decomposition short matrix diagonal entry matrix called singular value column v resp w called left resp right singular vector obtain unitary diagonalization matrix h ah h h h v w v singular value therefore uniquely determined positive square root positive eigenvalue h h unitary matrix v w singular value decomposition however eigenvectors general uniquely determined development decomposition special case middle century current general form many important player history linear algebra played role historical note concerning singular value decomposition one find contribution jordan sylvester schmidt current form shown carl henry eckart gale young</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>19 The Singular Value Decomposition\\n\\n297\\n\\nIf we write the SVD of A in the form\\n\u0003\\n\u0006\u0005 \u0002\\n\u0003\\n\u0006\\n\u0005 \u0002\\n\u0002r 0\\nIm\\nWH\\nW\\nW H =: U P,\\nA = V \u0002W H = V\\n0n−m,m\\n0 0m−r then U ∈ Cn,m has orthonormal columns, i.e., U H U = Im , and P = P H ∈ Cm,m is positive semidefinite with the inertia (r, 0, m − r ).\\nThe factorization A = U P is called a polar decomposition of A. It can be viewed as a generalization of the polar representation of complex numbers, z = eiϕ |z|.\\nLemma 19.3 Suppose that the matrix A ∈ Cn,m with rank(A) = r has an SVD of the form (19.1) with V = [v1 , . . . , vn ] and W = [w1 , . . . , wm ].\\nConsidering\\nA as an element of L(Cm,1 , Cn,1 ), we then have im(A) = span{v1 , . . . , vr } and ker(A) = span{wr +1 , . . . , wm }.\\nProof For j = 1, . . . , r we have Aw j = V \u0002W H w j = V \u0002e j = σ j v j \u0004= 0, since\\nσ j \u0004= 0.\\nHence these r linear independent vectors satisfy v1 , . . . , vr ∈ im(A).\\nNow r = rank(A) = dim(im(A)) implies that im(A) = span{v1 , . . . , vr }.\\nFor j = r +1, . . . , m we have Aw j = 0, and hence these m −r linear independent vectors satisfy wr +1 , . . . , wm ∈ ker(A).\\nThen dim(ker(A)) = m − dim(im(A)) =\\n\b\\n\u0007 m − r implies that ker(A) = span{wr +1 , . . . , wm }.\\nAn SVD of the form (19.1) can be written as\\nA= r\\n\u0007\\n\\nσ j v j w Hj .\\nj=1\\n\\nThus, A can be written as a sum of r matrices of the form σ j v j w Hj , where\\n\b rank σ j v j w Hj = 1.\\nLet\\nAk := k\\n\u0007\\n\\nσ j v j w Hj for some k, 1 ≤ k ≤ r.\\n\\n(19.2) j=1\\n\\nThen rank(Ak ) = k and, using that the matrix 2-norm is unitarily invariant (cp.\\nExercise 19.1), we get\\n\u0006A − Ak \u00062 = \u0006diag(σk+1 , . . . , σr )\u00062 = σk+1 .\\n\\n(19.3)\\n\\nHence A is approximated by the matrix Ak , where the rank of the approximating matrix and the approximation error in the matrix 2-norm are explicitly known.\\nThe singular value decomposition, furthermore, yields the best possible approximation of A by a matrix of rank k with respect to the matrix 2-norm.\\nTheorem 19.4 With Ak as in (19.2), we have \u0006A − Ak \u00062 ≤ \u0006A − B\u00062 for every matrix B ∈ Cn,m with rank(B) = k.</td>\n",
       "      <td>295.0</td>\n",
       "      <td>295</td>\n",
       "      <td>19 The Singular Value Decomposition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>singular value decomposition write svd form im wh w w h u p v h v u cn orthonormal column u h u im p p h cm positive semidefinite inertia r r factorization u p called polar decomposition viewed generalization polar representation complex number z eiϕ lemma suppose matrix cn rank r svd form v vn w wm considering element l im span vr ker span wr wm proof j r aw j v h w j v j σ j v j since σ j hence r linear independent vector satisfy vr im r rank dim im implies im span vr j r aw j hence linear independent vector satisfy wr wm ker dim ker dim im r implies ker span wr wm svd form written r σ j v j w hj thus written sum r matrix form σ j v j w hj rank σ j v j w hj let ak k σ j v j w hj k k rank ak k using matrix unitarily invariant cp exercise get ak σr hence approximated matrix ak rank approximating matrix approximation error matrix explicitly known singular value decomposition furthermore yield best possible approximation matrix rank k respect matrix theorem ak ak every matrix b cn rank b k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>298\\n\\n19 The Singular Value Decomposition\\n\\nProof The assertion is clear for k = rank(A), since then Ak = A and \u0006A− Ak \u00062 = 0.\\nLet k &lt; rank(A) ≤ m.\\nLet B ∈ Cn,m with rank(B) = k be given, then dim(ker(B)) = m − k, where we consider B as an element of L(Cm,1 , Cn,1 ).\\nIf w1 , . . . , wm are the right singular vectors of A from (19.1), then U := span{w1 , . . . , wk+1 } has the dimension k + 1.\\nSince ker(B) and U are subspaces of Cm,1 with dim(ker(B)) + dim(U) = m + 1, we have ker(B) ∩ U \u0004= {0}.\\nLet v ∈ ker(B) ∩ U with \u0006v\u00062 = 1 be given.\\nThen there exist α1 , . . . , αk+1 ∈ C k+1\\n2\\n2 with v = k+1 j=1 α j w j and j=1 |α j | = \u0006v\u00062 =1.\\nHence\\n(A − B)v = Av − Bv\u000e =\\n=0 k+1\\n\u0007 j=1\\n\\nα j Aw j = k+1\\n\u0007\\n\\nαjσjvj j=1 and, therefore, k+1\\n\u000f\u0007\\n\u000f\\nα j σ j v j \u000f2\\n\u0006A − B\u00062 = max \u0006(A − B)y\u00062 ≥ \u0006(A − B)v\u00062 = \u000f\\n\u0006y\u00062 =1\\n\\n= k+1\\n\b\u0007 j=1\\n\\n|α j σ j |2\\n\\n1/2\\n\\n(since v1 , . . . , vk+1 are pairwise orthonormal) j=1\\n\\n≥ σk+1 k+1\\n\b\u0007\\n\\n|α j |2\\n\\n1/2\\n\\n(since σ1 ≥ · · · ≥ σk+1 ) j=1\\n\\n= σk+1 = \u0006A − Ak \u00062 , which completes the proof.\\n\\n\b\\n\u0007\\n\\nMATLAB-Minute.\\nThe command A=magic(n) generates for n ≥ 3 an n × n matrix A with entries from 1 to n 2 , so that all row, column and diagonal sums of A are equal.\\nThe entries of A therefore from a “magic square”.\\nCompute the SVD of A=magic(10) using the command [V,S,W]=svd(A).\\nWhat can be said about the singular values of A and what is rank(A)?\\nForm\\nAk for k = 1, 2, . . . , rank(A) as in (19.2) and verify numerically the equation\\n(19.3).\\n\\nThe SVD is one of the most important and practical mathematical tools in almost all areas of science, engineering and social sciences, in medicine and even in psychology.\\nIts great importance is due to the fact that the SVD allows to distinguish between\\n“important” and “non-important” information in a given data.\\nIn practice, the latter</td>\n",
       "      <td>296.0</td>\n",
       "      <td>296</td>\n",
       "      <td>19 The Singular Value Decomposition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>singular value decomposition proof assertion clear k rank since ak ak let k rank let b cn rank b k given dim ker b k consider b element l wm right singular vector u span dimension k since ker b u subspace dim ker b dim u ker b u let v ker b u given exist c v α j w j j hence b v av α j aw j αjσjvj therefore α j σ j v j max b b j σ j since pairwise orthonormal j since ak completes proof matlab-minute command n generates n n n matrix entry n row column diagonal sum equal entry therefore magic square compute svd using command v w said singular value rank form ak k rank verify numerically equation svd one important practical mathematical tool almost area science engineering social science medicine even psychology great importance due fact svd allows distinguish important non-important information given data practice latter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>19 The Singular Value Decomposition\\n\\n299 corresponds, e.g., to measurement errors, noise in the transmission of data, or fine details in a signal or an image that do not play an important role.\\nOften, the “important” information corresponds to the large singular values, and the “non-important” information to the small ones.\\nIn many applications one sees, furthermore, that the singular values of a given matrix decay rapidly, so that there exist only few large and many small singular values.\\nIf this is the case, then the matrix can be approximated well by a matrix with low rank, since already for a small k the approximation error \u0006A − Ak \u00062 = σk+1 is small.\\nA low rank approximation Ak requires little storage capacity in the computer; only k scalars and 2k vectors have to be stored.\\nThis makes the SVD a powerful tool in all applications where data compression is of interest.\\nExample 19.5 We illustrate the use of the SVD in image compression with a picture that we obtained from the research center Matheon: Mathematics for Key Technologies2 .\\nThe greyscale picture is shown on the left of the figure below.\\nIt consists of 286 × 152 pixels, where each of the pixels is given by a value between 0 and 64.\\nThese values are stored in a real 286 × 152 matrix A which has (full) rank 152.\\n\\nWe compute an SVD A = V \u0002W T using the command [V,S,W]=svd(A) in MATLAB.\\nThe diagonal entries of the matrix S, i.e., the singular values of A, are ordered decreasingly by MATLAB (as in Theorem 19.1).\\nFor k = 100, 20, 10 we now compute matrices Ak with rank k as in (19.2) using the command Ak=V(:,1:k)∗\\nS(1:k,1:k)∗W(:,1:k)’.\\nThese matrices represent approximations of the original picture based on the k largest singular values and the corresponding singular vectors.\\nThe three approximations are shown next to the original picture above.\\nThe quality of the approximation decreases with decreasing k, but even the approximation for k = 10 shows the essential features of the “Matheon bear”.\\nAnother important application of the SVD arises in the solution of linear systems of equations.\\nIf A ∈ Cn,m has an SVD of the form (19.1), we define the matrix\\nA := W \u0002 V\\n†\\n\\n†\\n\\nH\\n\\n∈C m,n\\n\\n\u0002 −1 \u0003\\n\u0002r 0\\n, where \u0002 :=\\n∈ Rm,n .\\n0 0\\n†\\n\\n(19.4)\\n\\n2 We thank Falk Ebert for his help.\\nThe original bear can be seen in front of the Mathematics building of the TU Berlin.\\nMore information on MATHEON can be found at www.matheon.de.</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297</td>\n",
       "      <td>19 The Singular Value Decomposition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>singular value decomposition corresponds measurement error noise transmission data fine detail signal image play important role often important information corresponds large singular value non-important information small one many application one see furthermore singular value given matrix decay rapidly exist large many small singular value case matrix approximated well matrix low rank since already small k approximation error ak small low rank approximation ak requires little storage capacity computer k scalar vector stored make svd powerful tool application data compression interest example illustrate use svd image compression picture obtained research center matheon mathematics key greyscale picture shown left figure consists pixel pixel given value value stored real matrix full rank compute svd v using command v w matlab diagonal entry matrix singular value ordered decreasingly matlab theorem k compute matrix ak rank k using command k k k matrix represent approximation original picture based k largest singular value corresponding singular vector three approximation shown next original picture quality approximation decrease decreasing k even approximation k show essential feature matheon bear another important application svd arises solution linear system equation cn svd form define matrix w v h n rm n thank falk ebert help original bear seen front mathematics building tu berlin information matheon found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>300\\n\\n19 The Singular Value Decomposition\\n\\nOne easily sees that\\n\\n\u0002\\n\\n\u0003\\nIr 0\\nA A=W\\nW H ∈ Rm,m .\\n0 0\\n†\\n\\nIf r = m = n, then A is invertible and the right hand side of the above equation is equal to the identity matrix In .\\nIn this case we have A† = A−1 .\\nThe matrix A† can therefore be viewed as a generalized inverse, that in the case of an invertible matrix\\nA is equal to the inverse of A.\\nDefinition 19.6 The matrix A† in (19.4) is called Moore-Penrose inverse3 or pseudoinverse of A.\\nLet A ∈ Cn,m and b ∈ Cn,1 be given.\\nIf the linear system of equations Ax = b has x is “as close as possible” no solution, then we can try to find an \u0010 x ∈ Cm,1 such that A\u0010 to b.\\nUsing the Moore-Penrose inverse we obtain the best possible approximation with respect to the Euclidean norm.\\nTheorem 19.7 Let A ∈ Cn,m with n ≥ m and b ∈ Cn,1 be given.\\nIf A = V \u0002W H is x = A† b satisfies an SVD, and A† is as in (19.4), then \u0010\\n\u0006b − A\u0010 x \u00062 ≤ \u0006b − Ay\u00062 for all y ∈ Cm,1 , and\\n\\n⎛\\n\u0013\\n\u0013 ⎞1/2 r \u0013 H \u00132\\n\u0007 v b\\n\u0013 j \u0013 ⎠\\n\u0006\u0010 x \u00062 = ⎝\\n≤ \u0006y\u00062\\n\u0013\\n\u0013\\n\u0013 σj \u0013 j=1 for all y ∈ Cm,1 with \u0006b − A\u0010 x \u00062 = \u0006b − Ay\u00062 .\\nProof Let y ∈ Cm,1 be given and let z = [ξ1 , . . . , ξm ]T := W H y.\\nThen\\n\u0006b − Ay\u000622 = \u0006b − V \u0002W Hy\u000622 = \u0006V (V H b − \u0002z)\u000622 = \u0006V H b − \u0002z\u000622\\n= r n\\n\u0007\\n\u0007\\n\u0013 H\\n\u0013\\n\u0013 H \u00132\\n\u0013v b − σ j ξ j \u00132 +\\n\u0013v b\u0013 j j j=r +1 j=1 n\\n\u0007\\n\u0013 H \u00132\\n\u0013v b \u0013 .\\n≥ j\\n\\n(19.5) j=r +1\\n\\n\b\\nEquality holds if and only if ξ j = v Hj b /σ j for all j = 1, . . . , r .\\nThis is satisfied if z = W H y = \u0002 † V H b.\\nThe last equation holds if and only if x.\\ny = W \u0002 † V H b = A† b = \u0010\\n3 Eliakim\\n\\nHastings Moore (1862–1932) and Sir Roger Penrose (1931–).</td>\n",
       "      <td>298.0</td>\n",
       "      <td>298</td>\n",
       "      <td>19 The Singular Value Decomposition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>singular value decomposition one easily see ir w h rm r n invertible right hand side equation equal identity matrix case matrix therefore viewed generalized inverse case invertible matrix equal inverse definition matrix called moore-penrose pseudoinverse let cn b given linear system equation ax b x close possible solution try find x b using moore-penrose inverse obtain best possible approximation respect euclidean norm theorem let cn n b given v h x b satisfies svd x r h v b j x σj x proof let given let z ξm w h v v h b h b r n h h b σ j ξ j j j n h b j equality hold ξ j v hj b j j r satisfied z w h v h b last equation hold w v h b b eliakim hastings moore sir roger penrose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>19 The Singular Value Decomposition\\n\\n301\\n\\nThe vector \u0010 x therefore attains the lower bound (19.5).\\nThe equation\\n⎛\\n\u0013\\n\u0013 ⎞1/2 r \u0013 H \u00132\\n\u0007 vj b\u0013\\n\u0013\\n\u0006\u0010 x \u00062 = ⎝\\n\u0013\\n\u0013 ⎠\\n\u0013 σj \u0013 j=1 is easily checked.\\nEvery vector y ∈ Cm,1 that attains the lower bound (19.5) must have the form\\n\u0003T\\n\u0002 H vH b v1 b\\n, . . . , r , yr +1 , . . . , ym y=W\\nσ1\\nσr for some yr +1 , . . . , ym ∈ C, which implies that \u0006y\u00062 ≥ \u0006\u0010 x \u00062 .\\n\\n\b\\n\u0007\\n\\nThe minimization problem for the vector \u0010 x can be written as\\n\u0006b − A\u0010 x \u00062 = min \u0006b − Ay\u00062 .\\ny∈Cm,1\\n\\n⎡\\n\\nIf\\n\\nτ1\\n⎢ ..\\nA=⎣ .\\n\\n⎤\\n1\\n.. ⎥ ∈ Rm,2\\n.⎦\\n\\nτm 1 for (pairwise distinct) τ1 , . . . , τm ∈ R, then this minimization problem corresponds to the problem of linear regression and the least squares approximation in Example 12.16, that we have solved with the Q R-decomposition of A. If A = Q R is this decomposition, then A† = (A H A)−1 A H (cp.\\nExercise 19.5) and we have\\nA† = (R H Q H Q R)−1 R H Q H = R −1 (R H )−1 R H Q H = R −1 Q H .\\nThus, the solution of the least-squares approximation in Example 12.16 is identical to the solution of the above minimization problem using the SVD of A.\\nExercises\\n19.1 Show that the Frobenius norm and the matrix 2-norm are unitarily invariant, i.e., that \u0006P AQ\u0006 F = \u0006A\u0006 F and \u0006P AQ\u00062 = \u0006A\u00062 for all A ∈ Cn,m and unitary matrices P ∈ Cn,n , Q ∈ Cm,m .\\n(Hint: For the Frobenius norm one can use that \u0006A\u00062F = trace(A H A).)\\n\u001d",
       "1/2\\n\u001c",
       "\\n19.2 Use the result of Exercise 19.1 to show that \u0006A\u0006 F = σ12 + . . . + σr2 and\\n\u0006A\u00062 = σ1 , where σ1 ≥ · · · ≥ σr &gt; 0 are the singular values of A ∈ Cn,m .\\n19.3 Show that \u0006A\u00062 = \u0006A H \u00062 and \u0006A\u000622 = \u0006A H A\u00062 for all A ∈ Cn,m .\\n19.4 Show that \u0006A\u000622 ≤ \u0006A\u00061 \u0006A\u0006∞ for all A ∈ Cn,m .</td>\n",
       "      <td>299.0</td>\n",
       "      <td>299</td>\n",
       "      <td>19 The Singular Value Decomposition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>singular value decomposition vector x therefore attains lower bound equation r h vj x σj easily checked every vector attains lower bound must form h vh b b r yr ym σr yr ym c implies x minimization problem vector x written x min τm pairwise distinct τm r minimization problem corresponds problem linear regression least square approximation example solved q r-decomposition q r decomposition h h cp exercise r h q h q r r h q h r r h r h q h r q h thus solution least-squares approximation example identical solution minimization problem using svd exercise show frobenius norm matrix unitarily invariant f f cn unitary matrix p cn n q cm hint frobenius norm one use trace h use result exercise show f σr singular value cn show h h cn show cn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>302\\n\\n19 The Singular Value Decomposition\\n\\n19.5 Let A ∈ Cn,m and let A† be the Moore-Penrose inverse of A. Show the following assertions:\\n(a) If rank(A) = m, then A† = (A H A)−1 A H .\\n(b) The matrix X = A† is the uniquely determined matrix that satisfies the following four matrix equations:\\n(1) AX A = A,\\n(2) X AX = X ,\\n(3) (AX ) H = AX ,\\n(4) (X A) H = X A.\\n19.6 Let\\n⎡\\n\\n⎤\\n⎡\\n⎤\\n2 1\\n5\\nA = ⎣ 0 3 ⎦ ∈ R3,2 , b = ⎣ 2 ⎦ ∈ R3,1 .\\n1 −2\\n−5\\nCompute the Moore-Penrose inverse of A and a vector \u0010 x ∈ R2,1 such that\\n(a) \u0006b − A\u0010 x \u00062 ≤ \u0006b − Ay\u00062 for all y ∈ R2,1 , and x \u00062 .\\n(b) \u0006\u0010 x \u00062 ≤ \u0006y\u00062 for all y ∈ R2,1 with \u0006b − Ay\u00062 = \u0006b − A\u0010\\n19.7 Prove the following theorem:\\nLet A ∈ Cn,m and B ∈ C\u0003,m with m ≤ n ≤ \u0003.\\nThen A H A = B H B if and only if B = U A for a matrix U ∈ C\u0003,n with U H U = In .\\nIf A and B are real, then\\nU can also be chosen to be real.\\n(Hint: One direction is trivial.\\nFor the other direction consider the unitary diagonalization of A H A = B H B. This yields the matrix W in the SVD of A and of B. Show the assertion using these two decompositions.\\nThis theorem and its applications can be found in the article [HorO96].)</td>\n",
       "      <td>300.0</td>\n",
       "      <td>300</td>\n",
       "      <td>19 The Singular Value Decomposition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>singular value decomposition let cn let moore-penrose inverse show following assertion rank h h b matrix x uniquely determined matrix satisfies following four matrix equation ax x ax x ax h ax x h x let b compute moore-penrose inverse vector x x x b x prove following theorem let cn b n h b h b b u matrix u n u h u b real u also chosen real hint one direction trivial direction consider unitary diagonalization h b h b yield matrix w svd b show assertion using two decomposition theorem application found article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>Chapter 20\\n\\nThe Kronecker Product and Linear Matrix\\nEquations\\n\\nMany applications, in particular the stability analysis of differential equations, lead to linear matrix equations, such as AX + X B = C. Here the matrices A, B, C are given and the goal is to determine a matrix X that solves the equation (we will give a formal definition below).\\nIn the description of the solutions of such equations, the Kronecker product,1 another product of matrices, is useful.\\nIn this chapter we develop the most important properties of this products and we study its application in the context of linear matrix equations.\\nMany more results on this topic can be found in the books [HorJ91, LanT85].\\nDefinition 20.1 If K is a field, A = [ai j ] ∈ K m,m and B ∈ K n,n , then\\n⎤ a11 B · · · a1m B\\n⎢\\n.. ⎥ ,\\nA ⊗ B := [ai j B] = ⎣ ...\\n. ⎦ am1 B · · · amm B\\n⎡ is called the Kronecker product of A and B.\\nThe Kronecker product is sometimes called the tensor product of matrices.\\nThis product defines a map from K m,m × K n,n to K mn,mn .\\nThe definition can be extended to non-square matrices, but for simplicity we consider here only the case of square matrices.\\nThe following lemma presents the basic computational rules of the Kronecker product.\\nLemma 20.2 For all square matrices A, B, C over K , the following computational rules hold:\\n(1) A ⊗ (B ⊗ C) = (A ⊗ B) ⊗ C.\\n1 Leopold\\n\\nKronecker (1832–1891) is said to have used this product in his lectures in Berlin in the\\n1880s.\\nIt was defined formally for the first time in 1858 by Johann Georg Zehfuss (1832–1901).\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_20\\n\\n303</td>\n",
       "      <td>301.0</td>\n",
       "      <td>301</td>\n",
       "      <td>20 The Kronecker Product and Linear Matrix Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chapter kronecker product linear matrix equation many application particular stability analysis differential equation lead linear matrix equation ax x b matrix b c given goal determine matrix x solves equation give formal definition description solution equation kronecker another product matrix useful chapter develop important property product study application context linear matrix equation many result topic found book definition k field ai j k b k n n b b b ai j b b amm b called kronecker product b kronecker product sometimes called tensor product matrix product defines map k k n n k mn mn definition extended non-square matrix simplicity consider case square matrix following lemma present basic computational rule kronecker product lemma square matrix b c k following computational rule hold b c b leopold kronecker said used product lecture berlin defined formally first time johann georg zehfuss springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>304\\n\\n20 The Kronecker Product and Linear Matrix Equations\\n\\n(2)\\n(3)\\n(4)\\n(5)\\n\\n(μA) ⊗ B = A ⊗ (μB) = μ(A ⊗ B) for all μ ∈ K .\\n(A + B) ⊗ C = (A ⊗ C) + (B ⊗ C), whenever A + B is defined.\\nA ⊗ (B + C) = (A ⊗ B) + (A ⊗ C), whenever B + C is defined.\\n(A ⊗ B)T = A T ⊗ B T , and therefore the Kronecker product of two symmetric matrices is symmetric.\\n\u0005\\n\u0004\\n\\nProof Exercise.\\n\\nIn particular, in contrast to the standard matrix multiplication, the order of the factors in the Kronecker product does not change under transposition.\\nThe following result describes the matrix multiplication of two Kronecker products.\\nLemma 20.3 For A, C ∈ K m,m and B, D ∈ K n,n we have\\n(A ⊗ B)(C ⊗ D) = (AC) ⊗ (B D).\\nHence, in particular,\\n(1) A ⊗ B = (A ⊗ In )(Im ⊗ B) = (Im ⊗ B)(A ⊗ In ),\\n(2) (A ⊗ B)−1 = A−1 ⊗ B −1 , if A and B are invertible.\\nProof Since A ⊗ B = [ai j B] and C ⊗ D = [ci j D], the block Fi j ∈ K n,n in the block matrix [Fi j ] = (A ⊗ B)(C ⊗ D) is given by\\nFi j = m m m\\n\b\\n\b\\n\b\\n(aik B)(ck j D) = aik ck j B D = aik ck j B D.\\nk=1 k=1 k=1\\n\\nFor the block matrix [G i j ] = (AC) ⊗ (B D) with G i j ∈ K n,n we obtain\\nG i j = gi j B D, where gi j = m\\n\b aik ck j , k=1 which shows (A ⊗ B)(C ⊗ D) = (AC) ⊗ (B D).\\nNow (1) and (2) easily follow from this equation.\\n\u0005\\n\u0004\\nIn general the Kronecker product is non-commutative (cp.\\nExercise 20.2), but we have the following relationship between A ⊗ B and B ⊗ A.\\nLemma 20.4 For A ∈ K m,m and B ∈ K n,n there exists a permutation matrix\\nP ∈ K mn,mn with\\nP T (A ⊗ B)P = B ⊗ A.\\nProof Exercise.\\n\\n\u0005\\n\u0004\\n\\nFor the computation of the determinant, trace and rank of a Kronecker product there exist simple formulas.</td>\n",
       "      <td>302.0</td>\n",
       "      <td>302</td>\n",
       "      <td>20 The Kronecker Product and Linear Matrix Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kronecker product linear matrix equation μa b μb μ b μ k b c c b c whenever b defined b c b c whenever b c defined b b therefore kronecker product two symmetric matrix symmetric proof exercise particular contrast standard matrix multiplication order factor kronecker product change transposition following result describes matrix multiplication two kronecker product lemma c k b k n n b c ac b hence particular b im b im b b b b invertible proof since b ai j b c ci j block fi j k n n block matrix fi j b c given fi j aik b ck j aik ck j b aik ck j b block matrix g j ac b g j k n n obtain g j gi j b gi j aik ck j show b c ac b easily follow equation general kronecker product non-commutative cp exercise following relationship b b lemma k b k n n exists permutation matrix p k mn mn p b p b proof exercise computation determinant trace rank kronecker product exist simple formula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>20 The Kronecker Product and Linear Matrix Equations\\n\\n305\\n\\nTheorem 20.5 For A ∈ K m,m and B ∈ K n,n the following rules hold:\\n(1) det(A ⊗ B) = (det A)n (det B)m = det(B ⊗ A).\\n(2) trace(A ⊗ B) = trace(A) trace(B) = trace(B ⊗ A).\\n(3) rank(A ⊗ B) = rank(A) rank(B) = rank(B ⊗ A).\\nProof (1) From (1) in Lemma 20.3 and the multiplication theorem for determinants\\n(cp.\\nTheorem 7.15) we get det(A ⊗ B) = det ((A ⊗ In ) (Im ⊗ B)) = det(A ⊗ In ) det(Im ⊗ B).\\nBy Lemma 20.4 there exists a permutation matrix P with A⊗ In = P(In ⊗ A)P T .\\nThis implies that det(A ⊗ In ) = det P(In ⊗ A)P T = det(In ⊗ A) = (det A)n .\\nSince det(Im ⊗ B) = (det B)m , it then follows that det(A ⊗ B) = (det A)n\\n(det B)m , and therefore also det(A ⊗ B) = det(B ⊗ A).\\n(2) From (A ⊗ B) = [ai j B] we obtain trace(A ⊗ B) = m \b n\\n\b i=1 j=1 aii b j j = m\\n\b i=1 aii n\\n\b b j j = trace(A) trace(B) j=1\\n\\n= trace(B) trace(A) = trace(B ⊗ A).\\n\u0005\\n\u0004\\n\\n(3) Exercise.\\n\\nFor a matrix A = [a1 , . . . , an ] ∈ K m,n with columns a j ∈ K m,1 , j = 1, . . . , n, we define\\n⎡ ⎤ a1\\n⎢ a2 ⎥\\n⎢ ⎥ vec(A) := ⎢ . ⎥ ∈ K mn,1 .\\n⎣ .. ⎦ an\\nThe application of vec turns the matrix A into a “column vector” and thus “vectorizes”\\nA.\\nLemma 20.6 The map vec : K m,n → K mn,1 is an isomorphism.\\nIn particular,\\nA1 , . . . , Ak ∈ K m,n are linearly independent if and only if vec(A1 ), . . . , vec(Ak ) ∈\\nK mn,1 are linearly independent.\\nProof Exercise.\\n\\n\u0005\\n\u0004\\n\\nWe now consider the relationship between the Kronecker product and the vec map.</td>\n",
       "      <td>303.0</td>\n",
       "      <td>303</td>\n",
       "      <td>20 The Kronecker Product and Linear Matrix Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kronecker product linear matrix equation theorem k b k n n following rule hold det b det n det b det b trace b trace trace b trace b rank b rank rank b rank b proof lemma multiplication theorem determinant cp theorem get det b det im b det det im b lemma exists permutation matrix p p p implies det det p p det det n since det im b det b follows det b det n det b therefore also det b det b b ai j b obtain trace b n aii b j j aii n b j j trace trace b trace b trace trace b exercise matrix k n column j k j n define vec k application vec turn matrix column vector thus vectorizes lemma map vec k n k isomorphism particular ak k n linearly independent vec vec ak k linearly independent proof exercise consider relationship kronecker product vec map</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>306\\n\\n20 The Kronecker Product and Linear Matrix Equations\\n\\nTheorem 20.7 For A ∈ K m,m , B ∈ K n,n and C ∈ K m,n we have vec(AC B) = (B T ⊗ A)vec(C).\\nHence, in particular,\\n(1) vec(AC) = (In ⊗ A)vec(C) and vec(C B) = (B T ⊗ Im )vec(C),\\n(2) vec(AC + C B) = (In ⊗ A) + (B T ⊗ Im ) vec(C).\\nProof For j = 1, . . . , n, the jth column of AC B is given by\\n(AC B)e j = (AC)(Be j ) = n\\n\b bk j (AC)ek = n\\n\b k=1\\n\\n(bk j A)(Cek ) k=1\\n\\n= [ b1 j A, b2 j A, . . . , bn j A ] vec(C), which implies that vec(AC B) = (B T ⊗ A)vec(C).\\nWith B = In resp.\\nA = Im we obtain (1), while (1) and the linearity of vec yield (2).\\n\u0005\\n\u0004\\nIn order to study the relationship between the eigenvalues of the matrices A, B and those of the Kronecker product A⊗B, we use bivariate polynomials, i.e., polynomials in two variables (cp.\\nExercise 9.10).\\nIf p(t1 , t2 ) = l\\n\b j\\n\\nαi j t1i t2 ∈ K [t1 , t2 ] i, j=0 is such a polynomial, then for A ∈ K m,m and B ∈ K n,n we define the matrix p(A, B) := l\\n\b\\n\\nαi j Ai ⊗ B j .\\n\\n(20.1) i, j=0\\n\\nHere we have to be careful with the order of the factors, since in general Ai ⊗ B j \u0007=\\nB j ⊗ Ai (cp.\\nExercise 20.2).\\nExample 20.8 For A ∈ Rm,m , B ∈ Rn,n and p(t1 , t2 ) = 2t1 +3t1 t22 = 2t11 t20 +3t11 t22 ∈\\nR[t1 , t2 ] we get the matrix p(A, B) = 2 A ⊗ In + 3A ⊗ B 2 .\\nThe following result is known as Stephanos’ theorem.2\\n\\n2 Named after Cyparissos Stephanos (1857–1917) who in 1900 showed besides this result also the assertion of Lemma 20.3.</td>\n",
       "      <td>304.0</td>\n",
       "      <td>304</td>\n",
       "      <td>20 The Kronecker Product and Linear Matrix Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kronecker product linear matrix equation theorem k b k n n c k n vec ac b b vec c hence particular vec ac vec c vec c b b im vec c vec ac c b b im vec c proof j n jth column ac b given ac b e j ac j n bk j ac ek n bk j cek j j bn j vec c implies vec ac b b vec c b resp im obtain linearity vec yield order study relationship eigenvalue matrix b kronecker product use bivariate polynomial polynomial two variable cp exercise p l j αi j k polynomial k b k n n define matrix p b l αi j ai b j careful order factor since general ai b j b j ai cp exercise example rm b rn n p r get matrix p b b following result known stephanos named cyparissos stephanos showed besides result also assertion lemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>20 The Kronecker Product and Linear Matrix Equations\\n\\n307\\n\\nTheorem 20.9 Let A ∈ K m,m and B ∈ K n,n be two matrices that have Jordan normal forms and the eigenvalues λ1 , . . . , λm ∈ K and μ1 , . . . , μn ∈ K , respectively.\\nIf p(A, B) is defined as in (20.1), then the following assertions hold:\\n(1) The eigenvalues of p(A, B) are p(λk , μ\u0002 ) for k = 1, . . . , m and \u0002 = 1, . . . , n.\\n(2) The eigenvalues of A ⊗ B are λk · μ\u0002 for k = 1, . . . , m and \u0002 = 1, . . . , n.\\n(3) The eigenvalues of A⊗In +Im ⊗B are λk +μ\u0002 for k = 1, . . . , m and \u0002 = 1, . . . , n.\\nProof Let S ∈ G L m (K ) and T ∈ G L n (K ) be such that S −1 AS = J A and T −1 BT =\\nJ B are in Jordan canonical form.\\nThe matrices J A and J B are upper triangular.\\nThus, j j for all i, j ∈ N0 the matrices J Ai , J B and J Ai ⊗ J B are upper triangular.\\nThe eigenvalues j j j i i i of J A and J B are λ1 , . . . , λm and μ1 , . . . , μn , respectively.\\nThus, p(λk , μ\u0002 ), k =\\n1, . . . , m, \u0002 = 1, . . . , n, are the diagonal entries of the matrix p(J A , J B ).\\nUsing\\nLemma 20.3 we obtain p(A, B) = l\\n\b\\n\\nαi j (S J A S −1 )i ⊗ (T J B T −1 ) j = i, j=0\\n\\n= l\\n\b l\\n\b\\n\\nαi j (S J Ai S −1 ) ⊗ (T J B T −1 ) j i, j=0\\n\\nαi j (S J Ai ) ⊗ (T J B ) (S −1 ⊗ T −1 ) j i, j=0\\n\\n= l\\n\b i, j=0\\n\\nαi j (S ⊗ T )(J Ai ⊗ J B )(S ⊗ T )−1 j\\n\\n⎛\\n\\n= (S ⊗ T ) ⎝ l\\n\b\\n\\n⎞\\nαi j (J Ai ⊗ J B )⎠ (S ⊗ T )−1 j i, j=0\\n\\n= (S ⊗ T ) p(J A , J B )(S ⊗ T )−1 , which implies (1).\\nThe assertions (2) and (3) follow from (1) with p(t1 , t2 ) = t1 t2 and p(t1 , t2 ) =\\n\u0005\\n\u0004 t1 + t2 , respectively.\\nThe following result on the matrix exponential function of a Kronecker product is helpful in applications that involve systems of linear differential equations.\\nLemma 20.10 For A ∈ Cm,m , B ∈ Cn,n and C := (A ⊗ In ) + (Im ⊗ B) we have exp(C) = exp(A) ⊗ exp(B).\\nProof From Lemma 20.3 we know that the matrices A ⊗ In and Im ⊗ B commute.\\nUsing Lemma 17.6 we obtain</td>\n",
       "      <td>305.0</td>\n",
       "      <td>305</td>\n",
       "      <td>20 The Kronecker Product and Linear Matrix Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kronecker product linear matrix equation theorem let k b k n n two matrix jordan normal form eigenvalue λm k μn k respectively p b defined following assertion hold eigenvalue p b p λk k eigenvalue b λk k eigenvalue λk k proof let g l k g l n k j bt j b jordan canonical form matrix j j b upper triangular thus j j j matrix j ai j b j ai j b upper triangular eigenvalue j j j j j b λm μn respectively thus p λk k n diagonal entry matrix p j j b using lemma obtain p b l αi j j j b j l l αi j j ai j b j αi j j ai j b j l αi j j ai j b j l αi j j ai j b j p j j b implies assertion follow p p respectively following result matrix exponential function kronecker product helpful application involve system linear differential equation lemma cm b cn n c im b exp c exp exp b proof lemma know matrix im b commute using lemma obtain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>308\\n\\n20 The Kronecker Product and Linear Matrix Equations exp(C) = exp(A ⊗ In + Im ⊗ B) = exp(A ⊗ In ) exp(Im ⊗ B)\\n⎛\\n⎞\u0011\\n\u0012\\n∞\\n∞\\n\b\\n\b\\n1\\n1 j i\\n(A ⊗ In ) ⎠\\n(Im ⊗ B)\\n=⎝ j!\\ni!\\nj=0 i=0\\n=\\n\\n∞\\n∞\\n\b\\n1 \b1\\n(A ⊗ In ) j (Im ⊗ B)i j!\\ni!\\nj=0 i=0\\n\\n=\\n\\n∞\\n∞\\n\b\\n1 \b1 j\\n(A ⊗ B i ) j!\\ni!\\nj=0 i=0\\n\\n= exp(A) ⊗ exp(B), where we have used the properties of the matrix exponential series\\n(cp.\\nSect.\\n17.1).\\n\u0005\\n\u0004\\nFor given matrices A j ∈ K m,m , B j ∈ K n,n , j = 1, . . . , q, and C ∈ K m,n an equation of the form\\nA1 X B1 + A2 X B2 + . . . + Aq X Bq = C\\n\\n(20.2) is called a linear matrix equation for the unknown matrix X ∈ K m,n .\\nTheorem 20.11 A matrix \u0013\\nX ∈ K m,n solves (20.2) if and only if \u0013 x := vec( \u0013\\nX) ∈ mn,1 solves the linear system of equations\\nK\\nGx = vec(C), where G := q\\n\b\\n\\nB Tj ⊗ A j .\\nj=1\\n\\n\u0005\\n\u0004\\n\\nProof Exercise.\\nWe now consider two special cases of (20.2).\\nTheorem 20.12 For A ∈ Cm,m , B ∈ Cn,n and C ∈ Cm,n the Sylvester equation3\\nAX + X B = C\\n\\n(20.3) has a unique solution if and only if A and −B have no common eigenvalue.\\nIf all eigenvalues of A and B have negative real parts, then the unique solution of (20.3) is given by\\n\u0014∞\\n\u0013\\nX = − exp(t A)C exp(t B)dt.\\n0\\n\\n(As in Sect.\\n17.2 the integral is defined entrywise.)\\n3 James\\n\\nJoseph Sylvester (1814–1897).</td>\n",
       "      <td>306.0</td>\n",
       "      <td>306</td>\n",
       "      <td>20 The Kronecker Product and Linear Matrix Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kronecker product linear matrix equation exp c exp im b exp exp im b j im b j j im b j j b j exp exp b used property matrix exponential series cp sect given matrix j k b j k n n j q c k n equation form x x aq x bq c called linear matrix equation unknown matrix x k n theorem matrix x k n solves x vec x solves linear system equation k gx vec c g q b tj j proof exercise consider two special case theorem cm b cn n c cm n sylvester ax x b c unique solution common eigenvalue eigenvalue b negative real part unique solution given x exp c exp b dt sect integral defined entrywise james joseph sylvester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>20 The Kronecker Product and Linear Matrix Equations\\n\\n309\\n\\nProof Analogous to the representation in Theorem 20.11, we can write the Sylvester equation (20.3) as\\n(In ⊗ A + B T ⊗ Im )x = vec(C).\\nIf A and B have the eigenvalues λ1 , . . . , λm and μ1 , . . . , μn , respectively, then G =\\nIn ⊗ A + B T ⊗ Im by (3) in Theorem 20.9 has the eigenvalues λk + μ\u0002 , k = 1, . . . , m,\\n\u0002 = 1, . . . , n.\\nThus, G is invertible, and the Sylvester equation is uniquely solvable, if and only if λk + μ\u0002 \u0007= 0 for all k = 1, . . . , m and \u0002 = 1, . . . , n.\\nLet A and B be matrices with eigenvalues that have negative real parts.\\nThen A and\\n−B have no common eigenvalues and (20.3) has a unique solution.\\nLet J A = S −1 AS and J B = T −1 BT be Jordan canonical forms of A and B. We consider the linear differential equation dZ\\n= AZ + Z B, Z (0) = C,\\n(20.4) dt that is solved by the function\\nZ : [0, ∞) → Cm,n ,\\n\\nZ (t) := exp(t A)C exp(t B)\\n\\n(cp.\\nExercise 20.10).\\nThis function satisfies lim Z (t) = lim exp(t A)C exp(t B) t→∞ t→∞\\n\\n= lim S exp(t J A ) \u0015S −1\u0016\u0017C T\u0018 exp(t J B ) T −1 = 0.\\nt→∞\\n\u0015 \u0016\u0017 \u0018\\n\u0015 \u0016\u0017 \u0018 constant\\n\\n→0\\n\\n→0\\n\\nIntegration of equation (20.4) from t = 0 to t = ∞ yields\\n\u0014∞\\n−C = − Z (0) = lim (Z (t) − Z (0)) = A t→∞\\n\\n⎛∞\\n⎞\\n\u0014\\nZ (t)dt + ⎝ Z (t)dt ⎠ B.\\n\\n0\\n\\n0\\n\\n(Here we use without proof the existence of the infinite integrals.)\\nThis implies that\\n\u0013\\nX := −\\n\\n\u0014∞\\n\\n\u0014∞\\nZ (t)dt = −\\n\\n0 exp(t A)C exp(t B)dt\\n0\\n\\n\u0005\\n\u0004 is the unique solution of (20.3).\\nTheorem 20.12 also gives the solution of another important matrix equation.\\nCorollary 20.13 For A, C ∈ Cn,n the Lyapunov equation4\\nAX + X A H = −C\\n4 Alexandr\\n\\nMikhailovich Lyapunov (also Ljapunov or Liapunov; 1857–1918).\\n\\n(20.5)</td>\n",
       "      <td>307.0</td>\n",
       "      <td>307</td>\n",
       "      <td>20 The Kronecker Product and Linear Matrix Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kronecker product linear matrix equation proof analogous representation theorem write sylvester equation b im x vec c b eigenvalue λm μn respectively g b im theorem eigenvalue λk k thus g invertible sylvester equation uniquely solvable λk k let b matrix eigenvalue negative real part common eigenvalue unique solution let j j b bt jordan canonical form b consider linear differential equation dz az z b z c dt solved function z cm n z exp c exp b cp exercise function satisfies lim z lim exp c exp b lim exp j exp j b constant integration equation yield z lim z z z dt z dt b use without proof existence infinite integral implies x z dt exp c exp b dt unique solution theorem also give solution another important matrix equation corollary c cn n lyapunov ax x h alexandr mikhailovich lyapunov also ljapunov liapunov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>310\\n\\n20 The Kronecker Product and Linear Matrix Equations has a unique solution \u0013\\nX ∈ Cn,n if the eigenvalues of A have negative real parts.\\nIf, furthermore, C is Hermitian positive definite, then also \u0013\\nX is Hermitian positive definite.\\nProof Since by assumption A and −A H have no common eigenvalues, the unique solvability of (20.5) follows from Theorem 20.12, and the solution is given by the matrix\\n\u0013\\nX =−\\n\\n\u0014∞\\n\\n\u0014∞ exp(t A)(−C) exp t A\\n\\nH dt =\\n\\n0 exp(t A)C exp t A H dt.\\n0\\n\\nIf C is Hermitian positive definite, then \u0013\\nX is Hermitian and for x ∈ Cn,1 \\ {0} we have\\n⎞\\n⎛∞\\n\u0014\\n\u0014∞\\nH\u0013\\nH⎝\\nH x Xx = x exp(t A)C exp t A dt ⎠ x = x H exp(t A)C exp t A H x dt &gt; 0.\\n\u0016\u0017\\n\u0018\\n\u0015\\n0\\n\\n0\\n\\n&gt;0\\n\\nThe last inequality follows from the monotonicity of the integral and the fact that for\\n\u0005\\n\u0004 x \u0007= 0 also exp(t A H )x \u0007= 0, since exp t A H is invertible for every real t.\\nExercises\\n20.1\\n20.2\\n20.3\\n20.4\\n20.5\\n20.6\\n20.7\\n20.8\\n20.9\\n20.10\\n\\nProve Lemma 20.2.\\nConstruct two square matrices A, B with A ⊗ B \u0007= B ⊗ A.\\nProve Lemma 20.4.\\nProve Theorem 20.5 (3).\\nProve Lemma 20.6.\\nShow that A ⊗ B is normal if A ∈ Cm,m and B ∈ Cn,n are normal.\\nIs it true that if A ⊗ B is unitary, then A and B are unitary?\\nUse the singular value decompositions of A = V A \u0003 A W AH ∈ Cm,m and B =\\nVB \u0003 B W BH ∈ Cn,n to derive the singular value decomposition of A ⊗ B.\\nShow that for A ∈ Cm,m and B ∈ Cn,n and the matrix 2-norm, the equation\\nA ⊗ B 2 = A 2 B 2 holds.\\nProve Theorem 20.11.\\nLet A ∈ Cm,m , B ∈ Cn,n and C ∈ Cm,n .\\nShow that Z (t) = exp(t A)C exp(t B) is the solution of the matrix differential equation ddtZ = AZ + Z B with the initial condition Z (0) = C.</td>\n",
       "      <td>308.0</td>\n",
       "      <td>308</td>\n",
       "      <td>20 The Kronecker Product and Linear Matrix Equations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kronecker product linear matrix equation unique solution x cn n eigenvalue negative real part furthermore c hermitian positive definite also x hermitian positive definite proof since assumption h common eigenvalue unique solvability follows theorem solution given matrix x exp exp h dt exp c exp h dt c hermitian positive definite x hermitian x h x xx x exp c exp dt x x h exp c exp h x dt last inequality follows monotonicity integral fact x also exp h x since exp h invertible every real exercise prove lemma construct two square matrix b b b prove lemma prove theorem prove lemma show b normal cm b cn n normal true b unitary b unitary use singular value decomposition v w ah cm b vb b w bh cn n derive singular value decomposition b show cm b cn n matrix equation b b hold prove theorem let cm b cn n c cm n show z exp c exp b solution matrix differential equation ddtz az z b initial condition z c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Appendix A\\n\\nA Short Introduction to MATLAB\\n\\nMATLAB1 is an interactive software system for numerical computations, simulations and visualizations.\\nIt contains a large number of predefined functions and allows users to implement their programs in so-called m-files.\\nThe name MATLAB originates from MATrix LABoratory, which indicates the matrix orientation of the software.\\nIndeed, matrices are the major objects in MATLAB.\\nDue to the simple and intuitive use of matrices, we consider MATLAB well suited for teaching in the field of Linear Algebra.\\nIn this short introduction we explain the most important ways to enter and operate with matrices in MATLAB.\\nOne can learn the essential matrix operations as well as important algorithms and concepts in the context of matrices (and Linear Algebra in general) by actively using the MATLAB-Minutes in this book.\\nThese only use predefined functions.\\nA matrix in MATLAB can be entered in form of a list of entries enclosed by square brackets.\\nThe entries in the list are ordered by rows in the natural order of the indices, i.e., from “top to bottom” and “left to right”).\\nA new row starts after every semicolon.\\nFor example, the matrix\\n⎡\\n\\n⎤\\n123\\nA = ⎣4 5 6⎦ is entered in MATLAB by typing A=[1 2 3;4 5 6;7 8 9];\\n789\\n\\nA semicolon after the matrix A suppresses the output in MATLAB.\\nIf it is omitted then MATLAB writes out all the entered or computed quantities.\\nFor example, after entering\\nA=[1 2 3;4 5 6;7 8 9]\\n\\n1 MATLAB® is a registered trademark of The MathWorks Inc.\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7\\n\\n311</td>\n",
       "      <td>309.0</td>\n",
       "      <td>309</td>\n",
       "      <td>Appendix A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>appendix short introduction matlab interactive software system numerical computation simulation visualization contains large number predefined function allows user implement program so-called m-files name matlab originates matrix laboratory indicates matrix orientation software indeed matrix major object matlab due simple intuitive use matrix consider matlab well suited teaching field linear algebra short introduction explain important way enter operate matrix matlab one learn essential matrix operation well important algorithm concept context matrix linear algebra general actively using matlab-minutes book use predefined function matrix matlab entered form list entry enclosed square bracket entry list ordered row natural order index top bottom left right new row start every semicolon example matrix entered matlab typing semicolon matrix suppresses output matlab omitted matlab writes entered computed quantity example entering registered trademark mathworks springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>312\\n\\nAppendix A: A Short Introduction to MATLAB\\n\\nMATLAB gives the output\\nA\\n\\n=\\n1\\n4\\n7\\n\\n2\\n5\\n8\\n\\n3\\n6\\n9\\n\\nOne can access parts of matrices by the corresponding indices.\\nThe list of indices from k to m is abbreviated by k:m .\\nA colon : means all rows for given column indices, or all columns for given row indices.\\nIf A is as above, then for example\\nA(2,1) is the matrix\\n\\nA(3,1:2) is the matrix\\n\\nA(:,2:3) is the matrix\\n\\n[4],\\n[7 8],\\n\u0006\\n\u0007\\n2\\n5\\n8\\n\\n3\\n6 .\\n9\\n\\nThere are several predefined functions that produce matrices.\\nIn particular, for given positive integers n and m, eye(n) the identity matrix In , zeros(n,m) ones(n,m) an n × m matrix with all zeros, an n × m matrix with all ones, rand(n,m) an n × m “random matrix”.\\n\\nSeveral matrices (of appropriate sizes) be combined to a new matrix.\\nFor example, the commands\\nA=eye(2);\\n\\nB=[4;3];\\n\\nC=[2 -1];\\n\\nD=[-5]; E=[A B;C D] lead to\\nE\\n\\n=\\n1\\n0\\n2\\n\\n0\\n1\\n-1\\n\\n4\\n3\\n-5\\n\\nThe help function in MATLAB is started with the command help.\\nIn order to get information about specific functions one adds the name of the function.\\nFor example:</td>\n",
       "      <td>310.0</td>\n",
       "      <td>310</td>\n",
       "      <td>Appendix A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>appendix short introduction matlab matlab give output one access part matrix corresponding index list index k abbreviated k colon mean row given column index column given row index example matrix matrix matrix several predefined function produce matrix particular given positive integer n eye n identity matrix zero n one n n matrix zero n matrix one rand n n random matrix several matrix appropriate size combined new matrix example command b c lead e help function matlab started command help order get information specific function one add name function example</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Appendix A: A Short Introduction to MATLAB\\n\\nInput: help ops\\n\\nInformation on: operations and operators in MATLAB\\n(in particular addition, multiplication, transposition) help matfun MATLAB functions that operate with matrices help gallery collection of example matrices help det determinant help expm matrix exponential function\\n\\n313</td>\n",
       "      <td>311.0</td>\n",
       "      <td>311</td>\n",
       "      <td>Appendix A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>appendix short introduction matlab input help ops information operation operator matlab particular addition multiplication transposition help matfun matlab function operate matrix help gallery collection example matrix help det determinant help expm matrix exponential function</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     content  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Chapter 1\\n\\nLinear Algebra in Every Day Life\\n\\nOne has to familiarize the student with actual questions from applications, so that he learns to deal with real world problems.1\\nLothar Collatz (1910–1990)\\n\\n1.1 The PageRank Algorithm\\nThe PageRank algorithm is a method to assess the “importance” of documents with mutual links, such as web pages, on the basis of the link structure.\\nIt was developed by Sergei Brin and Larry Page, the founders of Google Inc., at Stanford University in the late 1990s.\\nThe basic idea of the algorithm is the following:\\nInstead of counting links, PageRank essentially interprets a link of page A to page\\nB as a vote of page A for page B. PageRank then assesses the importance of a page by the number of received votes.\\nPageRank also considers the importance of the page that casts the vote, since votes of some pages have a higher value, and thus also assign a higher value to the page they point to.\\nImportant pages will be rated higher and thus lead to a higher position in the search results.2\\nLet us describe (model) this idea mathematically.\\nOur presentation uses ideas from the article [BryL06].\\nFor a given set of web pages, every page k will be assigned an importance value xk ≥ 0.\\nA page k is more important than a page j if xk > x j .\\nIf a page k has a link to a page j, we say that page j has a backlink from page k.\\nIn the above description these backlinks are the votes.\\nAs an example, consider the following link structure:\\n\\n1 “Man muss den Lernenden mit konkreten Fragestellungen aus den Anwendungen vertraut machen, dass er lernt, konkrete Fragen zu behandeln.” of a text found in 2010 on http://www.google.de/corporate/tech.html.\\n\\n2 Translation\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_1\\n\\n1   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      2\\n\\n1 Linear Algebra in Every Day Life\\n\\nHere the page 1 has links to the pages 2, 3 and 4, and a backlink from page 3.\\nThe easiest approach to define importance of web pages is to count its backlinks; the more votes are cast for a page, the more important the page is.\\nIn our example this gives the importance values x1 = 1, x2 = 3, x3 = 2, x4 = 3.\\nThe pages 2 and 4 are thus the most important pages, and they are equally important.\\nHowever, the intuition and also the above description from Google suggests that backlinks from important pages are more important for the value of a page than those from less important pages.\\nThis idea can be modeled by defining xk as the sum of all importance values of the backlinks of the page k.\\nIn our example this results in four equations that have to be satisfied simultaneously, x1 = x3 , x2 = x1 + x3 + x4 , x3 = x1 + x4 , x4 = x1 + x2 + x3 .\\nA disadvantage of this approach is that it does not consider the number of links of the pages.\\nThus, it would be possible to (significantly) increase the importance of a page just by adding links to that page.\\nIn order to avoid this, the importance values of the backlinks in the PageRank algorithm are divided by the number of links of the corresponding page.\\nThis creates a kind of “internet democracy”: Every page can vote for other pages, where in total it can cast one vote.\\nIn our example this gives the equations x1 = x3 x4 x4 x3 x1 x1 x1 x3\\n, x2 =\\n+\\n+ , x3 =\\n+ , x4 =\\n+ x2 + .\\n3\\n3\\n3\\n2\\n3\\n2\\n3\\n3\\n\\n(1.1)\\n\\nThese are four equations for the four unknowns, and all equations are linear,3 i.e., the unknowns occur only in first power.\\nIn Chap.\\n6 we will see how to write the equations in (1.1) in form of a linear system of equations.\\nAnalyzing and solving such systems is one of the most important tasks of Linear Algebra.\\nThe example of the PageRank algorithm shows that Linear Algebra presents a powerful modeling\\n3 The term “linear” originates from the Latin word “linea”, which means “(straight) line”, and\\n“linearis” means “consisting of (straight) lines”.   \n",
       "2                                                                                                                                                                                                                         1.1 The PageRank Algorithm\\n\\n3 tool: We have turned the real world problem of assessing the importance of web pages into a problem of Linear Algebra.\\nThis problem will be examined further in\\nSect.\\n8.3.\\nFor completeness, we mention that a solution for the four unknowns (computed with MATLAB and rounded to the second significant digit) is given by x1 = 0.14, x2 = 0.54, x3 = 0.41, x4 = 0.72.\\nThus, page 4 is the most important one.\\nIt is possible to multiply the solution, i.e., the importance values xk , by a positive constant.\\nSuch a multiplication or scaling is often advantageous for computational methods or for the visual display of the results.\\nFor example, the scaling could be used to give the most important page the value 1.00.\\nA scaling is allowed, since it does not change the ranking of the pages, which is the essential information provided by the PageRank algorithm.\\n\\n1.2 No Claim Discounting in Car Insurances\\nInsurance companies compute the premiums for their customers on the basis of the insured risk: the higher the risk, the higher the premium.\\nIt is therefore important to identify the factors that lead to higher risk.\\nIn the case of a car insurance these factors include the number of miles driven per year, the distance between home and work, the marital status, the engine power, or the age of the driver.\\nUsing such information, the company calculates the initial premium.\\nUsually the best indicator for future accidents, and hence future insurance claims, is the number of accidents of the individual customer in the past, i.e., the claims history.\\nIn order to incorporate this information into the premium rates, insurers establish a system of risk classes, which divide the customers into homogeneous risk groups with respect to their previous claims history.\\nCustomers with fewer accidents in the past get a discount on their premium.\\nThis approach is called a no claims discounting scheme.\\nFor a mathematical model of this scheme we need a set of risk classes and a transition rule for moving between the classes.\\nAt the end of a policy year, the customer may move to a different class depending on the claims made during the year.\\nThe discount is given in percent of the premium in the initial class.\\nAs a simple example we consider four risk classes,\\nC1 C2 C3 C4\\n% discount 0 10 20 40 and the following transition rules:\\n• No accident: Step up one class (or stay in C4 ).   \n",
       "3                                                                                                          4\\n\\n1 Linear Algebra in Every Day Life\\n\\n• One accident: Step back one class (or stay in C1 ).\\n• More than one accident: Step back to class C1 (or stay in C1 ).\\nNext, the insurance company has to estimate the probability that a customer who is in the class Ci in this year will move to the class C j .\\nThis probability is denoted by pi j .\\nLet us assume, for simplicity, that the probability of exactly one accident for every customer is 0.1, i.e., 10 %, and the probability of two or more accidents for every customer is 0.05, i.e., 5 %.\\n(Of course, in practice the insurance companies determine these probabilities in dependence of the classes.)\\nFor example, a customer in the class C1 will stay in C1 in case of at least one accident.\\nThis happens with the probability 0.15, so that p11 = 0.15.\\nA customer in\\nC1 has no accident with the probability 0.85, so that p12 = 0.85.\\nThere is no chance to move from C1 to C3 or C4 in the next year, so that p13 = p14 = 0.00.\\nIn this way we obtain 16 values pi j , i, j = 1, 2, 3, 4, which we can arrange in a 4 × 4 matrix as follows:\\n⎡\\n⎤\\n⎡\\n⎤ p11 p12 p13 p14\\n0.15 0.85 0.00 0.00\\n⎢ p21 p22 p23 p24 ⎥\\n⎢0.15 0.00 0.85 0.00⎥\\n⎢\\n⎥\\n⎢\\n⎥\\n(1.2)\\n⎣ p31 p32 p33 p34 ⎦ = ⎣0.05 0.10 0.00 0.85⎦.\\np41 p42 p43 p44\\n0.05 0.00 0.10 0.85\\nAll entries of this matrix are nonnegative real numbers, and the sum of all entries in each row is equal to 1.00, i.e., pi1 + pi2 + pi3 + pi4 = 1.00 for each i = 1, 2, 3, 4.\\nSuch a matrix is called row-stochastic.\\nThe analysis of matrix properties is a central topic of Linear Algebra that is developed throughout this book.\\nAs in the example with the PageRank algorithm, we have translated a practical problem into the language of Linear Algebra, and we can now study it using Linear Algebra techniques.\\nThis example of premium rates will be discussed further in Example 4.7.\\n\\n1.3 Production Planning in a Plant\\nThe production planning in a plant has to consider many different factors, in particular commodity prices, labor costs, and available capital, in order to determine a production plan.\\nWe consider a simple example:\\nA company produces the products P1 and P2 .\\nIf xi units of the product Pi are produced, where i = 1, 2, then the pair (x1 , x2 ) is called a production plan.\\nSuppose that the raw materials and labor for the production of one unit of the product Pi cost a1i and a2i Euros, respectively.\\nIf b1 Euros are available for the purchase of raw materials and b2 Euros for the payment of labor costs, then a production plan must   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1.3 Production Planning in a Plant\\n\\n5 satisfy the constraint inequalities a11 x1 + a12 x2 ≤ b1 , a21 x1 + a22 x2 ≤ b2 .\\nIf a production plan satisfies these constraints, it is called feasible.\\nLet pi be the profit from selling one unit of product Pi .\\nThen the goal is to determine a production plan that maximizes the profit function\\n\u0002(x1 , x2 ) = p1 x1 + p2 x2 .\\nHow can we find this maximum?\\nThe two equations a11 x1 + a12 x2 = b1 and a21 x1 + a22 x2 = b2 describe straight lines in the coordinate system that has the variables x1 and x2 on its axes.\\nThese two lines form boundary lines of the feasible production plans, which are\\n“below” the lines; see the figure below.\\nNote that we also must have xi ≥ 0, since we cannot produce negative units of a product.\\nFor planned profits yi , i = 1, 2, 3, . . . , the equations p1 x1 + p2 x2 = yi describe parallel straight lines in the coordinate system; see the dashed lines in the figure.\\nIf x1 and x2 satisfy p1 x1 + p2 x2 = yi , then\\n\u0002(x1 , x2 ) = yi .\\nThe profit maximization problem can now be solved by moving the dashed lines until one of them reaches the corner with the maximal y:\\n\\nIn case of more variables we cannot draw such a simple figure and obtain the solution “graphically”.\\nBut the general idea of finding a corner with the maximum profit is still the same.\\nThis is an example of a linear optimization problem.\\nAs before, we have formulated a real world problem in the language of Linear Algebra, and we can use mathematical methods for its solution.\\n\\n1.4 Predicting Future Profits\\nThe prediction of profits or losses of a company is a central planning instrument of economics.\\nAnalogous problems arise in many areas of political decision making,   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                   6\\n\\n1 Linear Algebra in Every Day Life for example in budget planning, tax estimates or the planning of new infrastructures.\\nWe consider a specific example:\\nIn the four quarters of a year a company has profits of 10, 8, 9, 11 million Euros.\\nThe board now wants to predict the future profits development on the basis of these values.\\nEvidence suggests, that the profits behave linearly.\\nIf this was true, then the profits would form a straight line y(t) = αt + β that connects the points\\n(1, 10), (2, 8), (3, 9), (4, 11) in the coordinate system having “time” and “profit” as its axes.\\nThis, however, does neither hold in this example nor in practice.\\nTherefore one tries to find a straight line that deviates “as little as possible” from the given points.\\nOne possible approach is to choose the parameters α and β in order to minimize the sum of the squared distances between the given points and the straight line.\\nOnce the parameters α and β have been determined, the resulting line y(t) can be used for estimating or predicting the future profits, as illustrated in the following figure:\\n\\nThe determination of the parameters α and β that minimize a sum of squares is called a least squares problem.\\nWe will solve least squares problems using methods of Linear Algebra in Example 12.16.\\nThe approach itself is sometimes called a parameter identification.\\nIn Statistics, the modeling of given data (here the company profits) using a linear predictor function (here y(t) = αt + β) is known as linear regression.\\n\\n1.5 Circuit Simulation\\nThe current development of electronic devices is very rapid.\\nIn short intervals, nowadays often less than a year, new models of laptops or mobile phones have to be issued to the market.\\nTo achieve this, continuously new generations of computer chips have to be developed.\\nThese typically become smaller and more powerful, and naturally should use as little energy as possible.\\nAn important factor in this development is to plan and simulate the chips virtually, i.e., in the computer and without producing a physical prototype.\\nThis model-based planning and optimization of products is a central method in many high technology areas, and it is based on modern mathematics.   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1.5 Circuit Simulation\\n\\n7\\n\\nUsually, the switching behavior of a chip is modeled by a mathematical system consisting of differential and algebraic equations that describe the relation between currents and voltages.\\nWithout going into details, consider the following circuit:\\n\\nIn this circuit description, VS (t) is the given input current at time t, and the characteristic values of the components are R for the resistor, L for the inductor, and\\nC for the capacitor.\\nThe functions for the potential differences at the three components are denoted by VR (t), VL (t), and VC (t); I (t) is the current.\\nApplying the Kirchhoff laws4 of electrical engineering leads to the following system of linear equations and differential equations that model the dynamic behavior of the circuit: d\\nI = VL , dt d\\nC VC = I, dt\\nR I = VR ,\\nL\\n\\nVL + VC + VR = VS .\\nIn this example it is easy to solve the last two equations for VL and VR , and hence to obtain a system of differential equations\\nR\\n1\\n1 d\\nI = − I − VC + VS , dt\\nL\\nL\\nL d\\n1\\nVC = − I, dt\\nC for the functions I und VC .\\nWe will discuss and solve this system in Example 17.13.\\nThis simple example demonstrates that for the simulation of a circuit a system of linear differential equations and algebraic equations has to be solved.\\nModern computer chips in industrial practice require solving such systems with millions of differential-algebraic equations.\\nLinear Algebra is one of central tools for the theoretical analysis of such systems as well as the development of efficient solution methods.\\n\\n4 Gustav\\n\\nRobert Kirchhoff (1824–1887).   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Chapter 2\\n\\nBasic Mathematical Concepts\\n\\nIn this chapter we introduce the mathematical concepts that form the basis for the developments in the following chapters.\\nWe begin with sets and basic mathematical logic.\\nThen we consider maps between sets and their most important properties.\\nFinally we discuss relations and in particular equivalence relations on a set.\\n\\n2.1 Sets and Mathematical Logic\\nWe begin our development with the concept of a set and use the following definition of Cantor.1\\nDefinition 2.1 A set is a collection M of well determined and distinguishable objects x of our perception or our thinking.\\nThe objects are called the elements of M.\\nThe objects x in this definition are well determined, and therefore we can uniquely decide whether x belongs to a set M or not.\\nWe write x ∈ M if x is an element of the set M, otherwise we write x ∈\\n/ M. Furthermore, the elements are distinguishable, which means that all elements of M are (pairwise) distinct.\\nIf two objects x and y are equal, then we write x = y, otherwise x \u0003= y.\\nFor mathematical objects we usually have to give a formal definition of equality.\\nAs an example consider the equality of sets; see Definition 2.2 below.\\nWe describe sets with curly brackets { } that contain either a list of the elements, for example\\n{red, yellow, green}, {1, 2, 3, 4}, {2, 4, 6, . . . },\\n\\n1 Georg\\n\\nCantor (1845–1918), one of the founders of set theory.\\nCantor published this definition in the journal “Mathematische Annalen” in 1895.\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_2\\n\\n9   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             10\\n\\n2 Basic Mathematical Concepts or a defining property, for example\\n{x | x is a positive even number},\\n{x | x is a person owning a bike}.\\nSome of the well known sets of numbers are denoted as follows:\\nN = {1, 2, 3, . . . }\\nN0 = {0, 1, 2, . . . }\\n\\n(the natural numbers),\\n(the natural numbers including zero),\\n\\nZ = {. . . , −2, −1, 0, 1, 2, . . . }\\n(the integers),\\nQ = {x | x = a/b with a ∈ Z and b ∈ N} (the rational numbers),\\nR = {x | x is a real number}\\n(the real numbers).\\nThe construction and characterization of the real numbers R is usually done in an introductory course in Real Analysis.\\nTo describe a set via its defining property we formally write {x | P(x)}.\\nHere\\nP is a predicate which may hold for an object x or not, and P(x) is the assertion\\n“P holds for x”.\\nIn general, an assertion is a statement that can be classified as either “true” or\\n“false”.\\nFor instance the statement “The set N has infinitely many elements” is true.\\nThe sentence “Tomorrow the weather will be good” is not an assertion, since the meaning of the term “good weather” is unclear and the weather prediction in general is uncertain.\\nThe negation of an assertion A is the assertion “not A”, which we denote by ¬A.\\nThis assertion is true if and only if A is false, and false if and only if A is true.\\nFor instance, the negation of the true assertion “The set N has infinitely many elements” is given by “The set N does not have infinitely many elements” (or “The set N has finitely many elements”), which is false.\\nTwo assertions A and B can be combined via logical compositions to a new assertion.\\nThe following is a list of the most common logical compositions, together with their mathematical short hand notation:\\nComposition conjunction disjunction implication\\n\\nNotation\\n∧\\n∨\\n⇒ equivalence\\n\\n⇔\\n\\nWording\\nA and B\\nA or B\\nA implies B\\nIf A then B\\nA is a sufficient condition for B\\nB is a necessary condition for A\\nA and B are equivalent\\nA is true if and only if B is true\\nA is necessary and sufficient for B\\nB is necessary and sufficient for A   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2.1 Sets and Mathematical Logic\\n\\n11\\n\\nFor example, we can write the assertion “x is a real number and x is negative” as x ∈ R ∧ x < 0.\\nWhether an assertion that is composed of two assertions A and B is true or false, depends on the logical values of A and B. We have the following table of logical values (“t” and “f” denote true and false, respectively):\\nA t t f f\\n\\nB t f t f\\n\\nA∧B t f f f\\n\\nA∨B t t t f\\n\\nA⇒B t f t t\\n\\nA⇔B t f f t\\n\\nFor example, the assertion A ∧ B is true only when A and B are both true.\\nThe assertion A ⇒ B is false only when A is true and B is false.\\nIn particular, if A is false, then A ⇒ B is true, independent of the logical value of B.\\nThus, 3 < 5 ⇒ 2 < 4 is true, since 3 < 5 and 2 < 4 are both true.\\nBut\\n3 < 5 ⇒ 2 > 4 is false, since 2 > 4 is false.\\nOn the other hand, the assertions\\n4 < 2 ⇒ 3 > 5 and 4 < 2 ⇒ 3 < 5 are both true, since 4 < 2 is false.\\nIn the following we often have to prove that certain implications A ⇒ B are true.\\nAs the table of logical values shows and the example illustrates, we then only have to prove that under the assumption that A is true the assertion B is true as well.\\nInstead of “Assume that A is true” we will often write “Let A hold”.\\nIt is easy to see that\\n(A ⇒ B)\\n\\n⇔\\n\\n(¬B ⇒ ¬A).\\n\\n(As an exercise create the table of logical values for ¬B ⇒ ¬A and compare it with the table for A ⇒ B.)\\nThe truth of A ⇒ B can therefore be proved by showing that the truth of ¬B implies the truth of ¬A, i.e., that “B is false” implies “A is false”.\\nThe assertion ¬B ⇒ ¬A is called the contraposition of the assertion A ⇒ B and the conclusion from A ⇒ B to ¬B ⇒ ¬A is called proof by contraposition.\\nTogether with assertions we also often use so-called quantifiers:\\nQuantifier universal existential\\n\\nNotation\\n∀\\n∃\\n\\nWording\\nFor all\\nThere exists\\n\\nNow we return to set theory and introduce subsets and the equality of sets.\\nDefinition 2.2 Let M, N be sets.\\n(1) M is called a subset of N , denoted by M ⊆ N , if every element of M is also an element of N .\\nWe write M \u0002 N , if this does not hold.\\n(2) M and N are called equal, denoted by M = N , if M ⊆ N and N ⊆ M. We write M \u0003= N is this does not hold.   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        12\\n\\n2 Basic Mathematical Concepts\\n\\n(3) M is called a proper subset of N , denoted by M ⊂ N , if both M ⊆ N and\\nM \u0003= N hold.\\nUsing the notation of mathematical logic we can write this definition as follows:\\n(1) M ⊆ N\\n(2) M = N\\n(3) M ⊂ N\\n\\n⇔\\n⇔\\n⇔\\n\\n(∀ x : x ∈ M ⇒ x ∈ N ).\\n(M ⊆ N ∧ N ⊆ M).\\n(M ⊆ N ∧ M \u0003= N ).\\n\\nThe assertion on the right side of the equivalence in (1) reads as follows: For all objects x the truth of x ∈ M implies the truth of x ∈ N .\\nOr shorter: For all x, if x ∈ M holds, then x ∈ N holds.\\nA very special set is the set with no elements, which we define formally as follows.\\nDefinition 2.3 The set Ø := {x | x \u0003= x} is called the empty set.\\nThe notation “:=” means is defined as.\\nWe have introduced the empty set by a defining property: Every object x with x \u0003= x is any element of Ø.\\nThis cannot hold for any object, and hence Ø does not contain any element.\\nA set that contains at least one element is called nonempty.\\nTheorem 2.4 For every set M the following assertions hold:\\n(1) Ø ⊆ M.\\n(2) M ⊆ Ø ⇒ M = Ø.\\nProof\\n(1) We have to show that the assertion “∀ x : x ∈ Ø ⇒ x ∈ M” is true.\\nSince there is no x ∈ Ø, the assertion “x ∈ Ø” is false, and therefore “x ∈ Ø ⇒ x ∈ M” is true for every x (cp. the remarks on the implication A ⇒ B).\\n(2) Let M ⊆ Ø.\\nFrom (1) we know that Ø ⊆ M and hence M = Ø follows by (2) in Definition 2.2.\\nTheorem 2.5 Let M, N , L be sets.\\nThen the following assertions hold for the subset relation “⊆”:\\n(1) M ⊆ M (reflexivity).\\n(2) If M ⊆ N and N ⊆ L, then M ⊆ L (transitivity).\\nProof\\n(1) We have to show that the assertion “∀ x : x ∈ M ⇒ x ∈ M” is true.\\nIf “x ∈ M” is true, then “x ∈ M ⇒ x ∈ M” is an implication with two true assertions, and hence it is true.\\n(2) We have to show that the assertion “∀ x : x ∈ M ⇒ x ∈ L” is true.\\nIf “x ∈ M” is true, then also “x ∈ N ” is true, since M ⊆ N .\\nThe truth of “x ∈ N ” implies that “x ∈ L” is true, since N ⊆ L. Hence the assertion “x ∈ M ⇒ x ∈ L” is true.   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 2.1 Sets and Mathematical Logic\\n\\n13\\n\\nDefinition 2.6 Let M, N be sets.\\n(1) The union2 of M and N is M ∪ N := {x | x ∈ M ∨ x ∈ N }.\\n(2) The intersection of M and N is M ∩ N := {x | x ∈ M ∧ x ∈ N }.\\n(3) The difference of M and N is M \\ N := {x | x ∈ M ∧ x ∈\\n/ N }.\\nIf M ∩ N = Ø, then the sets M and N are called disjoint.\\nThe set operations union and intersection can be extended to more than two sets: If I \u0003= Ø is a set and if for all i ∈ I there is a set Mi , then\\n\u0002\\n\\nMi := {x | ∃ i ∈ I with x ∈ Mi } and i∈I\\n\\n\u0003\\n\\nMi := {x | ∀ i ∈ I we have x ∈ Mi }.\\ni∈I\\n\\nThe set I is called an index set.\\nFor I = {1, 2, . . . , n} ⊂ N we write the union and intersection of the sets M1 , M2 , . . . , Mn as n\\n\u0002 i=1\\n\\nMi and n\\n\u0003\\n\\nMi .\\ni=1\\n\\nTheorem 2.7 Let M ⊆ N for two sets M, N .\\nThen the following are equivalent:\\n(1) M ⊂ N .\\n(2) N \\ M \u0003= Ø.\\nProof We show that (1) ⇒ (2) and (2) ⇒ (1) hold.\\n(1) ⇒ (2): Since M \u0003= N , there exists an x ∈ N with x ∈\\n/ M. Thus x ∈ N \\ M, so that N \\ M \u0003= Ø holds.\\n(2) ⇒ (1): There exists an x ∈ N with x ∈\\n/ M, and hence N \u0003= M. Since M ⊆ N holds, we see that M ⊂ N holds.\\nTheorem 2.8 Let M, N , L be sets.\\nThen the following assertions hold:\\nM ∩ N ⊆ M and M ⊆ M ∪ N .\\nCommutativity: M ∩ N = N ∩ M and M ∪ N = N ∪ M.\\nAssociativity: M ∩ (N ∩ L) = (M ∩ N ) ∩ L and M ∪ (N ∪ L) = (M ∪ N ) ∪ L.\\nDistributivity: M ∪ (N ∩ L) = (M ∪ N ) ∩ (M ∪ L) and M ∩ (N ∪ L) =\\n(M ∩ N ) ∪ (M ∩ L).\\n(5) M \\ N ⊆ M.\\n(6) M \\ (N ∩ L) = (M \\ N ) ∪ (M \\ L) and M \\ (N ∪ L) = (M \\ N ) ∩ (M \\ L).\\n(1)\\n(2)\\n(3)\\n(4)\\n\\nProof Exercise.\\nnotations M ∪ N and M ∩ N for union and intersection of sets M and N were introduced in 1888 by Giuseppe Peano (1858–1932), one of the founders of formal logic.\\nThe notation of the\\n“smallest common multiple M(M, N )” and “largest common divisor D(M, N )” of the sets M and\\nN suggested by Georg Cantor (1845–1918) did not catch on.\\n\\n2 The   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 14\\n\\n2 Basic Mathematical Concepts\\n\\nDefinition 2.9 Let M be a set.\\n(1) The cardinality of M, denoted by |M|, is the number of elements of M.\\n(1) The power set of M, denoted by P(M), is the set of all subsets of M, i.e.,\\nP(M) := {N | N ⊆ M}.\\nThe empty set Ø has cardinality zero and P(Ø) = {Ø}, thus |P(Ø)| = 1.\\nFor\\nM = {1, 3} the cardinality is |M| = 2 and\\nP(M) = { Ø, {1}, {3}, M }, and hence |P(M)| = 4 = 2|M| .\\nOne can show that for every set M with finitely many elements, i.e., finite cardinality, |P(M)| = 2|M| holds.\\n\\n2.2 Maps\\nIn this section we discuss maps between sets.\\nDefinition 2.10 Let X, Y be nonempty sets.\\n(1) A map f from X to Y is a rule that assigns to each x ∈ X exactly one y = f (x) ∈ Y .\\nWe write this as f : X → Y, x \u0011→ y = f (x).\\n\\nInstead of x \u0011→ y = f (x) we also write f (x) = y.\\nThe sets X and Y are called domain and codomain of f .\\n(2) Two maps f : X → Y and g : X → Y are called equal when f (x) = g(x) holds for all x ∈ X .\\nWe then write f = g.\\nIn Definition 2.10 we have assumed that X and Y are nonempty, since otherwise there can be no rule that assigns an element of Y to each element of X .\\nIf one of these sets is empty, one can define an empty map.\\nHowever, in the following we will always assume (but not always explicitly state) that the sets between which a given map acts are nonempty.\\nExample 2.11 Two maps from X = R to Y = R are given by f : X → Y, f (x) = x 2 ,\\n\u0004\\n0, x ≤ 0, g : X → Y, x \u0011→\\n1, x > 0.\\nTo analyze the properties of maps we need some further terminology.\\n\\n(2.1)\\n(2.2)   \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                2.2 Maps\\n\\n15\\n\\nDefinition 2.12 Let X, Y be nonempty sets.\\n(1) The map Id X : X → X , x \u0011→ x, is called the identity on X .\\n(2) Let f : X → Y be a map and let M ⊆ X and N ⊆ Y .\\nThen f (M) := { f (x) | x ∈ M } ⊆ Y is called the image of M under f , f\\n\\n−1\\n\\n(N ) := { x ∈ X | f (x) ∈ N } is called the pre-image of N under f .\\n\\n(3) If f : X → Y , x \u0011→ f (x) is a map and Ø \u0003= M ⊆ X , then f | M : M → Y , x \u0011→ f (x), is called the restriction of f to M.\\nOne should note that in this definition f −1 (N ) is a set, and hence the symbol f −1 here does not mean the inverse map of f .\\n(This map will be introduced below in\\nDefinition 2.21.)\\nExample 2.13 For the maps with domain X = R in (2.1) and (2.2) we have the following properties: f (X ) = {x ∈ R | x ≥ 0}, f −1 (R− ) = {0}, f −1 ({−1}) = Ø, g(X ) = {0, 1}, g −1 (R− ) = g −1 ({0}) = R− , where R− := {x ∈ R | x ≤ 0}.\\nDefinition 2.14 Let X, Y be nonempty sets.\\nA map f : X → Y is called\\n(1) injective, if for all x1 , x2 ∈ X the equality f (x1 ) = f (x2 ) implies that x1 = x2 ,\\n(2) surjective, if f (X ) = Y ,\\n(3) bijective, if f is injective and surjective.\\nFor every nonempty set X the simplest example of a bijective map from X to X is Id X , the identity on X .\\nExample 2.15 Let R+ := {x ∈ R | x ≥ 0}, then f : R → R, f (x) = x 2 , is neither injective nor surjective.\\nf : R → R+ , f (x) = x 2 , is surjective but not injective.\\nf : R+ → R, f (x) = x 2 , is injective but not surjective.\\nf : R+ → R+ , f (x) = x 2 , is bijective.\\nIn these assertions we have used the continuity of the map f (x) = x 2 that is discussed in the basic courses on analysis.\\nIn particular, we have used the fact that continuous functions map real intervals to real intervals.\\nThe assertions also show why it is important to include the domain and codomain in the definition of a map.\\nTheorem 2.16 A map f : X → Y is bijective if and only if for every y ∈ Y there exists exactly one x ∈ X with f (x) = y.\\nProof ⇒: Let f be bijective and let y1 ∈ Y .\\nSince f is surjective, there exists an x1 ∈ X with f (x1 ) = y1 .\\nIf some x2 ∈ X also satisfies f (x2 ) = y1 , then x1 = x2   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  16\\n\\n2 Basic Mathematical Concepts follows from the injectivity of f .\\nTherefore, there exists a unique x1 ∈ X with f (x1 ) = y1 .\\n⇐: Since for all y ∈ Y there exists a unique x ∈ X with f (x) = y, it follows that f (X ) = Y .\\nThus, f surjective.\\nLet now x1 , x2 ∈ X with f (x1 ) = f (x2 ) = y ∈ Y .\\nThen the assumption implies x1 = x2 , so that f is also injective.\\nOne can show that between two sets X and Y of finite cardinality there exists a bijective map if and only if |X | = |Y |.\\nLemma 2.17 For sets X, Y with |X | = |Y | = m ∈ N, there exist exactly m! :=\\n1 · 2 · . . . · m pairwise distinct bijective maps between X and Y .\\nProof Exercise.\\nDefinition 2.18 Let f : X → Y , x \u0011→ f (x), and g : Y → Z , y \u0011→ g(y) be maps.\\nThen the composition of f and g is the map g ◦ f : X → Z, x \u0011→ g( f (x)).\\n\\nThe expression g ◦ f should be read “g after f ”, which stresses the order of the composition: First f is applied to x and then g to f (x).\\nOne immediately sees that f ◦ Id X = f = IdY ◦ f for every map f : X → Y .\\nTheorem 2.19 Let f : W → X , g : X → Y , h : Y → Z be maps.\\nThen\\n(1) h ◦ (g ◦ f ) = (h ◦ g) ◦ f , i.e., the composition of maps is associative.\\n(2) If f and g are injective/surjective/bijective, then g ◦ f is injective/ surjective/bijective.\\nProof Exercise.\\nTheorem 2.20 A map f : X → Y is bijective if and only if there exists a map g : Y → X with g ◦ f = Id X and f ◦ g = IdY .\\nProof ⇒: If f is bijective, then by Theorem 2.16 for every y ∈ Y there exists an x = x y ∈ X with f (x y ) = y.\\nWe define the map g by g : Y → X, g(y) = x y .\\nLet \u0005 y ∈ Y be given, then y, hence f ◦ g = IdY .\\n( f ◦ g)(\u0005 y) = f (g(\u0005 y)) = f (x\u0005y ) = \u0005\\nIf, on the other hand, \u0005 x ∈ X is given, then \u0005 y = f (\u0005 x ) ∈ Y .\\nBy Theorem 2.16, there y such that \u0005 x = x\u0005y .\\nSo with exists a unique x\u0005y ∈ X with f (x\u0005y ) = \u0005 y) = x\u0005y = \u0005 x,\\n(g ◦ f )(\u0005 x ) = (g ◦ f )(x\u0005y ) = g( f (x\u0005y )) = g(\u0005   \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         2.2 Maps\\n\\n17 we have g ◦ f = Id X .\\n⇐: By assumption g ◦ f = Id X , thus g ◦ f is injective and thus also f is injective\\n(see Exercise 2.7).\\nMoreover, f ◦ g = IdY , thus f ◦ g is surjective and hence also f is surjective (see Exercise 2.7).\\nTherefore, f is bijective.\\nThe map g : Y → X that was characterized in Theorem 2.20 is unique: If there were another map h : Y → X with h ◦ f = Id X and f ◦ h = IdY , then h = Id X ◦ h = (g ◦ f ) ◦ h = g ◦ ( f ◦ h) = g ◦ IdY = g.\\nThis leads to the following definition.\\nDefinition 2.21 If f : X → Y is a bijective map, then the unique map g : Y → X from Theorem 2.20 is called the inverse (or inverse map) of f .\\nWe denote the inverse of f by f −1 .\\nTo show that a given map g : Y → X is the unique inverse of the bijective map f : X → Y , it is sufficient to show one of the equations g ◦ f = Id X or f ◦ g = IdY .\\nIndeed, if f is bijective and g ◦ f = Id X , then g = g ◦ IdY = g ◦ ( f ◦ f −1 ) = (g ◦ f ) ◦ f −1 = Id X ◦ f −1 = f −1 .\\nIn the same way g = f −1 follows from the assumption f ◦ g = IdY .\\nTheorem 2.22 If f : X → Y and g : Y → Z are bijective maps, then the following assertions hold:\\n(1) f −1 is bijective with ( f −1 )−1 = f .\\n(2) g ◦ f is bijective with (g ◦ f )−1 = f −1 ◦ g −1 .\\nProof\\n(1) Exercise.\\n(2) We know from Theorem 2.19 that g ◦ f : X → Z is bijective.\\nTherefore, there exists a (unique) inverse of g ◦ f .\\nFor the map f −1 ◦ g −1 we have\\n\u0006\\n\u0007\\n\u0006\\n\u0007\\n( f −1 ◦ g −1 ) ◦ (g ◦ f ) = f −1 ◦ g −1 ◦ (g ◦ f ) = f −1 ◦ (g −1 ◦ g) ◦ f\\n= f −1 ◦ (IdY ◦ f ) = f −1 ◦ f = Id X .\\nHence, f −1 ◦ g −1 is the inverse of g ◦ f .\\n\\n2.3 Relations\\nWe first introduce the cartesian product3 of two sets.\\n3 Named after René Descartes (1596–1650), the founder of Analytic Geometry.\\nGeorg Cantor (1845–\\n\\n1918) used in 1895 the name “connection set of M and N ” and the notation (M.N ) = {(m, n)}.   \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        18\\n\\n2 Basic Mathematical Concepts\\n\\nDefinition 2.23 If M, N are nonempty sets, then the set\\nM × N := {(x, y) | x ∈ M ∧ y ∈ N } is the cartesian product of M and N .\\nAn element (x, y) ∈ M × N is called an\\n(ordered) pair.\\nWe can easily generalize this definition to n ∈ N nonempty sets M1 , . . . , Mn :\\nM1 × . . . × Mn := {(x1 , . . . , xn ) | xi ∈ Mi for i = 1, . . . , n}, where an element (x1 , . . . , xn ) ∈ M1 × · · · × Mn is called an (ordered) n-tuple.\\nThe n-fold cartesian product of a single nonempty set M is\\nM n := \bM × . . . × M = {(x1 , . . . , xn ) | xi ∈ M for i = 1, . . . , n}.\\nn times\\n\\nIf in these definitions at least one of the sets is empty, then the resulting cartesian product is the empty set as well.\\nDefinition 2.24 If M, N are nonempty sets then a set R ⊆ M × N is called a relation between M and N .\\nIf M = N , then R is called a relation on M. Instead of (x, y) ∈ R we also write x ∼ R y or x ∼ y, if it is clear which relation is considered.\\nIf in this definition at least one of the sets M and N is empty, then every relation between M and N is also the empty set, since then M × N = Ø.\\nIf, for instance M = N and N = Q, then\\nR = {(x, y) ∈ M × N | x y = 1} is a relation between M and N that can be expressed as\\nR = {(1, 1), (2, 1/2), (3, 1/3), . . . } = {(n, 1/n) | n ∈ N}.\\nDefinition 2.25 A relation R on a set M is called\\n(1) reflexive, if x ∼ x holds for all x ∈ M,\\n(2) symmetric, if (x ∼ y) ⇒ (y ∼ x) holds for all x, y ∈ M,\\n(3) transitive, if (x ∼ y ∧ y ∼ z) ⇒ (x ∼ z) holds for all x, y, z ∈ M.\\nIf R is reflexive, transitive and symmetric, then it is called an equivalence relation on M.\\nExample 2.26\\n(1) Let R = {(x, y) ∈ Q2 | x = −y}.\\nThen R is not reflexive, since x = −x holds only for x = 0.\\nIf x = −y, then also y = −x, and hence R is symmetric.\\nFinally, R is not transitive.\\nFor example, (x, y) = (1, −1) ∈ R and (y, z) =\\n(−1, 1) ∈ R, but (x, z) = (1, 1) ∈\\n/ R.   \n",
       "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 2.3 Relations\\n\\n19\\n\\n(2) The relation R = {(x, y) ∈ Z2 | x ≤ y} is reflexive and transitive, but not symmetric.\\n(3) If f : R → R is a map, then R = {(x, y) ∈ R2 | f (x) = f (y)} is an equivalence relation on R.\\nDefinition 2.27 let R be an equivalence relation on the set M. Then, for x ∈ M the set\\n[x] R := {y ∈ M | (x, y) ∈ R} = {y ∈ M | x ∼ y} is called the equivalence class of x with respect to R. The set of equivalence classes\\nM/R := {[x] R | x ∈ M} is called the quotient set of M with respect to R.\\nThe equivalence class [x] R of elements x ∈ M is never the empty set, since always x ∼ x (reflexivity) and therefore x ∈ [x] R .\\nIf it is clear which equivalence relation\\nR is meant, we often write [x] instead oft [x] R and also skip the additional “with respect to R”.\\nTheorem 2.28 If R is an equivalence relation on the set M and if x, y ∈ M, then the following are equivalent:\\n(1) [x] = [y].\\n(2) [x] ∩ [y] \u0003= Ø.\\n(3) x ∼ y.\\nProof\\n(1) ⇒ (2) : Since x ∼ x, it follows that x ∈ [x].\\nFrom [x] = [y] it follows that x ∈ [y] and thus x ∈ [x] ∩ [y].\\n(2) ⇒ (3) : Since [x] ∩ [y] \u0003= Ø, there exists a z ∈ [x] ∩ [y].\\nFor this element z we have x ∼ z and y ∼ z, and thus x ∼ z and z ∼ y (symmetry) and, therefore, x ∼ y (transitivity).\\n(3) ⇒ (1) : Let x ∼ y and z ∈ [x], i.e., x ∼ z.\\nUsing symmetry and transitivity, we obtain y ∼ z, and hence z ∈ [y].\\nThis means that [x] ⊆ [y].\\nIn an analogous way one shows that [y] ⊆ [x], and hence [x] = [y] holds.\\nTheorem 2.28 shows that for two equivalence classes [x] and [y] we have either\\n[x] = [y] or [x]∩[y] = Ø.\\nThus every x ∈ M is contained in exactly one equivalence class (namely in [x]), so that an equivalence relation R yields a partitioning or decomposition of M into mutually disjoint subsets.\\nEvery element of [x] is called a representative of the equivalence class [x].\\nA very useful and general approach that we will often use in this book is to partition a set of objects (e.g. sets of matrices) into equivalence classes, and to find in each such class a representative with a particularly simple structure.\\nSuch a representative is called a normal form with respect to the given equivalence relation.   \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               20\\n\\n2 Basic Mathematical Concepts\\n\\nExample 2.29 For a given number n ∈ N the set\\nRn := {(a, b) ∈ Z2 | a − b is divisible by n without remainder} is an equivalence relation on Z, since the following properties hold:\\n• Reflexivity: a − a = 0 is divisible by n without remainder.\\n• Symmetry: If a − b is divisible by n without remainder, then also b − a.\\n• Transitivity: Let a − b and b − c be divisible by n without remainder and write a − c = (a − b) + (b − c).\\nBoth summands on the right are divisible by n without remainder and hence this also holds for a − c.\\nFor a ∈ Z the equivalence class [a] is called residue class of a modulo n, and\\n[a] = a + nZ := {a + nz | z ∈ Z}.\\nThe equivalence relation Rn yields a partitioning of Z into n mutually disjoint subsets.\\nIn particular, we have n−1\\n\u0002\\n\\n[0] ∪ [1] ∪ · · · ∪ [n − 1] =\\n\\n[a] = Z.\\na=0\\n\\nThe set of all residue classes modulo n, i.e., the quotient set with respect to Rn , is often denoted by Z/nZ.\\nThus, Z/nZ := {[0], [1], . . . , [n − 1]}.\\nThis set plays an important role in the mathematical field of Number Theory.\\nExercises\\n2.1 Let A, B, C be assertions.\\nShow that the following assertions are true:\\n(a) For ∧ and ∨ the associative laws\\n[(A ∧ B) ∧ C] ⇔ [A ∧ (B ∧ C)],\\n\\n[(A ∨ B) ∨ C] ⇔ [A ∨ (B ∨ C)] hold.\\n(b) For ∧ and ∨ the commutative laws\\n(A ∧ B) ⇔ (B ∧ A),\\n\\n(A ∨ B) ⇔ (B ∨ A) hold.\\n(c) For ∧ and ∨ the distributive laws\\n[(A ∧ B) ∨ C] ⇔ [(A ∨ C) ∧ (B ∨ C)],\\n\\n[(A ∨ B) ∧ C] ⇔ [(A ∧ C) ∨ (B ∧ C)] hold.\\n2.2 Let A, B, C be assertions.\\nShow that the following assertions are true:\\n(a) A ∧ B ⇒ A.\\n(b) [A ⇔ B] ⇔ [(A ⇒ B) ∧ (B ⇒ A)].   \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             2.3 Relations\\n\\n(c)\\n(d)\\n(e)\\n(f)\\n\\n21\\n\\n¬(A ∨ B) ⇔ [(¬A) ∧ (¬B)].\\n¬(A ∧ B) ⇔ [(¬A) ∨ (¬B)].\\n[(A ⇒ B) ∧ (B ⇒ C)] ⇒ [A ⇒ C].\\n[A ⇒ (B ∨ C)] ⇔ [(A ∧ ¬B) ⇒ C].\\n\\n(The assertions (c) and (d) are called the De Morgan laws for ∧ and ∨.)\\n2.3 Prove Theorem 2.8.\\n2.4 Show that for two sets M, N the following holds:\\nN⊆M\\n\\n⇔\\n\\nM∩N =N\\n\\n⇔\\n\\nM ∪ N = M.\\n\\n2.5 Let X, Y be nonempty sets, U, V ⊆ Y nonempty subsets and let f : X → Y be a map.\\nShow that f −1 (U ∩ V ) = f −1 (U ) ∩ f −1 (V ).\\nLet U, V ⊆ X be nonempty.\\nCheck whether f (U ∪ V ) = f (U ) ∪ f (V ) holds.\\n2.6 Are the following maps injective, surjective, bijective?\\n(a) f 1 : R \\ {0} → R, x \u0011→ x1 .\\n(b) f 2 : R2 → R, (x, y) \u0011→ x + y.\\n2\\n2\\n(c) f 3 : R2 → R, (x, y)\\n\u0004 \u0011→ x + y − 1.\\nn\\n, n even,\\n(d) f 4 : N → Z, n \u0011→ 2 n−1\\n− 2 , n odd.\\n2.7 Show that for two maps f : X → Y and g : Y → Z the following assertions hold:\\n(a) g ◦ f is surjective ⇒ g is surjective.\\n(b) g ◦ f is injective ⇒ f is injective.\\n2.8 Let a ∈ Z be given.\\nShow that the map f a : Z → Z, f a (x) = x + a is bijective.\\n2.9 Prove Lemma 2.17.\\n2.10 Prove Theorem 2.19.\\n2.11 Prove Theorem 2.22 (1).\\n2.12 Find two maps f, g : N → N, so that simultaneously\\n(a) f is not surjective,\\n(b) g is not injective, and\\n(c) g ◦ f is bijective.\\n2.13 Determine all equivalence relations on the set {1, 2}.\\n2.14 Determine a symmetric and transitive relation on the set {a, b, c} that is not reflexive.   \n",
       "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Chapter 3\\n\\nAlgebraic Structures\\n\\nAn algebraic structure is a set with operations between its elements that follow certain rules.\\nAs an example of such a structure consider the integers and the operation ‘+.’\\nWhat are the properties of this addition?\\nAlready in elementary school one learns that the sum a + b of two integers a and b is another integer.\\nMoreover, there is a number 0 such that 0 + a = a for every integer a, and for every integer a there exists an integer −a such that (−a) + a = 0.\\nThe analysis of the properties of such concrete examples leads to definitions of abstract concepts that are built on a few simple axioms.\\nFor the integers and the operation addition, this leads to the algebraic structure of a group.\\nThis principle of abstraction from concrete examples is one of the strengths and basic working principles of Mathematics.\\nBy “extracting and completely exposing the mathematical kernel” (David Hilbert) we also simplify our further work:\\nEvery proved assertion about an abstract concept automatically holds for all concrete examples.\\nMoreover, by combining defined concepts we can move to further generalizations and in this way extend the mathematical theory step by step.\\nHermann Günther Graßmann (1809–1877) described this procedure as follows1 : “... the mathematical method moves forward from the simplest concepts to combinations of them and gains via such combinations new and more general concepts.”\\n\\n3.1 Groups\\nWe begin with a set and an operation with specific properties.\\nDefinition 3.1 A group is a set G with a map, called operation,\\n⊕ : G × G → G, (a, b) \u0004→ a ⊕ b,\\n1 “...\\ndie mathematische Methode hingegen schreitet von den einfachsten Begriffen zu den zusammengesetzteren fort, and gewinnt so durch Verknüpfung des Besonderen neue and allgemeinere\\nBegriffe.”\\n© Springer International Publishing Switzerland 2015\\n23\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_3   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       24\\n\\n3 Algebraic Structures that satisfies the following:\\n(1) The operation ⊕ is associative, i.e., (a ⊕ b) ⊕ c = a ⊕ (b ⊕ c) holds for all a, b, c ∈ G.\\n(2) There exists an element e ∈ G, called a neutral element, for which\\n(a) e ⊕ a = a for all a ∈ G, and\\n(b) for every a ∈ G there exists an \u0002 a ∈ G, called an inverse element of a, with\\n\u0002 a ⊕ a = e.\\nIf a ⊕ b = b ⊕ a holds for all a, b ∈ G, then the group is called commutative or\\nAbelian.2\\nAs short hand notation for a group we use (G, ⊕) or just G, if is clear which operation is used.\\nTheorem 3.2 For every group (G, ⊕) the following assertions hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n\\nIf e ∈ G is a neutral element and if a,\u0002 a ∈ G with\u0002 a ⊕a = e, then also a ⊕\u0002 a = e.\\nIf e ∈ G is a neutral element and if a ∈ G, then also a ⊕ e = a.\\nG contains exactly one neutral element.\\nFor every a ∈ G there exists a unique inverse element.\\n\\nProof\\n(1) Let e ∈ G be a neutral element and let a,\u0002 a ∈ G satisfy \u0002 a ⊕ a = e.\\nThen by\\nDefinition 3.1 there exists an element a1 ∈ G with a1 ⊕ \u0002 a = e.\\nThus, a ) ⊕ (a ⊕ \u0002 a )a1 ⊕ ((\u0002 a ⊕ a) ⊕ \u0002 a) a ⊕\u0002 a = e ⊕ (a ⊕ \u0002 a ) = (a1 ⊕ \u0002\\n= a1 ⊕ (e ⊕ \u0002 a ) = a1 ⊕ \u0002 a = e.\\n(2) Let e ∈ G be a neutral element and let a ∈ G. Then there exists \u0002 a ∈ G with\\n\u0002 a ⊕ a = e.\\nBy (1) then also a ⊕ \u0002 a = e and it follows that a ⊕ e = a ⊕ (\u0002 a ⊕ a) = (a ⊕ \u0002 a ) ⊕ a = e ⊕ a = a.\\n(3) Let e, e1 ∈ G be two neutral elements.\\nThen e1 ⊕ e = e, since e1 is a neutral element.\\nSince e is also a neutral element, it follows that e1 = e ⊕ e1 = e1 ⊕ e, where for the second identity we have used assertion (2).\\nHence, e = e1 .\\n(4) Let \u0002 a , a1 ∈ G be two inverse elements of a ∈ G and let e ∈ G be the (unique) neutral element.\\nThen with (1) and (2) it follows that a = a1 ⊕ (a ⊕ \u0002 a ) = a1 ⊕ e = a1 .\\n\u0002 a = e ⊕\u0002 a = (a1 ⊕ a) ⊕ \u0002\\n\\n2 Named after Niels Henrik Abel (1802–1829), the founder of group theory.\\n\\n\u0007\\n\u0006   \n",
       "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                3.1 Groups\\n\\n25\\n\\nExample 3.3\\n(1) (Z, +), (Q, +) and (R, +) are commutative groups.\\nIn all these groups the neutral element is the number 0 (zero) and the inverse of a is the number −a.\\nInstead of a + (−b) we usually write a − b.\\nSince the operation is the addition, these groups are also called additive groups.\\nThe natural numbers N with the addition do not form a group, since there is no neutral element in N. If we consider the set N0 , which includes also the number\\n0 (zero), then 0 + a = a + 0 = a for all a ∈ N0 , but only a = 0 has an inverse element in N. Hence also N0 with the addition does not form a group.\\n(2) The sets Q \\ {0} and R \\ {0} with the usual multiplication form commutative groups.\\nIn these multiplicative groups, the neutral element is the number 1 (one) and the inverse element of a is the number a1 (or a −1 ).\\nInstead of a · b−1 we also write ab or a/b.\\nThe integers Z with the multiplication do not form a group.\\nThe set Z includes the number 1, for which 1 · a = a · 1 = a for all a ∈ Z, but no a ∈ Z \\ {−1, 1} has an inverse element in Z.\\nDefinition 3.4 Let (G, ⊕) be a group and H ⊆ G. If (H, ⊕) is a group, then it is called a subgroup of (G, ⊕).\\nThe next theorem gives an alternative characterization of a subgroup.\\nTheorem 3.5 (H, ⊕) is a subgroup of the group (G, ⊕) if and only if the following properties hold:\\n(1) Ø = H ⊆ G.\\n(2) a ⊕ b ∈ H for all a, b ∈ H .\\n(3) For every a ∈ H also the inverse element satisfies \u0002 a ∈ H.\\n\u0007\\n\u0006\\n\\nProof Exercise.\\n\\nThe following definition characterizes maps between two groups which are compatible with the respective group operations.\\nDefinition 3.6 Let (G 1 , ⊕) and (G 2 , \u0002) be groups.\\nA map\\nϕ : G 1 → G 2 , g \u0004→ ϕ(g), is called a group homomorphism, if\\nϕ(a ⊕ b) = ϕ(a) \u0002 ϕ(b) for all a, b ∈ G 1 .\\nA bijective group homomorphism is called a group isomorphism.   \n",
       "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                     26\\n\\n3 Algebraic Structures\\n\\n3.2 Rings and Fields\\nIn this section we extend the concept of a group and discuss mathematical structures that are characterized by two operations.\\nAs motivating example consider the integers with the addition, i.e., the group (Z, +).\\nWe can multiply the elements of Z and this multiplication is associative, i.e., (a ·b)·c = a ·(b ·c) for all a, b, c ∈ Z. Furthermore the addition and multiplication satisfy the distributive laws a · (b + c) = a · b + a · c and (a + b) · c = a · c + b · c for all integers a, b, c.\\nThese properties make Z with addition and multiplication into a ring.\\nDefinition 3.7 A ring is a set R with two operations\\n+ : R × R → R,\\n\\n(a, b) \u0004→ a + b,\\n\\n(addition)\\n\\n∗ : R × R → R,\\n\\n(a, b) \u0004→ a ∗ b,\\n\\n(multiplication) that satisfy the following:\\n(1) (R, +) is a commutative group.\\nWe call the neutral element in this group zero, and write 0.\\nWe denote the inverse element of a ∈ R by −a, and write a − b instead of a + (−b).\\n(2) The multiplication is associative, i.e., (a ∗ b) ∗ c = a ∗ (b ∗ c) for all a, b, c ∈ R.\\n(3) The distributive laws hold, i.e., for all a, b, c ∈ R we have a ∗ (b + c) = a ∗ b + a ∗ c,\\n(a + b) ∗ c = a ∗ c + b ∗ c.\\nA ring is called commutative if a ∗ b = b ∗ a for all a, b ∈ R.\\nAn element 1 ∈ R is called unit if 1 ∗ a = a ∗ 1 = a for all a ∈ R. In this case R is called a ring with unit.\\nOn the right hand side of the two distributive laws we have omitted the parentheses, since multiplication is supposed to bind stronger than addition, i.e., a + (b ∗ c) = a + b ∗ c.\\nIf it is useful for illustration purposes we nevertheless use parentheses, e.g., we sometimes write (a ∗ b) + (c ∗ d) instead of a ∗ b + c ∗ d.\\nAnalogous to the notation for groups we denote a ring with (R, +, ∗) or just with\\nR, if the operations are clear from the context.\\nIn a ring with unit, the unit element is unique: If 1, e ∈ R satisfy 1 ∗ a = a ∗ 1 = a and e ∗ a = a ∗ e = a for all a ∈ R, then in particular e = e ∗ 1 = 1.\\nFor a1 , a2 , . . . , an ∈ R we use the following abbreviations for the sum and product of these elements: n\\n\u0003 j=1 a j := a1 + a2 + . . . + an and n\\n\u0004 j=1 a j := a1 ∗ a2 ∗ . . . ∗ an .   \n",
       "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         3.2 Rings and Fields\\n\\nMoreover, a n := empty sum as\\n\\n27\\n\\n\u0005n j=1 a for all a ∈ R and n ∈ N. If \u0002 > k, then we define the k\\n\u0003 a j := 0.\\nj=\u0002\\n\\nIn a ring with unit we also define for \u0002 > k the empty product as k\\n\u0004 a j := 1.\\nj=\u0002\\n\\nTheorem 3.8 For every ring R the following assertions hold:\\n(1) 0 ∗ a = a ∗ 0 = 0 for all a ∈ R.\\n(2) a ∗ (−b) = −(a ∗ b) = (−a) ∗ b and (−a) ∗ (−b) = a ∗ b for all a, b ∈ R.\\nProof\\n(1) For every a ∈ R we have 0 ∗ a = (0 + 0) ∗ a = (0 ∗ a) + (0 ∗ a).\\nAdding\\n−(0 ∗ a) on the left and right hand sides of this equality we obtain 0 = 0 ∗ a.\\nIn the same way we can show that a ∗ 0 = 0 for all a ∈ R.\\n(2) Since (a ∗ b) + (a ∗ (−b)) = a ∗ (b + (−b)) = a ∗ 0 = 0, it follows that a ∗ (−b) is the (unique) additive inverse of a ∗ b, i.e., a ∗ (−b) = −(a ∗ b).\\nIn the same way we can show that (−a) ∗ b = −(a ∗ b).\\nFurthermore, we have\\n0 = 0 ∗ (−b) = (a + (−a)) ∗ (−b) = a ∗ (−b) + (−a) ∗ (−b)\\n= −(a ∗ b) + (−a) ∗ (−b), and thus (−a) ∗ (−b) = a ∗ b.\\n\\n\u0007\\n\u0006\\n\\nIt is immediately clear that (Z, +, ∗) is a commutative ring with unit.\\nThis is the standard example, by which the concept of a ring was modeled.\\nExample 3.9 Let M be a nonempty set and let R be the set of maps f : M → R.\\nThen (R, +, ∗) with the operations\\n+ : R × R → R,\\n∗ : R × R → R,\\n\\n( f, g) \u0004→ f + g,\\n( f, g) \u0004→ f ∗ g,\\n\\n( f + g)(x) := f (x) + g(x),\\n( f ∗ g)(x) := f (x) · g(x), is a commutative ring with unit.\\nHere f (x) + g(x) and f (x) · g(x) are the sum and product of two real numbers.\\nThe zero in this ring is the map 0 R : M → R, x \u0004→ 0, and the unit is the map 1 R : M → R, x \u0004→ 1, where 0 and 1 are the real numbers zero and one.\\nIn the definition of a ring only additive inverse elements occur.\\nWe will now formally define the concept of a multiplicative inverse.   \n",
       "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       28\\n\\n3 Algebraic Structures\\n\\nDefinition 3.10 Let (R, +, ∗) be a ring with unit.\\nAn element b ∈ R is called an inverse of a ∈ R (with respect to ∗), if a ∗ b = b ∗ a = 1.\\nAn element of R that has an inverse is called invertible.\\nIt is clear from the definition that b ∈ R is an inverse of a ∈ R if and only if a ∈ R is an inverse of b ∈ R. In general, however, not every element in a ring must be (or is) invertible.\\nBut if an element is invertible, then it has a unique inverse, as shown in the following theorem.\\nTheorem 3.11 Let (R, +, ∗) be a ring with unit.\\n(1) If a ∈ R is invertible, then the inverse is unique and we denote it by a −1 .\\n(2) If a, b ∈ R are invertible then a ∗ b ∈ R is invertible and (a ∗ b)−1 = b−1 ∗ a −1 .\\nProof\\n(1) If b, \u0002 b ∈ R are inverses of a ∈ R, then b = b ∗ 1 = b ∗ (a ∗ \u0002 b) = (b ∗ a) ∗ \u0002 b=\\n\u0002\\n\u0002\\n1 ∗ b = b.\\n(2) Since a and b are invertible, b−1 ∗ a −1 ∈ R is well defined and\\n(b−1 ∗ a −1 )∗(a ∗ b) = ((b−1 ∗ a −1 )∗ a)∗b = (b−1 ∗ (a −1 ∗ a))∗ b = b−1 ∗ b = 1.\\n\\nIn the same way we can show that (a ∗ b) ∗ (b−1 ∗ a −1 ) = 1, and thus\\n\u0007\\n\u0006\\n(a ∗ b)−1 = b−1 ∗ a −1 .\\nFrom an algebraic point of view the difference between the integers on the one hand, and the rational or real numbers on the other, is that in the sets Q and R every element (except for the number zero) is invertible.\\nThis “additional structure” makes\\nQ and R into fields.\\nDefinition 3.12 A commutative ring R with unit is called a field, if 0 = 1 and every a ∈ R \\ {0} is invertible.\\nBy definition, every field is a commutative ring with unit, but the converse does not hold.\\nOne can also introduce the concept of a field based on the concept of a group (cp.\\nExercise 3.15).\\nDefinition 3.13 A field is a set K with two operations\\n+ : K × K → K,\\n∗ : K × K → K,\\n\\n(a, b) \u0004→ a + b,\\n(a, b) \u0004→ a ∗ b,\\n\\n(addition)\\n(multiplication)   \n",
       "26                                                                                                                                                                                                                                                                                                                                                                                                                         3.2 Rings and Fields\\n\\n29 that satisfy the following:\\n(1) (K , +) is a commutative group.\\nWe call the neutral element in this group zero, and write 0.\\nWe denote the inverse element of a ∈ K by −a, and write a − b instead of a + (−b).\\n(2) (K \\ {0}, ∗) is a commutative group.\\nWe call the neutral element in this group unit, and write 1.\\nWe denote the inverse element of a ∈ K \\ {0} by a −1 .\\n(3) The distributive laws hold, i.e., for all a, b, c ∈ K we have a ∗ (b + c) = a ∗ b + a ∗ c,\\n(a + b) ∗ c = a ∗ c + b ∗ c.\\nWe now show a few useful properties of fields.\\nLemma 3.14 For every field K the following assertions hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n\\nK has at least two elements.\\n0 ∗ a = a ∗ 0 = 0 for all a ∈ K .\\na ∗ b = a ∗ c and a = 0 imply that b = c for all a, b, c ∈ K .\\na ∗ b = 0 imply that a = 0 or b = 0 for all a, b ∈ K .\\n\\nProof\\n(1) This follows from the definition, since 0, 1 ∈ K with 0 = 1.\\n(2) This has already been shown for rings (cp.\\nTheorem 3.8).\\n(3) Since a = 0, we know that a −1 exists.\\nMultiplying both sides of a ∗ b = a ∗ c from the left with a −1 yields b = c.\\n(4) Suppose that a ∗ b = 0.\\nIf a = 0, then we are finished.\\nIf a = 0, then a −1 exists\\n\u0007 and multiplying both sides of a ∗ b = 0 from the left with a −1 yields b = 0. \u0006\\nFor a ring R an element a ∈ R is called a zero divisor,3 if a b ∈ R \\ {0} exists with a ∗ b = 0.\\nThe element a = 0 is called the trivial zero divisor.\\nProperty (4) in\\nLemma 3.14 means that fields contain only the trivial zero divisor.\\nThere are also rings in which property (4) holds, for instance the ring of integers Z. In later chapters we will encounter rings of matrices that contain non-trivial zero divisors (see e.g. the proof of Theorem 4.9 below).\\nThe following definition is analogous to the concepts of a subgroup (cp.\\nDefinition 3.4) and a subring (cp.\\nExcercise 3.14).\\nDefinition 3.15 Let (K , +, ∗) be a field and L ⊆ K .\\nIf (L , +, ∗) is a field, then it is called a subfield of (K , +, ∗).\\nAs two very important examples for algebraic concepts discussed above we now discuss the field of complex numbers and the ring of polynomials.\\n3 The concept of zero divisors was introduced in 1883 by Karl Theodor Wilhelm Weierstraß (1815–\\n\\n1897).   \n",
       "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           30\\n\\n3 Algebraic Structures\\n\\nExample 3.16 The set of complex numbers is defined as\\nC := { (x, y) | x, y ∈ R } = R × R.\\nOn this set we define the following operations as addition and multiplication:\\n+ : C × C → C, (x1 , y1 ) + (x2 , y2 ) := (x1 + x2 , y1 + y2 ),\\n· : C × C → C, (x1 , y1 ) · (x2 , y2 ) := (x1 · x2 − y1 · y2 , x1 · y2 + x2 · y1 ).\\nOn the right hand sides we here use the addition and the multiplication in the field\\nR. Then (C, +, ·) is a field with the neutral elements with respect to addition and multiplication given by\\n0C = (0, 0),\\n1C = (1, 0), and the inverse elements with respect to addition and multiplication given by\\n−(x, y) = (−x, −y) for all (x, y) ∈ C,\\n\u0007\\n\u0006 x y\\n−1 for all (x, y) ∈ C \\ {(0, 0)}.\\n,− 2\\n(x, y) = x 2 + y2 x + y2\\nIn the multiplicative inverse element we have written ab instead of a · b−1 , which is the common notation in R.\\nConsidering the subset L := {(x, 0) | x ∈ R} ⊂ C, we can identify every x ∈ R with an element of the set L via the (bijective) map x \u0004→ (x, 0).\\nIn particular,\\n0R \u0004→ (0, 0) = 0C and 1R \u0004→ (1, 0) = 1C .\\nThus, we can interpret R as subfield of C\\n(although R is not really a subset of C), and we do not have to distinguish between the zero and unit elements in R and C.\\nA special complex number is the imaginary unit (0, 1), which satisfies\\n(0, 1) · (0, 1) = (0 · 0 − 1 · 1, 0 · 1 + 0 · 1) = (−1, 0) = −1.\\nHere again we have identified the real number −1 with the complex number (−1, 0).\\nThe imaginary unit is denoted by i, i.e., i := (0, 1), and hence we can write i2 = −1.\\nUsing the identification of x ∈ R with (x, 0) ∈ C we can write z = (x, y) ∈ C as\\n(x, y) = (x, 0) + (0, y) = (x, 0) + (0, 1) · (y, 0) = x + iy = Re(z) + i Im(z).   \n",
       "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       3.2 Rings and Fields\\n\\n31\\n\\nIn the last expression Re(z) = x and Im(z) = y are the abbreviations for real part and imaginary part of the complex number z = (x, y).\\nSince (0, 1) · (y, 0) =\\n(y, 0) · (0, 1), i.e., iy = yi, it is justified to write the complex number x + iy as x + yi.\\nFor a given complex number z = (x, y) or z = x + iy the number z := (x, −y), respectively z := x − iy, is called the associated complex conjugate number.\\nUsing the (real) square root, the modulus or absolute value of a complex number is defined as\\n\b\\n|z| := (zz)1/2 = (x + iy) (x − iy)\\n\\n1/2\\n\\n\b\\n= x 2 − ix y + iyx − i2 y 2\\n\\n1/2\\n\\n= (x 2 + y 2 )1/2 .\\n\\n(Again, for simplification we have omitted the multiplication sign between two complex numbers.)\\nThis equation shows that the absolute value of a complex number is a nonnegative real number.\\nFurther properties of complex numbers are stated in the exercises at the end of this chapter.\\nExample 3.17 Let (R, +, ·) be a commutative ring with unit.\\nA polynomial over R and in the indeterminate or variable t is an expression of the form p = α0 · t 0 + α1 · t 1 + . . . + αn · t n , where α0 , α1 , . . . , αn ∈ R are the coefficients of the polynomial.\\nInstead of α0 · t 0 , t 1 and α j · t j we often just write α0 , t and α j t j .\\nThe set of all polynomials over R is denoted by R[t].\\nLet p = α0 + α1 · t + . . . + αn · t n , q = β0 + β1 · t + . . . + βm · t m be two polynomials in R[t] with n ≥ m.\\nIf n > m, then we set β j = 0 for j = m + 1, . . . , n and call p and q equal, written p = q, if α j = β j for j = 0, 1, . . . , n.\\nIn particular, we have\\nα0 + α1 · t + . . . + αn · t n = αn · t n + . . . + α1 · t + α0 ,\\n0 + 0 · t + . . . + 0 · t n = 0.\\nThe degree of the polynomial p = α0 + α1 · t + . . . + αn · t n , denoted by deg( p), is defined as the largest index j, for which α j = 0.\\nIf no such index exists, then the polynomial is the zero polynomial p = 0 and we set deg( p) := −∞.\\nLet p, q ∈ R[t] as above have degrees n, m, respectively, with n ≥ m.\\nIf n > m, then we again set β j = 0, j = m + 1, . . . , n.\\nWe define the following operations on\\nR[t]:   \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           32\\n\\n3 Algebraic Structures p + q := (α0 + β0 ) + (α1 + β1 ) · t + . . . + (αn + βn ) · t n ,\\n\u0003\\nαi β j .\\np ∗ q := γ0 + γ1 · t + . . . + γn+m · t n+m , γk := i+ j=k\\n\\nWith these operations (R[t], +, ∗) is a commutative ring with unit.\\nThe zero is given by the polynomial p = 0 and the unit is p = 1 · t 0 = 1.\\nBut R[t] it is not a field, since not every polynomial p ∈ R[t] \\ {0} is invertible, not even if R is a field.\\nFor example, for p = t and any other polynomial q = β0 + β1 t + . . . + βm t m ∈ R[t] we have p ∗ q = β0 t + β1 t 2 + . . . + βm t m+1 = 1, and hence p is not invertible.\\nIn a polynomial we can “substitute” the variable t by some other object when the resulting expression can be evaluated algebraically.\\nFor example, we may substitute t by any λ ∈ R and interpret the addition and multiplication as the corresponding operations in the ring R. This defines a map from R to R by\\nλ \u0004→ p(λ) = α0 · λ0 + α1 · λ1 + . . . + αn · λn , λk := λ · . . . · λ, k = 0, 1, . . . , n, k times where λ0 = 1 ∈ R (this is an empty product).\\nHere one should not confuse the ring element p(λ) with the polynomial p itself, but rather think of p(λ) as an evaluation of p at λ.\\nWe will study the properties of polynomials in more detail later on, and we will also evaluate polynomials at other objects such as matrices or endomorphisms.\\nExercises\\n3.1 Determine for the following (M, ⊕) whether they form a group:\\n(a) M = {x ∈ R | x > 0} and ⊕ : M × M → M, (a, b) \u0004→ a b .\\n(b) M = R \\ {0} and ⊕ : M × M → M, (a, b) \u0004→ ab .\\n3.2 Let a, b ∈ R, the map f a,b : R × R → R × R, (x, y) \u0004→ (ax − by, ay), and the set G = { f a,b | a, b ∈ R, a = 0} be given.\\nShow that (G, ◦) is a commutative group, when the operation ◦ : G × G → G is defined as the composition of two maps (cp.\\nDefinition 2.18).\\n3.3 Let X = Ø be a set and let S(X ) = { f : X → X | f is bijective}.\\nShow that\\n(S(X ), ◦) is a group.\\n3.4 Let (G, ⊕) be a group.\\nFor a ∈ G denote by −a ∈ G the (unique) inverse element.\\nShow the following rules for elements of G:\\n(a)\\n(b)\\n(c)\\n(d)\\n\\n−(−a) = a.\\n−(a ⊕ b) = (−b) ⊕ (−a).\\na ⊕ b1 = a ⊕ b2 ⇒ b1 = b2 .\\na1 ⊕ b = a2 ⊕ b ⇒ a1 = a2 .   \n",
       "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  3.2 Rings and Fields\\n\\n33\\n\\n3.5 Prove Theorem 3.5.\\n3.6 Let (G, ⊕) be a group and for a fixed a ∈ G let Z G (a) = {g ∈ G | a ⊕ g = g ⊕ a}.\\nShow that Z G (a) is a subgroup of G.\\n(This subgroup of all elements of G that commute with a is called centralizer of a.)\\n3.7 Let ϕ : G → H be a group homomorphism.\\nShow the following assertions:\\n(a) If U ⊆ G is a subgroup, then also ϕ(U ) ⊆ H is a subgroup.\\nIf, furthermore, G is commutative, then also ϕ(U ) is commutative (even if H is not commutative).\\n(b) If V ⊆ H is a subgroup, then also ϕ−1 (V ) ⊆ G is a subgroup.\\n3.8 Let ϕ : G → H be a group homomorphism and let eG and e H be the neutral elements of the groups G and H , respectively.\\n(a) Show that ϕ(eG ) = e H .\\n(b) Let ker(ϕ) := {g ∈ G | ϕ(g) = e H }.\\nShow that ϕ is injective if and only if ker(ϕ) = {eG }.\\n3.9 Show the properties in Definition 3.7 for (R, +, ∗) from Example 3.9 in order to show that (R, +, ∗) is a commutative ring with unit.\\nSuppose that in Example\\n3.9 we replace the codomain R of the maps by a commutative ring with unit.\\nIs (R, +, ∗) then still a commutative ring with unit?\\n3.10 Let R be a ring and n ∈ N. Show the following assertions:\\n\u000e an , if n is even,\\n(a) For all a ∈ R we have (−a)n =\\n−a n , if n is odd.\\n(b) If there exists a unit in R and if a n = 0 for a ∈ R, then 1 − a is invertible.\\n(An element a ∈ R with a n = 0 for some n ∈ N is called nilpotent.)\\n3.11 Let R be a ring with unit.\\nShow that 1 = 0 if and only if R = {0}.\\n3.12 Let (R, +, ∗) be a ring with unit and let R × denote the set of all invertible elements of R.\\n(a) Show that (R × , ∗) is a group (called the group of units of R).\\n(b) Determine the sets Z× , K × , and K [t]× , when K is a field.\\n3.13 For fixed n ∈ N let nZ = {nk | k ∈ Z} and Z/nZ = {[0], [1], . . . , [n − 1]} be as in Example 2.29.\\n(a) Show that nZ is a subgroup of Z.\\n(b) Define by\\n⊕ : Z/nZ × Z/nZ → Z/nZ, ([a], [b]) \u0004→ [a] ⊕ [b] = [a + b],\\n\u0010 : Z/nZ × Z/nZ → Z/nZ, ([a], [b]) \u0004→ [a] \u0010 [b] = [a · b], an addition and multiplication in Z/nZ, (with + and · being the addition and multiplication in Z).\\nShow the following assertions:   \n",
       "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              34\\n\\n3 Algebraic Structures\\n\\n(i) ⊕ and \u0010 are well defined.\\n(ii) (Z/nZ, ⊕, \u0010) is a commutative ring with unit.\\n(iii) (Z/nZ, ⊕, \u0010) is a field if and only if n is a prime number.\\n3.14 Let (R, +, ∗) be a ring.\\nA subset S ⊆ R is called a subring of R, if (S, +, ∗) is a ring.\\nShow that S is a subring of R if and only if the following properties hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n\\nS ⊆ R.\\n0 R ∈ S.\\nFor all r, s ∈ S also r + s ∈ S and r ∗ s ∈ S.\\nFor all r ∈ S also −r ∈ S.\\n\\n3.15 Show that the Definitions 3.12 and 3.13 of a field describe the same mathematical structure.\\n3.16 Let (K , +, ∗) be a field.\\nShow that (L , +, ∗) is a subfield of (K , +, ∗) (cp.\\nDefinition 3.15), if and only if the following properties hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\n\\nL ⊆ K.\\n0 K , 1 K ∈ L.\\na + b ∈ L and a ∗ b ∈ L for all a, b ∈ L.\\n−a ∈ L for all a ∈ L.\\na −1 ∈ L for all a ∈ L \\ {0}.\\n\\n3.17 Show that in a field 1 + 1 = 0 holds if and only if 1 + 1 + 1 + 1 = 0.\\n3.18 Let (R, +, ∗) be a commutative ring with 1 = 0 that does not contain non-trivial zero divisors.\\n(Such a ring is called an integral domain.)\\n(a) Define on M = R × R \\ {0} a relation by\\n(x, y) ∼ (\u000f x,\u000f y)\\n\\n⇔ x ∗\u000f y = y ∗\u000f x.\\n\\nShow that this is an equivalence relation.\\n(b) Denote the equivalence class [(x, y)] by xy .\\nShow that the following maps are well defined:\\n\u000f x x ∗\u000f y + y ∗\u000f x x\\n⊕ :=\\n, y \u000f y y ∗\u000f y x\\n\u000f x x ∗\u000f x\\n\u0010 : (M/ ∼) × (M/ ∼) → (M/ ∼) with\\n\u0010 :=\\n, y \u000f y y ∗\u000f y\\n\\n⊕ : (M/ ∼) × (M/ ∼) → (M/ ∼) with where M/ ∼ denotes the quotient set with respect to ∼ (cp.\\nDefinition 2.27).\\n(c) Show that (M/ ∼, ⊕, \u0010) is a field.\\n(This field is called the quotient field associated with R.)\\n(d) Which field is (M/ ∼, ⊕, \u0010) for R = Z?\\n3.19 In Exercise 3.18 consider R = K [t], the ring of polynomials over the field K , and construct in this way the field of rational functions.   \n",
       "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               3.2 Rings and Fields\\n\\n35\\n\\n3.20 Let a = 2 + i ∈ C and b = 1 − 3i ∈ C. Determine −a, −b, a + b, a − b, a −1 , b−1 , a −1 a, b−1 b, ab, ba.\\n3.21 Show the following rules for the complex numbers:\\n(a) z 1 + z 2 = z 1 + z 2 and z 1 z 2 = z 1 z 2 for all z 1 , z 2 ∈ C.\\n(b) z −1 = (z)−1 and Re(z −1 ) = |z|1 2 Re(z) for all z ∈ C \\ {0}.\\n3.22 Show that the absolute value of complex numbers satisfies the following properties:\\n(a) |z 1 z 2 | = |z 1 | |z 2 | for all z 1 , z 2 ∈ C.\\n(b) |z| ≥ 0 for all z ∈ C with equality if and only if z = 0.\\n(c) |z 1 + z 2 | ≤ |z 1 | + |z 2 | for all z 1 , z 2 ∈ C.   \n",
       "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Chapter 4\\n\\nMatrices\\n\\nIn this chapter we define matrices with their most important operations and we study several groups and rings of matrices.\\nJames Joseph Sylvester (1814–1897) coined the term matrix 1 in 1850 and described matrices as “an oblong arrangement of terms”.\\nThe matrix operations defined in this chapter were introduced by Arthur Cayley\\n(1821–1895) in 1858.\\nHis article “A memoir on the theory of matrices” was the first to consider matrices as independent algebraic objects.\\nIn our book matrices form the central approach to the theory of Linear Algebra.\\n\\n4.1 Basic Definitions and Operations\\nWe begin with a formal definition of matrices.\\nDefinition 4.1 Let R be a commutative ring with unit and let n, m ∈ N0 .\\nAn array of the form\\n⎤\\n⎡ a11 a12 · · · a1m\\n⎢a21 a22 · · · a2m ⎥\\n⎥\\n⎢\\nA = [ai j ] = ⎢ . .\\n.. ⎥\\n⎣ .. ..\\n. ⎦ an1 an2 · · · anm\\n\\n1 The\\n\\nLatin word “matrix” means “womb”.\\nSylvester considered matrices as objects “out of which we may form various systems of determinants” (cp.\\nChap.\\n5).\\nInterestingly, the English writer\\nCharles Lutwidge Dodgson (1832–1898), better known by his pen name Lewis Carroll, objected to\\nSylvester’s term and wrote in 1867: “I am aware that the word ‘Matrix’ is already in use to express the very meaning for which I use the word ‘Block’; but surely the former word means rather the mould, or form, into which algebraic quantities may be introduced, than an actual assemblage of such quantities”.\\nDodgson also objected to the notation ai j for the matrix entries: “…most of the space is occupied by a number of a’s, which are wholly superfluous, while the only important part of the notation is reduced to minute subscripts, alike difficult to the writer and the reader.”\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_4\\n\\n37   \n",
       "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                38\\n\\n4 Matrices with ai j ∈ R, i = 1, . . . , n, j = 1, . . . , m, is called a matrix of size n × m over R.\\nThe ai j are called the entries or coefficients of the matrix.\\nThe set of all such matrices is denoted by R n,m .\\nIn the following we usually assume (without explicitly mentioning it) that 1 \u0003= 0 in R. This excludes the trivial case of the ring that contains only the zero element\\n(cp.\\nExercise 3.11).\\nFormally, in Definition 4.1 for n = 0 or m = 0 we obtain “empty matrices” of the size 0 × m, n × 0 or 0 × 0.\\nWe denote such matrices by [ ].\\nThey will be used for technical reasons in some of the proofs below.\\nWhen we analyze algebraic properties of matrices, however, we always consider n, m ≥ 1.\\nThe zero matrix in R n,m , denoted by 0n,m or just 0, is the matrix that has all its entries equal to 0 ∈ R.\\nA matrix of size n × n is called a square matrix or just square.\\nThe entries aii for i = 1, . . . , n are called the diagonal entries of A. The identity matrix in R n,n is the matrix In := [δi j ], where\\n\b\\n1, if i = j,\\nδi j :=\\n(4.1)\\n0, if i \u0003= j.\\nis the Kronecker delta-function.2 If it is clear which n is considered, then we just write I instead of In .\\nFor n = 0 we set I0 := [ ].\\nThe ith row of A ∈ R n,m is [ai1 , ai2 , . . . , aim ] ∈ R 1,m , i = 1, . . . , n, where we use commas for the optical separation of the entries.\\nThe jth column of A is\\n⎡ ⎤ a1 j\\n⎢a2 j ⎥\\n⎢ ⎥\\n⎢ .. ⎥ ∈ R n,1 ,\\n⎣ . ⎦ j = 1, . . . , m.\\nan j\\nThus, the rows and columns of a matrix are again matrices.\\nIf 1 × m matrices ai := [ai1 , ai2 , . . . , aim ] ∈ R 1,m , i = 1, . . . , n, are given, then we can combine them to the matrix\\n⎡ ⎤ ⎡\\n⎤ a11 a12 · · · a1m a1\\n⎢a2 ⎥ ⎢a21 a22 · · · a2m ⎥\\n⎢ ⎥ ⎢\\n⎥ n,m\\nA=⎢.⎥=⎢ . .\\n.. ⎥ ∈ R .\\n⎣ .. ⎦ ⎣ .. ..\\n. ⎦ an\\n\\n2 Leopold\\n\\nKronecker (1823–1891).\\nan1 an2 · · · anm   \n",
       "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  4.1 Basic Definitions and Operations\\n\\n39\\n\\nWe then do not write square brackets around the rows of A. In the same way we can combine the n × 1 matrices\\n⎡ ⎤ a1 j\\n⎢a2 j ⎥\\n⎢ ⎥ a j := ⎢ . ⎥ ∈ R n,1 , j = 1, . . . , m,\\n⎣ .. ⎦ an j to the matrix\\n⎡ a11\\n⎢a21\\n⎢\\nA = [a1 , a2 , . . . , am ] = ⎢ .\\n⎣ ..\\na12 · · · a22 · · ·\\n..\\n.\\n\\n⎤ a1m a2m ⎥\\n⎥ n,m\\n.. ⎥ ∈ R .\\n. ⎦ an1 an2 · · · anm\\n\\nIf n 1 , n 2 , m 1 , m 2 ∈ N0 and Ai j ∈ R ni ,m j , i, j = 1, 2, then we can combine these four matrices to the matrix\\nA=\\n\\nA11 A12\\nA21 A22\\n\\n∈ R n 1 +n 2 ,m 1 +m 2 .\\n\\nThe matrices Ai j are then called blocks of the block matrix A.\\nWe now introduce four operations for matrices and begin with the addition:\\n+ : R n,m × R n,m → R n,m ,\\n\\n(A, B) \u0006→ A + B := [ai j + bi j ].\\n\\nThe addition in R n,m operates entrywise, based on the addition in R. Note that the addition is only defined for matrices of equal size.\\nThe multiplication of two matrices is defined as follows: m\\n\\n∗ : R n,m × R m,s → R n,s , (A, B) \u0006→ A ∗ B = [ci j ], ci j := aik bk j .\\nk=1\\n\\nThus, the entry ci j of the product A ∗ B is constructed by successive multiplication and summing up the entries in the ith row of A and the jth column of B. Clearly, in order to define the product A ∗ B, the number of columns of A must be equal to the number of rows in B.\\nIn the definition of the entries ci j of the matrix A ∗ B we have not written the multiplication symbol for the elements in R. This follows the usual convention of omitting the multiplication sign when it is clear which multiplication is considered.\\nEventually we will also omit the multiplication sign between matrices.   \n",
       "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       40\\n\\n4 Matrices\\n\\nWe can illustrate the multiplication rule “cij equals ith row of A times jth column of B” as follows:\\n⎡\\n⎤\\n⎡ ⎤ b11 · · · b1 j\\n· · · b1s\\n⎢ ..\\n⎢ .. ⎥\\n.. ⎥\\n⎣ .\\n⎣ . ⎦\\n. ⎦\\n·\\n·\\n·\\n·\\n·\\n· b b b mj ms\\n⎡\\n⎤ ⎡m1\\n⎤ a11 · · · a1m\\n⎢ ..\\n.. ⎥ ⎢\\n⎥\\n⎢ .\\n. ⎥\\n⎥\\n↓\\n⎢\\n⎥ ⎢\\n⎢\\n⎥\\n⎢[ ai1 · · · aim ]⎥ ⎢\\n⎥ ci j\\n⎢\\n⎥ ⎢ −→\\n⎥\\n⎢ .\\n.. ⎥ ⎣\\n⎦\\n.\\n⎣ .\\n. ⎦ an1 · · · anm\\nIt is important to note that the matrix multiplication in general is not commutative.\\nExample 4.2 For the matrices\\n⎡\\n\\nA=\\n\\n⎤\\n−1 1\\nB = ⎣ 0 0 ⎦ ∈ Z3,2\\n1 −1\\n\\n123\\n∈ Z2,3 ,\\n456 we have\\nA∗B =\\n\\n2 −2\\n∈ Z2,2 .\\n2 −2\\n\\nOn the other hand, B ∗ A ∈ Z3,3 .\\nAlthough A ∗ B and B ∗ A are both defined, we obviously have A ∗ B \u0003= B ∗ A. In this case one recognizes the non-commutativity of the matrix multiplication from the fact that A ∗ B and B ∗ A have different sizes.\\nBut even if A ∗ B and B ∗ A are both defined and have the same size, in general\\nA ∗ B \u0003= B ∗ A. For example,\\nA=\\n\\n12\\n∈ Z2,2 ,\\n03\\n\\nB=\\n\\n40\\n∈ Z2,2\\n56 yield the two products\\nA∗B =\\n\\n14 12\\n15 18 and B ∗ A =\\n\\n4 8\\n.\\n5 28\\n\\nThe matrix multiplication is, however, associative and distributive with respect to the matrix addition.   \n",
       "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    4.1 Basic Definitions and Operations\\n\\n41\\n\\nLemma 4.3 For A, A ∈ R n,m , B, B ∈ R m,\u0002 and C ∈ R \u0002,k the following assertions hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n\\nA ∗ (B ∗ C) = (A ∗ B) ∗ C.\\n(A + A) ∗ B = A ∗ B + A ∗ B.\\nA ∗ (B + B) = A ∗ B + A ∗ B.\\nIn ∗ A = A ∗ Im = A.\\n\\nProof We only show property (1); the others are exercises.\\nLet A ∈ R n,m , B ∈ R m,\u0002 ,\\nC ∈ R \u0002,k as well as (A ∗ B) ∗ C = [di j ] and A ∗ (B ∗ C) = [di j ].\\nBy the definition of the matrix multiplication and using the associative and distributive law in R, we get\\n\u0002\\n\\n\u000e m di j = s=1 t=1 ait t=1 bts cs j\\n\\n\u0002 m m\\n\\n(ait bts ) cs j = s=1 t=1\\n\\n\u000e\\n\\n\u0002 m\\n\\n=\\n\\n\u0002 cs j = ait bts\\n\\n\u000f\\n\u0010 ait bts cs j s=1 t=1\\n\\n= di j , s=1 for 1 ≤ i ≤ n and 1 ≤ j ≤ k, which implies that (A ∗ B) ∗ C = A ∗ (B ∗ C).\\nOn the right hand sides of (2) and (3) in Lemma 4.3 we have not written parentheses, since we will use the common convention that the multiplication of matrices binds stronger than the addition.\\nFor A ∈ R n,n we define\\n. . ∗ A\u0014 for k ∈ N,\\nAk := \u0011A ∗ .\u0012\u0013 k times\\n\\nA := In .\\n0\\n\\nAnother multiplicative operation for matrices is the multiplication with a scalar,3 which is defined as follows:\\n· : R × R n,m → R n,m , (λ, A) \u0006→ λ · A := [λai j ].\\n\\n(4.2)\\n\\nWe easily see that 0 · A = 0n,m and 1 · A = A for all A ∈ R n,m .\\nIn addition, the scalar multiplication has the following properties.\\nLemma 4.4 For A, B ∈ R n,m , C ∈ R m,\u0002 and λ, μ ∈ R the following assertions hold:\\n(1) (λμ) · A = λ · (μ · A).\\n(2) (λ + μ) · A = λ · A + μ · A.\\n3 The term “scalar” was introduced in 1845 by Sir William Rowan Hamilton (1805–1865).\\nIt originates from the Latin word “scale” which means “ladder”.   \n",
       "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         42\\n\\n4 Matrices\\n\\n(3) λ · (A + B) = λ · A + λ · B.\\n(4) (λ · A) ∗ C = λ · (A ∗ C) = A ∗ (λ · C).\\nProof Exercise.\\nThe fourth matrix operation that we introduce is the transposition:\\nT : R n,m → R m,n ,\\n\\nA = [ai j ] \u0006→ A T = [bi j ], bi j := a ji .\\n\\nFor example,\\n\\n⎡\\n\\nA=\\n\\n⎤\\n14\\nA T = ⎣2 5⎦ ∈ Z3,2 .\\n36\\n\\n123\\n∈ Z2,3 ,\\n456\\n\\nThe matrix A T is called the transpose of A.\\nDefinition 4.5 If A ∈ R n,n satisfies A = A T , then A is called symmetric.\\nIf A =\\n−A T , then A is called skew-symmetric.\\nFor the transposition we have the following properties.\\nLemma 4.6 For A, A ∈ R n,m , B ∈ R m,\u0002 and λ ∈ R the following assertions hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n\\n(A T )T = A.\\n(A + A)T = A T + A T .\\n(λ · A)T = λ · A T .\\n(A ∗ B)T = B T ∗ A T .\\n\\nProof \u0015\\nProperties (1)–(3) are exercises.\\nFor the proof of (4) let A ∗ B = [ci j ] with\\nT ai j ], B T = [\u0016 bi j ] and (A ∗ B)T = [\u0016 ci j ].\\nThen ci j = m k=1 aik bk j , A = [\u0016 m\\n\\n\u0016 ci j = c ji = m a jk bki = k=1\\n\\n\u0016 ak j \u0016 bik = k=1 m\\n\\n\u0016 ak j , bik\u0016 k=1 from which we see that (A ∗ B)T = B T ∗ A T .\\n\\nMATLAB-Minute.\\nCarry out the following commands in order to get used to the matrix operations of this chapter in MATLAB notation: A=ones(5,2), A+A, A-3∗A, A’, A’∗A,\\nA∗A’.\\n(In order to see MATLAB’s output, do not put a semicolon at the end of the command.)   \n",
       "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         4.1 Basic Definitions and Operations\\n\\n43\\n\\nExample 4.7 Consider again the example of car insurance premiums from Chap.\\n1.\\nRecall that pi j denotes the probability that a customer in class Ci in this year will move to the class C j .\\nOur example consists of four such classes, and the 16 probabilities can be associated with a row-stochastic 4 × 4 matrix (cp.\\n(1.2)), which we denote by\\nP. Suppose that the insurance company has the following distribution of customers in the four classes: 40 % in class C1 , 30 % in class C2 , 20 % in class C3 , and 10 % in class C4 .\\nThen the 1 × 4 matrix p0 := [0.4, 0.3, 0.2, 0.1] describes the initial customer distribution.\\nUsing the matrix multiplication we now compute\\n⎡\\n0.15\\n⎢0.15 p1 := p0 ∗ P = [0.4, 0.3, 0.2, 0.1] ∗ ⎢\\n⎣0.05\\n0.05\\n\\n0.85\\n0.00\\n0.10\\n0.00\\n\\n0.00\\n0.85\\n0.00\\n0.10\\n\\n⎤\\n0.00\\n0.00⎥\\n⎥\\n0.85⎦\\n0.85\\n\\n= [0.12, 0.36, 0.265, 0.255].\\nThen p1 contains the distribution of the customers in the next year.\\nAs an example, consider the entry of p0 ∗ P in position (1, 4), which is computed by\\n0.4 · 0.00 + 0.3 · 0.00 + 0.2 · 0.85 + 0.1 · 0.85 = 0.255.\\nA customer in the classes C1 or C2 in this year cannot move to the class C4 .\\nThus, the respective initial percentages are multiplied by the probabilities p14 = 0.00 and p24 = 0.00.\\nA customer in the class C3 or C4 will be in the class C4 with the probabilities p34 = 0.85 or p44 = 0.85, respectively.\\nThis yields the two products\\n0.2 · 0.85 and 0.1 · 0.85.\\nContinuing in the same way we obtain after k years the distribution pk := p0 ∗P k , k = 0, 1, 2, . . . .\\n(This formula also holds for k = 0, since P 0 = I4 .)\\nThe insurance company can use this formula to compute the revenue from the payments of premium rates in the coming years.\\nAssume that the full premium rate (class C1 ) is 500 Euros per year.\\nThen the rates in classes C2 , C3 , and C4 are 450, 400 and 300 Euros (10, 20 and\\n40 % discount).\\nIf there are 1000 customers initially, then the revenue in the first year\\n(in Euros) is\\n\u0010\\n\u000f\\n1000 · p0 ∗ [500, 450, 400, 300]T = 445000.   \n",
       "40                                                                                                                                                                                                                                                                                                                                                                                                                                                           44\\n\\n4 Matrices\\n\\nIf no customer cancels the contract, then this model yields the revenue in year k ≥ 0 as\\n1000 ·\\n\\n\u0017 pk ∗ [500, 450, 400, 300]T\\n\\n\u0018\\n\\n\u0017\\n\u0018\\n= 1000 · p0 ∗ (P k ∗ [500, 450, 400, 300]T ) .\\n\\nFor example, the revenue in the next 4 years is 404500, 372025, 347340 and 341819\\n(rounded to full Euros).\\nThese numbers decrease annually, but the rate of the decrease seems to slow down.\\nDoes there exists a “stationary state”, i.e., a state when the revenue is not changing (significantly) any more?\\nWhich properties of the model guarantee the existence of such a state?\\nThese are important practical questions for the insurance company.\\nOnly the existence of a stationary state guarantees significant revenues in the long-time future.\\nSince the formula depends essentially on the entries of the matrix P k , we have reached an interesting problem of Linear Algebra: the analysis of the properties of row-stochastic matrices.\\nWe will analyze these properties in Sect.\\n8.3.\\n\\n4.2 Matrix Groups and Rings\\nIn this section we study algebraic structures that are formed by certain sets of matrices and the matrix operations introduced above.\\nWe begin with the addition in R n,m .\\nTheorem 4.8 (R n,m , +) is a commutative group.\\nThe neutral element is 0 ∈ R n,m\\n(the zero matrix) and for A = [ai j ] ∈ R n,m the inverse element is −A := [−ai j ] ∈\\nR n,m .\\n(We write A − B instead of A + (−B).)\\nProof Using the associativity of the addition in R, for arbitrary A, B, C ∈ R n,m , we obtain\\n(A + B) + C = [ai j + bi j ] + [ci j ] = [(ai j + bi j ) + ci j ] = [ai j + (bi j + ci j )]\\n= [ai j ] + [bi j + ci j ] = A + (B + C).\\nThus, the addition in R n,m is associative.\\nThe zero matrix 0 ∈ R n,m satisfies 0 + A = [0] + [ai j ] = [0 + ai j ] = [ai j ] = A.\\nFor a given A = [ai j ] ∈ R n,m and −A := [−ai j ] ∈ R n,m we have −A + A =\\n[−ai j ] + [ai j ] = [−ai j + ai j ] = [0] = 0.\\nFinally, the commutativity of the addition in R implies that A+ B = [ai j ]+[bi j ] =\\n[ai j + bi j ] = [bi j + ai j ] = B + A.\\nNote that (2) in Lemma 4.6 implies that the transposition is a homomorphism (even an isomorphism) between the groups (R n,m , +) and (R m,n , +) (cp.\\nDefinition 3.6).   \n",
       "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               4.2 Matrix Groups and Rings\\n\\n45\\n\\nTheorem 4.9 (R n,n , +, ∗) is a ring with unit given by the identity matrix In .\\nThis ring is commutative only for n = 1.\\nProof We have already shown that (R n,n , +) is a commutative group (cp.\\nTheorem 4.8).\\nThe other properties of a ring (associativity, distributivity and the existence of a unit element) follow from Lemma 4.3.\\nThe commutativity for n = 1 holds because of the commutativity of the multiplication in the ring R. The example\\n01\\n10\\n00\\n01\\n10\\n01\\n∗\\n=\\n\u0003=\\n=\\n∗\\n00\\n00\\n00\\n00\\n00\\n00 shows that the ring R n,n is not commutative for n ≥ 2.\\nThe example in the proof of Theorem 4.9 shows that for n ≥ 2 the ring R n,n has non-trivial zero-divisors, i.e., there exist matrices A, B ∈ R n,n \\ {0} with A ∗ B = 0.\\nThese exist even when R is a field.\\nLet us now consider the invertibility of matrices in the ring R n,n (with respect to the\\n\u0016 ∈ R n,n must satisfy matrix multiplication).\\nFor a given matrix A ∈ R n,n , an inverse A\\n\u0016 = In (cp.\\nDefinition 3.10).\\nIf an inverse of\\n\u0016 ∗ A = In and A ∗ A the two equations A\\nA ∈ R n,n exists, i.e., if A is invertible, then the inverse is unique and denoted by A−1\\n(cp.\\nTheorem 3.11).\\nAn invertible matrix is sometimes called non-singular, while a non-invertible matrix is called singular.\\nWe will show in Corollary 7.20 that the\\n\u0016 ∗ A = In existence of the inverse already is implied by one of the two equations A\\n\u0016 = In , i.e., if one of them holds, then A is invertible and A−1 = A.\\n\u0016 Until and A ∗ A then, to be correct, we will have to check the validity of both equations.\\nNot all matrices A ∈ R n,n are invertible.\\nSimple examples are the non-invertible matrices\\n10\\nA = [0] ∈ R 1,1 and A =\\n∈ R 2,2 .\\n00\\nAnother non-invertible matrix is\\n11\\n∈ Z2,2 .\\n02\\n\\nA=\\n\\nHowever, considered as an element of Q2,2 , the (unique) inverse of A is given by\\n\u0019\\nA\\n\\n−1\\n\\n=\\n\\n1 − 21\\n0\\n\\n1\\n2\\n\\n\u001a\\n∈ Q2,2 .\\n\\nLemma 4.10 If A, B ∈ R n,n are invertible, then the following assertions hold:\\n(1) A T is invertible with (A T )−1 = (A−1 )T .\\n(We also write this matrix as A−T .)\\n(2) A ∗ B is invertible with (A ∗ B)−1 = B −1 ∗ A−1 .   \n",
       "42                                                                                                                                                                                                                                                                                                                                                                                                                               46\\n\\n4 Matrices\\n\\nProof\\n(1) Using (4) in Lemma 4.6 we have\\n(A−1 )T ∗ A T = (A ∗ A−1 )T = InT = In = InT = (A−1 ∗ A)T = A T ∗ (A−1 )T , and thus (A−1 )T is the inverse of A T .\\n(2) This was already shown in Theorem 3.11 for general rings with unit and thus it holds, in particular, for the ring (R n,n , +, ∗).\\nOur next result shows that the invertible matrices form a multiplicative group.\\nTheorem 4.11 The set of invertible n×n matrices over R forms a group with respect to the matrix multiplication.\\nWe denote this group by G L n (R) (“GL” abbreviates\\n“general linear (group)”).\\nProof The associativity of the multiplication in G L n (R) is clear.\\nAs shown in (2) in Lemma 4.10, the product of two invertible matrices is an invertible matrix.\\nThe neutral element in G L n (R) is the identity matrix In , and since every A ∈ G L n (R) is assumed to be invertible, A−1 exists with (A−1 )−1 = A ∈ G L n (R).\\nWe now introduce some important classes of matrices.\\nDefinition 4.12 Let A = [ai j ] ∈ R n,n .\\n(1) A is called upper triangular, if ai j = 0 for all i > j.\\nA is called lower triangular, if ai j = 0 for all j > i (i.e., A T is upper triangular).\\n(2) A is called diagonal, if ai j = 0 for all i \u0003= j (i.e., A is upper and lower triangular).\\nWe write a diagonal matrix as A = diag(a11 , . . . , ann ).\\nWe next investigate these sets of matrices with respect to their group properties, beginning with the invertible upper and lower triangular matrices.\\nTheorem 4.13 The sets of the invertible upper triangular n × n matrices and of the invertible lower triangular n × n matrices over R form subgroups of G L n (R).\\nProof We will only show the result for the upper triangular matrices; the proof for the lower triangular matrices is analogous.\\nIn order to establish the subgroup property we will prove the three properties from Theorem 3.5.\\nSince In is an invertible upper triangular matrix, the set of the invertible upper triangular matrices is a nonempty subset of G L n (R).\\nNext we show that for two invertible upper triangular matrices A, B ∈ R n,n the product C = A ∗ B is again an invertible upper triangular matrix.\\nThe invertibility of C = [ci j ] follows from (2) in Lemma 4.10.\\nFor i > j we have   \n",
       "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              4.2 Matrix Groups and Rings\\n\\n47 n ci j = aik bk j\\n\\n(here bk j = 0 for k > j) aik bk j\\n\\n(here aik = 0 for k = 1, . . . , j, since i > j) k=1 j\\n\\n= k=1\\n\\n= 0.\\nTherefore, C is upper triangular.\\nIt remains to prove that the inverse A−1 of an invertible upper triangular matrix A is an upper triangular matrix.\\nFor n = 1 the assertion holds trivially, so we assume that n ≥ 2.\\nLet A−1 = [ci j ], then the equation A ∗ A−1 = In can be written as a system of n equations\\n⎡ a11\\n⎢\\n⎢0\\n⎢\\n⎢ ..\\n⎣ .\\n0\\n\\n⎡ ⎤\\n⎤ ⎡ ⎤ c1 j\\nδ1 j\\n· · · · · · a1n\\n⎢ .. ⎥\\n.. ⎥ ⎢ .. ⎥\\n..\\n⎢ ⎥\\n⎢ ⎥\\n.\\n. ⎥\\n⎥ ∗ ⎢ . ⎥ = ⎢ . ⎥,\\n⎢\\n⎢ . ⎥\\n⎥\\n⎥\\n.\\n.\\n.. .. . ⎦ ⎣ . ⎦\\n⎣ .. ⎦\\n. . .\\n.\\n· · · 0 ann cn j\\nδn j j = 1, . . . , n.\\n\\n(4.3)\\n\\nHere, δi j is the Kronecker delta-function defined in (4.1).\\nWe will now prove inductively for i = n, n − 1, . . . , 1 that the diagonal entry aii of A is invertible with aii−1 = cii , and that n ci j = aii−1\\n\\nδi j −\\n\\n\u000e ai\u0002 c\u0002j , j = 1, . . . , n.\\n\\n(4.4)\\n\\n\u0002=i+1\\n\\nThis formula implies, in particular, that ci j = 0 for i > j.\\nFor i = n the last row of (4.3) is given by ann cn j = δn j , j = 1, . . . , n.\\n\\nFor j = n we have ann cnn = 1 = cnn ann , where in the second equation we use the\\n−1\\n= cnn , commutativity of the multiplication in R. Therefore, ann is invertible with ann and thus\\n−1\\nδn j , j = 1, . . . , n.\\ncn j = ann\\nThis is equivalent to (4.4) for i = n.\\n(Note that for i = n in (4.4) the sum is empty and thus equal to zero.)\\nIn particular, cn j = 0 for j = 1, . . . , n − 1.\\nNow assume that our assertion holds for i = n, . . . , k + 1, where 1 ≤ k ≤ n − 1.\\nThen, in particular, ci j = 0 for k + 1 ≤ i ≤ n and i > j.\\nIn words, the rows i = n, . . . , k + 1 of A−1 are in “upper triangular from”.\\nIn order to prove the assertion for i = k, we consider the kth row in (4.3), which is given by   \n",
       "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            48\\n\\n4 Matrices akk ck j + ak,k+1 ck+1, j + . . . + akn cn j = δk j , j = 1, . . . , n.\\n\\n(4.5)\\n\\nFor j = k (< n) we obtain akk ckk + ak,k+1 ck+1,k + . . . + akn cnk = 1.\\nBy the induction hypothesis, we have ck+1,k = · · · = cn,k = 0.\\nThis implies akk ckk =\\n1 = ckk akk , where we have used the commutativity of the multiplication in R. Hence\\n−1\\n= ckk .\\nFrom (4.5) we get akk is invertible with akk\\n\u000f\\n\u0010\\n−1\\nδk j − ak,k+1 ck+1, j − . . . − akn cn j , ck j = akk j = 1, . . . , n, and hence (4.4) holds for i = k.\\nIf k > j, then δk j = 0 and ck+1, j = · · · = cn j = 0, which gives ck j = 0.\\nWe point out that (4.4) represents a recursive formula for computing the entries of the inverse of an invertible upper triangular matrix.\\nUsing this formula the entries are computed “from bottom to top” and “from right to left”.\\nThis process is sometimes called backward substitution.\\nIn the following we will frequently partition matrices into blocks and make use of the block multiplication: For every k ∈ {1, . . . , n − 1}, we can write A ∈ R n,n as\\nA=\\n\\nA11 A12\\nA21 A22 with A11 ∈ R k,k and A22 ∈ R n−k,n−k .\\n\\nIf A, B ∈ R n,n are both partitioned like this, then the product A ∗ B can be evaluated blockwise, i.e.,\\nB11 B12\\nA11 A12\\n∗\\nA21 A22\\nB21 B22\\n\\n=\\n\\nA11 ∗ B11 + A12 ∗ B21 A11 ∗ B12 + A12 ∗ B22\\n.\\nA21 ∗ B11 + A22 ∗ B21 A21 ∗ B12 + A22 ∗ B22\\n(4.6)\\n\\nIn particular, if\\nA=\\n\\nA11 A12\\n0 A22 with A11 ∈ G L k (R) and A22 ∈ G L n−k (R), then A ∈ G L n (R) and a direct computation shows that\\n−1\\n−1\\nA−1\\n11 −A11 ∗ A12 ∗ A22\\n.\\n(4.7)\\nA−1 =\\n−1\\n0\\nA22   \n",
       "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                4.2 Matrix Groups and Rings\\n\\n49\\n\\nMATLAB-Minute.\\nCreate block matrices in MATLAB by carrying out the following commands: k=5;\\nA11=gallery(’tridiag’,-ones(k-1,1),2∗ones(k,1),-ones(k-1,1));\\nA12=zeros(k,2); A12(1,1)=1; A12(2,2)=1;\\nA22=-eye(2);\\nA=full([A11 A12; A12’ A22])\\nB=full([A11 A12; zeros(2,k) -A22])\\nInvestigate the meaning of the command full.\\nCompute the products A∗B and B∗A as well as the inverses inv(A) and inv(B).\\nCompute the inverse of\\nB in MATLAB with the formula (4.7).\\nCorollary 4.14 The set of the invertible diagonal n × n matrices over R forms a commutative subgroup (with respect to the matrix multiplication) of the invertible upper (or lower) triangular n × n matrices over R.\\nProof Since In is an invertible diagonal matrix, the invertible diagonal n ×n matrices form a nonempty subset of the invertible upper (or lower) triangular n × n matrices.\\nIf A = diag(a11 , . . . , ann ) and B = diag(b11 , . . . , bnn ) are invertible, then A ∗ B is invertible (cp.\\n(2) in Lemma 4.10) and diagonal, since\\nA ∗ B = diag(a11 , . . . , ann ) ∗ diag(b11 , . . . , bnn ) = diag(a11 b11 , . . . , ann bnn ).\\nMoreover, if A = diag(a11 , . . . , ann ) is invertible, then aii ∈ R is invertible for all i = 1, . . . , n (cp. the proof of Theorem 4.13).\\nThe inverse A−1 is given by the\\n−1\\n−1\\n, . . . , ann\\n).\\nFinally, the commutativity property invertible diagonal matrix diag(a11\\nA ∗ B = B ∗ A follows directly from the commutativity in R.\\nDefinition 4.15 A matrix P ∈ R n,n is called a permutation matrix, if in every row and every column of P there is exactly one unit and all other entries are zero.\\nThe term “permutation” means “exchange”.\\nIf a matrix A ∈ R n,n is multiplied with a permutation matrix from the left or from the right, then its rows or columns, respectively, are exchanged (or permuted).\\nFor example, if\\n⎡\\n⎤\\n001\\nP = ⎣0 1 0⎦,\\n100\\n\\n⎡\\n\\n⎤\\n123\\nA = ⎣4 5 6⎦ ∈ Z3,3 ,\\n789   \n",
       "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               50\\n\\n4 Matrices\\n\\n⎡\\n\\n⎡\\n⎤\\n⎤\\n789\\n321\\nP ∗ A = ⎣4 5 6⎦ and A ∗ P = ⎣6 5 4⎦ .\\n123\\n987 then\\n\\nTheorem 4.16 The set of the n × n permutation matrices over R forms a subgroup of G L n (R).\\nIn particular, if P ∈ R n,n is a permutation matrix, then P is invertible with P −1 = P T .\\nProof Exercise.\\nFrom now on we will omit the multiplication sign in the matrix multiplication and write AB instead of A ∗ B.\\nExercises\\n(In the following exercises R is a commutative ring with unit.)\\n4.1 Consider the following matrices over Z:\\n⎡\\n\\nA=\\n\\n1 −2 4\\n,\\n−2 3 −5\\n\\n⎤\\n2 4\\n−1 0\\nB = ⎣3 6⎦, C =\\n.\\n11\\n1 −2\\n\\nDetermine, if possible, the matrices C A, BC, B T A, A T C, (−A)T C, B T A T ,\\nAC and C B.\\n4.2 Consider the matrices\\n⎡ ⎤ x1\\n\u001b \n",
       "\\n⎢ ⎥\\nA = ai j ∈ R n,m , x = ⎣ ... ⎦ ∈ R n,1 , y = [y1 , . . . , ym ] ∈ R 1,m .\\nxn\\nWhich of the following expressions are well defined for m \u0003= n or m = n?\\n\\n(a) x y,\\n(b) x T y,\\n(c) yx, (d) yx T , (e) x Ay, (f) x T Ay,\\nT\\nT\\nT\\n(g) x Ay , (h) x Ay , (i) x y A, (j) x y A T , (k) Ax y, (l) A T x y.\\n4.3 Show the following computational rules:\\nμ1 x1 + μ2 x2 = [x1 , x2 ]\\n\\nμ1\\nμ2 and A[x1 , x2 ] = [Ax1 , Ax2 ] for A ∈ R n,m , x1 , x2 ∈ R m,1 and μ1 , μ2 ∈ R.   \n",
       "47                                                                                                                                                                                                                                                                                                                                                                                                                   4.2 Matrix Groups and Rings\\n\\n51\\n\\n4.4 Prove Lemma 4.3 (2)–(4).\\n4.5 Prove Lemma 4.4.\\n4.6 Prove Lemma\\n⎡ 4.6⎤(1)–(3).\\n011\\n4.7 Let A = ⎣0 0 1⎦ ∈ Z3,3 .\\nDetermine An for all n ∈ N ∪ {0}.\\n000\\n4.8 Let p = αn t n + . . . + α1 t + α0 t 0 ∈ R[t] be a polynomial (cp.\\nExample 3.17) and A ∈ R m,m .\\nWe define p(A) ∈ R m,m as p(A) := αn An +. . . +α1 A +α0 Im .\\n10\\n∈ Z2,2 .\\n31\\n(b) For a fixed matrix A ∈ R m,m consider the map f A : R[t] → R m,m , p \u0006→ p(A).\\nShow that f A ( p + q) = f A ( p) + f A (q) and f A ( pq) = f A ( p) f A (q) for all p, q ∈ R[t].\\n(The map f A is a ring homomorphism between the rings R[t] and R m,m .)\\n(c) Show that f A (R[t]) = { p(A) | p ∈ R[t]} is a commutative subring of R m,m , i.e., that f A (R[t]) is a subring of R m,m (cp.\\nExercise 3.14) and that the multiplication in this subring is commutative.\\n(d) Is the map f A surjective?\\n(a) Determine p(A) for p = t 2 − 2t + 1 ∈ Z[t] and A =\\n\\n4.9 Let K be a field with 1 + 1 \u0003= 0.\\nShow that every matrix A ∈ K n,n can be written as A = M + S with a symmetric matrix M ∈ K n,n (i.e., M T = M) and a skew-symmetric matrix S ∈ K n,n (i.e., S T = −S).\\nDoes this also hold in a field with 1+1 = 0?\\nGive a proof or a counterexample.\\n4.10 Show the binomial formula for commuting matrices: If\u0017 A,\u0018 B ∈ R n,n with\\n\u0015k \u0017 k \u0018 j k− j k!\\nk\\n.\\nAB = B A, then (A + B) = j=0 j A B , where kj := j! (k− j)!\\nn,n for which In − A is invertible.\\nShow that (In −\\n4.11 Let A ∈ R be a matrix\\n\u0015\\nA)−1 (In − Am+1 ) = mj=0 A j holds for every m ∈ N.\\n4.12 Let A ∈ R n,n be a matrix for which an m ∈ N with Am = In exists and let m be smallest natural number with this property.\\n(a) Investigate whether A is invertible, and if so, give a particularly simple representation of the inverse.\\n(b) Determine the cardinality of the set {Ak | k ∈ N}.\\n\n",
       "\\n\n",
       "\\n\u001f\\n4.13 Let A = [ai j ] ∈ R n,n \n",
       " an j = 0 for j = 1, . . . , n .\\n(a) Show that A is a subring of R n,n .\\n(b) Show that AM ∈ A for all M ∈ R n,n and A ∈ A.\\n(A subring with this property is called a left ideal of R n,n .)\\n(c) Determine an analogous subring B of R n,n , such that M B ∈ B for all\\nM ∈ R n,n and B ∈ B.\\n(A subring with this property is called a left ideal of R n,n .)\\n4.14 Examine whether (G, ∗) with   \n",
       "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              52\\n\\n4 Matrices\\n\\nG=\\n\\n\n",
       "\\n!\\ncos(α) − sin(α) \n",
       "\n",
       "\\nα\\n∈\\nR sin(α) cos(α) is a subgroup of G L 2 (R).\\n4.15 Generalize the block multiplication (4.6) to matrices A ∈ R n,m and B ∈ R m,\u0002 .\\n4.16 Determine all invertible upper triangular matrices A ∈ R n,n with A−1 = A T .\\n4.17 Let A11 ∈ R n 1 ,n 1 , A12 ∈ R n 1 ,n 2 , A21 ∈ R n 2 ,n 1 , A22 ∈ R n 2 ,n 2 and\\nA=\\n(a) Let A11 ∈\\nA21 A−1\\n11 A12\\n(b) Let A22 ∈\\nA12 A−1\\n22 A21\\n\\nA11 A12\\n∈ R n 1 +n 2 ,n 1 +n 2 .\\nA21 A22\\n\\nG L n 1 (R).\\nShow that A is invertible if and only if A22 − is invertible and derive in this case a formula for A−1 .\\nG L n 2 (R).\\nShow that A is invertible if and only if A11 − is invertible and derive in this case a formula for A−1 .\\n\\n4.18 Let A ∈ G L n (R), U ∈ R n,m and V ∈ R m,n .\\nShow the following assertions:\\n(a) A + U V ∈ G L n (R) holds if and only if Im + V A−1 U ∈ G L m (R).\\n(b) If Im + V A−1 U ∈ G L m (R), then\\n(A + U V )−1 = A−1 − A−1 U (Im + V A−1 U )−1 V A−1 .\\n(This last equation is called the Sherman-Morrison-Woodbury formula; named after Jack Sherman, Winifred J. Morrison and Max A. Woodbury.)\\n4.19 Show that the set of block upper triangular matrices with invertible 2 × 2 diagonal blocks, i.e., the set of matrices\\n⎡\\n\\n⎤\\n· · · A1m\\n· · · A2m ⎥\\n⎥\\n.. ⎥,\\n..\\n. . ⎦\\n0 · · · 0 Amm\\n\\nA11\\n⎢ 0\\n⎢\\n⎢ ..\\n⎣ .\\n\\nA12\\nA22\\n..\\n.\\n\\nAii ∈ G L 2 (R), i = 1, . . . , m, is a group with respect to the matrix multiplication.\\n4.20 Prove Theorem 4.16.\\nIs the group of permutation matrices commutative?\\n4.21 Show that the following is an equivalence relation on R n,n :\\nA∼B\\n\\n⇔\\n\\nThere exists a permutation matrix P with A = P T B P.\\n\\n4.22 A company produces from four raw materials R1 , R2 , R3 , R4 five intermediate products Z 1 , Z 2 , Z 3 , Z 4 , Z 5 , and from these three final products E 1 , E 2 , E 3 .\\nThe following tables show how many units of Ri and Z j are required for producing one unit of Z k and E \u0002 , respectively:   \n",
       "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                4.2 Matrix Groups and Rings\\n\\nR1\\nR2\\nR3\\nR4\\n\\n53\\n\\nZ1\\n0\\n5\\n1\\n0\\n\\nZ2\\n1\\n0\\n1\\n2\\n\\nZ3\\n1\\n1\\n1\\n0\\n\\nZ4\\n1\\n2\\n1\\n1\\n\\nZ5\\n2\\n1\\n0\\n0\\n\\nZ1\\nZ2\\nZ3\\nZ4\\nZ5\\n\\nE1\\n1\\n1\\n0\\n4\\n3\\n\\nE2\\n1\\n2\\n1\\n1\\n1\\n\\nE3\\n1\\n0\\n1\\n1\\n1\\n\\nFor instance, five units of R2 and one unit of R3 are required for producing one unit of Z 1 .\\n(a) Determine, with the help of matrix operations, a corresponding table which shows how many units of Ri are required for producing one unit of E \u0002 .\\n(b) Determine how many units of the four raw materials are required for producing 100 units of E 1 , 200 units of E 2 and 300 units of E 3 .   \n",
       "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Chapter 5\\n\\nThe Echelon Form and the Rank of Matrices\\n\\nIn this chapter we develop a systematic method for transforming a matrix A with entries from a field into a special form which is called the echelon form of A. The transformation consists of a sequence of multiplications of A from the left by certain\\n“elementary matrices”.\\nIf A is invertible, then its echelon form is the identity matrix, and the inverse A−1 is the product of the inverses of the elementary matrices.\\nFor a non-invertible matrix its echelon form is, in some sense, the “closest possible” matrix to the identity matrix.\\nThis form motivates the concept of the rank of a matrix, which we introduce in this chapter and will use frequently later on.\\n\\n5.1 Elementary Matrices\\nLet R be a commutative ring with unit, n ∈ N and i, j ∈ {1, . . . , n}.\\nLet In ∈ R n,n be the identity matrix and let ei be its ith column, i.e., In = [e1 , . . . , en ].\\nWe define\\nE i j := ei e Tj = [0, . . . , 0, ei , 0, . . . , 0] ∈ R n,n ,\\n\u0002\u0003\u0004\u0005 column j i.e., the entry (i, j) of E i j is 1, all other entries are 0.\\nFor n ≥ 2 and i < j we define\\nPi j := [e1 , . . . , ei−1 , e j , ei+1 , . . . , e j−1 , ei , e j+1 , . . . , en ] ∈ R n,n .\\n\\n(5.1)\\n\\nThus, Pi j is a permutation matrix (cp.\\nDefinition 4.12) obtained by exchanging the columns i and j of In .\\nA multiplication of A ∈ R n,m from the left with Pi j means an exchange ofthe rows i and j of A. For example,\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_5\\n\\n55   \n",
       "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               56\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\n⎡\\n\\n⎤\\n123\\nA = ⎣4 5 6⎦ ,\\n789\\n\\n⎡\\n⎤\\n001\\nP13 = [e3 , e2 , e1 ] = ⎣0 1 0⎦ ,\\n100\\n\\n⎡\\n\\n⎤\\n789\\nP13 A = ⎣4 5 6⎦ .\\n123\\n\\nFor λ ∈ R we define\\nMi (λ) := [e1 , . . . , ei−1 , λei , ei+1 , . . . , en ] ∈ R n,n .\\n\\n(5.2)\\n\\nThus, Mi (λ) is a diagonal matrix obtained by replacing the ith column of In by λei .\\nA multiplication of A ∈ R n,m from the left with Mi (λ) means a multiplication of the ith row of A by λ.\\nFor example,\\n⎡\\n\\n⎤\\n123\\nA = ⎣4 5 6⎦ ,\\n789\\n\\n⎡\\n\\n⎤\\n1 00\\nM2 (−1) = [e1 , −e2 , e3 ] = ⎣ 0 −1 0 ⎦ ,\\n0 01\\n\\n⎡\\n\\n⎤\\n1 2 3\\nM2 (−1)A = ⎣ −4 −5 −6 ⎦ .\\n7 8 9\\n\\nFor n ≥ 2, i < j and λ ∈ R we define\\nG i j (λ) := In + λE ji = [e1 , . . . , ei−1 , ei + λe j , ei+1 , . . . , en ] ∈ R n,n .\\n\\n(5.3)\\n\\nThus, the lower triangular matrix G i j (λ) is obtained by replacing the ith column of\\nIn by ei + λe j .\\nA multiplication of A ∈ R n,m from the left with G i j (λ) means that\\nλ times the ith row of A is added to the jth row of A. Similarly, a multiplication of\\nA ∈ R n,m from the left by the upper triangular matrix G i j (λ)T means that λ times the jth row of A is added to the ith row of A. For example,\\n⎡\\n\\n1\\nA = ⎣4\\n7\\n⎡\\n1\\nG 23 (−1)A = ⎣4\\n3\\n\\n⎤\\n⎡\\n⎤\\n3\\n1 00\\n6⎦ , G 23 (−1) = [e1 , e2 − e3 , e3 ] = ⎣ 0 1 0 ⎦ ,\\n9\\n0 −1 1\\n⎡\\n⎤\\n⎤\\n23\\n1 2 3\\n5 6⎦ , G 23 (−1)T A = ⎣ −3 −3 −3 ⎦ .\\n33\\n7 8 9\\n\\n2\\n5\\n8\\n\\nLemma 5.1 The elementary matrices Pi j , Mi (λ) for invertible λ ∈ R, and G i j (λ) defined in (5.1), (5.2), and (5.3), respectively, are invertible and have the following inverses:\\nT\\n(1) Pi−1 j = Pi j = Pi j .\\n(2) Mi (λ)−1 = Mi (λ−1 ).\\n(3) G i j (λ)−1 = G i j (−λ).\\n\\nProof\\nT\\n(1) The invertibility of Pi j with Pi−1 j = Pi j was already shown in Theorem 4.16; the symmetry of Pi j is easily seen.   \n",
       "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 5.1 Elementary Matrices\\n\\n57\\n\\n(2) Since λ ∈ R is invertible, the matrix Mi (λ−1 ) is well defined.\\nA straightforward computation now shows that Mi (λ−1 )Mi (λ) = Mi (λ)Mi (λ−1 ) = In .\\n(3) Since e Tj ei = 0 for i < j, we have E 2ji = (ei e Tj )(ei e Tj ) = 0, and therefore\\nG i j (λ)G i j (−λ) = (In + λE ji )(In + (−λ)E ji )\\n= In + λE ji + (−λ)E ji + (−λ2 )E 2ji = In .\\nA similar computation shows that G i j (−λ)G i j (λ) = In .\\n\\n\u0005\\n\u0004\\n\\n5.2 The Echelon Form and Gaussian Elimination\\nThe constructive proof of the following theorem relies on the Gaussian elimination algorithm.1 For a given matrix A ∈ K n,m , where K is a field, this algorithm constructs a matrix S ∈ G L n (K ) such that S A = C is quasi-upper triangular.\\nWe obtain this special form by left-multiplication of A with elementary matrices Pi j , Mi j (λ) and\\nG i j (λ).\\nEach of these left-multiplications corresponds to the application of one of the so-called “elementary row operations” to the matrix A:\\n• Pi j : exchange two rows of A.\\n• Mi (λ): multiply a row of A with an invertible scalar.\\n• G i j (λ): add a multiple of one row of A to another row of A.\\nWe assume that the entries of A are in a field (rather than a ring) because in the proof of the theorem we require that nonzero entries of A are invertible.\\nA generalization of the result which holds over certain rings (e.g. the integers Z) is given by the Hermite normal form,2 which plays an important role in Number Theory.\\nTheorem 5.2 Let K be a field and let A ∈ K n,m .\\nThen there exist invertible matrices\\nS1 , . . . , St ∈ K n,n (these are products of elementary matrices) such that C :=\\nSt · · · S1 A is in echelon form, i.e., either C = 0 or\\n\\n1 Named after Carl Friedrich Gauß (1777–1855).\\nA similar method was already described in Chap.\\n8,\\n\\n“Rectangular Arrays”, of the “Nine Chapters on the Mathematical Art”.\\nThis text developed in ancient China over several decades BC stated problems of every day life and gave practical mathematical solution methods.\\nA detailed commentary and analysis was written by Liu Hui (approx.\\n220–280 AD) around 260 AD.\\n2 Charles Hermite (1822–1901).   \n",
       "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             58\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\n⎡\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\nC = ⎢\\n⎢ 0\\n⎢\\n⎢\\n⎢\\n⎣\\n\\n1\\n\\n\u0002\\n\\n0\\n1\\n\\n\u0002\\n\\n0\\n0\\n\\n\u0002\\n\\n..\\n.\\n\\n1\\n0\\n\\n0\\n\\n⎤\\n\\n0\\n\\n..\\n0\\n\\n.\\n0\\n1\\n0\\n\\n⎥\\n⎥\\n⎥\\n\u0002⎥\\n⎥\\n⎥.\\n⎥\\n⎥\\n⎥\\n⎥\\n⎦\\n\\nHere \u0002 denotes an arbitrary (zero or nonzero) entry of C.\\nMore precisely, C = [ci j ] is either the zero matrix, or there exists a sequence of natural numbers j1 , . . . , jr (these are called the “steps” of the echelon form), where\\n1 ≤ j1 < · · · < jr ≤ m and 1 ≤ r ≤ min{n, m}, such that\\n(1) ci j = 0 for 1 ≤ i ≤ r and 1 ≤ j < ji ,\\n(2) ci j = 0 for r < i ≤ n and 1 ≤ j ≤ m,\\n(3) ci, ji = 1 for 1 ≤ i ≤ r and all other entries in column ji are zero.\\nIf n = m, then A ∈ K n,n is invertible if and only if C = In .\\nIn this case A−1 =\\nSt · · · S1 .\\nProof If A = 0, then we set t = 1, S1 = In , C = 0 and we are done.\\nNow let A \u0007= 0 and let j1 be the index of the first column of\\n:= A\\nA(1) = ai(1) j that does not consist of all zeros.\\nLet ai(1) be the first entry in this column that is\\n1 , j1 nonzero, i.e., A(1) has the form\\n\u000e\\n\u000e\\n\u000e 0 \u000e\\n\u000e\\n\u000e\\n\u000e .. \u000e\\n⎢\\n\u000e . \u000e\\n⎢\\n\u000e\\n\u000e\\n⎢\\n\u000e 0 \u000e\\n⎢\\n\u000e (1) \u000e\\n⎢\\n\u000e\\n\u000e\\n= ⎢\\n⎢ 0 \u000e ai1 , j1 \u000e\\n\u000e \u0002 \u000e\\n⎢\\n\u000e\\n\u000e\\n⎢\\n\u000e . \u000e\\n⎢\\n\u000e .. \u000e\\n⎣\\n\u000e\\n\u000e\\n\u000e \u0002 \u000e j1\\n\\n⎤\\n\\n⎡\\n\\nA(1)\\n\\n\u0002\\n\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥.\\n⎥\\n⎥\\n⎥\\n⎥\\n⎦\\n\\nWe then proceed as follows: First we permute the rows i 1 and 1 (if i 1 > 1).\\nThen we\\n\u0010−1\\n\u000f\\n.\\nFinally we eliminate normalize the new first row, i.e., we multiply it with ai(1)\\n, j\\n1 1 the nonzero elements below the first entry in column j1 .\\nPermuting and normalizing leads to   \n",
       "54                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 5.2 The Echelon Form and Gaussian Elimination\\n\\n59\\n\\n\u000e\\n\u000e\\n\u000e 1 \u000e\\n\u000e (1) \u000e\\n\u000e\u0011\\n\u000e\\n⎢\\n\u000e a2, j1 \u000e\\n⎢\\n= ⎢ 0 \u000e . \u000e\\n\u000e .. \u000e\\n⎣\\n\u000e\\n\u000e\\n(1) \u000e\\n\u000e\u0011 an, j1 j1\\n\\n⎤\\n\\n⎡\\n\\n\u0011(1) = ãi,(1)j := M1\\nA\\n\\n\u0012\u000f\\n\u0010−1 \u0013\\n(1) ai1 , j1\\nP1,i1 A(1)\\n\\n\u0002\\n\\n⎥\\n⎥\\n⎥.\\n⎦\\n\\nIf i 1 = 1, then we set P1,1 := In .\\nIn order to eliminate below the 1 in column j1 , we\\n\u0011(1) from the left with the matrices multiply A\\n\u000f\\n\u0010\\n\u000f\\n\u0010\\n(1)\\n(1) an,\\n,\\n.\\n.\\n.\\n,\\nG\\n−\u0011 a\\nG 1,n −\u0011\\n1,2 j1\\n2, j1 .\\n⎡\\n\\nThen we have\\n\\n0\\n\\n⎢\\n⎢\\nS1 A(1) = ⎢\\n⎣0\\n\\n1\\n0\\n..\\n.\\n\\n\u0002\\nA(2)\\n\\n⎤\\n⎥\\n⎥\\n⎥,\\n⎦\\n\\n0 j1 where\\n\\n\u0012\u000f\\n\u000f\\n\u0010\\n\u000f\\n\u0010\\n\u0010−1 \u0013\\n(1)\\n(1)\\n(1) a\\nP1,i1\\n·\\n·\\n·\\nG\\n− ã\\nM\\nS1 := G 1,n −ãn,\\n1,2\\n1 j1\\n2, j1 i 1 , j1 and A(2) = [ai(2) j ] with i = 2, . . . , n, j = j1 + 1, . . . , m, i.e., we keep the indices of the larger matrix A(1) in the smaller matrix A(2) .\\nIf A(2) = [ ] or A(2) = 0, then we are finished, since then C := S1 A(1) is in echelon form.\\nIn this case r = 1.\\nIf at least one of the entries of A(2) is nonzero, then we apply the steps described above to the matrix A(2) .\\nFor k = 2, 3, . . . we define the matrices Sk recursively as\\n⎡\\n\u0014\\nSk =\\n\\n0\\n\\n\u0015\\n⎢\\nIk−1 0\\n⎢\\n(k)\\n\u0011\\n, where\\nS\\nA\\n=\\n⎢ k\\n0 \u0011\\nSk\\n⎣0\\n\\n1\\n0\\n..\\n.\\n\\n\u0002\\nA\\n\\n(k+1)\\n\\n⎤\\n⎥\\n⎥\\n⎥.\\n⎦\\n\\n0 jk\\nEach matrix S̃k is constructed analogous to S1 : First we identify the first column jk of A(k) that is not completely zero, as well as the first nonzero entry ai(k) in that k , jk column.\\nThen permuting and normalizing yields the matrix\\n\u0011(k) = [\u0011 ai(k)\\nA j ] := Mk\\n\\n\u0012\u000f\\n\u0010−1 \u0013\\n(k) aik , jk\\nPk,ik A(k) .   \n",
       "55                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         60\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\nIf k = i k , then we set Pk,k := In−k+1 .\\nNow\\n\u0012\u000f\\n\u000f\\n\u0010\\n\u000f\\n\u0010\\n\u0010−1 \u0013\\n(k)\\n(k)\\n(k)\\n\u0011\\nPk,ik , an, jk · · · G k,k+1 −\u0011 ak+1, jk Mk aik , jk\\nSk = G k,n −\u0011 so that Sk is indeed a product of elementary matrices of the form\\n\u0014\\n\\n\u0015\\nIk−1 0\\n,\\n0 T where T is an elementary matrix of size (n − k + 1) × (n − k + 1).\\nIf we continue this procedure inductively, it will end after r ≤ min{n, m} steps with either A(r +1) = 0 or A(r +1) = [ ].\\nAfter r steps we have\\nSr · · · S1 A(1) =\\n⎡\\n1\\n\u0002 \u0002\\n\u0002\\n⎢\\n1\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢ 0\\n⎢\\n0\\n⎢\\n0\\n⎢\\n⎢\\n⎣\\n\\n\u0002\\n\\n⎤\\n\\n\u0002\\n\\n\u0002\\n\\n\u0002\\n\\n1\\n\\n..\\n.\\n..\\n\\n0\\n\\n. \u0002\\n1\\n0\\n\\n⎥\\n⎥\\n⎥\\n\u0002 ⎥\\n⎥\\n⎥.\\n⎥\\n⎥\\n⎥\\n⎥\\n⎦\\n\\n(5.4)\\n\\nBy construction, the entries 1 in (5.4) are in the positions\\n(1, j1 ), (2, j2 ), . . . , (r, jr ).\\nIf r = 1, then S1 A(1) is in echelon form (see the discussion at the beginning of the proof).\\nIf r > 1, then we still have to eliminate the nonzero entries above the 1 in columns j2 , . . . , jr .\\nTo do this, we denote the matrix in (5.4) by R (1) = [ri(1) j ] and form for k = 2, . . . , r recursively\\n(k−1)\\n,\\nR (k) = [ri(k) j ] := Sr +k−1 R where\\n\\n\u000f\\n\u0010T\\n\u000f\\n\u0010T\\n(k−1)\\n(k−1)\\n· · · G k−1,k −rk−1,\\n.\\nSr +k−1 := G 1,k −r1, jk jk\\n\\nFor t := 2r − 1 we have C := St St−1 · · · S1 A in echelon form.\\nSuppose now that n = m and that C = St St−1 · · · S1 A is in echelon form.\\nIf A is invertible, then C is a product of invertible matrices and thus invertible.\\nAn invertible matrix cannot have a row containing only zeros, so that r = n and hence C = In .\\nIf, on the other hand, C = In , then the invertibility of the elementary matrices   \n",
       "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   5.2 The Echelon Form and Gaussian Elimination\\n\\n61 implies that S1−1 · · · St−1 = A. As a product of invertible matrices, A is invertible and\\n\u0005\\n\u0004\\nA−1 = St · · · S1 .\\nIn the literature, the echelon form is sometimes called reduced row echelon form.\\nExample 5.3 Transformation of a matrix from Q3,5 to echelon form via left multiplication with elementary matrices:\\n⎡\\n⎤\\n02133\\n⎣0 2 0 1 1⎦\\n02011\\n⎤\\n⎡\\n0 1 21 23 23 j1 = 2, i 1 = 1\\n⎣0 2 0 1 1 ⎦\\n−→\\n\u0016 \u0017\\nM1 21\\n02011\\n⎡\\n−→\\nG 12 (−2)\\n\\n1\\n2\\n\\n3\\n2\\n\\n3\\n2\\n\\n−→\\nG 13 (−2)\\n\\n⎤\\n\\n⎤\\n⎡\\n0 1 21 23 23 j2 = 3, i 2 = 2\\n⎣0 0 1 2 2⎦\\n−→\\nM2 (−1)\\n0 0 −1 −2 −2\\n\\n⎣ 0 0 −1 −2 −2 ⎦\\n0 0 −1 −2 −2\\n⎡\\n\\n−→\\nG 23 (1)\\n\\n01\\n\\n3\\n2\\n\\n⎤\\n\\n⎢\\n⎣ 0\\n\\n1\\n\\n1\\n2\\n\\n0\\n\\n1\\n\\n⎥\\n2 2 ⎦\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n⎤\\n0 1 21 23 23\\n⎣0 2 0 1 1⎦\\n0 0 −1 −2 −2\\n⎡\\n\\n3\\n2\\n\\n0\\n\\n⎡\\n\\n0\\n\\n−→\\n\u0016 \u0017T ⎢\\n⎣ 0\\nG 12 − 21\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n2\\n\\n1\\n2\\n\\n⎤\\n\\n⎥\\n2 2 ⎦.\\n0 0\\n\\nMATLAB-Minute.\\nThe echelon form is computed in MATLAB with the command rref (“reduced row echelon form”).\\nApply rref to [A eye(n+1)] in order to compute the inverse of the matrix A=full(gallery(’tridiag’,\\n-ones(n,1),2∗ones(n+1,1),-ones(n,1))) for n=1,2,3,4,5 (cp.\\nExercise 5.5).\\nFormulate a conjecture about the general form of A−1 .\\n(Can you prove your conjecture?)\\nThe proof of Theorem 5.2 leads to the so-called LU -decomposition of a square matrix.\\nTheorem 5.4 For every matrix A ∈ K n,n , there exists a permutation matrix P ∈\\nK n,n , a lower triangular matrix L ∈ GLn (K ) with ones on the diagonal and an upper triangular matrix U ∈ K n,n , such that A = PLU.\\nThe matrix U is invertible if and only if A is invertible.   \n",
       "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      62\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\n\u0011, where U\\n\u0011 is upper\\nProof For A ∈ K n,n the Eq.\\n(5.4) has the form Sn · · · S1 A = U triangular.\\nIf r < n, then we set Sn = Sn−1 = · · · = Sr +1 = In .\\nSince the matrices\\n\u0011 is invertible if and only if A is invertible.\\nS1 , . . . , Sn are invertible, it follows that U\\nFor i = 1, . . . , n every matrix Si has the form\\n⎤\\n⎡\\n1\\n⎥\\n⎢ ..\\n⎥\\n⎢\\n.\\n⎥\\n⎢\\n⎥\\n⎢\\n1\\n⎥\\n⎢\\n⎥ Pi, j ,\\n⎢ si,i\\nSi = ⎢ i\\n⎥\\n⎥\\n⎢ s\\n1 i+1,i\\n⎥\\n⎢\\n⎢\\n..\\n.. ⎥\\n⎣\\n. ⎦\\n.\\n1 sn,i where ji ≥ i for i = 1, . . . , n and Pi,i := In (if ji = i, then no permutation was necessary).\\nTherefore,\\n⎡\\n1\\n⎢ .\\n⎢ .\\n.\\n⎢\\nSn · · · S1 = ⎢\\n⎢\\n1\\n⎢\\n⎣\\n1\\n\\n⎤⎡\\n1\\n⎥⎢ .\\n⎥⎢ .\\n.\\n⎥⎢\\n⎥⎢\\n⎥⎢\\n1\\n⎥⎢\\n⎦⎣ sn−1,n−1 sn,n sn,n−1\\n⎤\\n\\n⎤\\n⎥\\n⎥\\n⎥\\n⎥ Pn−1, j\\n⎥ n−1\\n⎥\\n⎦\\n1\\n\\n⎡\\n⎡\\n1\\n1\\n⎢ .\\n⎥\\n⎢ .\\n⎥\\n⎢\\n.\\n⎢\\n⎥\\n⎢\\n⎢\\n⎥\\n⎢\\n⎢\\n⎥ Pn−2, j\\n1\\n···⎢\\n⎢\\n⎥\\n⎢ n−2\\n⎢\\n⎥\\n⎢ sn−2,n−2\\n⎢\\n⎥\\n⎣\\n⎣ sn−1,n−2 1 ⎦ sn,n−2 0 1\\n\\n⎤\\n\\n⎤ s11\\n⎥\\n⎥\\n⎢s 1 s22\\n⎥\\n⎥\\n⎢ 21\\n⎥\\n⎥\\n⎢ s32 1\\n⎥ P2, j ⎢ s31 1\\n⎥ P1, j .\\n⎥\\n⎥\\n⎢\\n2\\n1\\n.\\n.\\n.\\n.\\n⎥\\n⎢ .\\n.. ⎥\\n.\\n.\\n⎦\\n⎣ .\\n. ⎦\\n.\\nsn,2 sn,1\\n1\\n1\\n⎡\\n\\nThe form of the permutation matrices for k = 2, . . . , n − 1 and \u0003 = 1, . . . , k − 1 implies that\\n⎡\\n⎤\\n⎤\\n⎡\\n1\\n1\\n⎢ ..\\n⎥\\n⎥\\n⎢ ..\\n⎢\\n⎥\\n⎥\\n⎢\\n.\\n.\\n⎢\\n⎥\\n⎥\\n⎢\\n⎢\\n⎥\\n⎥\\n⎢\\n1\\n1\\n⎢\\n⎥\\n⎥\\n⎢\\n⎢\\n⎥\\n⎥ Pk, j s s\\n=\\nPk, jk ⎢\\n\u0003,\u0003\\n\u0003,\u0003 k\\n⎢\\n⎥\\n⎥\\n⎢\\n⎢\\n⎥\\n⎥\\n⎢ s\\n1\\n\u0011 s\\n1\\n\u0003+1,\u0003\\n\u0003+1,\u0003\\n⎢\\n⎥\\n⎥\\n⎢\\n⎢\\n⎢\\n..\\n..\\n.. ⎥\\n.. ⎥\\n⎣\\n⎣\\n. ⎦\\n. ⎦\\n.\\n.\\n1\\n1 sn,\u0003\\n\u0011 sn,\u0003   \n",
       "58                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             5.2 The Echelon Form and Gaussian Elimination\\n\\n63 holds for certain \u0011 s j,\u0003 ∈ K , j = \u0003 + 1, . . . , n.\\nHence,\\nSn · · · S1 =\\n⎡\\n⎤\\n⎤ 1\\n⎡\\n1\\n⎢\\n⎥\\n⎥ ⎢ ...\\n⎢ ..\\n⎥\\n⎥⎢\\n⎢\\n⎥\\n.\\n⎥⎢\\n⎢\\n⎥\\n1\\n⎥⎢\\n⎢\\n⎥···\\n1\\n⎥⎢\\n⎢\\n⎥ s n−2,n−2\\n⎥\\n⎦⎢\\n⎣ sn−1,n−1\\n⎣\\n\u0011 sn−1,n−2 1 ⎦ sn,n sn,n−1 sn,n\\n1\\n\u0011 sn,n−2\\n⎡\\n⎤⎡\\n⎤ s11\\n1\\n⎢ s22\\n⎥ ⎢\u0011\\n⎥\\n⎢\\n⎥ ⎢ s21 1\\n⎥\\n⎢\\n⎢ \u0011\\n⎥\\n⎥ s31 1\\n⎢ s32 1\\n⎥ ⎢\u0011\\n⎥ Pn−1, jn−1 · · · P1, j1 .\\n⎢\\n⎢ ..\\n⎥\\n.\\n.. ⎦ ⎣ .\\n.. ⎥\\n⎣ .\\n.\\n. ⎦\\n.\\n1 \u0011\\n1\\n\u0011 sn2 sn,1\\nThe invertible lower triangular matrices and the permutation matrices form groups with respect to the matrix multiplication (cp.\\nTheorems 4.13 and 4.16).\\nThus,\\n\u0011 where \u0011\\n\u0011 is a permutaSn · · · S1 = \u0011\\nL P,\\nL is invertible and lower triangular, and P l11 , . . . ,\u0011 lnn ) is invertible, tion matrix.\\nSince \u0011\\nL = [\u0011 li j ] is invertible, also D := diag(\u0011\\n\u0011T , L := \u0011\\n\u0011−1 = P\\n\u0011.\\nBy\\nL −1 D and U := D −1 U and we obtain A = P LU with P := P construction, all diagonal entries of L are equal to one.\\n\u0005\\n\u0004\\nExample 5.5 Computation of an LU -decomposition of a matrix from Q3,3 :\\n⎡\\n⎤\\n224\\n⎣2 2 1⎦\\n201\\n⎡\\n⎤ j1 = 2, i 1 = 1\\n112\\n−→\\n⎣2 2 1⎦\\n−→\\n\u00161\u0017\\nG 13 (−2)\\nM1 2\\n⎡2 0 1\\n⎤\\n1 1 2\\n−→\\n⎣ 0 0 −3 ⎦ −→\\nG 12 (−2)\\nP23\\n0 −2 −3\\n\\n⎡\\n\\n⎤\\n1 1 2\\n⎣2 2 1⎦\\n⎡ 0 −2 −3 ⎤\\n1 1 2\\n\u0011.\\n⎣ 0 −2 −3 ⎦ = U\\n0 0 −3\\n\\n\u0011 = P23 ,\\nHence, P\\n⎡ 1\\n⎤\\n\u0012 \u0013\\n00\\n2\\n1\\n\u0011\\n= ⎣ −2 1 0 ⎦ ,\\nL = G 12 (−2)G 13 (−2)M1\\n2\\n−2 1 1\\nT\\n\u0011T = P23 and thus, P = P\\n= P23 ,\\n\\n\u0013\\n1\\n, 1, 1 ,\\nD = diag\\n2\\n\u0012   \n",
       "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     64\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\n⎡\\n⎤\\n⎡\\n⎤\\n100\\n2 2 4\\n\u0011 = ⎣ 0 −2 −3 ⎦.\\nL=\u0011\\nL −1 D = ⎣1 1 0⎦ , U = D −1 U\\n101\\n0 0 −3\\nIf A ∈ GLn (K ), then the LU -decomposition yields A−1 = U −1 L −1 P T .\\nHence after computing the LU -decomposition, one obtains the inverse of A essentially by inverting the two triangular matrices.\\nSince this can be achieved by the efficient recursive formula (4.4), the LU -decomposition is a popular method in scientific computing applications that require the inversion of matrices or the solution of linear systems of equations (cp.\\nChap.\\n6).\\nIn this context, however, alternative strategies for the choice of the permutation matrices are used.\\nFor example, instead of the first nonzero entry in a column one chooses an entry with large (or largest) absolute value for the row exchange and the subsequent elimination.\\nBy this strategy the influence of rounding errors in the computation is reduced.\\n\\nMATLAB-Minute.\\nThe Hilbert matrix 3 A = [ai j ] ∈ Qn,n has the entries ai j = 1/(i + j − 1) for i, j = 1, . . . , n.\\nIt can be generated in MATLAB with the command hilb(n).\\nCarry out the command [L,U,P]=lu(hilb(4)) in order to compute an LU -decomposition of the matrix hilb(4).\\nHow do the matrices P, L and U look like?\\nCompute also the LU -decomposition of the matrix full(gallery(’tridiag’,-ones(3,1),2∗ones(4,1),-ones(3,1))) and study the corresponding matrices P, L and U.\\nWe will now show that, for a given matrix A, the matrix C in Theorem 5.2 is uniquely determined in a certain sense.\\nFor this we need the following definition.\\nDefinition 5.6 If C ∈ K n,m is in echelon form (as in Theorem 5.2), then the positions of (1, j1 ), . . . , (r, jr ) are called the pivot positions of C.\\nWe also need the following results.\\nLemma 5.7 If Z ∈ GLn (K ) and x ∈ K n,1 , then Z x = 0 if and only if x = 0.\\nProof Exercise.\\n\\n\u0005\\n\u0004\\n\\nTheorem 5.8 Let A, B ∈ K n,m be in echelon form.\\nIf A = Z B for a matrix Z ∈\\nGLn (K ), then A = B.\\n\\n3 David\\n\\nHilbert (1862–1943).   \n",
       "60                                                                                                                                                                                                                                                                                                                                                                                                                                                                    5.2 The Echelon Form and Gaussian Elimination\\n\\n65\\n\\nProof If B is the zero matrix, then A = ZB = 0, and hence A = B.\\nLet now B \u0007= 0 and let A, B have the respective columns ai , bi , 1 ≤ i ≤ m.\\nFurthermore, let (1, j1 ), . . . , (r, jr ) be the r ≥ 1 pivot positions of B. We will show that every matrix Z ∈ GLn (K ) with A = Z B has the form\\n\u0014\\nZ=\\n\\n\u0015\\nIr \u0002\\n,\\n0 Z n−r where Z n−r ∈ GLn−r (K ).\\nSince B is in echelon form and all entries of B below its row r are zero, it then follows that B = Z B = A.\\nSince (1, j1 ) is the first pivot position of B, we have bi = 0 ∈ K n,1 for 1 ≤ i ≤ j1 − 1 and b j1 = e1 (the first column of In ).\\nThen A = Z B implies ai = 0 ∈ K n,1 for 1 ≤ i ≤ j1 − 1 and a j1 = Z b j1 = Z e1 .\\nSince Z is invertible, Lemma 5.7 implies that a j1 \u0007= 0 ∈ K n,1 .\\nSince A is in echelon form, a j1 = e1 = b j1 .\\nFurthermore,\\n\u0014\\n\\n1\\nZ = Z n :=\\n0\\n\\n\u0015\\n\u0002\\n,\\nZ n−1 where Z n−1 ∈ GLn−1 (K ) (cp.\\nExercise 5.3).\\nIf r = 1, then we are done.\\nIf r > 1, then we proceed with the other pivot positions in an analogous way:\\nSince B is in echelon form, the kth pivot position gives b jk = ek .\\nFrom a jk = Z b jk and the invertibility of Z n−k+1 we obtain a jk = b jk and\\n⎡\\n\\n⎤\\nIk−1 0 \u0002\\nZ = ⎣ 0 1 \u0002 ⎦,\\n0 0 Z n−k where Z n−k ∈ GLn−k (K ).\\n\\n\u0005\\n\u0004\\n\\nThis result yields the uniqueness of the echelon form of a matrix and its invariance under left-multiplication with invertible matrices.\\nCorollary 5.9 For A ∈ K n,m the following assertions hold:\\n(1) There is a unique matrix C ∈ K n,m in echelon form to which A can be transformed by elementary row operations, i.e., by left-multiplication with elementary matrices.\\nThis matrix C is called the echelon form of A.\\n(2) If M ∈ GLn (K ), then the matrix C in (1) is also the echelon form of M A, i.e., the echelon form of a matrix is invariant under left-multiplication with invertible matrices.\\nProof\\n(1) If S1 A = C1 and S2 A\u0016 = C2 ,\u0017 where C1 , C2 are in echelon form and S1 , S2 are invertible, then C1 = S1 S2−1 C2 .\\nTheorem 5.8 now gives C1 = C2 .\\n(2) If M ∈ GLn (K ) and S\u00163 (M A) =\\n\u0017 C3 is in echelon form, then with S1 A = C1 from (1) we get C3 = S3 M S1−1 C1 .\\nTheorem 5.8 now gives C3 = C1 .\\n\u0005\\n\u0004   \n",
       "61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             66\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\n5.3 Rank and Equivalence of Matrices\\nAs we have seen in Corollary 5.9, the echelon form of A ∈ K n,m is unique.\\nIn particular, for every matrix A ∈ K n,m , there exists a unique number of pivot positions\\n(cp.\\nDefinition 5.6) in its echelon form.\\nThis justifies the following definition.\\nDefinition 5.10 The number r of pivot positions in the echelon form of A ∈ K n,m is called the rank4 of A and denoted by rank(A).\\nWe see immediately that for A ∈ K n,m always 0 ≤ rank(A) ≤ min{n, m}, where rank(A) = 0 if and only if A = 0.\\nMoreover, Theorem 5.2 shows that A ∈ K n,n is invertible if and only if rank(A) = n.\\nFurther properties of the rank are summarized in the following theorem.\\nTheorem 5.11 For A ∈ K n,m the following assertions hold:\\n(1) There exist matrices Q ∈ GLn (K ) and Z ∈ GLm (K ) with\\n\u0014\\nQ AZ =\\n\\nIr\\n0n−r,r\\n\\n0r,m−r\\n0n−r,m−r\\n\\n\u0015 if and only if rank(A) = r .\\n(2) If Q ∈ GLn (K ) and Z ∈ GLm (K ), then rank(A) = rank(Q AZ ).\\n(3) If A = BC with B ∈ K n,\u0003 and C ∈ K \u0003,m , then\\n(a) rank(A) ≤ rank(B),\\n(b) rank(A) ≤ rank(C).\\n(4) rank(A) = rank(A T ).\\n(5) There exist matrices B ∈ K n,\u0003 and C ∈ K \u0003,m with A = BC if and only if rank(A) ≤ \u0003.\\nProof\\n(3a) Let Q ∈ G L n (K ) be such that Q B is in echelon form.\\nThen Q A = Q BC.\\nIn the matrix Q BC at most the first rank(B) rows contain nonzero entries.\\nBy\\nCorollary 5.9, the echelon form of Q A is equal to the echelon form of A. Thus, in the normal echelon form of A also at most the first rank(B) rows will be nonzero, which implies rank(A) ≤ rank(B).\\n(1) ⇐: If rank(A) = r = 0, i.e., A = 0, then Ir = [ ] and the assertion holds for arbitrary matrices Q ∈ G L n (K ) and Z ∈ G L m (K ).\\nIf r ≥ 1, then there exists a matrix Q ∈ G L n (K ) such that Q A is in echelon form with r pivot positions.\\nThen there exists a permutation matrix P ∈ K m,m , that is a product of elementary permutation matrices Pi j , with\\n4 The concept of the rank was introduced (in the context of bilinear forms) first in 1879 by Ferdinand\\n\\nGeorg Frobenius (1849–1917).   \n",
       "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         5.3 Rank and Equivalence of Matrices\\n\\n67\\n\\n\u0014\\nP AT Q T =\\n\\nIr 0r,n−r\\nV 0m−r,n−r\\n\\n\u0015 for some matrix V ∈ K m−r,r .\\nIf r = m, then V = [ ].\\nIn the following, for simplicity, we omit the sizes of the zero matrices.\\nThe matrix\\n\u0014\\nY := is invertible with\\nY −1 =\\nThus,\\n\\nIr 0\\n−V Im−r\\n\u0014\\n\\nIr 0\\nV Im−r\\n\\n\u0015\\n∈ K m,m\\n\u0015\\n∈ K m,m .\\n\\n\u0015\\nIr 0\\n,\\nYPA Q =\\n0 0\\n\u0014\\n\\nT\\n\\nT and with Z := P T Y T ∈ G L m (K ) we obtain\\n\u0015\\nIr 0\\n.\\nQ AZ =\\n0 0\\n\u0014\\n\\n(5.5)\\n\\n⇒: Suppose that (5.5) holds for A ∈ K n,m and matrices Q ∈ G L n (K ) and\\nZ ∈ G L m (K ).\\nThen with (3a) we obtain rank(A) = rank(AZ Z −1 ) ≤ rank(AZ) ≤ rank(A), and thus, in particular, rank(A) = rank(AZ ).\\nDue to the invariance of the echelon form (and hence the rank) under left-multiplication with invertible matrices (cp.\\nCorollary 5.9), we get\\n\u0012\u0014 rank(A) = rank(AZ ) = rank(Q AZ ) = rank\\n\\nIr 0\\n0 0\\n\\n\u0015\u0013\\n= r.\\n\\n(2) If A ∈ K n×n , Q ∈ GLn (K ) and Z ∈ GLm (K ), then the invariance of the rank under left-multiplication with invertible matrices and (3a) can again be used for showing that rank(A) = rank(Q AZ Z −1 ) ≤ rank(Q AZ ) = rank(AZ ) ≤ rank(A), and hence, in particular, rank(A) = rank(Q AZ ).\\n(4) If rank(A) = r , then by\\n\u0014 (1)\u0015 there exist matrices Q ∈ G L n (K ) and Z ∈\\nI 0\\nG L m (K ) with Q AZ = r .\\nTherefore,\\n0 0   \n",
       "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              68\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\n\u0012\u0014 rank(A) = rank(Q AZ ) = rank\\n\\nIr 0\\n0 0\\n\\n\u0018\u0014\\n\\n\u0015\u0013\\n= rank\\n\\nIr 0\\n0 0\\n\\n\u0015T \u0019\\n\\n= rank((Q AZ )T )\\n\\n= rank(Z T A T Q T ) = rank(A T ).\\n\\n(3b) Using (3a) and (4), we obtain rank(A) = rank(A T ) = rank(C T B T ) ≤ rank(C T ) = rank(C).\\n(5) Let A = BC with B ∈ K n,\u0003 , C ∈ K \u0003,m .\\nThen by (3a), rank(A) = rank(BC) ≤ rank(B) ≤ \u0003.\\nLet, on the other hand, rank(A) = r ≤ \u0014\u0003.\\nThen\\n\u0015 there exist matrices Q ∈\\nIr 0\\n.\\nThus, we obtain\\nG L n (K ) and Z ∈ G L m (K ) with Q AZ =\\n0 0\\n\u0012\\nA=\\n\\nQ −1\\n\\n\u0014\\n\\nIr\\n\\n0n−r,r\\n\\n0r,\u0003−r\\n0n−r,\u0003−r\\n\\n\u0015\u0013 \u0012\u0014\\n\\nIr\\n\\n0\u0003−r,r\\n\\n\u0015\\n\u0013\\n0r,m−r\\nZ −1 =: BC,\\n0\u0003−r,m−r where B ∈ K n,\u0003 and C ∈ K \u0003,m .\\n\\n\u0005\\n\u0004\\n\\nExample 5.12 The matrix\\n⎡\\n⎤\\n02133\\nA = ⎣0 2 0 1 1⎦ ∈ Q3,5\\n02011 from Example 5.3 has the echelon form\\n⎡\\n\\n010\\n\\n1 1\\n2 2\\n\\n⎤\\n\\n⎢\\n⎥\\n⎣0 0 1 2 2 ⎦.\\n00000\\nSince there are two pivot positions, we have rank(A) = 2.\\nMultiplying A from the right by\\n⎡\\n⎤\\n100 0 0\\n⎢0 0 0 0 0⎥\\n⎢\\n⎥\\n5,5\\n⎥\\nB=⎢\\n⎢0 0 0 0 0⎥ ∈ Q ,\\n⎣ 0 0 0 −1 −1 ⎦\\n0 0 0 −1 −1 yields AB = 0 ∈ Q3,5 , and hence rank(AB) = 0 < rank(A).\\nAssertion (1) in Theorem 5.11 motivates the following definition.   \n",
       "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 5.3 Rank and Equivalence of Matrices\\n\\n69\\n\\nDefinition 5.13 Two matrices A, B ∈ K n,m are called equivalent, if there exist matrices Q ∈ G L n (K ) and Z ∈ G L m (K ) with A = Q B Z .\\nAs the name suggests, this defines an equivalence relation on the set K n,m , since the following properties hold:\\n• Reflexivity: A = Q AZ with Q = In and Z = Im .\\n• Symmetry: If A = Q B Z , then B = Q −1 AZ −1 .\\n• Transitivity: If A = Q 1 B Z 1 and B = Q 2 C Z 2 , then A = (Q 1 Q 2 )C(Z 2 Z 1 ).\\nThe equivalence class of A ∈ K n,m is given by\\n\u001b\\n\u001a\\n[A] = Q AZ | Q ∈ G L n (K ) and Z ∈ G L m (K ) .\\nIf rank(A) = r , then by (1) in Theorem 5.11 we have\\n\u0014\\n\\nIr\\n0n−r,r and, therefore,\\n\\n0r,m−r\\n0n−r,m−r\\n\u0014\u0014\\n\\nIr 0\\n0 0\\n\\n\u0015\\n\\n\u0015\\nIr 0\\n∈ [A]\\n=\\n0 0\\n\u0014\\n\\n\u0015\u0015\\n= [A].\\n\\nConsequently, the rank of A fully determines the equivalence class [A].\\nThe matrix\\n\u0014\\n\\n\u0015\\nIr 0\\n∈ K n,m\\n0 0 is called the equivalence normal form of A. We obtain\\n\u0015\u0015\\nIr 0\\n, where\\nK\\n=\\n0 0 r =0\\n\u0014\u0014\\n\u0015\u0015\\n\u0014\u0014\\n\u0015\u0015\\nIr 0 \n",
       " I\u0003 0\\n= Ø, if r \u0007= \u0003.\\n0 0\\n0 0 n,m min{n,m}\\n\n",
       " \u0014\u0014\\n\\nHence there are 1 + min{n, m} pairwise distinct equivalence classes, and\\n\u000e\\n\u001f\\n\n",
       "\u0014\\n\u0015\\n\u000e\\nIr 0 n,m \u000e\\n∈K\\n\u000e r = 0, 1, . . . , min{n, m}\\n0 0 is a complete set of representatives.\\nFrom the proof of Theorem 4.9 we know that (K n,n , +, ∗) for n ≥ 2 is a noncommutative ring with unit that contains non-trivial zero divisors.\\nUsing the equivalence normal form these can be characterized as follows:\\n• If A ∈ K n,n is invertible, then A cannot be a zero divisor, since then AB = 0 implies that B = 0.   \n",
       "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                70\\n\\n5 The Echelon Form and the Rank of Matrices\\n\\n• If A ∈ K n,n \\ {0} is a zero divisor, then A cannot be invertible, and hence 1 ≤ rank(A) = r < n, so that the equivalence normal form of A is not the identity matrix In .\\nLet Q, Z ∈ G L n (K ) be given with\\n\u0015\\n\u0014\\nI 0\\n.\\nQ AZ = r\\n0 0\\nThen for every matrix\\n\\n\u0014\\n\\n0 0\\nV := r,r r,n−r\\nV21 V22 and B := Z V we have\\nAB = Q\\n\\n−1\\n\\n\u0014\\n\\nIr 0\\n0 0\\n\\n\u0015\\n∈ K n,n\\n\\n\u0015\u0014\\n\u0015\\n0r,r 0r,n−r\\n= 0.\\nV21 V22\\n\\nIf V \u0007= 0, then B \u0007= 0, since Z is invertible.\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n5.1 Compute the echelon forms of the matrices\\n\\n⎡\\n1 i −i\\n\u0014 \u0015\\n⎢\\n12 3\\n1 i\\n0\\n0 0\\nA=\\n∈ Q2,3 , B =\\n∈ C2,2 , C = ⎢\\n⎣ 5 0 −6i\\n2 4 48 i 1\\n01 0\\n⎡ ⎤\\n⎡\\n⎤\\n10\\n1020\\nD = ⎣1 1⎦ ∈ (Z/2Z)3,2 , E = ⎣2 0 1 1⎦ ∈ (Z/3Z)3,4 .\\n01\\n1202\\n\u0014\\n\\n\u0015\\n\\n⎤\\n0\\n1⎥\\n⎥ ∈ C4,4 ,\\n0⎦\\n0\\n\\n(Here for simplicity the elements of Z/nZ are denoted by k instead of [k].)\\nState the elementary matrices that carry out the transformations.\\nIf one of the matrices is invertible, then compute its inverse as a product of the elementary matrices. \u0014\\n\u0015\\nαβ\\n5.2 Let A =\\n∈ K 2,2 with αδ \u0007= βγ.\\nDetermine the echelon form of A and\\nγ δ a formula\u0014for A−1\u0015.\\n1 A12\\n∈ K n,n with A12 ∈ K 1,n−1 and B ∈ K n−1,n−1 .\\nShow that\\n5.3 Let A =\\n0 B\\nA ∈ G L n (K ) if and only if B ∈ G L n−1 (K ).\\n5.4 Consider the matrix\\n!\\nA= t+1 t−1 t−1 t 2 t 2 t−1 t+1 t+1\\n\\n∈ (K (t))2,2 ,   \n",
       "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               5.3 Rank and Equivalence of Matrices\\n\\n5.5\\n\\n5.6\\n\\n5.7\\n5.8\\n\\n71 where K (t) is the field of rational functions (cp.\\nExercise 3.19).\\nExamine whether A is invertible and determine, if possible, A−1 .\\nVerify your result by computing A−1 A and A A−1 .\\nShow that if A ∈ G L n (K ), then the echelon form of [A, In ] ∈ K n,2n is given by [In , A−1 ].\\n(The inverse of an invertible matrix A can thus be computed via the transformation of [A, In ] to its echelon form.)\\nTwo matrices A, B ∈ K n,m are called left equivalent, if there exists a matrix\\nQ ∈ G L n (K ) with A = Q B. Show that this defines an equivalence relation on\\nK n,m and determine a most simple representative for each equivalence class.\\nProve Lemma 5.7.\\nDetermine LU -decompositions (cp.\\nTheorem 5.4) of the matrices\\n⎡\\n\\n1\\n⎢4\\nA=⎢\\n⎣5\\n0\\n\\n2\\n0\\n0\\n1\\n\\n3\\n0\\n6\\n0\\n\\n⎤\\n0\\n1⎥\\n⎥,\\n0⎦\\n0\\n\\n⎡\\n\\n⎤\\n2 0 −2 0\\n⎢ −4 0 4 −1 ⎥\\n4,4\\n⎥\\nB=⎢\\n⎣ 0 −1 −1 −2 ⎦ ∈ R .\\n0 0 1 1\\n\\nIf one of these matrices is invertible, then determine its inverse using its LU decomposition.\\n5.9 Let A be the 4 × 4 Hilbert matrix (cp. the MATLAB-Minute above Definition 5.6).\\nDetermine rank(A).\\nDoes A have an LU -decomposition as in Theorem 5.4 with P = I4 ?\\n5.10 Determine the rank of the matrix\\n⎡\\n⎤\\n0 αβ\\nA = ⎣ −α 0 γ ⎦ ∈ R3,3\\n−β −γ 0 in dependence of α, β, γ ∈ R.\\n5.11 Let A, B ∈ K n,n be given.\\nShow that\\n\u0012\u0014 rank(A) + rank(B) ≤ rank\\n\\nAC\\n0 B\\n\\n\u0015\u0013 for all C ∈ K n,n .\\nExamine when this inequality is strict.\\n5.12 Let a, b, c ∈ Rn,1 .\\n(a) Determine rank(ba T ).\\n(b) Let M(a, b) := ba T − ab T .\\nShow the following assertions:\\n(i) M(a, b) = −M(b, a) and M(a, b)c + M(b, c)a + M(c, a)b = 0,\\n(ii) M(λa + μb, c) = λM(a, c) + μM(b, c) for λ, μ ∈ R,\\n(iii) rank(M(a, b)) = 0 if and only if there exist λ, μ ∈ R with λ \u0007= 0 or\\nμ \u0007= 0 and λa + μb = 0,\\n(iv) rank(M(a, b)) ∈ {0, 2}.   \n",
       "67                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Chapter 6\\n\\nLinear Systems of Equations\\n\\nSolving linear systems of equations is a central problem of Linear Algebra that we discuss in an introductory way in this chapter.\\nSuch systems arise in numerous applications from engineering to the natural and social sciences.\\nMajor sources of linear systems of equations are the discretization of differential equations and the linearization of nonlinear equations.\\nIn this chapter we analyze the solution sets of linear systems of equations and we characterize the number of solutions using the echelon form from Chap.\\n5.\\nWe also develop an algorithm for the computation of the solutions.\\nDefinition 6.1 A linear system (of equations) over a field K with n equations in m unknowns x1 , . . . , xm has the form a11 x1 + . . . + a1m xm = b1 , a21 x1 + . . . + a2m xm = b2 ,\\n..\\n.\\nan1 x1 + . . . + anm xm = bn or\\nAx = b, where the coefficient matrix A = [ai j ] ∈ K n,m and the right hand side b = [bi ] ∈\\nK n,1 are given.\\nIf b = 0, then the linear system is called homogeneous, otherwise x = b is called a solution of the linear non-homogeneous.\\nEvery \u0002 x ∈ K m,1 with A\u0002 system.\\nAll these \u0002 x form the solution set of the linear system, which we denote by\\nL (A, b).\\nThe next result characterizes the solution set L (A, b) of the linear system Ax = b using the solution set L (A, 0) of the associated homogeneous linear system Ax = 0.\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_6\\n\\n73   \n",
       "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       74\\n\\n6 Linear Systems of Equations\\n\\nLemma 6.2 Let A ∈ K n,m and b ∈ K n,1 with L (A, b) \u0003= Ø be given.\\nIf \u0002 x ∈\\nL (A, b), then\\nL (A, b) = \u0002 x + L (A, 0) := {\u0002 x +\u0002 z |\u0002 z ∈ L (A, 0)}.\\nProof If \u0002 z ∈ L (A, 0), and thus \u0002 x +\u0002 z ∈\u0002 x + L (A, 0), then\\nA(\u0002 x +\u0002 z) = A\u0002 x + A\u0002 z = b + 0 = b.\\nHence \u0002 x +\u0002 z ∈ L (A, b), which shows that \u0002 x + L (A, 0) ⊆ L (A, b).\\nz := \u0002 x1 − \u0002 x .\\nThen\\nLet now \u0002 x1 ∈ L (A, b) and let \u0002 x = b − b = 0,\\nA\u0002 z = A\u0002 x1 − A\u0002 x +\u0002 z ∈\u0002 x + L (A, 0), which shows that L (A, b) ⊆ i.e.,\u0002 z ∈ L (A, 0).\\nHence \u0002 x1 = \u0002\\n\u0002 x + L (A, 0).\\n\u0002\\nWe will have a closer look at the set L (A, 0): Clearly, 0 ∈ L (A, 0) \u0003= Ø.\\nIf\\n\u0002 z ∈ L (A, 0), then for all λ ∈ K we have A(λ\u0002 z) = λ(A\u0002 z) = λ · 0 = 0, and hence\\nλ\u0002 z ∈ L (A, 0).\\nFurthermore, for \u0002 z 1 ,\u0002 z 2 ∈ L (A, 0) we have z 2 ) = A\u0002 z 1 + A\u0002 z 2 = 0 + 0 = 0,\\nA(\u0002 z 1 +\u0002 z 2 ∈ L (A, 0).\\nThus, L (A, 0) is a nonempty subset of K m,1 that is and hence \u0002 z 1 +\u0002 closed under scalar multiplication and addition.\\nLemma 6.3 If A ∈ K n,m , b ∈ K n,1 and S ∈ K n,n , then L (A, b) ⊆ L (S A, Sb).\\nMoreover, if S is invertible, then L (A, b) = L (S A, Sb).\\nProof If \u0002 x ∈ L (A, b), then also S A\u0002 x = Sb, and thus \u0002 x ∈ L (S A, Sb), which shows that L (A, b) ⊆ L (S A, Sb).\\nIf S is invertible and \u0002 y ∈ L (S A, Sb), then\\nS A\u0002 y = Sb.\\nMultiplying from the left with S −1 yields A\u0002 y = b.\\nSince \u0002 y ∈ L (A, b), we have L (S A, Sb) ⊆ L (A, b).\\n\u0002\\nConsider the linear system of equations Ax = b.\\nBy Theorem 5.2 we can find a matrix S ∈ G L n (K ) such that S A is in echelon form.\\nLet \u0003 b = [\u0003 bi ] := Sb, then\\n\u0003\\nL (A, b) = L (S A, b) by Lemma 6.3, and the linear system S Ax = \u0003 b takes the form\\n⎤\\n⎡\\n⎡ ⎤\\n1 \u0002 0\\n0\\n0\\n\u0003 b1\\n\u0002\\n⎥\\n⎢\\n⎢\\n⎥\\n1\\n0 \u0002\\n⎥\\n⎢\\n..\\n⎢ ⎥\\n⎥\\n⎢\\n.\\n⎢\\n⎥\\n⎢\\n1\\n\u0002⎥\\n⎢ ⎥\\n⎥\\n⎢\\n⎢\\n⎥ x = ⎢ .. ⎥\\n⎢ 0\\n..\\n⎥.\\n⎥\\n⎢\\n.\\n⎢.⎥\\n0\\n0\\n⎥\\n⎢\\n⎢\\n⎥\\n0\\n⎥\\n⎢\\n⎢ ⎥\\n1\\n0\\n⎥\\n⎢\\n⎣\\n⎦\\n⎦\\n⎣\\n0\\n\u0003 bn   \n",
       "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         6 Linear Systems of Equations\\n\\n75\\n\\nSuppose that rank(A) = r , and let j1 , j2 , . . . , jr be the pivot columns.\\nUsing a rightmultiplication with a permutation matrix we can move the r pivot columns of S A to the first r columns.\\nThis is achieved by\\nP T := [e j1 , . . . , e jr , e1 , . . . , e j1 −1 , e j1 +1 , . . . , e j2 −1 , e j2 +1 , . . . , e jr −1 , e jr +1 , . . . , em ] ∈ K m,m , which yields\\n\u0003 := S A P T =\\nA\\n\\n\u000312\\nIr\\nA\\n,\\n0n−r,r 0n−r,m−r\\n\\n\u000312 ∈ K r,m−r .\\nIf r = m, then we have A\\n\u000312 = [ ].\\nThis permutation leads to where A a simplification of the following presentation, but it is usually omitted in practical computations.\\n\u0003 =\u0003 b as (S A P T )(P x) = \u0003 b, or Ay b,\\nSince P T P = Im , we can write S Ax = \u0003 which has the form\\n⎡\\n⎤ ⎡\u0003 ⎤\\n⎡\\n⎤ y1 b1\\n⎢ .. ⎥ ⎢ .. ⎥\\n⎢ Ir\\n\u000312 ⎥ ⎢ . ⎥ ⎢ . ⎥\\nA\\n⎥ ⎢\\n⎥\\n⎢\\n⎥⎢\\n⎥\\n⎢\\n⎥ ⎢ yr ⎥ ⎢ \u0003\\n⎥ = ⎢ br ⎥.\\n⎢\\n⎥⎢\\n(6.1)\\n⎥\\n⎢\\n⎥ ⎢ yr +1 ⎥ ⎢\u0003 b\\n⎥ ⎢ r +1 ⎥\\n⎢\\n⎥⎢\\n⎣ 0n−r,r 0n−r,m−r ⎦ ⎢ .. ⎥ ⎢ .. ⎥\\n⎣ . ⎦ ⎣ . ⎦\\n\u0003 bn\\n\u000e\\n\u000f ym\\n\u000e \u000f\\n\u000e \u000f\\n\u0003\\n= A:=S\\nAPT\\n=y:=P x\\n\\n=\u0003 b:=Sb\\n\\nThe left-multiplication of x with P just means a different ordering of the unknowns x1 , . . . , xm .\\nThus, the solutions of Ax = b can be easily recovered from the solutions\\n\u0003 = \u0003\\n\u0003\u0003 y ∈ of Ay b, and vice versa: We have \u0002 y ∈ L ( A, b) if and only if \u0002 x := P T \u0002\\n\u0003\\nL (S A, b) = L (A, b).\\nThe solutions of (6.1) can now be determined using the extended coefficient matrix\\n\u0003\u0003\\n[ A, b] ∈ K n,m+1 ,\\n\u0003 Note that rank( A)\\n\u0003 ≤ which is obtained by attaching \u0003 b as an extra column to A.\\n\u0003\\n\u0003\\n\u0003\\n\u0003 rank([ A, b]), with equality if and only if br +1 = · · · = bn = 0.\\n\u0003 < rank([ A,\\n\u0003\u0003 bn is nonzero.\\nThen\\nIf rank( A) b]), then at least one of \u0003 br +1 , . . . , \u0003\\n\u0003\\n(6.1) cannot have a solution, since all entries of A in the rows r + 1, . . . , n are zero.\\n\u0003 = rank([ A,\\n\u0003\u0003 bn = 0 and\\nIf, on the other hand, rank( A) b]), then \u0003 br +1 = · · · = \u0003\\n(6.1) can be written as\\n⎡ ⎤ ⎡ ⎤\\n⎡\\n⎤\\n\u0003 y1 b1 yr +1\\n⎢ .. ⎥ ⎢ .. ⎥ \u0003 ⎢ .. ⎥\\n⎣ . ⎦ = ⎣ . ⎦ − A12 ⎣ . ⎦ .\\n\u0003 yr ym br\\n\\n(6.2)   \n",
       "70                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   76\\n\\n6 Linear Systems of Equations\\n\\nThis representation implies, in particular, that\\n\u0003\u0003\\n\u0003 b1 , . . . , \u0003 br , 0, . . . , 0]T ∈ L ( A, b) \u0003= Ø.\\nb(m) := [\u0003\\n\u000e \u000f m−r\\n\\n\u0003\u0003\\n\u0003 0).\\nIn order to determine\\nFrom Lemma 6.2 we know that L ( A, b) = \u0003 b(m) +L ( A,\\n\u0003\\n\u0003\\n\u0003\\nL ( A, 0) we set b1 = · · · = br = 0 in (6.2), which yields\\n\u0003 0) =\\nL ( A,\\n\\n\u0010 ym ]T | \u0002 yr +1 , . . . , \u0002 ym arbitrary and\\n(6.3)\\n[\u0002 y1 , . . . , \u0002\\n\u0011\\nT\\nT\\n\u0003\\n[\u0002 y1 , . . . , \u0002 yr ] = 0 − A12 [\u0002 yr +1 , . . . , \u0002 ym ] .\\n\\n\u000312 = [ ], L ( A,\\n\u0003 0) = {0} and thus |L ( A,\\n\u0003\u0003\\nIf r = m, then A b)| = 1, i.e., the solution\\n\u0003\\n\u0003 of Ay = b is uniquely determined.\\nExample 6.4 For the extended coefficient matrix\\n⎡\\n\\n⎤\\n103\u0003 b1\\n\u0003\u0003\\n[ A, b] = ⎣ 0 1 4 \u0003 b2 ⎦ ∈ Q3,4\\n000\u0003 b3\\n\u0003 = rank([ A,\\n\u0003\u0003\\n\u0003\u0003 we have rank( A) b]) if and only if \u0003 b3 = 0.\\nIf \u0003 b3 \u0003= 0, then L ( A, b) = Ø.\\n\u0003 =\u0003 b can be written as\\nIf \u0003 b3 = 0, then Ay\\n\u0003\\n3 b y1\\n= \u00031 −\\n[y3 ].\\ny2\\n4 b2\\n\u0003\u0003\\nHence, \u0003 b(3) = [\u0003 b1 , \u0003 b2 , 0]T ∈ L ( A, b) and\\n\u0003 0) =\\nL ( A,\\n\\n\u0010\\n\\n\u0011 y2 , \u0002 y3 ]T | \u0002 y3 arbitrary and [\u0002 y1 , \u0002 y2 ]T = −[3, 4]T [\u0002 y3 ] .\\n[\u0002 y1 , \u0002\\n\\nSummarizing our considerations we have the following algorithm for solving a linear system of equations.\\nAlgorithm 6.5 Let A ∈ K n,m and b ∈ K n,1 be given.\\n(1) Determine S ∈ G L n (K ) such that S A is in echelon form and define \u0003 b := Sb.\\n(2a) If rank(S A) < rank([S A, \u0003 b]), then L (S A, \u0003 b) = L (A, b) = Ø.\\n\u0003 := S A P T as in (6.1).\\n(2b) If r = rank(A) = rank([S A, \u0003 b]), then define A\\n\u0003\u0003\\n\u0003\u0003\\n\u0003 0), where L ( A,\\n\u0003 0) is\\nWe have \u0003 b(m) ∈ L ( A, b) and L ( A, b) = \u0003 b(m) + L ( A,\\nT\\n\u0003\\n\u0003 y |\u0002 y ∈ L ( A, b)}.\\ndetermined as in (6.3), as well as L (A, b) = {P \u0002\\n\u0003 and rank([A, b]) = rank([S A, \u0003\\nSince rank(A) = rank(S A) = rank( A) b]) =\\n\u0003\u0003 rank([ A, b]), the discussion above also yields the following result about the different cases of the solvability of a linear system of equations.   \n",
       "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              6 Linear Systems of Equations\\n\\n77\\n\\nCorollary 6.6 For A ∈ K n,m and b ∈ K n,1 the following assertions hold:\\n(1) If rank(A) < rank([A, b]), then L (A, b) = Ø.\\n(2) If rank(A) = rank([A, b]) = m, then |L (A, b)| = 1 (i.e., there exists a unique solution).\\n(3) If rank(A) = rank([A, b]) < m, then there exist many solutions.\\nIf the field K has infinitely many elements (e.g., when K = Q, K = R or K = C), then there exist infinitely many pairwise distinct solutions.\\nThe different cases in Corollary 6.6 will be studied again in Example 10.8.\\nExample 6.7 Let K = Q and consider the linear system of equations Ax = b with\\n⎡\\n\\n1\\n⎢0\\n⎢\\nA=⎢\\n⎢1\\n⎣2\\n1\\n\\n2\\n1\\n0\\n3\\n1\\n\\n2\\n0\\n3\\n5\\n3\\n\\n⎡ ⎤\\n⎤\\n1\\n1\\n⎢0⎥\\n3⎥\\n⎢ ⎥\\n⎥\\n⎢ ⎥\\n0⎥\\n⎥, b = ⎢2⎥ .\\n⎣ 3⎦\\n⎦\\n4\\n2\\n3\\n\\nWe form [A, b] and apply the Gaussian elimination algorithm in order to transform\\nA into echelon form:\\n⎡\\n⎡\\n⎤\\n⎤\\n1 22 11\\n12211\\n⎢0 1 0 3 0⎥\\n⎢0 1 0 3 0⎥\\n⎢\\n⎢\\n⎥\\n⎥\\n⎢\\n⎥\\n⎥\\n[A, b] \u0003 ⎢ 0 −2 1 −1 1 ⎥ \u0003 ⎢\\n⎢0 0 1 5 1⎥\\n⎣ 0 −1 1 2 1 ⎦\\n⎣0 0 1 5 1⎦\\n0 −1 1 2 1\\n00151\\n⎡\\n⎡\\n⎤\\n⎤\\n12211\\n1 0 2 −5 1\\n⎢0 1 0 3 0⎥\\n⎢0 1 0 3 0⎥\\n⎢\\n⎢\\n⎥\\n⎥\\n⎢\\n⎥\\n⎥\\n\u0003 ⎢0 0 1 5 1⎥ \u0003 ⎢\\n⎢0 0 1 5 1⎥\\n⎣0 0 0 0 0⎦\\n⎣0 0 0 0 0⎦\\n00000\\n000 00\\n⎡\\n⎤\\n1 0 0 −15 −1\\n⎢0 1 0\\n3 0⎥\\n⎢\\n⎥\\n⎢\\n\u0003\\n5 1⎥\\n\u0003 ⎢0 0 1\\n⎥ = [S A|b].\\n⎣0 0 0\\n⎦\\n0 0\\n000\\n0 0\\nHere rank(S A) = rank([S A, \u0003 b]) = 3, and hence there exist solutions.\\nThe pivot\\n\u0003 = S A. Now columns are ji = i for i = 1, 2, 3, so that P = P T = I4 and A\\nS Ax = \u0003 b can be written as\\n⎤ ⎡\\n⎤\\n⎡ ⎤ ⎡\\n−1\\n−15 x1\\n⎣x2 ⎦ = ⎣ 0 ⎦ − ⎣ 3 ⎦ [x4 ].\\n1\\n5 x3   \n",
       "72                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           78\\n\\n6 Linear Systems of Equations\\n\\nConsequently, \u0003 b(4) = [−1, 0, 1, 0]T ∈ L (A, b) and L (A, b) = \u0003 b(4) + L (A, 0), where\\nL (A, 0) =\\n\\n\u0010\\n\\n\u0011 x 4 ]T | \u0002 x4 arbitrary and [\u0002 x1 , \u0002 x2 , \u0002 x3 ]T = −[−15, 3, 5]T [\u0002 x4 ] .\\n[\u0002 x1 , . . . , \u0002\\n\\nExercises\\n6.1 Find a field K and matrices A ∈ K n,m , S ∈ K n,n and b ∈ K n,1 with L (A, b) \u0003=\\nL (S A, Sb).\\n6.2 Determine L (A, b) for the following A and b:\\n⎡\\n\\n1\\nA = ⎣1\\n1\\n⎡\\n1\\nA = ⎣1\\n1\\n⎡\\n1\\n⎢1\\nA=⎢\\n⎣1\\n1\\n⎡\\n1\\n⎢1\\nA=⎢\\n⎣1\\n1\\n\\n⎤\\n⎡\\n⎤\\n1 1\\n1\\n2 −1 ⎦ ∈ R3,3 , b = ⎣ −2 ⎦ ∈ R3,1 ,\\n−1 6\\n3\\n⎤\\n⎡\\n⎤\\n1\\n1\\n0\\n1\\n2 −1 −1⎦ ∈ R3,4 , b = ⎣ −2 ⎦ ∈ R3,1 ,\\n−1\\n6\\n2\\n3\\n⎡\\n⎤\\n⎤\\n1\\n1\\n1\\n⎢ −2 ⎥\\n2 −1⎥\\n4,3\\n4,1\\n⎥∈R , b=⎢\\n⎥\\n⎣ 3⎦ ∈ R ,\\n−1\\n6⎦\\n1\\n1\\n1\\n⎤\\n⎤\\n⎡\\n1\\n1\\n1\\n⎥\\n⎢\\n2 −1⎥\\n⎥ ∈ R4,3 , b = ⎢ −2 ⎥ ∈ R4,1 .\\n⎦\\n⎣\\n−1\\n6\\n3⎦\\n1\\n1\\n0\\n\\n6.3 Let α ∈ Q,\\n⎡\\n\\n⎤\\n⎡ ⎤\\n321\\n6\\nA = ⎣1 1 1⎦ ∈ Q3,3 , bα = ⎣ 3 ⎦ ∈ Q3,1 .\\n210\\nα\\nDetermine L (A, 0) and L (A, bα ) in dependence of α.\\n6.4 Let A ∈ K n,m and B ∈ K n,s .\\nFor i = 1, . . . , s denote by bi the ith column of\\nB. Show that the linear system of equations AX = B has at least one solution\\n\u0002\\nX ∈ K m,s if and only if rank(A) = rank([A, b1 ]) = rank([A, b2 ]) = · · · = rank([A, bs ]).\\nFind conditions under which this solution is unique.   \n",
       "73                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                6 Linear Systems of Equations\\n\\n6.5 Let\\n\\n⎡\\n\\n0 β1\\n\\n79\\n\\n⎤\\n\\n⎡ ⎤ b1\\n⎢\\n⎥\\n.\\n.\\n⎢α2 0 . ⎥\\n⎢ .. ⎥ n,n n,1\\n⎢\\n⎥\\nA=⎢\\n⎥∈K , b=⎣.⎦∈K\\n⎣ . . . . . . βn ⎦ bn\\nαn 0 be given with βi , αi \u0003= 0 for all i.\\nDetermine a recursive formula for the entries of the solution of the linear system Ax = b.   \n",
       "74                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Chapter 7\\n\\nDeterminants of Matrices\\n\\nThe determinant is a map that assigns to every square matrix A ∈ R n,n , where R is a commutative ring with unit, an element of R. This map has very interesting and important properties.\\nFor instance it yields a necessary and sufficient condition for the invertibility of A ∈ R n,n .\\nMoreover, it forms the basis for the definition of the characteristic polynomial of a matrix in Chap.\\n8.\\n\\n7.1 Definition of the Determinant\\nThere are several different approaches to define the determinant of a matrix.\\nWe use the constructive approach via permutations.\\nDefinition 7.1 Let n ∈ N be given.\\nA bijective map\\nσ : {1, 2, . . . , n} → {1, 2, . . . , n}, j \u0004→ σ( j), is called a permutation of the numbers {1, 2, . . . , n}.\\nWe denote the set of all these maps by Sn .\\nA permutation σ ∈ Sn can be written in the form\\n\u0002\\n\u0003\\nσ(1) σ(2) . . . σ(n) .\\nFor example S1 = {[1]}, S2 = {[1 2], [2 1]}, and\\nS3 = { [1 2 3], [1 3 2], [2 1 3], [2 3 1], [3 1 2], [3 2 1] }.\\nFrom Lemma 2.17 we know that |Sn | = n! = 1 · 2 · . . . · n.\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_7\\n\\n81   \n",
       "75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               82\\n\\n7 Determinants of Matrices\\n\\nThe set Sn with the composition of maps “◦” forms a group (cp.\\nExercise 3.3), which is sometimes called the symmetric group.\\nThe neutral element in this group is the permutation [1 2 . . . n].\\nWhile S1 and S2 are commutative groups, the group Sn for n ≥ 3 is noncommutative.\\nAs an example consider n = 3 and the permutations σ1 = [2 3 1],\\nσ2 = [1 3 2].\\nThen\\nσ1 ◦ σ2 = [σ1 (σ2 (1)) σ1 (σ2 (2)) σ1 (σ2 (3))] = [σ1 (1) σ1 (3) σ1 (2)] = [2 1 3],\\nσ2 ◦ σ1 = [σ2 (σ1 (1)) σ2 (σ1 (2)) σ2 (σ1 (3))] = [σ2 (2) σ2 (3) σ2 (1)] = [3 2 1].\\nDefinition 7.2 Let n ≥ 2 and σ ∈ Sn .\\nA pair (σ(i), σ( j)) with 1 ≤ i < j ≤ n and\\nσ(i) > σ( j) is called an inversion of σ.\\nIf k is the number of inversions of σ, then sgn(σ) := (−1)k is called the sign of σ.\\nFor n = 1 we define sgn([1]) := 1= (−1)0 .\\nIn short, an inversion of a permutation σ is a pair that is “out of order”.\\nThe term inversion should not be confused with the inverse map σ −1 (which exists, since σ is bijective).\\nThe sign of a permutation is sometimes also called the signature.\\nExample 7.3 The permutation [2 3 1 4] ∈ S4 has the inversions (2, 1) and (3, 1), so that sgn([2 3 1 4]) = 1.\\nThe permutation [4 1 2 3] ∈ S4 has the inversions (4, 1),\\n(4, 2), (4, 3), so that sgn([4 1 2 3]) = −1.\\nWe can now define the determinant map.\\nDefinition 7.4 Let R be a commutative ring with unit and let n ∈ N. The map det : R n,n → R,\\n\\nA = [ai j ] \u0004→ det(A) :=\\n\\n\u0004\\nσ∈Sn sgn(σ) n\\n\u0005 ai,σ(i) ,\\n\\n(7.1) i=1 is called the determinant, and the ring element det(A) is called the determinant of A.\\nThe formula (7.1) for det(A) is called the signature formula of Leibniz.1 The term sgn(σ) in this definition is to be interpreted as an element of the ring R, i.e., either sgn(σ) = 1 ∈ R or sgn(σ) = −1 ∈ R, where −1 ∈ R is the unique additive inverse of the unit 1 ∈ R.\\nExample 7.5 For n = 1 we have A = [a11 ] and thus det(A) = sgn([1])a11 = a11 .\\nFor n = 2 we get det(A) = det\\n\\n\b\\n\u0006\u0007 a11 a12\\n= sgn([1 2])a11 a22 + sgn([2 1])a12 a21 a21 a22\\n\\n= a11 a22 − a12 a21 .\\n\\n1 Gottfried\\n\\nWilhelm Leibniz (1646–1716).   \n",
       "76                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  7.1 Definition of the Determinant\\n\\n83\\n\\nFor n = 3 we have the Sarrus rule2 : det(A) = a11 a22 a33 + a12 a23 a31 + a13 a21 a32\\n−a11 a23 a32 − a12 a21 a33 − a13 a22 a31 .\\nIn order to compute det(A) using the signature formula of Leibniz we have to form n! products with n factors each.\\nFor large n this is too costly even on modern computers.\\nAs we will see in Corollary 7.16, there are more efficient ways for computing det(A).\\nThe signature formula is mostly of theoretical relevance, since it represents the determinant of A explicitly in terms of the entries of A. Considering the n 2 entries as variables, we can interpret det(A) as a polynomial in these variables.\\nIf R = R or R = C, then standard techniques of Analysis show that det(A) is a continuous function of the entries of A.\\nWe will now study the group of permutations in more detail.\\nThe permutation\\nσ = [3 2 1] ∈ S3 has the inversions (3, 2), (3, 1) and (2, 1), so that sgn(σ) = −1.\\nMoreover,\\n\u0005\\n1≤i< j≤3\\n\\nσ(2) − σ(1) σ(3) − σ(1) σ(3) − σ(2)\\nσ( j) − σ(i)\\n= j −i\\n2−1\\n3−1\\n3−2\\n=\\n\\n2−3 1−3 1−2\\n= (−1)3 = −1 = sgn(σ).\\n2−1 3−1 3−2\\n\\nThis observation can be generalized as follows.\\nLemma 7.6 For each σ ∈ Sn we have sgn(σ) =\\n\\n\u0005\\n1≤i< j≤n\\n\\nσ( j) − σ(i)\\n.\\nj −i\\n\\n(7.2)\\n\\nProof If n = 1, then the left hand side of (7.2) is an empty product, which is defined to be 1 (cp.\\nSect.\\n3.2), so that (7.2) holds for n = 1.\\nLet n > 1 and σ ∈ Sn with sgn(σ) = (−1)k , i.e., k is the number of pairs\\n(σ(i), σ( j)) with i < j but σ(i) > σ( j).\\nThen\\n\u0005\\n\\n(σ( j) − σ(i)) = (−1)k\\n\\n1≤i< j≤n\\n\\n\u0005\\n1≤i< j≤n\\n\\n|σ( j) − σ(i)| = (−1)k\\n\\n\u0005\\n\\n( j − i).\\n\\n1≤i< j≤n\\n\\nIn the last equation we have used the fact that the two products have the same factors\\n(except possibly for their order).\\n\b\\n\\n2 Pierre\\n\\nFrédéric Sarrus (1798–1861).   \n",
       "77                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      84\\n\\n7 Determinants of Matrices\\n\\nTheorem 7.7 For all σ1 , σ2 ∈ Sn we have sgn(σ1 ◦ σ2 ) = sgn(σ1 ) sgn(σ2 ).\\nIn particular, sgn(σ −1 ) = sgn(σ) for all σ ∈ Sn .\\nProof By Lemma 7.6 we have\\n\u0005 sgn(σ1 ◦ σ2 ) =\\n\\n1≤i< j≤n\\n\\n⎛\\n\\nσ1 (σ2 ( j)) − σ1 (σ2 (i)) j −i\\n\\n\u0005\\n\\n⎞⎛\\n\\n\u0005\\n\\n⎞\\n\\nσ1 (σ2 ( j)) − σ1 (σ2 (i)) ⎠ ⎝\\nσ2 ( j) − σ2 (i) ⎠\\nσ\\n( j)\\n−\\nσ\\n(i) j −i\\n2\\n2\\n1≤i< j≤n\\n1≤i< j≤n\\n⎛\\n⎞\\n\u0005\\nσ1 (σ2 ( j)) − σ1 (σ2 (i)) ⎠\\n=⎝ sgn(σ2 )\\nσ2 ( j) − σ2 (i)\\n1≤σ2 (i)<σ2 ( j)≤n\\n⎛\\n⎞\\n\u0005 σ1 ( j) − σ1 (i)\\n⎠ sgn(σ2 )\\n=⎝ j −i\\n1≤i< j≤n\\n\\n=⎝\\n\\n= sgn(σ1 ) sgn(σ2 ).\\nFor each σ ∈ Sn we have 1 = sgn([1 2 . . . n]) = sgn(σ ◦ σ −1 ) = sgn(σ) sgn(σ −1 ),\\n\b so that sgn(σ) = sgn(σ −1 ).\\nTheorem 7.7 shows that the map sgn is a homomorphism between the groups\\n(Sn , ◦) and ({1, −1}, ·), where the operation in the second group is the standard multiplication of the integers 1 and −1.\\nDefinition 7.8 A transposition is a permutation τ ∈ Sn , n ≥ 2, that exchanges exactly two distinct elements k, \u0002 ∈ {1, 2, . . . , n}, i.e., τ (k) = \u0002, τ (\u0002) = k and\\nτ ( j) = j for all j ∈ {1, 2, . . . , n} \\ {k, \u0002}.\\nObviously τ −1 = τ for every transposition τ ∈ Sn .\\nLemma 7.9 Let τ ∈ Sn be the transposition, that exchanges k and \u0002 for some\\n1 ≤ k < \u0002 ≤ n.\\nThen τ has exactly 2(\u0002−k)−1 inversions and, hence, sgn(τ ) = −1.\\nProof We have \u0002 = k + j for a j ≥ 1 and thus τ is given by\\nτ = [1, . . . , k − 1, k + j, k + 1, . . . , k + ( j − 1), k, \u0002 + 1, . . . , n], where the points denote values of τ in increasing and thus “correct” order.\\nA simple counting argument shows that τ has exactly 2 j − 1 = 2(\u0002 − k) − 1 inversions. \b   \n",
       "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      7.2 Properties of the Determinant\\n\\n85\\n\\n7.2 Properties of the Determinant\\nIn this section we prove important properties of the determinant map.\\nLemma 7.10 For A ∈ R n,n the following assertions hold:\\n(1) For λ ∈ R,\\n\\n\u0006\u0007 det\\n\\n(2)\\n(3)\\n(4)\\n(5)\\n\\nλ \u0003\\n0n,1 A\\n\\n\b\\n\\n\u0006\u0007\\n= det\\n\\nλ 01,n\\n\u0003 A\\n\\n\b\\n= λ det(A).\\n\\n\u000en\\nIf A = [ai j ] is upper or lower triangular, then det(A) = i=1 aii .\\nIf A has a zero row or column, then det(A) = 0.\\nIf n ≥ 2 and A has two equal rows or two equal columns, then det(A) = 0.\\ndet(A) = det(A T ).\\n\\nProof\\n(1) Exercise.\\n(2) This follows by an application of (1) to the upper (or lower) triangular matrix A.\\n(3) If A has\u000e a zero row or column, then for every σ ∈ Sn at least one factor in the n ai,σ(i) is equal to zero and thus det(A) = 0.\\nproduct i=1\\n(4) Let the rows k and \u0002, with k < \u0002, of A = [ai j ] be equal, i.e., ak j = a\u0002j for j = 1, . . . , n.\\nLet τ ∈ Sn be the transposition that exchanges the elements k and\\n\u0002, and let\\nTn := {σ ∈ Sn | σ(k) < σ(\u0002)}.\\nSince the set Tn contains all permutations σ ∈ Sn for which σ(k) < σ(\u0002), we have |Tn | = |Sn |/2 and\\nSn \\ Tn = {σ ◦ τ | σ ∈ Tn }.\\nMoreover, ai,(σ◦τ )(i)\\n\\n⎧\\n⎪\\n⎨ai,σ(i) , i = k, \u0002,\\n= ak,σ(\u0002) , i = k,\\n⎪\\n⎩ a\u0002,σ(k) , i = \u0002.\\n\\nWe have ak,σ(\u0002) = a\u0002,σ(\u0002) and a\u0002,σ(k) = ak,σ(k) , Thus, using Theorem 7.7 and\\nLemma 7.9, we obtain\\n\u0004\\nσ∈Sn \\Tn sgn(σ) n\\n\u0005 ai,σ(i) =\\n\\n\u0004 sgn(σ ◦ τ )\\n\\nσ∈Tn i=1\\n\\n=\\n\\n\u0004 ai,(σ◦τ )(i) i=1\\n\\n(−sgn(σ))\\n\\nσ∈Tn\\n\\n=− n\\n\u0005\\n\\n\u0004\\nσ∈Tn n\\n\u0005 ai,(σ◦τ )(i) i=1 sgn(σ) n\\n\u0005 i=1 ai,σ(i) .   \n",
       "79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   86\\n\\n7 Determinants of Matrices\\n\\nThis implies det(A) =\\n\\n\u0004 sgn(σ)\\n\\nσ∈Sn\\n\\n=\\n\\n\u0004 n\\n\u0005 ai,σ(i) i=1 sgn(σ)\\n\\nσ∈Tn n\\n\u0005\\n\\n\u0004 ai,σ(i) + sgn(σ)\\n\\nσ∈Sn \\Tn i=1 n\\n\u0005 ai,σ(i) = 0.\\ni=1\\n\\nThe proof for the case of two equal columns is analogous.\\n(5) We observe first that\\n{ (σ(i), i) | 1 ≤ i ≤ n } = { (i, σ −1 (i)) | 1 ≤ i ≤ n } for every σ ∈ Sn .\\nTo see this, let i with 1 ≤ i ≤ n be fixed.\\nThen σ(i) = j if and only if i = σ −1 ( j).\\nThus, (σ(i), i) = ( j, i) is an element of the first set if and only if ( j, σ −1 ( j)) = ( j, i) is an element of the second set.\\nSince σ is bijective, the two sets are equal.\\nLet A = [ai j ] and A T = [bi j ] with bi j = a ji .\\nThen det(A T ) =\\n\\n\u0004 sgn(σ)\\n\\nσ∈Sn\\n\\n=\\n\\n\u0004\\n\u0004 bi,σ(i) = sgn(σ −1 ) n\\n\u0005 aσ(i),i = i=1 sgn(σ)\\n\\nσ∈Sn n\\n\u0005\\n\\n\u0004 sgn(σ)\\n\\nσ∈Sn i=1\\n\\nσ∈Sn\\n\\n= n\\n\u0005\\n\\n\u0004 n\\n\u0005 aσ(i),i i=1 sgn(σ −1 )\\n\\nσ∈Sn n\\n\u0005 ai,σ−1 (i) i=1 ai,σ(i) = det(A).\\ni=1\\n\\nHere we have used sgn(σ −1 ) (cp.\\nTheorem 7.7) and the fact that\\n\u000enthat sgn(σ) = \u000e n ai,σ−1 (i) have the same factors.\\n\b the two products i=1 aσ(i),i and i=1\\nExample 7.11 For the matrices\\n⎡\\n\\n⎤\\n123\\nA = ⎣0 4 5⎦ ,\\n006\\n\\n⎡\\n12\\nB = ⎣1 3\\n14\\n\\n⎡\\n⎤\\n⎤\\n0\\n112\\n0⎦ , C = ⎣1 1 3⎦\\n0\\n114 from Z3,3 we obtain det(A) = 1 · 4 · 6 = 24 by (2) in Lemma 7.10, and det(B) = det(C) = 0 by (3) and (4) in Lemma 7.10.\\nWe may also compute these determinants using the Sarrus rule from Example 7.5.\\nItem (2) in Lemma 7.10 shows in particular that det(In ) = 1 for the identity matrix In = [e1 , e2 , . . . , en ] ∈ R n,n .\\nFor this reason the determinant map is called normalized.   \n",
       "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               7.2 Properties of the Determinant\\n\\n87\\n\\nFor σ ∈ Sn the matrix\\nPσ := [eσ(1) , eσ(2) , . . . , eσ(n) ] is called the permutation matrix associated with σ.\\nThis map from the group Sn to the group of permutation matrices in R n,n is bijective.\\nThe inverse of a permutation matrix is its transpose (cp.\\nTheorem 4.16) and we can easily check that\\nPσ−1 = PσT = Pσ−1 .\\nIf A = [a1 , a2 , . . . , an ] ∈ R n,n , i.e., a j ∈ R n,1 is the jth column of A, then\\nA Pσ = [aσ(1) , aσ(2) , . . . , aσ(n) ], i.e., the right-multiplication of A with Pσ exchanges the columns of A according to the permutation σ.\\nIf, on the other hand, ai ∈ R 1,n is the ith row of A, then\\n⎤ aσ(1)\\n⎢aσ(2) ⎥\\n⎥\\n⎢\\nPσT A = ⎢ . ⎥ ,\\n⎣ .. ⎦\\n⎡ aσ(n) i.e., the left-multiplication of A by PσT exchanges the rows of A according to the permutation σ.\\nWe next study the determinants of the elementary matrices.\\nLemma 7.12 (1) For σ ∈ Sn and the associated permutation matrix Pσ ∈ R n,n we have sgn(σ) = det(Pσ ).\\nIf n ≥ 2 and Pi j is defined as in (5.1), then det(Pi j ) =\\n−1.\\n(2) If Mi (λ) and G i j (λ) are defined as in (5.2) and (5.3), respectively, then det(Mi (λ)) = λ and det(G i j (λ)) = 1.\\nProof\\n(1) If \u0019\\nσ ∈ Sn and P\u0019σ = [ai j ] ∈ R n,n , then a\u0019σ( j), j = 1 for j = 1, 2, . . . , n, and all other entries of P\u0019σ are zero.\\nHence\\nT det(P\u0019\\nσ ) = det(P\u0019\\nσ )=\\n\\n\u0004\\nσ∈Sn sgn(σ) n\\n\u0005 j=1\\n\\n\u001a aσ( j), j = sgn(\u0019\\nσ)\\n\u001b n\\n\u0005 j=1 a\u0019\\nσ ).\\nσ ( j), j = sgn(\u0019\\n\u001a \u001b\n",
       " \n",
       "\\n=1\\n\\n=0 for σ =\u0019\\nσ\\n\\nThe permutation matrix Pi j is associated with the transposition that exchanges i and j.\\nHence, det(Pi j ) = −1 follows from Lemma 7.9.\\n(2) Since Mi (λ) and G i j (λ) are lower triangular matrices, the assertion follows from\\n(2) in Lemma 7.10.\\n\b   \n",
       "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         88\\n\\n7 Determinants of Matrices\\n\\nThese results lead to some important computational rules for determinants.\\nLemma 7.13 For A ∈ R n,n , n ≥ 2, and λ ∈ R the following assertions hold:\\n(1) The multiplication of a row of A by λ leads to the multiplication of det(A) by λ: det(Mi (λ)A) = λ det(A) = det(Mi (λ)) det(A).\\n(2) The addition of the λ–multiple of a row of A to another row of A does not change det(A): det(G i j (λ)A) = det(A) = det(G i j (λ)) det(A), and det(G i j (λ)T A) = det(A) = det(G i j (λ)T ) det(A).\\n(3) Exchanging two rows of A changes the sign of det(A): det(Pi j A) = − det(A) = det(Pi j ) det A.\\nProof\\n\u0019 = Mi (λ)A = [\u0019\\n(1) If A = [amk ] and A amk ], then amk , m = i,\\n=\\nλamk , m = i,\\n\\n\u0019 amk and hence\\n\u0019 = det( A)\\n\\n\u0004 n\\n\u0005 sgn(σ)\\n\\nσ∈Sn m=1\\n\\n\u0019 am,σ(m) =\\n\\n\u0004\\nσ∈Sn sgn(σ) \u0019 ai,σ(i)\\n\u001a \u001b n\\n\u0005 m=1\\n=λai,σ(i) m =i\\n\\n\u0019 am,σ(m)\\n\u001a \u001b\n",
       " \n",
       "\\n=am,σ(m)\\n\\n= λ det(A).\\n\u0019 = G i j (λ)A = [\u0019\\n(2) If A = [amk ] and A amk ], then\\n\u0019 amk amk , m = j,\\n= a jk + λaik , m = j, and hence\\n\u0019 = det( A)\\n\\n\u0004 sgn(σ) (a j,σ( j) + λai,σ( j) )\\n\\nσ∈Sn\\n\\n=\\n\\n\u0004\\nσ∈Sn n\\n\u0005 am,σ(m) m=1 m= j sgn(σ) n\\n\u0005 m=1 am,σ(m) + λ\\n\\n\u0004\\nσ∈Sn sgn(σ)ai,σ( j) n\\n\u0005 am,σ(m) .\\nm=1 m= j\\n\\nThe first term is equal to det(A), and the second is equal to the determinant of a matrix with two equal columns, and thus equal to zero.\\nThe proof for the matrix\\nG i j (λ)T A is analogous.   \n",
       "82                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       7.2 Properties of the Determinant\\n\\n89\\n\\n(3) The permutation matrix Pi j exchanges rows i and j of A, where i < j.\\nThis exchange can be expressed by the following four elementary row operations:\\nMultiply row j by −1; add row i to row j; add the (−1)–multiple of row j to row i; add row i to row j.\\nTherefore,\\nPi j = G i j (1)(G i j (−1))T G i j (1)M j (−1).\\n(One may verify this also by carrying out the matrix multiplications.)\\nUsing (1) and (2) we obtain det(Pi j A) = det G i j (1)(G i j (−1))T G i j (1)M j (−1)A\\n= det(G i j (1)) det((G i j (−1))T ) det(G i j (1)) det(M j (−1)) det(A)\\n\b\\n= (−1) det(A).\\nSince det(A) = det(A T ) (cp.\\n(5) in Lemma 7.10), the results in Lemma 7.13 for the rows of A can be formulated analogously for the columns of A.\\nExample 7.14 Consider the matrices\\n⎡\\n13\\nA = ⎣1 2\\n12\\n\\n⎤\\n0\\n0⎦ ,\\n4\\n\\n⎡\\n\\n⎤\\n310\\nB = ⎣2 1 0⎦ ∈ Z3,3 .\\n214\\n\\nA simple calculation shows that det(A) = −4.\\nSince B is obtained from A by exchanging the first two columns we have det(B) = − det(A) = 4.\\nThe determinant map can be interpreted as a map of (R n,1 )n to R, i.e., as a map of the n columns of the matrix A ∈ R n,n to the ring R. If ai , a j ∈ R n,1 are two columns of A,\\nA = [. . . ai . . . a j . . .], then det(A) = − det([. . . a j . . . ai . . .]) by (3) in Lemma 7.13.\\nDue to this property the determinant map is called an alternating map of the columns of A. Analogously, the determinant map is an alternating map of the rows of A.\\n(1)\\n(2)\\n( j)\\n\" of A has the form λa + μa for some λ, μ ∈ R and a =\\n!\\nIf the kth row\\n( j)\\n( j)\\n1,n ak1 , . . . , akn ∈ R , j = 1, 2, then   \n",
       "83                                                                                                                                                                                                                                                                                                                                                                                                                        90\\n\\n7 Determinants of Matrices\\n\\n⎛⎡\\n\\n⎤⎞\\n..\\n.\\nn\\n⎜⎢\\n⎥⎟\\n&\u0005\\n%\\n\u0004\\n⎜⎢\\n⎥⎟\\n(1)\\n(2) det(A) = det ⎜⎢λa (1) + μa (2) ⎥⎟ = sgn(σ) λak,σ(k) + μak,σ(k) ai,σ(i)\\n⎝⎣\\n⎦⎠\\n..\\ni=1\\nσ∈Sn i =k\\n.\\n=λ\\n\\n\u0004\\nσ∈Sn\\n\\n(1) sgn(σ) ak,σ(k)\\n\\n⎛⎡ n\\n\u0005 i=1 i =k\\n\\n\u0004 ak,σ(k) + μ\\n\\nσ∈Sn\\n\\n(2) sgn(σ) ak,σ(k)\\n\\n⎛⎡\\n⎤⎞\\n⎤⎞\\n..\\n..\\n⎜⎢ . ⎥⎟\\n⎜⎢ . ⎥⎟\\n⎜⎢\\n⎜⎢\\n⎥⎟\\n⎥⎟\\n= λ det ⎜⎢a (1) ⎥⎟ + μ det ⎜⎢a (2) ⎥⎟ .\\n⎝⎣ . ⎦⎠\\n⎝⎣ . ⎦⎠\\n..\\n..\\nn\\n\u0005 ai,σ(i) i=1 i =k\\n\\nThis property is called the linearity of the determinant map with respect to the rows of A. Analogously we have the linearity with respect to the columns of A. Linear maps will be studied in detail in later chapters.\\nThe next result is called the multiplication theorem for determinants.\\nTheorem 7.15 If K is a field and A, B ∈ K n,n , then det(AB) = det(A) det(B).\\nMoreover, if A is invertible, then det(A−1 ) = (det(A))−1 .\\nProof By Theorem 5.2 we know that for A ∈ K n,n there exist invertible elementary\\n\u0019 = St . . .\\nS1 A is in echelon form.\\nBy Lemma 7.13 we matrices S1 , . . . , St such that A have\\n\u0019 det(A) = det(S1−1 ) · · · det(St−1 ) det( A), as well as\\n\u001f\\n\u0019 det(AB) = det S1−1 · · · St−1 AB\\n\u0019\\n= det(S1−1 ) · · · det(St−1 ) det( AB).\\n\u0019 and thus also AB\\n\u0019 have a zero\\nThere are two cases: If A is not invertible, then A\\n\u0019 = det( AB)\\n\u0019 row.\\nThen det( A)\\n= 0, which implies that det(A) = 0, and hence\\n\u0019 = In , det(AB) = 0 = det(A) det(B).\\nOn the other hand, if A is invertible, then A\\n\u0019 is in echelon form.\\nNow det(In ) = 1 again gives det(AB) = det(A) det(B).\\nsince A\\nFinally, if A is invertible, then 1 = det(In ) = det(A A−1 ) = det(A) det(A−1 ),\\n\b and hence det(A−1 ) = (det(A))−1 .\\nSince our proof relies on Theorem 5.2, which is valid for matrices over a field\\nK , we have formulated Theorem 7.15 for A, B ∈ K n,n .\\nHowever, the multiplication theorem for determinants also holds for matrices over a commutative ring R with unit.\\nA direct proof based on the signature formula of Leibniz can be found, for example, in the book “Advanced Linear Algebra” by Loehr [Loe14, Sect.\\n5.13].\\nThat book also contains a proof of the Cauchy-Binet formula for det(AB) with A ∈ R n,m and\\nB ∈ R m,n for n ≤ m.\\nBelow we will sometimes use that det(AB) = det(A) det(B)   \n",
       "84                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              7.2 Properties of the Determinant\\n\\n91 holds for all A, B ∈ R n,n , although we have shown the result in Theorem 7.15 only for A, B ∈ K n,n .\\nThe proof of Theorem 7.15 suggests that det(A) can be easily computed while transforming A ∈ K n,n into its echelon form using elementary row operations.\\nCorollary 7.16 For A ∈ K n,n let S1 , . . . , St ∈ K n,n be elementary matrices, such\\n\u0019 has a zero row and hence\\n\u0019 = St . . .\\nS1 A is in echelon form.\\nThen either A that A\\n−1\\n\u0019 det(A) = 0, or A = In and hence det(A) = (det(S1 )) · · · (det(St ))−1 .\\nAs shown in Theorem 5.4, every matrix A ∈ K n,n can be factorized as A = P LU , and hence det(A) = det(P) det(L) det(U ).\\nThe determinants of the matrices on the right hand side are easily computed, since these are permutation and triangular matrices.\\nAn LU -decomposition of a matrix A therefore yields an efficient way to compute det(A).\\nMATLAB-Minute.\\nLook at the matrices wilkinson(n) for n=2,3,. . .,10 in MATLAB.\\nCan you find a general formula for their entries?\\nFor n=2,3,. . .,10 compute\\nA=wilkinson(n)\\n[L,U,P]=lu(A) (LU -decomposition; cp. the MATLAB-Minute above Definition 5.6) det(L), det(U), det(P), det(P)∗det(L)∗det(U), det(A)\\nWhich permutation is associated with the computed matrix P?\\nWhy is det(A) an integer for odd n?\\n\\n7.3 Minors and the Laplace Expansion\\nWe now show that the determinant can be used for deriving formulas for the inverse of an invertible matrix and for the solution of linear systems of equations.\\nThese formulas are, however, more of theoretical than practical relevance.\\nDefinition 7.17 Let R be a commutative ring with unit and let A ∈ R n,n , n ≥ 2.\\nThen the matrix A( j, i) ∈ R n−1,n−1 that is obtained by deleting the jth row and ith column of A is called a minor 3 of A. The matrix adj(A) = [bi j ] ∈ R n,n with bi j := (−1)i+ j det(A( j, i)), is called the adjunct of A.\\nThe adjunct is also called adjungate or classical adjoint of A.\\n3 This term was introduced in 1850 by James Joseph Sylvester (1814–1897).   \n",
       "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  92\\n\\n7 Determinants of Matrices\\n\\nTheorem 7.18 For A ∈ R n,n , n ≥ 2, we have\\nA adj(A) = adj(A) A = det(A)In .\\nIn particular A is invertible if and only if det(A) ∈ R is invertible.\\nIn this case\\n(det(A))−1 = det(A−1 ) and A−1 = (det(A))−1 adj(A).\\nProof Let B = [bi j ] have the entries bi j = (−1)i+ j det(A( j, i)).\\nThen C = [ci j ] = adj(A)A satisfies ci j = n\\n\u0004 bik ak j = k=1 n\\n\u0004\\n\\n(−1)i+k det(A(k, i))ak j .\\nk=1\\n\\nLet a\u0002 be the \u0002th column of A and let\\n\u0019 i) := [a1 , . . . , ai−1 , ek , ai+1 , . . . , an ] ∈ R n,n ,\\nA(k, where ek is the kth column of the identity matrix In .\\nThen there exist permutation matrices P and Q that perform k − 1 row and i − 1 column exchanges, respectively, such that\\n\u0007\\n\b\\n\u0003\\n\u0019 i)Q = 1\\nP A(k,\\n.\\n0 A(k, i)\\nUsing (1) in Lemma 7.10 we obtain\\n\b\\n\u0003\\n1\\n\u0019 i)Q)\\n= det(P A(k,\\n0 A(k, i)\\n\u0019 i)) det(Q)\\n= det(P) det( A(k,\\n\u0006\u0007 det(A(k, i)) = det\\n\\n\u0019 i))\\n= (−1)(k−1)+(i−1) det( A(k,\\n\u0019 i)).\\n= (−1)k+i det( A(k,\\nThe linearity of the determinant with respect to the columns now gives ci j = n\\n\u0004\\n\u0019 i))\\n(−1)i+k (−1)k+i ak j det( A(k, k=1\\n\\n= det([a1 , . . . , ai−1 , a j , ai+1 , . . . , an ])\\n\n",
       "\\n0, i = j\\n= det(A), i = j\\n= δi j det(A), and thus adj(A)A = det(A)In .\\nAnalogously we can show that A adj(A) = det(A)In .   \n",
       "86                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         7.3 Minors and the Laplace Expansion\\n\\n93\\n\\nIf det(A) ∈ R is invertible, then\\nIn = (det(A))−1 adj(A)A = A(det(A))−1 adj(A), i.e., A is invertible with A−1 = (det(A))−1 adj(A).\\nIf, on the other hand, A is invertible, then\\n1 = det(In ) = det(A A−1 ) = det(A) det(A−1 ) = det(A−1 ) det(A), where we have used the multiplication theorem for determinants over R (cp. our comment following the proof of Theorem 7.15).\\nThus, det(A) is invertible with\\n\b\\n(det(A))−1 = det(A−1 ), and again A−1 = (det(A))−1 adj(A).\\nExample 7.19\\n(1) For\\n\\n\b\\n41\\n∈ Z2,2\\n21\\n\\n\u0007\\nA= we have det(A) = 2 and thus A is not invertible.\\nBut A is invertible when considered as an element of Q2,2 , since in this case det(A−1 ) = (det(A))−1 = 21 .\\n(2) For\\n\u0007\\n\b t −1 t −2\\nA=\\n∈ (Z[t])2,2 t t −1 we have det(A) = 1.\\nThe matrix A is invertible, since 1 ∈ Z[t] is invertible.\\nNote that if A ∈ R n,n is invertible, then Theorem 7.18 shows that A−1 can be obtained by inverting only one ring element, det(A).\\nWe now use Theorem 7.18 and the multiplication theorem for matrices over a commutative ring with unit to prove a result already announced in Sect.\\n4.2: In order\\n\u0019 ∈ R n,n is the (unique) inverse of A ∈ R n,n , only one of the two to show that A\\n\u0019 = In needs to be checked.\\n\u0019A = In or A A equations A\\n\u0019 ∈ R n,n exists with A\\n\u0019A = In or A A\\n\u0019 = In ,\\nCorollary 7.20 Let A ∈ R n,n .\\nIf a matrix A\\n\u0019 = A−1 .\\nthen A is invertible and A\\n\u0019A = In , then the multiplication theorem for determinants yields\\nProof If A\\n\u0019A) = det( A)\\n\u0019 det(A) = det(A) det( A),\\n\u0019\\n1 = det(In ) = det( A\\n\u0019 Thus also A is invertible i.e., det(A) ∈ R is invertible with (det(A))−1 = det( A).\\n−1 and has a unique inverse A .\\nFor n = 1 this is obvious and for n ≥ 2 it was shown\\n\u0019A = In from the right with A−1 we in Theorem 7.18.\\nIf we multiply the equation A\\n−1\\n\u0019 get A = A .\\n\u0019 = In is analogous.\\n\b\\nThe proof starting from A A   \n",
       "87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               94\\n\\n7 Determinants of Matrices\\n\\nLet us summarize the invertibility criteria for a square matrix over a field that we have shown so far:\\nA ∈ G L n (K )\\n\\nTheorem 5.2\\n\\n⇐⇒\\n\\nThe echelon form of A is the identity matrix In\\n\\nDefinition 5.10\\n\\n⇐⇒ clear\\n\\n⇐⇒ rank(A) = n rank(A) = rank([A, b]) = n for all b ∈ K n,1\\n\\nAlgorithm 6.6\\n\\n|L (A, b)| = 1 for all b ∈ K n,1\\n\\nTheorem 7.18 det(A) = 0.\\n\\n⇐⇒\\n⇐⇒\\n\\n(7.3)\\n\\nAlternatively we obtain:\\nA∈\\n/ G L n (K )\\n\\nTheorem 5.2\\n\\n⇐⇒\\n\\nThe echelon form of A has at least one zero row\\n\\nDefinition 5.10\\n\\n⇐⇒ clear\\n\\n⇐⇒ rank(A) < n rank([A, 0]) < n\\n\\nAlgorithm 6.6\\n\\nL (A, 0) = {0}\\n\\nTheorem 7.18 det(A) = 0.\\n\\n⇐⇒\\n⇐⇒\\n\\n(7.4)\\n\\nIn the fields Q, R and C we have the (usual) absolute value | · | of numbers and can formulate the following useful invertibility criterion for matrices.\\nTheorem 7.21 If A ∈ K n,n with K ∈ {Q, R, C} is diagonally dominant, i.e., if\\n|aii | > n\\n\u0004\\n\\n|ai j | for all i = 1, . . . , n, j=1 j =i then det(A) = 0.\\nProof We prove the assertion by contraposition, i.e., by showing that det(A) = 0 implies that A is not diagonally dominant.\\nIf det(A) = 0, then L (A, 0) = {0}, i.e., the homogeneous linear system of xn ]T = 0.\\nLet ' xm be an equations Ax = 0 has at least one solution ' x = [' x1 , . . . , ' entry of ' x with maximal absolute value, i.e., |' xm | ≥ |' x j | for all j = 1, . . . , n.\\nIn x = 0 is given by particular, we then have |' xm | > 0.\\nThe mth row of A' x1 + am2' x2 + . . . + amn' xn = 0 am1'\\n\\n⇔ amm ' xm = − n\\n\u0004 am j ' xj.\\nj=1 j =m\\n\\nWe now take absolute values on both sides and use the triangle inequality, which yields   \n",
       "88                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         7.3 Minors and the Laplace Expansion\\n\\n|amm | |' xm | ≤ n\\n\u0004\\n\\n|am j | |' xj| ≤ j=1 j =m\\n\\n95 n\\n\u0004\\n\\n|am j ||' xm |, hence |amm | ≤ n\\n\u0004 j=1 j =m\\n\\n|am j |, j=1 j =m\\n\\n\b so that A not diagonally dominant.\\nThe converse of this theorem does not hold: For example, the matrix\\nA=\\n\\n\u0007 \b\\n12\\n∈ Q2,2 ,\\n10 has det(A) = −2 = 0, but A is not diagonally dominant.\\nFrom Theorem 7.18 we obtain the Laplace expansion4 of the determinant, which is particularly useful when A contains many zero entries (cp.\\nExample 7.24 below).\\nCorollary 7.22 For A ∈ R n,n , n ≥ 2, the following assertions hold:\\n(1) For each i = 1, 2, . . . , n we have det(A) = n\\n\u0004\\n(−1)i+ j ai j det(A(i, j)).\\nj=1\\n\\n(Laplace expansion of det(A) with respect to the ith row A.)\\n(2) For each j = 1, 2, . . . , n we have det(A) = n\\n\u0004\\n(−1)i+ j ai j det(A(i, j)).\\ni=1\\n\\n(Laplace expansion of det(A) with respect to the jth column of A.)\\nProof The two expansions for det(A) follow immediately by comparison of the diagonal entries in the matrix equations det(A) In = A adj(A) and det(A) In = adj(A) A.\\n\b\\nThe Laplace expansions allows a recursive definition of the determinant: For A ∈\\nR n,n with n ≥ 2, let det(A) be defined as in (1) or (2) in Corollary 7.22.\\nWe can choose an arbitrary row or column of A. The formula for det(A) then contains only matrices of size (n−1)×(n−1).\\nFor each of these we can use the Laplace expansion again, now expressing each determinant in terms of determinants of (n − 2) × (n − 2) matrices.\\nWe can do this recursively until only 1 × 1 matrices remain.\\nFor A = [a11 ] ∈ R 1,1 we define det(A) := a11 .\\nFinally we state Cramer’s rule,5 which gives an explicit formula for the solution of a linear system in form of determinants.\\nThis rule is only of theoretical value, because in order to compute the n components of the solution it requires the evaluation of n + 1 determinants of n × n matrices.\\n4 Pierre-Simon\\n5 Gabriel\\n\\nLaplace (1749–1827) published this expansion in 1772.\\nCramer (1704–1752).   \n",
       "89                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               96\\n\\n7 Determinants of Matrices\\n\\nCorollary 7.23 Let K be a field, A ∈ G L n (K ) and b ∈ K n,1 .\\nThen the unique solution of the linear system of equations Ax = b is given by xn ]T = A−1 b = (det(A))−1 adj(A)b,\\n' x = [' x1 , . . . , ' with\\n' xi = det[a1 , . . . , ai−1 , b, ai+1 , . . . , an ]\\n, i = 1, . . . , n.\\ndet(A)\\n\\nExample 7.24 Consider\\n⎡\\n13\\n⎢1 2\\nA=⎢\\n⎣1 2\\n12\\n\\n0\\n0\\n1\\n3\\n\\n⎤\\n⎡ ⎤\\n0\\n1\\n⎢2⎥\\n0⎥\\n4,4\\n⎥ ∈ Q , b = ⎢ ⎥ ∈ Q4,1 .\\n⎣1⎦\\n0⎦\\n1\\n0\\n\\nThe Laplace expansion with respect to the last column yields\\n⎛⎡\\n⎤⎞\\n\u0006\u0007 \b\\n130\\n13 det(A) = 1 · det ⎝⎣1 2 0⎦⎠ = 1 · 1 · det\\n= 1 · 1 · (−1) = −1.\\n12\\n121\\nThus, A is invertible and Ax = b has a unique solution ' x = A−1 b ∈ Q4,1 , which by\\nCramer’s rule has the following entries:\\n⎛⎡\\n\\n' x1\\n\\n' x2\\n\\n' x3\\n\\n' x4\\n\\n⎤⎞\\n1300\\n⎜⎢2 2 0 0⎥⎟\\n⎢\\n⎥⎟\\n= det ⎜\\n⎝⎣1 2 1 0⎦⎠ / det(A) = −4/(−1) = 4,\\n0231\\n⎛⎡\\n⎤⎞\\n1100\\n⎜⎢1 2 0 0⎥⎟\\n⎢\\n⎥⎟\\n= det ⎜\\n⎝⎣1 1 1 0⎦⎠ / det(A) = 1/(−1) = −1,\\n1031\\n⎛⎡\\n⎤⎞\\n1310\\n⎜⎢1 2 2 0⎥⎟\\n⎢\\n⎥⎟\\n= det ⎜\\n⎝⎣1 2 1 0⎦⎠ / det(A) = 1/(−1) = −1,\\n1201\\n⎛⎡\\n⎤⎞\\n1301\\n⎜⎢1 2 0 2⎥⎟\\n⎢\\n⎥⎟\\n= det ⎜\\n⎝⎣1 2 1 1⎦⎠ / det(A) = −1/(−1) = 1.\\n1230   \n",
       "90                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               7.3 Minors and the Laplace Expansion\\n\\n97\\n\\nExercises\\n7.1 A permutation σ ∈ Sn is called an r -cycle if there exists a subset {i 1 , . . . , ir } ⊆\\n{1, 2, . . . , n} with r ≥ 1 elements and\\nσ(i k ) = i k+1 for k = 1, 2, . . . , r − 1, σ(ir ) = i 1 , σ(i) = i for i ∈\\n/ {i 1 , . . . , ir }.\\n\\nWe write an r -cycle as σ = (i 1 , i 2 , . . . , ir ).\\nIn particular, a transposition τ ∈ Sn is a 2-cycle.\\n(a) Let n = 4 and the 2-cycles τ1,2 = (1, 2), τ2,3 = (2, 3) and τ3,4 = (3, 4) be\\n−1\\n, and τ1,2 ◦ τ2,3 ◦ τ3,4 .\\ngiven.\\nCompute τ1,2 ◦ τ2,3 , τ1,2 ◦ τ2,3 ◦ τ1,2\\n(b) Let n ≥ 4 and σ = (1, 2, 3, 4).\\nDetermine σ j for j = 2, 3, 4, 5.\\n(c) Show that the inverse of the cycle (i 1 , . . . , ir ) is given by (ir , . . . , i 1 ).\\n(d) Show that two cycles with disjoint elements, i.e. (i 1 , . . . , ir ) and ( j1 , . . . , js ) with {i 1 , . . . , ir } ∩ { j1 , . . . , js } = Ø, commute.\\n(e) Show that every permutation σ ∈ Sn can be written as product of disjoint cycles that are, except for the order, uniquely determined by σ.\\n7.2 Prove Lemma 7.10 (1) using (7.1).\\n7.3 Show that the group homomorphism sgn : (Sn , ◦) → ({1, −1}, ·) satisfies the following assertions:\\n(a) The set An = {σ ∈ Sn | sgn(σ) = 1} is a subgroup of Sn (cp.\\nExercise 3.8).\\n(b) For all σ ∈ An and π ∈ Sn we have π ◦ σ ◦ π −1 ∈ An .\\n7.4 Compute the determinants of the following matrices:\\n(a) A = [en , en−1 , . . . , e1 ] ∈ Zn,n , where ei is the ith column of the identity matrix.\\n\u0002 \u0003\\n(b) B = bi j ∈ Zn,n with\\n⎧\\n⎪ for |i − j| = 0,\\n⎨2 bi j = −1 for |i − j| = 1,\\n⎪\\n⎩\\n0 for |i − j| ≥ 2.\\n(c)\\n\\n⎡\\n\\n1\\n⎢e\\n⎢ 2\\n⎢e\\n⎢ 3\\nC =⎢\\n⎢e 4\\n⎢e\\n⎢\\n⎣e 6\\n0\\n\\n0 1\\n0 eπ\\n1 17\\n31\\n0 −e\\n0 10001\\n√\\n2\\n0\\n0 1\\n\\n0 0\\n√4 √5\\n6 7\\nπ e\\n0 π −1\\n0 0\\n0 0\\n\\n⎤\\n0 √0\\n⎥\\n√1 √ π ⎥\\n8 10⎥\\n⎥\\n7,7\\n0 πe ⎥\\n⎥∈R .\\n2 ⎥\\n0 e π⎥\\n0 −1 ⎦\\n0 0   \n",
       "91                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         98\\n\\n7 Determinants of Matrices\\n\\n(d) The 4 × 4 Wilkinson matrix 6 (cp. the MATLAB-Minute at the end of\\nSect.\\n7.2).\\n7.5 Construct matrices A, B ∈ Rn,n for some n ≥ 2 and with det(A + B) = det(A) + det(B).\\n7.6 Let R be a commutative ring with unit, n ≥ 2 and A ∈ R n,n .\\nShow that the following assertions hold:\\n(a)\\n(b)\\n(c)\\n(d)\\n(e)\\n(f)\\n(g) adj(In ) = In .\\nadj(AB) = adj(B)adj(A), if A and B ∈ R n,n are invertible.\\nadj(λA) = λn−1 adj(A) for all λ ∈ R.\\nadj(A T ) = adj(A)T .\\ndet(adj(A)) = (det(A))n−1 , if A is invertible.\\nadj(adj(A)) = det(A)n−2 A.\\nadj(A−1 ) = adj(A)−1 , if A is invertible.\\n\\nCan one drop the requirement of invertibility in (b) or (e)?\\n1\\n7.7 Let n ≥ 2 and A = [ai j ] ∈ Rn,n with ai j = xi +y for some x1 , . . . , xn , j y1 , . . . , yn ∈ R. Hence, in particular, xi + y j = 0 for all i, j.\\n(Such a matrix A is called a Cauchy matrix.7 )\\n(a) Show that\\n\\n\u000e det(A) =\\n\\n1≤i< j≤n (x j − x i )(y j −\\n\u000en i, j=1 (x i + y j ) yi )\\n\\n.\\n\\n(b) Use (a) to derive a formula for the determinant of the n × n Hilbert matrix\\n(cp. the MATLAB-Minute above Definition 5.6).\\n7.8 Let R be a commutative ring with unit.\\nIf α1 , . . . , αn ∈ R, n ≥ 2, then\\n⎤\\nα1 · · · α1n−1\\nα2 · · · α2n−1 ⎥\\n⎥ n,n\\n.. ⎥ ∈ R\\n..\\n. ⎦\\n.\\n1 αn · · · αnn−1\\n\\n⎡\\n1\\n\" ⎢1\\n!\\n⎢\\nVn := αij−1 = ⎢ .\\n⎣ ..\\nis called a Vandermonde matrix.8\\n(a) Show that\\n\\n\u0005 det(Vn ) =\\n\\n1≤i< j≤n\\n\\n6 James\\n\\nHardy Wilkinson (1919–1986).\\nLouis Cauchy (1789–1857).\\n8 Alexandre-Théophile Vandermonde (1735–1796).\\n7 Augustin\\n\\n(α j − αi ).   \n",
       "92                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             7.3 Minors and the Laplace Expansion\\n\\n99\\n\\n(b) Let K be a field and let K [t]≤n−1 be the set of polynomials in the variable t of degree at most n − 1.\\nShow that two polynomials p, q ∈ K [t]≤n−1 are equal if there exist pairwise distinct β1 , . . . , βn ∈ K with p(β j ) = q(β j ).\\n7.9 Show the following assertions:\\n(a) Let K be a field with 1 + 1 = 0 and let A ∈ K n,n with A T = −A. If n is odd, then det(A) = 0.\\n(b) If A ∈ G L n (R) with A T = A−1 , then det(A) ∈ {1, −1}.\\n7.10 Let K be a field and\\n\\n\u0007\\n\\nA11 A12\\nA=\\nA21 A22\\n\\n\b for some A11 ∈ K n 1 ,n 1 , A12 ∈ K n 1 ,n 2 , A21 ∈ K n 2 ,n 1 , A22 ∈ K n 2 ,n 2 .\\nShow the following assertions:\\n\u001f\\n(a) If A11 ∈ G L n 1 (K ), then det(A) = det(A11 ) det A22 − A21 A−1\\n11 A12 .\\n\u001f\\n(b) If A22 ∈ G L n 2 (K ), then det(A) = det(A22 ) det A11 − A12 A−1\\n22 A21 .\\n(c) If A21 = 0, then det(A) = det(A11 ) det(A22 ).\\nCan you show this also when the matrices are defined over a commutative ring with unit?\\n7.11 Construct matrices A11 , A12 , A21 , A22 ∈ Rn,n for n ≥ 2 with\\n\b\\n\u0006\u0007\\nA11 A12\\n= det(A11 ) det(A22 ) − det(A12 ) det(A21 ).\\ndet\\nA21 A22\\n7.12 Let A = [ai j ] ∈ G L n (R) with ai j ∈ Z for i, j = 1, . . . , n.\\nShow that the following assertions hold:\\n(a) A−1 ∈ Qn,n .\\n(b) A−1 ∈ Zn,n if and only if det(A) ∈ {−1, 1}.\\n(c) The linear system of equations Ax = b has a unique solution ' x ∈ Zn,1 for every b ∈ Zn,1 if and only if det(A) ∈ {−1, 1}.\\n7.13 Show that G = {A ∈ Zn,n | det(A) ∈ {−1, 1} } is a subgroup of G L n (Q).   \n",
       "93                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Chapter 8\\n\\nThe Characteristic Polynomial and Eigenvalues of Matrices\\n\\nWe have already characterized matrices using their rank and their determinant.\\nIn this chapter we use the determinant map in order to assign to every square matrix a unique polynomial that is called the characteristic polynomial of the matrix.\\nThis polynomial contains important information about the matrix.\\nFor example, one can read off the determinant and thus see whether the matrix is invertible.\\nEven more important are the roots of the characteristic polynomial, which are called the eigenvalues of the matrix.\\n\\n8.1 The Characteristic Polynomial and the Cayley-Hamilton Theorem\\nLet R be a commutative ring with unit and let R[t] be the corresponding ring of polynomials (cp.\\nExample 3.17).\\nFor A = [ai j ] ∈ R n,n we set\\n⎡\\n⎤ t − a11 −a12\\n···\\n−a1n\\n⎢\\n.. ⎥\\n⎢ −a21 t − a22 . . .\\n. ⎥\\n⎥ ∈ (R[t])n,n .\\nt In − A := ⎢\\n⎢ ..\\n⎥\\n.\\n.\\n..\\n. . −a\\n⎣ .\\n⎦ n−1,n\\n−an1\\n· · · −an,n−1 t − ann\\nThe entries of the matrix t In − A are elements of the commutative ring with unit\\nR[t], where the diagonal entries are polynomials of degree 1, and the other entries are constant polynomials.\\nUsing Definition 7.4 we can form the determinant of the matrix t In − A, which is an element of R[t].\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_8\\n\\n101   \n",
       "94                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     102\\n\\n8 The Characteristic Polynomial and Eigenvalues of Matrices\\n\\nDefinition 8.1 Let R be a commutative ring with unit and A ∈ R n,n .\\nThen\\nPA := det(t In − A) ∈ R[t] is called the characteristic polynomial of A.\\nExample 8.2 If n = 1 and A = [a11 ], then\\nPA = det(t I1 − A) = det([t − a11 ]) = t − a11 .\\nFor n = 2 and\\nA=\\n\\n\b a11 a12 a21 a22 we obtain\\n\b t − a11 −a12\\n−a21 t − a22\\n\\nPA = det\\n\\n= t 2 − (a11 + a22 )t + (a11 a22 − a12 a21 ).\\n\\nUsing Definition 7.4 we see that the general form of PA for a matrix A ∈ R n,n is given by n\\n\u000e\\n\u000f sgn(σ)\\nδi,σ(i) t − ai,σ(i) .\\n(8.1)\\nPA =\\nσ∈Sn i=1\\n\\nThe following lemma presents basic properties of the characteristic polynomial.\\nLemma 8.3 For A ∈ R n,n we have PA = PAT and\\nPA = t n − αn−1 t n−1 + . . . + (−1)n−1 α1 t + (−1)n α0 with αn−1 =\\n\\n\u0010n i=1 aii and α0 = det(A).\\n\\nProof Using (5) in Lemma 7.10 we obtain\\nPA = det(t In − A) = det((t In − A)T ) = det(t In − A T ) = PAT .\\nUsing PA as in (8.1) we see that n\\n\\nPA = n\\n\\n(t − aii ) + i=1 sgn(σ)\\nσ∈Sn\\nσ\u0003 =[1 ··· n] i=1\\n\\n\u000e\\n\\n\u000f\\nδi,σ(i) t − ai,σ(i) .   \n",
       "95                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            8.1 The Characteristic Polynomial and the Cayley-Hamilton Theorem\\n\\n103\\n\\nThe first term on the right hand side is of the form\\n\u0011 n t −\\n\\n\u0012 aii t n−1 + (polynomial of degree ≤ n − 2), n i=1 and the second term is a polynomial of degree ≤ n − 2.\\nThus, αn−1 = claimed.\\nMoreover, Definition 8.1 yields\\n\\n\u0010n i=1 aii as\\n\\nPA (0) = det(−A) = (−1)n det(A), so that α0 = det(A).\\n\\n\u0006\\n\u0005\\n\\nThis lemma shows that the characteristic polynomial of A ∈ R n,n always is of degree n.\\nThe coefficient of t n is 1 ∈ R. Such a polynomial is called monic.\\nThe coefficient of t n−1 is given by the sum of the diagonal entries of A. This quantity is called the trace of A, i.e., n trace(A) := aii .\\ni=1\\n\\nThe following lemma shows that for every monic polynomial p ∈ R[t] of degree n ≥ 1 there exists a matrix A ∈ R n,n with PA = p.\\nLemma 8.4 If n ∈ N and p = t n + βn−1 t n−1 + . . . + β0 ∈ R[t], then p is the characteristic polynomial of the matrix\\n⎡\\n⎤\\n0\\n−β0\\n⎢ ..\\n.. ⎥\\n⎢1 .\\n. ⎥\\n⎥ ∈ R n,n .\\nA=⎢\\n⎢ .\\n⎥\\n.\\n⎣\\n.\\n0 −βn−2 ⎦\\n1 −βn−1\\n(For n = 1 we have A = [−β0 ].)\\nThe matrix A is called the companion matrix of p.\\nProof We prove the assertion by induction on n.\\nFor n = 1 we have p = t + β0 , A = [−β0 ] and PA = det([t + β0 ]) = p.\\nLet the assertion hold for some n ≥ 1.\\nWe consider p = t n+1 + βn t n + . . . + β0 and\\n⎡\\n⎤\\n0\\n−β0\\n⎢ ..\\n.. ⎥\\n⎢1 .\\n. ⎥\\n⎢\\n⎥ ∈ R n+1,n+1 .\\nA=⎢\\n⎥\\n.\\n⎣ . .\\n0 −βn−1 ⎦\\n1 −βn   \n",
       "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   104\\n\\n8 The Characteristic Polynomial and Eigenvalues of Matrices\\n\\nUsing the Laplace expansion with respect to the first row (cp.\\nCorollary 7.22) and the induction hypothesis we get\\nPA = det(t In+1 − A)\\n⎛⎡\\n⎤⎞ t\\nβ0\\n⎜⎢\\n.. ⎥⎟\\n⎟\\n⎜⎢−1 . . .\\n. ⎥\\n⎢\\n⎥⎟\\n= det ⎜\\n⎟\\n⎜⎢\\n⎥\\n..\\n⎝⎣\\n. t βn−1 ⎦⎠\\n−1 t + βn\\n⎛⎡\\n⎤⎞\\n⎛⎡\\n⎤⎞ t\\nβ1\\n−1 t\\n⎜⎢\\n⎥⎟\\n⎜⎢\\n.. ⎥⎟\\n.. ..\\n⎜⎢−1 . . .\\n⎥⎟\\n⎟\\n⎜⎢\\n. .\\n. ⎥ n+2\\n⎢\\n⎜\\n⎥⎟\\n⎢\\n⎟\\n⎜\\n⎥\\n= t · det ⎜⎢\\n+ (−1)\\n· β0 · det ⎜⎢\\n⎥⎟\\n⎟\\n⎥\\n..\\n..\\n⎝⎣\\n⎝⎣\\n. t ⎦⎠\\n. t βn−1 ⎦⎠\\n−1\\n−1 t + βn\\n= t · (t n + βn t n−1 + . . . + β1 ) + (−1)2n+2 β0\\n= t n+1 + βn t n + . . . + β1 t + β0\\n= p.\\n\\n\u0006\\n\u0005\\n\\nExample 8.5 The polynomial p = (t − 1)3 = t 3 − 3t 2 + 3t − 1 ∈ Z[t] has the companion matrix\\n⎡\\n⎤\\n00 1\\nA = ⎣ 1 0 −3 ⎦ ∈ Z3,3 .\\n01 3\\nThe identity matrix I3 has the characteristic polynomial\\nPI3 = det(t I3 − I3 ) = (t − 1)3 = PA .\\nThus, different matrices may have the same characteristic polynomial.\\nIn Example 3.17 we have seen how to evaluate a polynomial p ∈ R[t] at a scalar\\nλ ∈ R. Analogously, we can evaluate p at a matrix M ∈ R m,m (cp.\\nExercise 4.8).\\nFor p = βn t n + βn−1 t n−1 + . . . + β0 ∈ R[t] we define p(M) := βn M n + βn−1 M n−1 + . . . + β0 Im ∈ R m,m , where the multiplication on the right hand side is the scalar multiplication of β j ∈ R and M j ∈ R m,m , j = 0, 1, . . . , n.\\n(Recall that M 0 = Im .)\\nEvaluating a given polynomial at matrices M ∈ R m,m therefore defines a map from R m,m to R m,m .   \n",
       "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              8.1 The Characteristic Polynomial and the Cayley-Hamilton Theorem\\n\\n105\\n\\nIn particular, using (8.1), the characteristic polynomial PA of A ∈ R n,n satisfies n\\n\\nPA (M) = sgn(σ)\\nσ∈Sn\\n\\n\u000e\\n\u000f\\nδi,σ(i) M − ai,σ(i) Im for all M ∈ R m,m .\\ni=1\\n\\nNote that for M ∈ R n,n and PA = det(t In − A) the “obvious” equation PA (M) = det(M − A) is wrong.\\nBy definition, PA (M) ∈ R n,n and det(M − A) ∈ R, so that the two expressions cannot be the same, even for n = 1.\\nThe following result is called the Cayley-Hamilton theorem.1\\nTheorem 8.6 For every matrix A ∈ R n,n and its characteristic polynomial PA ∈\\nR[t] we have PA (A) = 0 ∈ R n,n .\\nProof For n = 1 we have A = [a11 ] and PA = t − a11 , so that PA (A) = [a11 ] −\\n[a11 ] = [0].\\nLet now n ≥ 2 and let ei be the ith column of the identity matrix In ∈ R n,n .\\nThen\\nAei = a1i e1 + a2i e2 + . . . + ani en , i = 1, . . . , n, which is equivalent to n\\n\\n(A − aii In )ei +\\n\\n(−a ji In )e j = 0, i = 1, . . . , n.\\nj=1 j\u0003 =i\\n\\nThe last n equations can be written as\\n⎡\\n\\n⎤⎡ ⎤ ⎡ ⎤\\n0 e1\\n⎥ ⎢e2 ⎥ ⎢0⎥\\n⎥⎢ ⎥ ⎢ ⎥\\n0.\\n⎥ ⎢ .. ⎥ = ⎢ .. ⎥ , or Bε = \u0019\\n⎦ ⎣ . ⎦ ⎣.⎦\\n0\\n· · · A − ann In en\\n\\nA − a11 In −a21 In · · ·\\n⎢ −a12 In A − a22 In · · ·\\n⎢\\n⎢\\n..\\n..\\n⎣\\n.\\n.\\n−a1n In\\n\\n−a2n In\\n\\n−an1 In\\n−an2 In\\n..\\n.\\n\\nHence B ∈ (R[A])n,n with R[A] := { p(A) | p ∈ R[t]} ⊂ R n,n .\\nThe set R[A] forms a commutative ring with unit given by the identity matrix In (cp.\\nExercise 4.8).\\nUsing\\nTheorem 7.18 we obtain adj(B)B = det(B)\u0019\\nIn ,\\n\\n= 2 and claimed that he had verified it for n = 3.\\nHe did not feel it necessary to give a proof for general n.\\nSir William Rowan Hamilton\\n(1805–1865) proved the theorem for the case n = 4 in 1853 in the context of his investigations of quaternions.\\nOne of the first proofs for general n was given by Ferdinand Georg Frobenius (1849–\\n1917) in 1878.\\nJames Joseph Sylvester (1814–1897) coined the name of the theorem in 1884 by calling it the “no-little-marvelous Hamilton-Cayley theorem”.\\n\\n1 Arthur Cayley (1821–1895) showed this theorem in 1858 for n   \n",
       "98                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                106\\n\\n8 The Characteristic Polynomial and Eigenvalues of Matrices where det(B) ∈ R[A] and \u0019\\nIn is the identity matrix in (R[A])n,n .\\n(This matrix has n times the identity matrix In on its diagonal.)\\nMultiplying this equation from the right by ε yields adj(B)Bε = det(B)\u0019\\nIn ε, which implies that det(B) = 0 ∈ R n,n .\\nFinally, using Lemma 8.3 gives n\\n\\n0 = det(B) =\\n\\n(δi,σ(i) A − aσ(i),i In ) sgn(σ)\\nσ∈Sn i=1 n\\n\\n=\\n\\n(δσ(i),i A − aσ(i),i In ) sgn(σ)\\nσ∈Sn i=1\\n\\n= PAT (A)\\n= PA (A),\\n\u0006\\n\u0005 which completes the proof.\\n\\n8.2 Eigenvalues and Eigenvectors\\nIn this section we present an introduction to the topic of eigenvalues and eigenvectors of square matrices over a field K .\\nThese concepts will be studied in more detail in later chapters.\\nDefinition 8.7 Let A ∈ K n,n .\\nIf λ ∈ K and v ∈ K n,1 \\ {0} satisfy Av = λv, then λ is called an eigenvalue of A and v is called an eigenvector of A corresponding to λ.\\nWhile by definition v = 0 can never be an eigenvector of a matrix, λ = 0 may be an eigenvalue.\\nFor example,\\n\b\\n\\n1 −1\\n−1 1\\n\\n\b\\n\b\\n1\\n1\\n= 0\\n.\\n1\\n1\\n\\nIf v is an eigenvector corresponding to the eigenvalue λ of A and α ∈ K \\ {0}, then\\nαv \u0003= 0 and\\nA (αv) = α (Av) = α (λv) = λ (αv).\\nThus, also αv is an eigenvector of A corresponding to λ.   \n",
       "99                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    8.2 Eigenvalues and Eigenvectors\\n\\n107\\n\\nTheorem 8.8 For A ∈ K n,n the following assertions hold:\\n(1) λ is an eigenvalue of A if and only if λ is a root of the characteristic polynomial of A, i.e., PA (λ) = 0 ∈ K .\\n(2) λ = 0 is an eigenvalue of A if and only if det(A) = 0.\\n(3) λ is an eigenvalue of A if and only if λ is an eigenvalue of A T .\\nProof\\n(1) The equation PA (λ) = det(λIn − A) = 0 holds if and only if the matrix λIn − A is not invertible (cp.\\n(7.4)), and this is equivalent to L (λIn − A, 0) \u0003= {0}.\\nx = 0, or\\nThis, however, means that there exists a vector \u0019 x \u0003= 0 with (λIn − A)\u0019\\nA\u0019 x = λ\u0019 x.\\n(2) By (1), λ = 0 is an eigenvalue of A if and only if PA (0) = 0.\\nThe assertion now follows from PA (0) = (−1)n det(A) (cp.\\nLemma 8.3).\\n\u0006\\n\u0005\\n(3) This follows from (1) and PA = PAT (cp.\\nLemma 8.3).\\nWhether a matrix A ∈ K n,n has eigenvalues or not may depend on the field K over which A is considered.\\nExample 8.9 The matrix\\n\b\\nA=\\n\\n01\\n∈ R2,2\\n−1 0 has the characteristic polynomial PA = t 2 + 1 ∈ R[t].\\nThis polynomial does not have roots, since the equation t 2 + 1 = 0 has no (real) solutions.\\nIf we consider A as an element of C2,2 , then PA ∈ C[t] has the roots i and −i.\\nThen these two complex numbers are the eigenvalues of A.\\nItem (3) in Theorem 8.8 shows that A and A T have the same eigenvalues.\\nAn eigenvector of A, however, may not be an eigenvector of A T .\\nExample 8.10 The matrix\\n\b\\nA=\\n\\n33\\n∈ R2,2\\n11 has the characteristic polynomial PA = t 2 −4t = t ·(t −4), and hence its eigenvalues are 0 and 4.\\nWe have\\n\b\\n\b\\n\b\\n\b\\n\b\\n1\\n2\\n1\\n1\\n1\\n\u0003= λ\\n= and A T\\n=0\\nA\\n−1\\n2\\n−1\\n−1\\n−1   \n",
       "100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              108\\n\\n8 The Characteristic Polynomial and Eigenvalues of Matrices for all λ ∈ R. Thus, [1, −1]T is an eigenvector of A corresponding to the eigenvalue 0, but it is not an eigenvector of A T .\\nOn the other hand,\\n\b\\nA\\n\\nT\\n\\n1\\n=0\\n−3\\n\\n\b\\n\\n1\\n−3\\n\\n\b\\n\\n\b\\n\b\\n1\\n−6\\n1 and A\\n=\\n\u0003= λ\\n−3\\n−2\\n−3 for all λ ∈ R. Thus, [1, −3]T is an eigenvector of A T corresponding to the eigenvalue 0, but it is not an eigenvector of A.\\nTheorem 8.8 implies further criteria for the invertibility of A ∈ K n,n (cp.\\n(7.3)):\\nA ∈ G L n (K ) ⇔ 0 is not an eigenvalue of A\\n⇔ 0 is not a root of PA .\\nDefinition 8.11 Two matrices A, B ∈ K n,n are called similar, if there exists a matrix\\nZ ∈ G L n (K ) with A = Z B Z −1 .\\nOne can easily show that this defines an equivalence relation on the set K n,n (cp.\\nthe proof following Definition 5.13).\\nTheorem 8.12 If two matrices A, B ∈ K n,n are similar, then PA = PB .\\nProof If A = Z B Z −1 , then the multiplication theorem for determinants yields\\nPA = det(t In − A) = det(t In − Z B Z −1 ) = det(Z (t In − B)Z −1 )\\n= det(Z ) det(t In − B) det(Z −1 ) = det(t In − B) det(Z Z −1 )\\n= PB\\n\u0006\\n\u0005\\n\\n(cp. the remarks below Theorem 7.15).\\n\\nTheorem 8.12 and (1) in Theorem 8.8 show that two similar matrices have the same eigenvalues.\\nThe condition that A and B are similar is sufficient, but not necessary for PA = PB .\\nExample 8.13 Let\\n\b\\n\\n11\\nA=\\n,\\n01\\n\\n\b\\nB=\\n\\n10\\n= I2 .\\n01\\n\\nThen PA = (t − 1)2 = PB , but for every matrix Z ∈ G L n (K ) we have Z B Z −1 =\\nI2 \u0003= A. Thus, we have PA = PB although A and B are not similar (cp. also\\nExample 8.5).   \n",
       "101                                                                                                                                                                                                                                                                                                                                                                                                                                     8.2 Eigenvalues and Eigenvectors\\n\\n109\\n\\nMATLAB-Minute.\\nThe roots of a polynomial p = αn t n + αn−1 t n−1 + . . . + α0 can be computed\\n(or approximated) in MATLAB using the command roots(p), where p is a\\n1×(n +1) matrix with the entries p(i)= αn+1−i for i = 1, . . . , n +1.\\nCompute roots(p) for the monic polynomial p = t 3 − 3t 2 + 3t − 1 ∈ R[t] and display the output using format long.\\nWhat are the exact roots of p and how large is the numerical error in the computation of the roots using roots(p)?\\nForm the matrix A=compan(p) and compare its structure with the one of the companion matrix from Lemma 8.4.\\nCan you transfer the proof of Lemma 8.4 to the structure of the matrix A?\\nCompute the eigenvalues of A with the command eig(A) and compare the output with the one of roots(p).\\nWhat do you observe?\\n\\n8.3 Eigenvectors of Stochastic Matrices\\nWe now consider the eigenvalue problem presented in Sect.\\n1.1 in the context of the PageRank algorithm.\\nThe mathematical modeling leads to the equations (1.1), which can be written in the form Ax = x.\\nHere A = [ai j ] ∈ Rn,n (n is the number of documents) satisfies n ai j ≥ 0 and ai j = 1 for j = 1, . . . , n.\\ni=1\\n\\nSuch a matrix A is called column-stochastic.\\nNote that A is column-stochastic if and only if A T is row-stochastic.\\nSuch matrices also occurred in the car insurance application considered in Sect.\\n1.2 and Example 4.7.\\nWe want to determine x =\\n[x1 , . . . , xn ]T ∈ Rn,1 \\ {0} with Ax = x, where the entry xi describes the importance of document i.\\nThe importance values should be nonnegative, i.e., xi ≥ 0 for i =\\n1, . . . , n.\\nThus, we want to determine an entrywise nonnegative eigenvector of A corresponding to the eigenvalue λ = 1.\\nWe first check whether this problem has a solution, and then study whether the solution is unique.\\nOur presentation is based on the article [BryL06].\\nLemma 8.14 A column-stochastic matrix A ∈ Rn,n has an eigenvector corresponding to the eigenvalue 1.\\nProof Since A is column-stochastic, we have A T [1, . . . , 1]T = [1, . . . , 1]T , so that 1 is an eigenvalue of A T .\\nNow (3) in Theorem 8.8 shows that also A has the eigenvalue\\n1, and hence there exists a corresponding eigenvector.\\n\u0006\\n\u0005   \n",
       "102                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            110\\n\\n8 The Characteristic Polynomial and Eigenvalues of Matrices\\n\\nA matrix with real entries is called positive, if all its entries are positive.\\nLemma 8.15 If A ∈ Rn,n is positive and column-stochastic and if x ∈ Rn,1 is an eigenvector of A corresponding to the eigenvalue 1, then either x or −x is positive.\\nProof If x = [x1 , . . . , xn ]T is an eigenvector of A = [ai j ] corresponding to the eigenvalue 1, then n xi = ai j x j , i = 1, . . . , n.\\nj=1\\n\\nSuppose that not all entries of x are positive or not all entries of x are negative.\\nThen there exists at least one index k with\\n\u001a\\n|xk |= \u001a n\\n\\n\u001a ak j x j \u001a < j=1 n ak j |x j |, j=1 which implies n n n\\n\\n|xi | < i=1 n n ai j |x j | = i=1 j=1 n j=1 i=1 n n\\n\\n|x j | · ai j |x j | = j=1 ai j i=1\\n\\n\u001b \n",
       "\n",
       " \n",
       "\\n\\n=\\n\\n|x j |.\\nj=1\\n\\n=1\\n\\nThis is impossible, so that indeed x or −x must be positive.\\n\\n\u0006\\n\u0005\\n\\nWe can now prove the following uniqueness result.\\nTheorem 8.16 If A ∈ Rn,n is positive and column-stochastic, then there exists a\\n\u0010n xi = 1 and Ax = x.\\nunique positive x = [x1 , . . . , xn ]T ∈ Rn,1 with i=1\\nProof By Lemma 8.15, A has a least one positive eigenvector corresponding to the\\n\u001f\\n\u001f\\nT\\nT eigenvalue 1.\\nSuppose that x (1) = x1(1) , . . . , xn(1) and x (2) = x1(2) , . . . , xn(2)\\n\u0010n\\n( j) are two such eigenvectors.\\nSuppose that these are normalized by i=1 xi = 1, j = 1, 2.\\nThis assumption can be made without loss of generality, since every nonzero multiple of an eigenvector is still an eigenvector.\\nWe will show that x (1) = x (2) .\\nFor α ∈ R we define x(α) := x (1) + αx (2) ∈ Rn,1 , then\\nAx(α) = Ax (1) + α Ax (2) = x (1) + αx (2) = x(α).\\nα) is equal to zero and thus, by\\nIf !\\nα := −x1(1) /x1(2) , then the first entry of x(!\\nLemma 8.15, x(!\\nα) cannot be an eigenvector of A corresponding to the eigenvalue 1.\\nNow Ax(!\\nα) = x(!\\nα) implies that x(!\\nα) = 0, and hence\\nα xi(2) = 0, i = 1, . . . , n.\\nxi(1) + !\\n\\n(8.2)   \n",
       "103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             8.3 Eigenvectors of Stochastic Matrices\\n\\n111\\n\\nSumming up these n equations yields n xi(1) + !\\nα i=1\\n\\n\u001b \n",
       "\n",
       " \n",
       "\\n=1 n xi(2) = 0, i=1\\n\\n\u001b \n",
       "\n",
       " \n",
       "\\n=1 so that !\\nα = −1.\\nFrom (8.2) we get xi(1) = xi(2) for i = 1, . . . , n, and therefore\\n(1)\\n\u0006\\n\u0005 x = x (2) .\\nThe unique positive eigenvector x in Theorem 8.16 is called the Perron eigenvector 2 of the positive matrix A. The theory of eigenvalues and eigenvectors of positive\\n(or more general nonnegative) matrices is an important area of Matrix Theory, since these matrices arise in many applications.\\nBy construction, the matrix A ∈ Rn,n in the PageRank algorithm is columnstochastic but not positive, since there are (usually many) entries ai j = 0.\\nIn order to obtain a uniquely solvable problem one can use the following trick:\\nLet S = [si j ] ∈ Rn,n with si j = 1/n.\\nObviously, S is positive and columnstochastic.\\nFor a real number α ∈ (0, 1] we define the matrix\\n\u0019\\nA(α)\\n:= (1 − α)A + αS.\\nThis matrix is positive and column-stochastic, and hence it has a unique positive eigenvector \u0019 u corresponding to the eigenvalue 1.\\nWe thus have\\nα\\n\u0019 u = (1 − α)A\u0019\\n\u0019 u = A(α)\u0019 u + αS\u0019 u = (1 − α)A\u0019 u + [1, . . . , 1]T.\\nn\\nFor a very large number of documents (e.g. the entire internet) the number α/n is very small, so that (1 − α)A\u0019 u ≈\u0019 u .\\nTherefore a solution of the eigenvalue problem\\n\u0019 u =\u0019\\nA(α)\u0019 u for small α potentially gives a good approximation of a u ∈ Rn,1 that satisfies Au = u.\\nThe practical solution of the eigenvalue problem with the matrix\\n\u0019\\nA(α) is a topic of the field of Numerical Linear Algebra.\\nThe matrix S represents a link structure where all document are mutually linked\\n\u0019 and thus all documents are equally important.\\nThe matrix A(α)\\n= (1 − α)A + αS therefore models the following internet “surfing behavior”: A user follows a proposed link with the probability 1−α and an arbitrary link with the probability α.\\nOriginally,\\nGoogle Inc. used the value α = 0.15.\\n\\n2 Oskar\\n\\nPerron (1880–1975).   \n",
       "104                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          112\\n\\n8 The Characteristic Polynomial and Eigenvalues of Matrices\\n\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n8.1 Determine the characteristic polynomials of the following matrices over Q:\\n\b\\nA=\\n\\n20\\n,\\n02\\n\\n\b\\nB=\\n\\n⎡\\n\\n\b\\n\\n44\\n21\\n, C=\\n,\\n−1 0\\n02\\n\\n⎤\\n2 0 −1\\nD = ⎣ 0 2 0⎦.\\n−4 0 2\\n\\nVerify the Cayley-Hamilton theorem in each case by direct computation.\\nAre two of the matrices A, B, C similar?\\n8.2 Let R be a commutative ring with unit and n ≥ 2.\\n(a) Show that for every A ∈ G L n (R) there exists a polynomial p ∈ R[t] of degree at most n − 1 with adj(A) = p(A).\\nConclude that A−1 = q(A) holds for a polynomial q ∈ R[t] of degree at most n − 1.\\n(b) Let A ∈ R n,n .\\nApply Theorem 7.18 to the matrix t In − A ∈ (R[t])n,n and derive an alternative proof of the Cayley-Hamilton theorem from the formula det(t In − A) In = (t In − A) adj(t In − A).\\n8.3 Let A ∈ K n,n be a matrix with Ak = 0 for some k ∈ N. (Such a matrix is called nilpotent.)\\n(a) Show that λ = 0 is the only eigenvalue of A.\\n(b) Determine PA and show that An = 0.\\nn\\n\"\\n(Hint: You may assume that PA has the form (t −λi ) for some λ1 , . . . , λn i=1\\n\\n∈ K .)\\n(c) Show that μIn − A is invertible if and only if μ ∈ K \\ {0}.\\n(d) Show that (In − A)−1 = In + A + A2 + . . . + An−1 .\\n8.4 Determine the eigenvalues and corresponding eigenvectors of the following matrices over R:\\n⎡\\n⎤\\n⎡\\n⎡\\n⎤\\n⎤\\n0 −1 0 0\\n111\\n3 8 16\\n⎢1 0 0 0⎥\\n⎥\\nA = ⎣0 1 1⎦ , B = ⎣ 0 7 8 ⎦ , C = ⎢\\n⎣ 0 0 −2 1 ⎦ .\\n001\\n0 −4 −5\\n0 0 0 −2\\nIs there any difference when you consider A, B, C as matrices over C?\\n8.5 Let n ≥ 3 and ε ∈ R. Consider the matrix\\n⎤\\n1 1\\n⎢ .. .. ⎥\\n⎢\\n. . ⎥\\n⎥\\nA(ε) = ⎢\\n⎢\\n.. ⎥\\n⎣\\n.\\n1⎦\\nε\\n1\\n⎡   \n",
       "105                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             8.3 Eigenvectors of Stochastic Matrices\\n\\n113 as an element of Cn,n and determine all eigenvalues in dependence of ε.\\nHow many pairwise distinct eigenvalues does A(ε) have?\\n8.6 Determine the eigenvalues and corresponding eigenvectors of\\n⎡\\n\\n⎤\\n2 2−a\\n2−a\\n2 − a ⎦ ∈ R3,3 ,\\nA = ⎣0 4 − a\\n0 −4 + 2a −2 + 2a\\n\\n⎡\\n\\n⎤\\n110\\nB = ⎣1 0 1⎦ ∈ (Z/2Z)3,3 .\\n011\\n\\n(For simplicity, the elements of Z/2Z are here denoted by k instead of [k].)\\n8.7 Let A ∈ K n,n , B ∈ K m,m , n ≥ m, and C ∈ K n,m with rank(C) = m and\\nAC = C B. Show that then every eigenvalue of B is an eigenvalue of A.\\n8.8 Show the following assertions:\\n(a) trace(λ A + μB) = λ trace(A) + μ trace(B) holds for all λ, μ ∈ K and\\nA, B ∈ K n,n .\\n(b) trace(AB) = trace(B A) holds for all A, B ∈ K n,n .\\n(c) If A, B ∈ K n,n are similar, then trace(A) = trace(B).\\n8.9 Prove or disprove the following statements:\\n(a) There exist matrices A, B ∈ K n,n with trace(AB) \u0003= trace(A) trace(B).\\n(b) There exist matrices A, B ∈ K n,n with AB − B A = In .\\n8.10 Suppose that the matrix A = [ai j ] ∈ Cn,n has only real entries ai j .\\nShow that if λ ∈ C\\R is an eigenvalue of A with corresponding eigenvector v =\\n[ν1 , . . . , νn ]T ∈ Cn,1 , then also λ is an eigenvalue of A with corresponding eigenvector v := [ν 1 , . . . , ν n ]T .   \n",
       "106                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Chapter 9\\n\\nVector Spaces\\n\\nIn the previous chapters we have focussed on matrices and their properties.\\nWe have defined algebraic operations with matrices and derived important concepts associated with them, including their rank, determinant, characteristic polynomial, and eigenvalues.\\nIn this chapter we place these concepts in a more abstract framework by introducing the idea of a vector space.\\nMatrices form one of the most important examples of vector spaces, and properties of certain (namely, finite dimensional) vector spaces can be studied in a transparent way using matrices.\\nIn the next chapter we will study (linear) maps between vector spaces, and there the connection with matrices will play a central role as well.\\n\\n9.1 Basic Definitions and Properties of Vector Spaces\\nWe begin with the definition of a vector space over a field K .\\nDefinition 9.1 Let K be a field.\\nA vector space over K , or shortly K -vector space, is a set V with two operations,\\n+ : V × V → V,\\n\\n(v, w) \u0003→ v + w,\\n\\n(addition)\\n\\n· : K × V → V,\\n\\n(λ, v) \u0003→ λ · v,\\n\\n(scalar multiplication) that satisfy the following:\\n(1) (V, +) is a commutative group.\\n(2) For all v, w ∈ V and λ, μ ∈ K the following assertions hold:\\n(a)\\n(b)\\n(c)\\n(d)\\n\\nλ · (μ · v) = (λμ) · v.\\n1 · v = v.\\nλ · (v + w) = λ · v + λ · w.\\n(λ + μ) · v = λ · v + μ · v.\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_9\\n\\n115   \n",
       "107                                                                                                                                                                                                                                                                                                                                                                              116\\n\\n9 Vector Spaces\\n\\nAn element v ∈ V is called a vector,1 an element λ ∈ K is called a scalar.\\nAgain, we usually omit the sign of the scalar multiplication, i.e., we usually write\\nλv instead of λ · v. If it is clear from the context (or not important) which field we are using, we often omit the explicit reference to K and simply write vector space instead of K -vector space.\\nExample 9.2\\n(1) The set K n,m with the matrix addition and the scalar multiplication forms a\\nK -vector space.\\nFor obvious reasons, the elements of K n,1 and K 1,m are sometimes called column and row vectors, respectively.\\n(2) The set K [t] forms a K -vector space, if the addition is defined as in Example 3.17 (usual addition of polynomials) and the scalar multiplication for p = α0 + α1 t + . . . + αn t n ∈ K [t] is defined by\\nλ · p := (λ α0 ) + (λ α1 )t + . . . + (λ αn )t n .\\n(3) The continuous and real valued functions defined on a real interval [α, β] with the pointwise addition and scalar multiplication, i.e.,\\n( f + g)(x) := f (x) + g(x) and (λ · f )(x) := λ f (x), form an R-vector space.\\nThis can be shown by using that the addition of two continuous functions as well as the multiplication of a continuous function by a real number yield again a continuous function.\\nSince, by definition, (V, +) is a commutative group, we already know some vector space properties from the theory of groups (cp.\\nChap.\\n3).\\nIn particular, every vector space contains a unique neutral element (with respect to addition) 0V , which is called the null vector.\\nEvery vector v ∈ V has a unique (additive) inverse −v ∈ V with v + (−v) = v − v = 0V .\\nAs usual, we will write v − w instead of v + (−w).\\nLemma 9.3 Let V be a K -vector space.\\nIf 0 K and 0V are the neutral (null) elements of K and V, respectively, then the following assertions hold:\\n(1) 0 K · v = 0V for all v ∈ V.\\n(2) λ · 0V = 0V for all λ ∈ K .\\n(3) −(λ · v) = (−λ) · v = λ · (−v) for all v ∈ V and λ ∈ K .\\n\\n1 This term was introduced in 1845 by Sir William Rowan Hamilton (1805–1865) in the context of his quaternions.\\nIt is motivated by the Latin verb “vehi” (“vehor”, “vectus sum”) which means to ride or drive.\\nAlso the term “scalar” was introduced by Hamilton; see the footnote on the scalar multiplication (4.2).   \n",
       "108                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                9.1 Basic Definitions and Properties of Vector Spaces\\n\\n117\\n\\nProof\\n(1) For all v ∈ V we have 0 K · v = (0 K + 0 K ) · v = 0 K · v + 0 K · v. Adding −(0 K · v) on both sides of this identity gives 0V = 0 K · v.\\n(2) For all λ ∈ K we have λ · 0V = λ · (0V + 0V ) = λ · 0V + λ · 0V .\\nAdding −(λ · 0V ) on both sides of this identity gives 0V = λ · 0V .\\n(3) For all λ ∈ K and v ∈ V we have λ · v + (−λ) · v = (λ − λ) · v = 0 K · v = 0V , as well as λ · v + λ · (−v) = λ · (v − v) = λ · 0V = 0V .\\n\u0006\\n\u0005\\nIn the following we will write 0 instead of 0 K and 0V when it is clear which null element is meant.\\nAs in groups, rings and fields we can identify substructures in vector spaces that are again vector spaces.\\nDefinition 9.4 Let (V, +, ·) be a K -vector space and let U ⊆ V. If (U, +, ·) is a\\nK -vector space, then it is called a subspace of (V, +, ·).\\nA substructure must be closed with respect to the given operations, which here are addition and scalar multiplication.\\nLemma 9.5 (U, +, ·) is a subspace of the K -vector space (V, +, ·) if and only if\\nØ \b= U ⊆ V and the following assertions hold:\\n(1) v + w ∈ U for all v, w ∈ U,\\n(2) λv ∈ U for all λ ∈ K and v ∈ U.\\n\u0006\\n\u0005\\n\\nProof Exercise.\\nExample 9.6\\n\\n(1) Every vector space V has the trivial subspaces U = V and U = {0}.\\n(2) Let A ∈ K n,m and U = L (A, 0) ⊆ K m,1 , i.e., U is the solution set of the homogeneous linear system Ax = 0.\\nWe have 0 ∈ U, so U is not empty.\\nIf v, w ∈ U, then\\nA(v + w) = Av + Aw = 0 + 0 = 0, i.e., v + w ∈ U. Furthermore, for all λ ∈ K ,\\nA(λ v) = λ (Av) = λ 0 = 0, i.e., λv ∈ U. Hence, U is a subspace of K m,1 .\\n(3) For every n ∈ N0 the set K [t]≤n := { p ∈ K [t] | deg( p) ≤ n} is a subspace of\\nK [t].\\nDefinition 9.7 Let V be a K -vector space, n ∈ N, and v1 , . . . , vn ∈ V. A vector of the form n\\n\u0002\\nλi vi ∈ V\\nλ1 v1 + . . . + λn vn = i=1   \n",
       "109                                                                                                                                                                                                                                                                                                                                                                                                                   118\\n\\n9 Vector Spaces is called a linear combination of v1 , . . . , vn with the coefficients λ1 , . . . , λn ∈ K .\\nThe (linear) span of v1 , . . . , vn is the set span{v1 , . . . , vn } := n\\n\u0003\u0002\\n\\n\u0004\\nλi vi | λ1 , . . . , λn ∈ K .\\ni=1\\n\\nLet M be a set and suppose that for every m ∈ M we have a vector vm ∈ V. Let the set of all these vectors, called the system of these vectors, be denoted by {vm }m∈M .\\nThen the (linear) span of the system {vm }m∈M , denoted by span {vm }m∈M , is defined as the set of all vectors v ∈ V that are linear combinations of finitely many vectors of the system.\\nThis definition can be consistently extended to the case n = 0.\\nIn this case v1 , . . . , vn is a list of length zero, or an empty list.\\nIf we define the empty sum of vectors as 0 ∈ V, then we obtain span{v1 , . . . , vn } = span Ø = {0}.\\nIf in the following we consider a list of vectors v1 , . . . , vn or a set of vectors\\n{v1 , . . . , vn }, we usually mean that n ≥ 1.\\nThe case of empty list and the associated zero vector space V = {0} will sometimes be discussed separately.\\nExample 9.8 The vector space K 1,3 = {[α1 , α2 , α3 ] | α1 , α2 , α3 ∈ K } is spanned by the vectors [1, 0, 0], [0, 1, 0], [0, 0, 1].\\nThe set {[α1 , α2 , 0] | α1 , α2 ∈ K } forms a subspace of K 1,3 that is spanned by the vectors [1, 0, 0], [0, 1, 0].\\nLemma 9.9 If V is a vector space and v1 , . . . , vn ∈ V, then span{v1 , . . . , vn } is a subspace of V.\\nProof It is clear that Ø \b= span{v1 , . . . , vn } ⊆ V. Furthermore, span{v1 , . . . , vn } is by definition closed with respect to addition and scalar multiplication, so that (1) and\\n(2) in Lemma 9.5 are satisfied.\\n\u0006\\n\u0005\\n\\n9.2 Bases and Dimension of Vector Spaces\\nWe will now discuss the central theory of bases and dimension of vector spaces, and start with the concept of linear independence.\\nDefinition 9.10 Let V be a K -vector space.\\n(1) The vectors v1 , . . . , vn ∈ V are called linearly independent if the equation n\\n\u0002\\n\\nλi vi = 0 with λ1 , . . . , λn ∈ K i=1\\n\\n\u0005n\\nλi vi = 0 always implies that λ1 = · · · = λn = 0.\\nOtherwise, i.e., when i=1 holds for some scalars λ1 , . . . , λn ∈ K that are not all equal to zero, then the vectors v1 , . . . , vn are called linearly dependent.   \n",
       "110                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 9.2 Bases and Dimension of Vector Spaces\\n\\n119\\n\\n(2) The empty list is linear independent.\\n(3) If M is a set and for every m ∈ M we have a vector vm ∈ V, the corresponding system {vm }m∈M is called linearly independent when finitely many vectors of the system are always linearly independent in the sense of (1).\\nOtherwise the system is called linearly dependent.\\nThe vectors v1 , . . . , vn are linearly independent if and only if the zero vector can be linearly combined only in the trivial way 0 = 0 · v1 + . . . + 0 · vn .\\nConsequently, if one of these vectors is the zero vector, then v1 , . . . , vn are linearly dependent.\\nA single vector v is linearly independent if and only if v \b= 0.\\nThe following result gives a useful characterization of the linear independence of finitely many (but at least two) given vectors.\\nLemma 9.11 The vectors v1 , . . . , vn , n ≥ 2, are linearly independent if and only if no vector vi , i = 1, . . . , n, can be written as a linear combination of the others.\\nProof We prove the assertion by contraposition.\\nThe vectors v1 , . . . , vn are linearly dependent if and only if n\\n\u0002\\nλi vi = 0 i=1 with at least one scalar λ j \b= 0.\\nEquivalently, vj = − n\\n\u0002\\n(λ−1 j λi ) vi , i=1 i\b = j so that v j is a linear combination of the other vectors.\\n\\n\u0006\\n\u0005\\n\\nUsing the concept of linear independence we can now define the concept of the basis of a vector space.\\nDefinition 9.12 Let V be a vector space.\\n(1) A set {v1 , . . . , vn } ⊆ V is called a basis of V, when v1 , . . . , vn are linearly independent and span{v1 , . . . , vn } = V.\\n(2) The set Ø is the basis of the zero vector space V = {0}.\\n(3) Let M be a set and suppose that for every m ∈ M we have a vector vm ∈ V. The set {vm | m ∈ M} is called a basis of V if the corresponding system {vm }m∈M is linearly independent and span {vm }m∈M = V.\\nIn short, a basis is a linearly independent spanning set of a vector space.\\nExample 9.13\\n(1) Let E i j ∈ K n,m be the matrix with entry 1 in position (i, j) and all other entries 0\\n(cp.\\nSect.\\n5.1).\\nThen the set   \n",
       "111                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        120\\n\\n9 Vector Spaces\\n\\n{E i j | 1 ≤ i ≤ n and 1 ≤ j ≤ m}\\n\\n(9.1) is a basis of the vector space K n,m (cp.\\n(1) in Example 9.2): The matrices E i j ∈\\nK n,m , 1 ≤ i ≤ n and 1 ≤ j ≤ m, are linearly independent, since\\n0= m n \u0002\\n\u0002\\n\\nλi j E i j = [λi j ] i=1 j=1 implies that λi j = 0 for i = 1, . . . , n and j = 1, . . . , m.\\nFor any A = [ai j ] ∈\\nK n,m we have\\nA= m n \u0002\\n\u0002 ai j E i j , i=1 j=1 and hence span{E i j | 1 ≤ i ≤ n and 1 ≤ j ≤ m} = K n,m .\\nThe basis (9.1) is called the canonical or standard basis of the vector space\\nK n,m .\\nFor m = 1 we denote the canonical basis vectors of K n,1 by\\n⎡ ⎤\\n⎡ ⎤\\n⎡ ⎤\\n1\\n0\\n0\\n⎢0⎥\\n⎢1⎥\\n⎢ .. ⎥\\n⎢ ⎥\\n⎢ ⎥\\n⎢.⎥\\n⎢ ⎥\\n⎢ ⎥\\n⎢ ⎥ e1 := ⎢0⎥ , e2 := ⎢0⎥ , . . . , en := ⎢0⎥ .\\n⎢ .. ⎥\\n⎢ .. ⎥\\n⎢ ⎥\\n⎣.⎦\\n⎣.⎦\\n⎣0⎦\\n0\\n0\\n1\\nThese vectors are also called unit vectors; they are the n columns of the identity matrix In .\\n(2) A basis of the vector space K [t] (cp.\\n(2) in Example 9.2) is given by the set\\n{t m | m ∈ N0 }, since the corresponding system {t m }m∈N0 is linearly independent, and every polynomial p ∈ K [t] is a linear combination of finitely many vectors of the system.\\nThe next result is called the basis extension theorem.\\nTheorem 9.14 Let V be a vector space and let v1 , . . . , vr , w1 , . . . , w\u0002 ∈ V, where r, \u0002 ∈ N0 .\\nIf v1 , . . . , vr are linearly independent and span{v1 , . . . , vr , w1 , . . . , w\u0002 } =\\nV, then the set {v1 , . . . , vr } can be extended to a basis of V using vectors from the set {w1 , . . . , w\u0002 }.\\nProof Note that for r = 0 the list v1 , . . . , vr is empty and hence linearly independent due to (2) in Definition 9.10.\\nWe prove the assertion by induction on \u0002.\\nIf \u0002 = 0, then span{v1 , . . . , vr } = V, and the linear independence of {v1 , . . . , vr } shows that this set is a basis of V.   \n",
       "112                                                                                                                                                                                                                                                                                 9.2 Bases and Dimension of Vector Spaces\\n\\n121\\n\\nLet the assertion hold for some \u0002 ≥ 0.\\nSuppose that v1 , . . . , vr , w1 , . . . , w\u0002+1 ∈ V are given, where v1 , . . . , vr are linearly independent and span{v1 , . . . , vr , w1 , . . . , w\u0002+1 } = V. If {v1 , . . . , vr } already is a basis of V, then we are done.\\nSuppose, therefore, that span{v1 , . . . , vr } ⊂ V. Then there exists at least one j, 1 ≤ j ≤ \u0002 + 1,\\n/ span{v1 , . . . , vr }.\\nIn particular, we have w j \b= 0.\\nThen such that w j ∈\\nλw j + r\\n\u0002\\n\\nλi vi = 0 i=1 implies that λ = 0 (otherwise we would have w j ∈ span{v1 , . . . , vr }) and, therefore, λ1 = · · · = λr = 0 due to the linear independence of v1 , . . . , vr .\\nThus, v1 , . . . , vr , w j are linearly independent.\\nBy the induction hypothesis we can extend the set {v1 , . . . , vr , w j } to a basis of V using vectors from the set\\n\u0006\\n\u0005\\n{w1 , . . . , w\u0002+1 } \\ {w j }, which contains \u0002 elements.\\nExample 9.15 Consider the vector space V = K [t]≤3 (cp.\\n(3) in Example 9.6) and the vectors v1 = t, v2 = t 2 , v3 = t 3 .\\nThese vectors are linearly independent, but {v1 , v2 , v3 } is not a basis of V, since span{v1 , v2 , v3 } \b= V. For example, the\\n/ vectors w1 = t 2 + 1 and w2 = t 3 − t 2 − 1 are elements of V, but w1 , w2 ∈ span{v1 , v2 , v3 }.\\nWe have span{v1 , v2 , v3 , w1 , w2 } = V. If we extend {v1 , v2 , v3 } by w1 , then we get the linearly independent vectors v1 , v2 , v3 , w1 which indeed span V.\\nThus, {v1 , v2 , v3 , w1 } is a basis of V.\\nBy the basis extension theorem every vector space that is spanned by finitely many vectors has a basis consisting of finitely many elements.\\nA central result of the theory of vector spaces is that every such basis has the same number of elements.\\nIn order to show this result we first prove the following exchange lemma.\\n\u0005m\\nλi vi ∈\\nLemma 9.16 Let V be a vector space, let v1 , . . . , vm ∈ V and let w = i=1\\nV with λ1 \b= 0.\\nThen span{w, v2 , . . . , vm } = span{v1 , v2 , . . . , vm }.\\nProof By assumption we have v1 = λ−1\\n1 w− m\\n\u0002\\n\\nλ−1\\n1 λi vi .\\ni=2\\n\\nIf y ∈ span{v1 , . . . , vm }, say y =\\n\u000e y = γ1 λ−1\\n1 w− m\\n\u0002 i=1\\n\\nγi vi , then\\n\u000f\\n\\nλ−1\\n1 λi vi i=2 m\\n\u0002 w +\\n= γ1 λ−1\\n1\\n\\n\u0005m\\n\\n+ m\\n\u0002\\n\\nγi vi i=2\\n\\nγi − γ1 λ−1\\n1 λi vi ∈ span{w, v2 , . . . , vm }.\\ni=2\\n\\nIf, on the other hand, y = α1 w +\\n\\n\u0005m i=2\\n\\nαi vi ∈ span{w, v2 , . . . , vm }, then   \n",
       "113                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           122\\n\\n9 Vector Spaces y = α1\\n\\n\u000e m\\n\u0002\\n\\n\u000f\\nλi vi\\n\\n+ i=1 m\\n\u0002\\n\\nαi vi i=2\\n\\n= α1 λ1 v1 + m\\n\u0002\\n\\n(α1 λi + αi ) vi ∈ span{v1 , . . . , vm }, i=2 and thus span{w, v2 , . . . , vm } = span{v1 , v2 , . . . , vm }.\\n\\n\u0006\\n\u0005\\n2\\n\\nUsing this lemma we now prove the exchange theorem.\\n\\nTheorem 9.17 Let W = {w1 , . . . , wn } and U = {u 1 , . . . , u m } be finite subsets of a vector space, and let w1 , . . . , wn be linearly independent.\\nIf W ⊆ span{u 1 , . . . , u m }, then n ≤ m, and n elements of U , if numbered appropriately the elements u 1 , . . . , u n , can be exchanged against n elements of W in such a way that span{w1 , . . . , wn , u n+1 , . . . , u m } = span{u 1 , . . . , u n , u n+1 , . . . , u m }.\\n\u0005m\\nλi u i for some scalars λ1 , . . . , λm that\\nProof By assumption we have w1 = i=1 are not all zero (otherwise w1 = 0, which contradicts the linear independence of w1 , . . . , wn ).\\nAfter an appropriate renumbering we have λ1 \b= 0, and Lemma 9.16 yields span{w1 , u 2 , . . . , u m } = span{u 1 , u 2 , . . . , u m }.\\nSuppose that for some r , 1 ≤ r ≤ n −1, we have exchanged the vectors u 1 , . . . , u r against w1 , . . . , wr so that span{w1 , . . . , wr , u r +1 , . . . , u m } = span{u 1 , . . . , u r , u r +1 , . . . , u m }.\\nIt is then clear that r ≤ m.\\nBy assumption we have wr +1 ∈ span{u 1 , . . . , u m }, and thus wr +1 = r\\n\u0002 i=1\\n\\nλi wi + m\\n\u0002\\n\\nλi u i i=r +1 for some scalars λ1 , . . . , λm .\\nOne of the scalars λr +1 , . . . , λm must be nonzero (otherwise wr +1 ∈ span{w1 , . . . , wr }, which contradicts the linear independence of w1 , . . . , wm ).\\nAfter an appropriate renumbering we have λr +1 \b= 0, and Lemma 9.16 yields span{w1 , . . . , wr +1 , u r +2 , . . . , u m } = span{w1 , . . . , wr , u r +1 , . . . , u m }.\\nIf we continue this construction until r = n − 1, then we obtain\\n2 In the literature, his theorem is sometimes called the Steinitz exchange theorem after Ernst Steinitz\\n\\n(1871–1928).\\nThe result was first proved in 1862 by Hermann Günther Graßmann (1809–1877).   \n",
       "114                                                                                                                                                                                                                                                                                                                     9.2 Bases and Dimension of Vector Spaces\\n\\n123 span{w1 , . . . , wn , u n+1 , . . . , u m } = span{u 1 , . . . , u n , u n+1 , . . . u m }, where in particular n ≤ m.\\n\\n\u0006\\n\u0005\\n\\nUsing this fundamental theorem, the following result about the unique number of basis elements is a simple corollary.\\nCorollary 9.18 If a vector space V is spanned by finitely many vectors, then V has a basis consisting of finitely many elements, and any two bases of V have the same number of elements.\\nProof The assertion is clear for V = {0} (cp.\\n(2) in Definition 9.12).\\nLet V = span{v1 , . . . , vm } with v1 \b= 0.\\nBy Theorem 9.14, we can extend span{v1 } using elements of {v2 , . . . , vm } to a basis of V. Thus, V has a basis with finitely many elements.\\nLet U := {u 1 , . . . , u \u0002 } and W := {w1 , . . . , wk } be two such bases.\\nThen\\nW ⊆ V = span{u 1 , . . . , u \u0002 }\\n\\nTheorem 9.18\\n\\n=⇒ k ≤ \u0002,\\n\\nU ⊆ V = span{w1 , . . . , wk }\\n\\nTheorem 9.18\\n\\n\u0002 ≤ k,\\n\\n=⇒ and thus \u0002 = k.\\n\\n\u0006\\n\u0005\\n\\nWe can now define the dimension of a vector space.\\nDefinition 9.19 If there exists a basis of a K -vector space V that consists of finitely many elements, then V is called finite dimensional, and the unique number of basis elements is called the dimension of V. We denote the dimension by dim K (V) or dim(V), if it is clear which field is meant.\\nIf V is not spanned by finitely many vectors, then V is called infinite dimensional, and we write dim K (V) = ∞.\\nNote that the zero vector space V = {0} has the basis Ø and thus it has dimension zero (cp.\\n(2) in Definition 9.12).\\nIf V is a finite dimensional vector space and if v1 , . . . , vm ∈ V with m > dim(V), then the vectors v1 , . . . , vm must be linearly dependent.\\n(If these vectors were linearly independent, then we could extend them via Theorem 9.14 to a basis of V that would contain more than dim(V) elements.)\\nExample 9.20 The set in (9.1) forms a basis of the vector space K n,m .\\nThis basis has n · m elements, and hence dim(K n,m ) = n · m.\\nOn the other hand, the vector space\\nK [t] is not spanned by finitely many vectors (cp.\\n(2) in Example 9.13) and hence it is infinite dimensional.\\nExample 9.21 Let V be the vector space of continuous and real valued functions on the real interval [0, 1] (cp.\\n(3) in Example 9.2).\\nDefine for n = 1, 2, . . . the function f n ∈ V by   \n",
       "115                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   124\\n\\n9 Vector Spaces f n (x) =\\n\\n⎧\\n⎪\\n⎪\\n⎪\\n⎪\\n⎪\\n⎪\\n⎪\\n⎨\\n⎪\\n⎪\\n⎪\\n⎪\\n⎪\\n⎪\\n⎪\\n⎩\\n\\n1\\n2\\n\\n\u0014\\n\\n1 j\\n\\n+\\n\\n1 j+1\\n\\n\u0015 x<\\n\\n0,\\n\\n1 n\\n\\n2n(n + 1)x − 2n,\\n−2n(n + 1)x + 2n + 2,\\n\\nEvery linear combination at\\n\\n0, k\\n\u0005\\n\\n< x,\\n\\n1 n+1\\n1\\n2\\n\\n1\\n, n+1\\n\\n1 n\\n\\n≤x≤\\n+\\n\\n1 n+1\\n\\n1\\n2\\n\\n1 n\\n\\n+\\n\\n1 n+1\\n\\n,\\n\\n< x ≤ n1 .\\n\\nλ j f j is a continuous function that has the value λ j j=1\\n\\n.\\nThus, the equation k\\n\u0005\\n\\nλ j f j = 0 ∈ V implies that all λ j must be j=1 zero, so that f 1 , . . . , f k ∈ V are linearly independent for all k ∈ N. Consequently, dim(V) = ∞.\\n\\n9.3 Coordinates and Changes of the Basis\\nWe will now study the linear combinations of basis vectors of a finite dimensional vector space.\\nIn particular, we will study what happens with a linear combination if we change to another basis of the vector space.\\nLemma 9.22 If {v1 , . . . , vn } is a basis of a K -vector space V, then for every v ∈ V there exist uniquely determined scalars λ1 , . . . , λn ∈ K with v = λ1 v1 + . . . + λn vn .\\nThese scalars are called the coordinates of v with respect to the basis {v1 , . . . , vn }.\\n\u0005n\\n\u0005n\\nProof Let v = i=1\\nλi vi = i=1\\nμi vi for some scalars λi , μi ∈ K , i = 1, . . . , n, then n\\n\u0002\\n(λi − μi )vi .\\n0=v−v = i=1   \n",
       "116                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                9.3 Coordinates and Changes of the Basis\\n\\n125\\n\\nThe linear independence of v1 , . . . , vn implies that λi = μi for i = 1, . . . , n.\\n\\n\u0006\\n\u0005\\n\\nBy definition, the coordinates of a vector depend on the given basis.\\nIn particular, they depend on the ordering (or numbering) of the basis vectors.\\nBecause of this, some authors distinguish between the basis as “set”, i.e., a collection of elements without a particular ordering, and an “ordered basis”.\\nIn this book we will keep the set notation for a basis {v1 , . . . , vn }, where the indices indicate the ordering of the basis vectors.\\nLet V be a K -vector space, v1 , . . . , vn ∈ V (they need not be linearly independent) and v = λ1 v1 + . . . + λn vn for some coefficients λ1 , . . . , λn ∈ K .\\nLet us write\\n⎤\\nλ1\\n⎢ ⎥\\n(v1 , . . . , vn ) ⎣ ... ⎦ := λ1 v1 + . . . + λn vn .\\n⎡\\n\\n(9.2)\\n\\nλn\\n\\nHere (v1 , . . . , vn ) is an n-tuple over V, i.e.,\\n. . × V\u0019 .\\n(v1 , . . . , vn ) ∈ V n = V\\n\u0016 × .\u0017\u0018 n times\\n\\nFor n = 1 we have V 1 = V. We then skip the parentheses and write v instead of\\n(v) for a 1-tuple.\\nThe notation (9.2) formally defines a “multiplication” as map from\\nV n × K n,1 to V.\\nFor all α ∈ K we have\\n⎤\\n⎡\\nαλ1\\n⎥\\n⎢\\nα · v = (α · λ1 )v1 + . . . + (α · λn )vn = (v1 , . . . , vn ) ⎣ ... ⎦ .\\nαλn\\n\\nIf μ1 , . . . , μn ∈ K and\\n⎤\\nμ1\\n⎢ ⎥ u = μ1 v1 + . . . + μn vn = (v1 , . . . , vn ) ⎣ ... ⎦ ,\\n⎡\\n\\nμn then\\n⎤\\nλ1 + μ1\\n⎥\\n⎢ v + u = (λ1 + μ1 )v1 + . . . + (λn + μn )vn = (v1 , . . . , vn ) ⎣ ... ⎦ .\\n⎡\\n\\nλn + μn   \n",
       "117                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     126\\n\\n9 Vector Spaces\\n\\nThis shows that if vectors are given by linear combinations, then the operations scalar multiplication and addition correspond to operations with the coefficients of the vectors with respect to the linear combinations.\\nWe can further extend this notation.\\nLet A = [ai j ] ∈ K n,m and let\\n⎡ ⎤ a1 j\\n⎢ .. ⎥ u j = (v1 , . . . , vn ) ⎣ . ⎦ , j = 1, . . . , m.\\nan j\\nThen we write the m linear combinations for u 1 , . . . , u m as the system\\n(u 1 , . . . , u m ) =: (v1 , . . . , vn )A.\\n\\n(9.3)\\n\\nOn both sides of this equation we have elements of V m .\\nThe right-multiplication of an arbitrary n-tuple (v1 , . . . , vn ) ∈ V n with a matrix A ∈ K n,m thus corresponds to forming m linear combinations of the vectors v1 , . . . , vn , with the corresponding coefficients given by the entries of A. Formally, this defines a “multiplication” as a map from V n × K n,m to V m .\\nLemma 9.23 Let V be a K -vector space, let v1 , . . . , vn ∈ V be linearly independent, let A ∈ K n,m , and let (u 1 , . . . , u m ) = (v1 , . . . , vn )A. Then the vectors u 1 , . . . , u m are linearly independent if and only if rank(A) = m.\\n\u0006\\n\u0005\\n\\nProof Exercise.\\nNow consider also a matrix B = [bi j ] ∈ K m,\u0002 .\\nUsing (9.3) we obtain\\n(u 1 , . . . , u m )B = ((v1 , . . . , vn )A)B.\\nLemma 9.24 In the previous notation,\\n((v1 , . . . , vn )A)B = (v1 , . . . , vn )(AB).\\n\\n\u0006\\n\u0005\\n\\nProof Exercise.\\n\\nLet {v1 , . . . , vn } and {w1 , . . . , wn } be bases of V and let v ∈ V. By Lemma 9.22 there exist (unique) coordinates λ1 , . . . , λn and μ1 , . . . , μn , respectively, with\\n⎤\\n⎡ ⎤\\nλ1\\nμ1\\n⎢ .. ⎥\\n⎢ .. ⎥ v = (v1 , . . . , vn ) ⎣ . ⎦ = (w1 , . . . , wn ) ⎣ . ⎦ .\\n⎡\\n\\nλn\\n\\nμn\\n\\nWe will now describe a method for transforming the coordinates λ1 , . . . , λn with respect to the basis {v1 , . . . , vn } into the coordinates μ1 , . . . , μn with respect to the basis {w1 , . . . , wn }, and vice versa.   \n",
       "118                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              9.3 Coordinates and Changes of the Basis\\n\\n127\\n\\nFor every basis vector v j , j = 1, . . . , n, there exist (unique) coordinates pi j , i = 1, . . . , n, such that\\n⎤ p1 j\\n⎢ ⎥ v j = (w1 , . . . , wn ) ⎣ ... ⎦ ,\\n⎡ j = 1, . . . , n.\\npn j\\nDefining P = [ pi j ] ∈ K n,n we can write these n equations for the vectors v j analogous to (9.3) as\\n(v1 , . . . , vn ) = (w1 , . . . , wn ) P.\\n\\n(9.4)\\n\\nIn the same way, for every basis vector w j , j = 1, . . . , n, there exist (unique) coordinates qi j , i = 1, . . . , n, such that\\n⎡ ⎤ q1 j\\n⎢ .. ⎥ w j = (v1 , . . . , vn ) ⎣ . ⎦ , j = 1, . . . , n.\\nqn j\\nIf we set Q = [qi j ] ∈ K n,n , then analogously to (9.4) we get\\n(w1 , . . . , wn ) = (v1 , . . . , vn )Q.\\nThus,\\n(w1 , . . . , wn ) = (v1 , . . . , vn )Q = ((w1 , . . . , wn )P)Q = (w1 , . . . , wn )(P Q), which implies that\\n(w1 , . . . , wn )(In − P Q) = (0, . . . , 0).\\nThis means that the n linear combinations of the basis vectors w1 , . . . , wn , with their corresponding coordinates given by the entries of the n columns of In − P Q, are all equal to the zero vector.\\nSince the basis vectors are linearly independent, all coordinates must be zero, and hence In − P Q = 0 ∈ K n,n , or P Q = In .\\nAnalogously we obtain the equation Q P = In .\\nTherefore the matrix P ∈ K n,n is invertible with\\nP −1 = Q. Furthermore, we have\\n⎤\\n⎡ ⎤\\n⎛ ⎡ ⎤⎞\\nλ1\\nλ1\\nλ1\\n⎢ .. ⎥\\n⎢ .. ⎥\\n⎜ ⎢ .. ⎥⎟ v = (v1 , . . . , vn ) ⎣ . ⎦ = ((w1 , . . . , wn )P) ⎣ . ⎦ = (w1 , . . . , wn ) ⎝ P ⎣ . ⎦⎠ .\\n⎡\\n\\nλn\\n\\nλn\\n\\nλn   \n",
       "119                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    128\\n\\n9 Vector Spaces\\n\\nDue to the uniqueness of the coordinates of v with respect to the basis {w1 , . . . , wn } we obtain\\n⎡ ⎤\\n⎡ ⎤\\n⎡ ⎤\\n⎡ ⎤\\nλ1\\nλ1\\nμ1\\nμ1\\n⎢ .. ⎥\\n⎢ .. ⎥\\n⎢ .. ⎥\\n−1 ⎢ .. ⎥\\n=\\nP\\n, or\\n=\\nP\\n⎣.⎦\\n⎣.⎦\\n⎣.⎦\\n⎣ . ⎦.\\nμn\\n\\nλn\\n\\nλn\\n\\nμn\\n\\nHence a multiplication with the matrix P transforms the coordinates of v with respect to the basis {v1 , . . . , vn } into those with respect to the basis {w1 , . . . , wn }; a multiplication with P −1 yields the inverse transformation.\\nTherefore, P and P −1 are called coordinate transformation matrices.\\nWe can summarize the results obtained above as follows.\\nTheorem 9.25 Let {v1 , . . . , vn } and {w1 , . . . , wn } be bases of a K -vector space V.\\nThen the uniquely determined matrix P ∈ K n,n is (9.4) is invertible and yields the coordinate transformation from {v1 , . . . , vn } to {w1 , . . . , wn }: If\\n⎤\\n⎡ ⎤\\nλ1\\nμ1\\n⎢ .. ⎥\\n⎢ .. ⎥ v = (v1 , . . . , vn ) ⎣ . ⎦ = (v1 , . . . , vn ) ⎣ . ⎦ ,\\n⎡\\n\\nλn then\\n\\nμn\\n\\n⎡\\n\\n⎡ ⎤\\n⎤\\nμ1\\nλ1\\n⎢ .. ⎥\\n⎢ .. ⎥\\n⎣ . ⎦ = P ⎣ . ⎦.\\nμn\\n\\nλn\\n\\nExample 9.26 Consider the vector space V = R2 = {(α1 , α2 ) | α1 , α2 ∈ R} with the entrywise addition and scalar multiplication.\\nA basis of V is given by the set\\n{e1 = (1, 0), e2 = (0, 1)}, and we have (α1 , α2 ) = α1 e1 +α2 e2 for all (α1 , α2 ) ∈ V.\\nAnother basis of V is the set {v1 = (1, 1), v2 = (1, 2)}.\\nThe corresponding coordinate transformation matrices can be obtained from the defining equations (v1 , v2 ) =\\n(e1 , e2 )P and (e1 , e2 ) = (v1 , v2 )Q as\\n!\\n11\\nP=\\n,\\n12\\n\\nQ=P\\n\\n−1\\n\\n!\\n2 −1\\n=\\n.\\n−1 1\\n\\n9.4 Relations Between Vector Spaces and Their Dimensions\\nOur first result describes the relation between a vector space and a subspace.\\nLemma 9.27 If V is a finite dimensional vector space and U ⊆ V is a subspace, then dim(U) ≤ dim(V) with equality if and only if U = V.   \n",
       "120                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             9.4 Relations Between Vector Spaces and Their Dimensions\\n\\n129\\n\\nProof Let U ⊆ V and let {u 1 , . . . , u m } be a basis of U, where {u 1 , . . . , u m } = Ø for U = {0}.\\nUsing Theorem 9.14 we can extend this set to a basis of V. If U is a proper subset of V, then at least one basis vector needs to be added and hence dim(U) < dim(V).\\nIf U = V, then every basis of V is also a basis of U, and thus dim(U) = dim(V).\\n\u0006\\n\u0005\\nIf U1 and U2 are subspaces of a vector space V, then their intersection is given by\\nU1 ∩ U2 = {u ∈ V | u ∈ U1 ∧ u ∈ U2 }\\n(cp.\\nDefinition 2.6).\\nThe sum of the two subspaces is defined as\\nU1 + U2 := {u 1 + u 2 ∈ V | u 1 ∈ U1 ∧ u 2 ∈ U2 }.\\nLemma 9.28 If U1 and U2 are subspaces of a vector space V, then the following assertions hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n\\nU1 ∩ U2 and U1 + U2 are subspaces of V.\\nU1 + U1 = U1 .\\nU1 + {0} = U1 .\\nU1 ⊆ U1 + U2 , with equality if and only if U2 ⊆ U1 .\\n\u0006\\n\u0005\\n\\nProof Exercise.\\nAn important result is the following dimension formula for subspaces.\\n\\nTheorem 9.29 If U1 and U2 are finite dimensional subspaces of a vector space V, then dim(U1 ∩ U2 ) + dim(U1 + U2 ) = dim(U1 ) + dim(U2 ).\\nProof Let {v1 , . . . , vr } be a basis of U1 ∩ U2 .\\nWe extend this set to a basis\\n{v1 , . . . , vr , w1 , . . . , w\u0002 } of U1 and to a basis {v1 , . . . , vr , x1 , . . . , xk } of U2 , where we assume that r, \u0002, k ≥ 1.\\n(If one of the lists is empty, then the following argument is easily modified.)\\nIf suffices to show that {v1 , . . . , vr , w1 , . . . , w\u0002 , x1 , . . . , xk } is a basis of U1 + U2 .\\nObviously, span{v1 , . . . , vr , w1 , . . . , w\u0002 , x1 , . . . , xk } = U1 + U2 , and hence it suffices to show that v1 , . . . , vr , w1 , . . . , w\u0002 , x1 , . . . , xk are linearly independent.\\nLet r\\n\u0002 k\\n\u0002\\n\u0002\\n\u0002\\nλi vi +\\nμi wi +\\nγi xi = 0, i=1 i=1 i=1   \n",
       "121                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          130\\n\\n9 Vector Spaces then k\\n\u0002\\n\\n\u000e\\nγi xi = − r\\n\u0002 i=1\\n\\nλi vi + i=1\\n\\n\u0002\\n\u0002\\n\\n\u000f\\nμi wi .\\ni=1\\n\\nOn the left hand side of this equation we have,\\n\u0005k by definition, a vector in U2 ; on the\\nγi xi ∈ U1 ∩ U2 .\\nBy construction, right hand side a vector in U1 .\\nTherefore, i=1\\nU1 ∩U2 and the vectors v1 , . . . , vr , w1 , . . . , w\u0002 are however, {v1 , . . . , vr } is a basis of\u0005\\n\u0002\\nμi wi = 0 implies that μ1 = · · · = μ\u0002 = 0.\\nlinearly independent.\\nTherefore, i=1\\nBut then also r\\n\u0002\\n\\nλi vi + i=1 k\\n\u0002\\n\\nγi xi = 0, i=1 and hence λ1 = · · · = λr = γ1 = · · · = γk = 0 due to the linear independence of\\n\u0006\\n\u0005 v1 , . . . , vr , x1 , . . . , xk .\\nIf at least one of the subspaces in Theorem 9.29 is infinite dimensional, then the assertion is still formally correct, since in this case dim(U1 + U2 ) = ∞ and dim(U1 ) + dim(U2 ) = ∞.\\nExample 9.30 For the subspaces\\nU1 = {[α1 , α2 , 0] | α1 , α2 ∈ K }, U2 = {[0, α2 , α3 ] | α2 , α3 ∈ K } ⊂ K 1,3 we have dim(U1 ) = dim(U2 ) = 2,\\nU1 ∩ U2 = {[0, α2 , 0] | α2 ∈ K }, dim(U1 ∩ U2 ) = 1,\\nU1 + U2 = K 1,3 , dim(U1 + U2 ) = 3.\\n\\nThe above definition of the sum can be extended to an arbitrary (but finite) number of subspaces: If U1 , . . . , Uk , k ≥ 2, are subspaces of the vector space V, then we define\\nU1 + . . . + Uk = k\\n\u0002\\n\\nU j := j=1 k\\n\u0003\u0002\\n\\n\u0004 u j | u j ∈ U j , j = 1, . . . , k .\\nj=1\\n\\nThis sum is called direct, if\\nUi ∩ k\\n\u0002\\n\\nU j = {0} for i = 1, . . . , k, j=1 j\b =i and in this case we write the (direct) sum as   \n",
       "122                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             9.4 Relations Between Vector Spaces and Their Dimensions\\n\\nU1 ⊕ . . . ⊕ Uk = k\\n\"\\n\\n131\\n\\nUj.\\nj=1\\n\\nIn particular, a sum U1 + U2 of two subspaces U1 , U2 ⊆ V is direct if U1 ∩ U2 = {0}.\\nThe following theorem presents two equivalent characterizations of the direct sum of subspaces.\\nTheorem 9.31 If U = U1 + . . . + Uk is a sum of k ≥ 2 subspaces of a vector space\\nV, then the following assertions are equivalent:\\n\u0005\\n(1) The sum U is direct, i.e., Ui ∩ j\b=i U j = {0} for i = 1, . . . , k.\\n\u0005\\n(2) Every vector u ∈ U has a representation of the form u = kj=1 u j with uniquely determined u j ∈ U j for j = 1, . . . , k.\\n\u0005k\\n(3) j=1 u j = 0 with u j ∈ U j for j = 1, . . . , k implies that u j = 0 for j =\\n1, . . . , k.\\nProof\\n\\n\u0005\\n\u0005\\n(1) ⇒ (2): Let u = kj=1 u j = kj=1 # u j with u j , # u j ∈ U j , j = 1, . . . , k.\\nFor every i = 1, . . . , k we then have ui = − ui − #\\n\\n\u0002\\n\\n(u j − # u j ) ∈ Ui ∩ j\b=i\\n\\n\u0002\\n\\nUj.\\nj\b=i\\n\\n\u0005 u i = 0, and hence u i = # u i for\\nNow Ui ∩ j\b=i U j = {0} implies that u i − # i = 1, . . . , k.\\n(2) ⇒ (3): This is obvious.\\n\u0005\\n\u0005\\n∩ j\b=i U j .\\nThen u =\\n(3) ⇒ (1): For a given i, let u ∈ Ui\u0005 j\b=i u j for some\\n−u\\n+ u\\n=\\n0.\\nIn particular, this implies that u j ∈ U j , j \b= i, and hence j\b=i j\\n\u0005\\n\u0006\\n\u0005 u = 0, and thus Ui ∩ j\b=i U j = {0}.\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n9.1.\\nWhich of the following sets (with the usual addition and scalar multiplication) are R-vector spaces?\\n\u0003\\n\\n\u0004 \u0003\\n\u0004\\n[α1 , α2 ] ∈ R1,2 | α1 = α2 , [α1 , α2 ] ∈ R1,2 | α12 + α22 = 1 ,\\n\u0004 \u0003\\n\u0004\\n\u0003\\n[α1 , α2 ] ∈ R1,2 | α1 ≥ α2 , [α1 , α2 ] ∈ R1,2 | α1 − α2 = 0 and 2α1 + α2 = 0 .\\n\\nDetermine, if possible, a basis and the dimension.\\n9.2.\\nDetermine a basis of the R-vector space C and dimR (C).\\nDetermine a basis of the C-vector space C and dimC (C).\\n9.3.\\nShow that a1 , . . . , an ∈ K n,1 are linearly independent if and only if det([a1 , …, an ]) \b= 0.   \n",
       "123                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           132\\n\\n9 Vector Spaces\\n\\n9.4.\\nLet V be a K -vector space, \u0003 a nonempty set and Map(\u0003, V) the set of maps from \u0003 to V. Show that Map(\u0003, V) with the operations\\n+ : Map(\u0003, V) × Map(\u0003, V) → Map(\u0003, V), ( f, g) \u0003→ f + g, with ( f + g)(x) := f (x) + g(x) for all x ∈ \u0003,\\n· : K × Map(\u0003, V) → Map(\u0003, V), (λ, f ) \u0003→ λ · f, with(λ · f )(x) := λ f (x) for all x ∈ \u0003, is a K -vector space.\\n9.5.\\nShow that the functions sin and cos in Map(R, R) are linearly independent.\\n9.6.\\nLet V be a vector space with n = dim(V) ∈ N and let v1 , . . . , vn ∈ V. Show that the following statements are equivalent:\\n(1) v1 , . . . , vn are linearly independent.\\n(2) span{v1 , . . . , vn } = V.\\n(3) {v1 , . . . , vn } is a basis of V.\\n9.7.\\nShow that (K n,m , +, ·) is a K -vector space (cp.\\n(1) in Example 9.2).\\nFind a subspace of this K -vector space.\\n9.8.\\nShow that (K [t], +, ·) is a K -vector space (cp.\\n(2) in Example 9.2).\\nShow further that K [t]≤n is a subspace of K [t] (cp.\\n(3) in Example 9.6) and determine dim(K [t]≤n ).\\n9.9.\\nShow that the polynomials p1 = t 5 + t 4 , p2 = t 5 − 7t 3 , p3 = t 5 − 1, p4 = t 5 + 3t are linearly independent in Q[t]≤5 and extend { p1 , p2 , p3 , p4 } to a basis of Q[t]≤5 .\\n9.10.\\nLet n ∈ N and n\\n\u0003\u0002\\n\u0004 j $\\nαi j t1i t2 $ αi j ∈ K .\\nK [t1 , t2 ] := i, j=0\\n\\n9.11.\\n9.12.\\n9.13.\\n9.14.\\n9.15.\\n\\nAn element of K [t1 , t2 ] is called bivariate polynomial over K in the unknowns t1 and t2 .\\nDefine a scalar multiplication and an addition so that K [t1 , t2 ] becomes a vector space.\\nDetermine a basis of K [t1 , t2 ].\\nShow Lemma 9.5.\\nLet A ∈ K n,m and b ∈ K n,1 .\\nIs the solution set L (A, b) of Ax = b a subspace of K m,1 ?\\nLet A ∈ K n,n and let λ ∈ K be an eigenvalue of A. Show that the set {v ∈\\nK n,1 | Av = λv} is a subspace of K n,1 .\\nLet A ∈ K n,n and let λ1 \b= λ2 be two eigenvalues of A. Show that any two associated eigenvectors v1 and v2 are linearly independent.\\nShow that B = {B1 , B2 , B3 , B4 } and C = {C1 , C2 , C3 , C4 } with\\nB1 =\\n\\n!\\n!\\n!\\n!\\n11\\n10\\n10\\n11\\n, B2 =\\n, B3 =\\n, B4 =\\n00\\n00\\n10\\n01   \n",
       "124                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          9.4 Relations Between Vector Spaces and Their Dimensions\\n\\n133 and\\nC1 =\\n\\n!\\n!\\n!\\n!\\n10\\n10\\n10\\n01\\n, C2 =\\n, C3 =\\n, C4 =\\n01\\n10\\n00\\n10 are bases of the vector space K 2,2 , and determine corresponding coordinate transformation matrices.\\n9.16.\\nExamine the elements of the following sets for linear independence in the vector space K [t]≤3 :\\nU1 = {t, t 2 + 2t, t 2 + 3t + 1, t 3 }, U2 = {1, t, t + t 2 , t 2 + t 3 },\\nU3 = {1, t 2 − t, t 2 + t, t 3 }.\\n\\n9.17.\\n\\n9.18.\\n9.19.\\n9.20.\\n9.21.\\n9.22.\\n9.23.\\n\\n9.24.\\n\\nDetermine the dimensions of the subspaces spanned by the elements of U1 ,\\nU2 , U3 .\\nIs one of these sets a basis of K [t]≤3 ?\\nShow that the set of sequences {(α1 , α2 , α3 , . . .) | αi ∈ Q, i ∈ N} with entrywise addition and scalar multiplication forms an infinite dimensional vector space, and determine a basis system.\\nProve Lemma 9.23.\\nProve Lemma 9.24.\\nProve Lemma 9.28.\\nLet U1 , U2 be finite dimensional subspaces of a vector space V. Show that the sum U1 + U2 is direct if dim(U1 + U2 ) = dim(U1 ) + dim(U2 ).\\nLet U1 , . . . , Uk , k ≥ 3, be finite dimensional subspaces of a vector space V.\\nSuppose that Ui ∩ U j = {0} for all i \b= j.\\nIs the sum U1 + . . . + Uk direct?\\nLet U be a subspace of a finite dimensional vector space V. Show that there\\n# with U ⊕ U\\n# = V. (The subspace U\\n# is called a exists another subspace U complement of U.)\\nDetermine three subspaces U1 , U2 , U3 of V = R3,1 with U2 \b= U3 and V =\\nU1 ⊕ U2 = U1 ⊕ U3 .\\nIs there a subspace U1 of V with a uniquely determined complement?   \n",
       "125                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Chapter 10\\n\\nLinear Maps\\n\\nIn this chapter we study maps between vector spaces that are compatible with the two vector space operations, addition and scalar multiplication.\\nThese maps are called linear maps or homomorphisms.\\nWe first investigate their most important properties and then show that in the case of finite dimensional vector spaces every linear map can be represented by a matrix, when bases in the respective spaces have been chosen.\\nIf the bases are chosen in a clever way, then we can read off important properties of a linear map from its matrix representation.\\nThis central idea will arise frequently in later chapters.\\n\\n10.1 Basic Definitions and Properties of Linear Maps\\nWe start our investigations with the definition of linear maps between vector spaces.\\nDefinition 10.1 Let V and W be K -vector spaces.\\nA map f : V → W is called linear, when\\n(1) f (λv) = λ f (v), and\\n(2) f (v + w) = f (v) + f (w), hold for all v, w ∈ V and λ ∈ K .\\nThe set of all these maps is denoted by L(V, W).\\nA linear map f : V → W is also called a linear transformation or (vector space) homomorphism.\\nA bijective linear map is called an isomorphism.\\nIf there exists an isomorphism between V and W, then the spaces V and W are called isomorphic, which we denote by\\nV∼\\n= W.\\nA map f ∈ L(V, V) is called an endomorphism, and a bijective endomorphism is called an automorphism.\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_10\\n\\n135   \n",
       "126                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       136\\n\\n10 Linear Maps\\n\\nIt is an easy exercise to show that the conditions (1) and (2) in Definition 10.1 hold if and only if f (λv + μw) = λ f (v) + μ f (w) holds for all λ, μ ∈ K and v, w ∈ V.\\nExample 10.2\\n(1) Every matrix A ∈ K n,m defines a map\\nA : K m,1 → K n,1 , x \u0005→ Ax.\\nThis map is linear, since\\nA(λx) = λAx for all x ∈ K m,1 and λ ∈ K ,\\nA(x + y) = Ax + Ay for all x, y ∈ K m,1\\n(cp.\\nLemmas 4.3 and 4.4).\\n\u0002n aii , is linear (cp.\\n(2) The map trace : K n,n → K , A = [ai j ] \u0005→ trace(A) := i=1\\nExercise 8.8).\\n(3) The map f : Q[t]≤3 → Q[t]≤2 , α3 t 3 + α2 t 2 + α1 t + α0 \u0005→ 2α2 t 2 + 3α1 t + 4α0 , is linear.\\n(Show this as an exercise).\\nThe map g : Q[t]≤3 → Q[t]≤2 , α3 t 3 + α2 t 2 + α1 t + α0 \u0005→ α2 t 2 + α1 t + α02 , is not linear.\\nFor example, if p1 = t + 2 and p2 = t + 1, then g( p1 + p2 ) =\\n2t + 9 \u0007= 2t + 5 = g( p1 ) + g( p2 ).\\nThe set of linear maps between vector spaces forms a vector space itself.\\nLemma 10.3 Let V and W be K -vector spaces.\\nFor f, g ∈ L(V, W) and λ ∈ K define f + g and λ · f by\\n( f + g)(v) := f (v) + g(v),\\n(λ · f )(v) := λ f (v), for all v ∈ V. Then (L(V, W), +, ·) is a K -vector space.\\nProof Cp.\\nExercise 9.4.\\n\\n\b\\n\\nThe next result deals with the existence and uniqueness of linear maps.\\nTheorem 10.4 Let V and W be K -vector spaces, let {v1 , . . . , vm } be a basis of V, and let w1 , . . . , wm ∈ W. Then there exists a unique linear map f ∈ L(V, W) with f (vi ) = wi for i = 1, . . . , m.   \n",
       "127                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           10.1 Basic Definitions and Properties of Linear Maps\\n\\n137\\n\\n(v)\\nProof For every v ∈ V there exist (unique) coordinates λ(v)\\n1 , . . . , λm with v =\\n\u0002m (v) i=1 λi vi (cp.\\nLemma 9.22).\\nWe define the map f : V → W by f (v) := m\\n\u0003\\n\\nλi(v) wi for all v ∈ V.\\ni=1\\n\\nBy definition, f (vi ) = wi for i = 1, . . . , m.\\n\u0002m\\n(λ λi(v) )vi ,\\nWe next show that f is linear.\\nFor every λ ∈ K we have λv = i=1 and hence m m\\n\u0003\\n\u0003\\n(λ λi(v) )wi = λ\\nλi(v) wi = λ f (v).\\nf (λv) = i=1\\n\\nIf u =\\n\\n\u0002m i=1 i=1\\n\\nλi(u) vi ∈ V, then v + u = f (v + u) = m\\n\u0003\\n\\n(λi(v)\\n\\n+ i=1\\n\\nλi(u) )wi\\n\\n=\\n\\n\u0002m\\n\\n(v) i=1 (λi m\\n\u0003\\n\\nλi(v) wi\\n\\n+ λi(u) )vi , and hence\\n+ m\\n\u0003 i=1\\n\\nλi(u) wi = f (v) + f (u).\\ni=1\\n\\nThus, f ∈ L(V, W).\\nSuppose that g ∈ L(V, W) also satisfies g(vi ) = wi for i = 1, . . . , m.\\nThen for\\n\u0002m (v) every v = i=1\\nλi vi we have f (v) = f m\\n\u0004\u0003 i=1 m m m m\\n\u0004\u0003\\n\u0005 \u0003\\n\u0005\\n\u0003\\n\u0003\\nλi(v) vi =\\nλi(v) f (vi ) =\\nλi(v) wi =\\nλi(v) g(vi ) = g\\nλi(v) vi = g(v), i=1 i=1 i=1 i=1 and hence f = g, so that f is indeed uniquely determined.\\n\\n\b\\n\\nTheorem 10.4 shows that the map f ∈ L(V, W) is uniquely determined by the images of f at the given basis vectors of V. Note that the image vectors w1 , . . . , wm ∈\\nW may be linearly dependent, and that W may be infinite dimensional.\\nIn Definition 2.12 we have introduced the image and pre-image of a map.\\nWe next recall these definitions for completeness and introduce the kernel of a linear map.\\nDefinition 10.5 If V and W are K -vector spaces and f ∈ L(V, W), then the kernel and the image of f are defined by ker( f ) := {v ∈ V | f (v) = 0}, im( f ) := { f (v) | v ∈ V}.\\nFor w ∈ W the pre-image of w in the space V is defined by f −1 (w) := f −1 ({w}) = {v ∈ V | f (v) = w}.\\nThe kernel of a linear map is sometimes called the null space (or nullspace) of the map, and some authors use the notation null( f ) instead of ker( f ).   \n",
       "128                                                                                                                                                                                                                                                                                                                                                                                                                                                                               138\\n\\n10 Linear Maps\\n\\nNote that the pre-image f −1 (w) is a set, and that f −1 here does not mean the inverse map of f (cp.\\nDefinition 2.12).\\nIn particular, we have f −1 (0) = ker( f ), and if w ∈\\n/ im( f ), then f −1 (w) = Ø,\\nExample 10.6 For A ∈ K n,m and the corresponding map A ∈ L(K m,1 , K n,1 ) from\\n(1) in Example 10.2 we have ker(A) = {x ∈ K m,1 | Ax = 0} and im(A) = {Ax | x ∈ K m,1 }.\\nNote that ker(A) = L (A, 0) (cp.\\nDefinition 6.1).\\nLet a j ∈ K n,1 denote the jth column of A, j = 1, . . . , m.\\nFor x = [x1 , . . . , xm ]T ∈ K m,1 we then can write\\nAx = m\\n\u0003 xjaj.\\nj=1\\n\\nClearly, 0 ∈ ker(A).\\nMoreover, we see from the representation of Ax that ker(A) =\\n{0} if and only if the columns of A are linearly independent.\\nThe set im(A) is given by the linear combinations of the columns of A, i.e., im(A) = span{a1 , . . . , am }.\\nLemma 10.7 If V and W are K -vector spaces, then for every f ∈ L(V, W) the following assertions hold: f (0) = 0 and f (−v) = − f (v) for all v ∈ V.\\nIf f is an isomorphism, then f −1 ∈ L(W, V).\\nker( f ) is a subspace of V and im( f ) is a subspace of W.\\nf is surjective if and only if im( f ) = W.\\nf is injective if and only if ker( f ) = {0}.\\nIf f is injective and if v1 , . . . , vm ∈ V are linearly independent, then f (v1 ), . . . , f (vm ) ∈ W are linearly independent.\\n(7) If v1 , . . . , vm ∈ V are linearly dependent, then f (v1 ), . . . , f (vm ) ∈ W are linearly dependent, or, equivalently, if f (v1 ), . . . , f (vm ) ∈ W are linearly independent, then v1 , . . . , vm ∈ V are linearly independent.\\n(8) If w ∈ im( f ) and if u ∈ f −1 (w) is arbitrary, then\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\n(6) f −1 (w) = u + ker( f ) := {u + v | v ∈ ker( f )}.\\nProof\\n(1) We have f (0V ) = f (0 K · 0V ) = 0 K · f (0V ) = 0V as well as f (v) + f (−v) = f (v + (−v)) = f (0) = 0 for all v ∈ V.\\n(2) The existence of the inverse map f −1 : W → V is guaranteed by Theorem 2.20, so we just have to show that f −1 is linear.\\nIf w1 , w2 ∈ W, then there exist uniquely determined v1 , v2 ∈ V with w1 = f (v1 ) and w2 = f (v2 ).\\nHence, f −1 (w1 + w2 ) = f −1 ( f (v1 ) + f (v2 )) = f −1 ( f (v1 + v2 )) = v1 + v2\\n= f −1 (w1 ) + f −1 (w2 ).   \n",
       "129                                                                                                                                                                                                                                                                                                                                                                                                                    10.1 Basic Definitions and Properties of Linear Maps\\n\\n139\\n\\nMoreover, for every λ ∈ K we have f −1 (λw1 ) = f −1 (λ f (v1 )) = f −1 ( f (λv1 )) = λv1 = λ f −1 (w1 ).\\n(3) and (4) are obvious from the corresponding definitions.\\n(5) Let f be injective and v ∈ ker( f ), i.e., f (v) = 0.\\nFrom (1) we know that f (0) = 0.\\nSince f (v) = f (0), the injectivity of f yields v = 0.\\nSuppose now that ker( f ) = {0} and let u, v ∈ V with f (u) = f (v).\\nThen f (u − v) = 0, i.e., u −\u0002 v ∈ ker( f ), which implies u − v = 0, i.e., u = v.\\nm\\nλi f (vi ) = 0.\\nThe linearity of f yields\\n(6) Let i=1 f m\\n\u0004\u0003 m\\n\u0005\\n\u0003\\nλi vi = 0, i.e.,\\nλi vi ∈ ker( f ).\\ni=1 i=1\\n\\n\u0002m\\nλi vi = 0 by (5), and hence λ1 = · · · =\\nSince f is injective, we have i=1\\nλm = 0 due to the linear independence of v1 , . . . , vm .\\nThus, f (v1 ), . . . , f (vm ) are linearly independent.\\n\u0002m\\nλi vi = 0 for some λ1 , . . . , λm ∈\\n(7) If v1 , . . . , vm are linearly dependent, then i=1\\nK that \u0002 are not all equal to zero.\\nApplying f on both sides and using the linearity m\\nλi f (vi ) = 0, hence f (v1 ), . . . , f (vm ) are linearly dependent.\\nyields i=1\\n(8) Let w ∈ im( f ) and u ∈ f −1 (w).\\nIf v ∈ f −1 (w), then f (v) = f (u), and thus f (v − u) = 0, i.e., v − u ∈ ker( f ) or v ∈ u + ker( f ).\\nThis shows that f −1 (w) ⊆ u + ker( f ).\\nIf, on the other hand, v ∈ u +ker( f ), then f (v) = f (u) = w, i.e., v ∈ f −1 (w).\\n\b\\nThis shows that u + ker( f ) ⊆ f −1 (w).\\nExample 10.8 Consider a matrix A ∈ K n,m and the corresponding map A ∈\\nL(K m,1 , K n,1 ) from (1) in Example 10.2.\\nFor a given b ∈ K n,1 we have A−1 (b) =\\nL (A, b).\\nIf b ∈\\n/ im(A), then L (A, b) = Ø (case (1) in Corollary 6.6).\\nNow suppose that b ∈ im(A) and let \u0006 x ∈ L (A, b) be arbitrary.\\nThen (8) in Lemma 10.7 yields\\nL (A, b) = \u0006 x + ker(A), which is the assertion of Lemma 6.2.\\nIf ker(A) = {0}, i.e., the columns of A are linearly independent, then |L (A, b)| = 1 (case (2) in Corollary 6.6).\\nIf ker(A) \u0007= {0}, i.e., the columns of A are linearly dependent, then |L (A, b)| > 1 (case (3) in\\nCorollary 6.6).\\nIf {w1 , . . . , w\u0002 } is a basis of ker(A), then\\n\u0002\\n\u0007\\n\u0003\\n\b\\nλi wi \b λ1 , . . . , λ\u0002 ∈ K .\\nL (A, b) = \u0006 x+ i=1\\n\\nThus, the solutions of Ax = b depend of \u0002 ≤ m parameters.   \n",
       "130                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         140\\n\\n10 Linear Maps\\n\\nThe following result, which gives an important dimension formula for linear maps, is also known as the rank-nullity theorem: The dimension of the image of f is equal to the rank of a matrix associated with f (cp.\\nTheorem 10.22 below), and the dimension of the kernel (or null space) of f is sometimes called the nullity1 of f .\\nTheorem 10.9 Let V and W be K -vector spaces and let V be finite dimensional.\\nThen for every f ∈ L(V, W) we have the dimension formula dim(V) = dim(im( f )) + dim(ker( f )).\\nProof Let v1 , . . . , vn ∈ V. If f (v1 ), . . . , f (vn ) ∈ W are linearly independent, then by (7) in Lemma 10.7 also v1 , . . . , vn are linearly independent, and thus dim(im( f )) ≤ dim(V).\\nSince ker( f ) ⊆ V, we have dim(ker( f )) ≤ dim(V), so that im( f ) and ker( f ) are both finite dimensional.\\nLet {w1 , . . . , wr } and {v1 , . . . , vk } be bases of im( f ) and ker( f ), respectively, and let u 1 ∈ f −1 (w1 ), . . . , u r ∈ f −1 (wr ).\\nWe will show that {u 1 , . . . , u r , v1 , . . . , vk } is a basis of V, which then implies the assertion.\\nIf v ∈ V, then\u0002by Lemma 9.22 there\u0002exist (unique) coordinates μ1 , . . . , μr ∈\\nK with f (v) = ri=1 μi wi .\\nLet v := ri=1 μi u i , then f (v) = f (v), and hence\\n\u0002k v − v ∈ ker( f ), which gives v − v = i=1\\nλi vi for some (unique) coordinates\\nλ1 , . . . , λk ∈ K .\\nTherefore, v=v + k\\n\u0003 r\\n\u0003\\n\\nλi vi = i=1\\n\\nμi u i + i=1 k\\n\u0003\\n\\nλi vi , i=1 and thus v ∈ span{u 1 , . . . , u r , v1 , . . . , vk }.\\nSince {u 1 , . . . , u r , v1 , . . . , vk } ⊂ V, we have\\nV = span{u 1 , . . . , u r , v1 , . . . , vk }, and it remains to show that u 1 , . . . , u r , v1 , . . . , vk are linearly independent.\\nIf r\\n\u0003\\n\\nαi u i + k\\n\u0003 i=1\\n\\nβi vi = 0, i=1 then\\n0 = f (0) = f r\\n\u0003 i=1\\n\\nαi u i + k\\n\u0003 i=1\\n\\nβi vi\\n\\n= r\\n\u0003\\n\\nαi f (u i ) = i=1 r\\n\u0003\\n\\nαi wi i=1 and thus α1 = · · · = αr = 0, because w1 , . . . , wr are linearly independent.\\nFinally,\\n\b the linear independence of v1 , . . . , vk implies that β1 = · · · = βk = 0.\\n1 This term was introduced in 1884 by James Joseph Sylvester (1814–1897).   \n",
       "131                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         10.1 Basic Definitions and Properties of Linear Maps\\n\\n141\\n\\nExample 10.10\\n(1) For the linear map\\n⎤\\n⎡ ⎤\\n\u0012\\n\u0012 α1\\n\u0011\\n\u0011\\nα1\\n1\\n0\\n1\\nα1 + α3\\n2,1\\n⎦\\n⎦\\n⎣\\n⎣\\nα2 \u0005→\\nα2 =\\n,\\n→Q ,\\nα1 + α3\\n101\\nα3\\nα3\\n⎡ f : Q3,1 we have\\n\\n⎧⎡\\n⎫\\n⎤\b\\n\u0014\\n\u0013\u0011 \u0012 \b\\nα1 \b\b\\n⎨\\n⎬\\nα \b\b\\n⎣ α2 ⎦ \b α1 , α2 ∈ Q .\\nα\\n∈\\nQ\\n, ker( f\\n)\\n= im( f ) =\\n\b\\nα \b\\n⎩\\n⎭\\n−α1 \b\\nHence dim(im( f )) = 1 and dim(ker( f )) = 2, so that indeed dim(im( f )) + dim(ker( f )) = dim(Q3,1 ).\\n(2) If A ∈ K n,m and A ∈ L(K m,1 , K n,1 ) are as in (1) in Example 10.2, then m = dim(K m,1 ) = dim(ker(A)) + dim(im(A)).\\nThus, dim(im(A)) = m if and only if dim(ker(A)) = 0.\\nThis holds if and only if ker(A) = {0}, i.e., if and only if the columns of A are linearly independent (cp.\\nExample 10.6).\\nIf, on the other hand, dim(im(A)) < m, then dim(ker(A)) = m − dim(im(A)) > 0, and thus ker(A) \u0007= {0}.\\nIn this case the columns of A are linearly dependent, since there exists an x ∈ K m,1 \\ {0} with Ax = 0.\\nCorollary 10.11 If V and W are K -vector spaces with dim(V) = dim(W) ∈ N and if f ∈ L(V, W), then the following statements are equivalent:\\n(1) f is injective.\\n(2) f is surjective.\\n(3) f is bijective.\\nProof If (3) holds, then (1) and (2) hold by definition.\\nWe now show that (3) is implied by (1) as well as by (2).\\nIf f is injective, then ker( f ) = {0} (cp.\\n(5) in Lemma 10.7) and the dimension formula of Theorem 10.9 yields dim(W) = dim(V) = dim(im( f )).\\nThus, im( f ) =\\nW (cp.\\nLemma 9.27), so that f is also surjective.\\nIf f is surjective, i.e., im( f ) = W, then the dimension formula and dim(W) = dim(V) yield dim(ker( f )) = dim(V) − dim(im( f )) = dim(W) − dim(im( f )) = 0.\\nThus, ker( f ) = {0}, so that f is also injective.\\n\\n\b\\n\\nUsing Theorem 10.9 we can also characterize when two finite dimensional vector spaces are isomorphic.   \n",
       "132  142\\n\\n10 Linear Maps\\n\\nCorollary 10.12 Two finite dimensional K -vector spaces V and W are isomorphic if and only if dim(V) = dim(W).\\nProof If V ∼\\n= W, then there exists a bijective map f ∈ L(V, W).\\nBy (4) and (5) in\\nLemma 10.7 we have im( f ) = W and ker( f ) = {0}, and the dimension formula of\\nTheorem 10.9 yields dim(V) = dim(im( f )) + dim(ker( f )) = dim(W) + dim({0}) = dim(W).\\nLet now dim(V) = dim(W).\\nWe need to show that there exists a bijective f ∈\\nL(V, W).\\nLet {v1 , . . . , vn } and {w1 , . . . , wn } be bases of V and W. By Theorem 10.4 there exists a unique f ∈ L(V, W) with f (vi ) = wi , i = 1, . . . , n.\\nIf v = λ1 v1 +\\n. . . + λn vn ∈ ker( f ), then\\n0 = f (v) = f (λ1 v1 + . . . + λn vn ) = λ1 f (v1 ) + . . . + λn f (vn )\\n= λ 1 w1 + . . . + λn wn .\\nSince w1 , . . . , wn are linearly independent, we have λ1 = · · · = λn = 0, hence v = 0 and ker( f ) = {0}.\\nThus, f is injective.\\nMoreover, the dimension formula yields dim(V) = dim(im( f )) = dim(W) and, therefore, im( f ) = W (cp.\\nLemma 9.27), so that f is also surjective.\\n\b\\nExample 10.13\\n(1) The vector spaces K n,m and K m,n both have the dimension n·m and are therefore isomorphic.\\nAn isomorphism is given by the linear map A \u0005→ A T .\\n(2) The R-vector spaces R1,2 and C = {x + iy | x, y ∈ R} both have the dimension 2 and are therefore isomorphic.\\nAn isomorphism is given by the linear map\\n[x, y] \u0005→ x + iy.\\n(3) The vector spaces Q[t]≤2 and Q1,3 both have dimension 3 and are therefore isomorphic.\\nAn isomorphism is given by the linear map α2 t 2 + α1 t + α0 \u0005→\\n[α2 , α1 , α0 ].\\nAlthough Mathematics is a formal and exact science, where smallest details matter, one sometimes uses an “abuse of notation” in order to simplify the presentation.\\nWe have used this for example in the inductive existence proof of the echelon form in Theorem 5.2.\\nThere we kept, for simplicity, the indices of the larger matrix A(1) in\\n(2) the smaller matrix A(2) = [ai(2) had, of course, an entry in position j ].\\nThe matrix A\\n(2)\\n(2)\\n.\\nKeeping the indices in the\\n(1, 1), but this entry was denoted by a22 rather than a11 induction made the argument much less technical, while the proof itself remained formally correct.\\nAn abuse of notation should always be justified and should not be confused with a “misuse” of notation.\\nIn the field of Linear Algebra a justification is often given by an isomorphism that identifies vector spaces with each other.\\nFor example, the constant polynomials over a field K , i.e., polynomials of the form αt 0 with α ∈ K , are often written simply as α, i.e., as elements of the field itself.\\nThis is justified since   \n",
       "133                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               10.1 Basic Definitions and Properties of Linear Maps\\n\\n143\\n\\nK [t]≤0 and K are isomorphic K -vector spaces (of dimension 1).\\nWe already used this identification above.\\nSimilarly, we have identified the vector space V with V 1 and written just v instead of (v) in Sect.\\n9.3.\\nAnother common example in the literature is the notation K n that in our text denotes the set of n-tuples with elements from\\nK , but which is often used for the (matrix) sets of the “column vectors” K n,1 or the\\n“row vectors” K 1,n .\\nThe actual meaning then should be clear from the context.\\nAn attentive reader can significantly benefit from the simplifications due to such abuses of notation.\\n\\n10.2 Linear Maps and Matrices\\nLet V and W be finite dimensional K -vector spaces with bases {v1 , . . . , vm } and\\n{w1 , . . . , wn }, respectively, and let f ∈ L(V, W).\\nBy Lemma 9.22, for every f (v j ) ∈\\nW, j = 1, . . . , m, there exist (unique) coordinates ai j ∈ K , i = 1, . . . , n, with f (v j ) = a1 j w1 + . . . + an j wn .\\nWe define A := [ai j ] ∈ K n,m and write, similarly to (9.3), the m equations for the vectors f (v j ) as\\n(10.1)\\n( f (v1 ), . . . , f (vm )) = (w1 , . . . , wn )A.\\nThe matrix A is determined uniquely by f and the given bases of V and W.\\nIf v = λ1 v1 + . . . + λm vm ∈ V, then f (v) = f (λ1 v1 + . . . + λm vm ) = λ1 f (v1 ) + . . . + λm f (vm )\\n⎡ ⎤\\nλ1\\n= ( f (v1 ), . . . , f (vm )) ⎣ ... ⎦\\nλm\\n⎤\\n\\n⎡\\n\\nλ1\\n= ((w1 , . . . , wn ) A) ⎣ ... ⎦\\nλm\\n⎛ ⎡ ⎤⎞\\nλ1\\n= (w1 , . . . , wn ) ⎝ A ⎣ ... ⎦⎠ .\\nλm\\n\\nThe coordinates of f (v) with respect to the given basis of W are therefore given by\\n⎤\\nλ1\\n.\\nA ⎣ .. ⎦ .\\nλm\\n⎡   \n",
       "134                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  144\\n\\n10 Linear Maps\\n\\nThus, we can compute the coordinates of f (v) simply by multiplying the coordinates of v with A. This motivates the following definition.\\nDefinition 10.14 The uniquely determined matrix in (10.1) is called the matrix representation of f ∈ L(V, W) with respect to the bases B1 = {v1 , . . . , vm } of V and\\nB2 = {w1 , . . . , wn } of W. We denote this matrix by [ f ] B1 ,B2 .\\nThe construction of the matrix representation and Definition 10.14 can be consistently extended to the case that (at least) one of the K -vector spaces has dimension zero.\\nIf, for instance, m = dim(V) ∈ N and W = {0}, then f (v j ) = 0 for every basis vector v j of V. Thus, every vector f (v j ) is an empty linear combination of vector of the basis Ø of W. The matrix representation of f then is an empty matrix of size 0 × m.\\nIf also V = {0}, then the matrix representation of f is an empty matrix of size 0 × 0.\\nThere are many different notations for the matrix representation of linear maps in the literature.\\nThe notation should reflect that the matrix depends on the linear map f and the given bases B1 and B2 .\\nExamples of alternative notations are [ f ] BB12 and\\nM( f ) B1 ,B2 (where “M” means “matrix”).\\nAn important special case is obtained for V = W, hence in particular m = n, and f = IdV , the identity on V. We then obtain\\n(v1 , . . . , vn ) = (w1 , . . . , wn )[IdV ] B1 ,B2 ,\\n\\n(10.2) so that [IdV ] B1 ,B2 is exactly the matrix P in (9.4), i.e., the coordinate transformation matrix in Theorem 9.25.\\nOn the other hand,\\n(w1 , . . . , wn ) = (v1 , . . . , vn ) [IdV ] B2 ,B1 , and thus\\n\u001f\\n[IdV ] B1 ,B2\\n\\n−1\\n\\n= [IdV ] B2 ,B1 .\\n\\nExample 10.15\\n(1) Consider the vector space Q[t]≤1 with the bases B1 = {1, t} and B2 = {t +\\n1, t − 1}.\\nThen the linear map f : Q[t]≤1 → Q[t]≤1 , α1 t + α0 \u0005→ 2α1 t + α0 , has the matrix representations\\n!\\n!\\n\"\\n\u0012\\n1\\n3\\n1\\n10\\n2\\n2\\n, [ f ] B1 ,B2 =\\n=\\n=\\n,\\n[ f\\n]\\nB2 ,B2\\n1\\n02\\n− 21 1\\n2\\n\u0011\\n\\n[ f ] B1 ,B1\\n\\n1\\n2\\n3\\n2\\n\\n\"\\n.\\n\\n(2) For the vector space K [t]≤n with the basis B = {t 0 , t 1 , . . . , t n } and the linear map   \n",
       "135                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        10.2 Linear Maps and Matrices\\n\\n145 f : K [t]≤n → K [t]≤n ,\\nαn t n + αn−1 t n−1 + . . . + α1 t + α0 \u0005→ α0 t n + α1 t n−1 + . . . + αn−1 t + αn , we have f (t j ) = t n− j for j = 0, 1, . . . , n, so that\\n⎡\\n\\n1\\n\\n⎤\\n\\n[ f ] B,B = ⎣ . . . ⎦ ∈ K n+1,n+1 .\\n1\\nThus, [ f ] B,B is a permutation matrix.\\nTheorem 10.16 Let V and W be finite dimensional K -vector spaces with bases\\nB1 = {v1 , . . . , vm } and B2 = {w1 , . . . , wn }, respectively.\\nThen the map\\nL(V, W) → K n,m , f \u0005→ [ f ] B1 ,B2 , is an isomorphism.\\nHence L(V, W) ∼\\n= K n,m and dim(L(V, W)) = dim(K n,m ) = n · m.\\nProof In this proof we denote the map f \u0005→ [ f ] B1 ,B2 by mat, i.e., mat( f ) =\\n[ f ] B1 ,B2 .\\nWe first show that this map is linear.\\nLet f, g ∈ L(V, W), mat( f ) = [ f i j ] and mat(g) = [gi j ].\\nFor j = 1, . . . , m we have\\n( f + g)(v j ) = f (v j ) + g(v j ) = n\\n\u0003 f i j wi + i=1 n\\n\u0003 gi j wi = i=1 n\\n\u0003\\n( f i j + gi j )wi , i=1 and thus mat( f + g) = [ f i j + gi j ] = [ f i j ] + [gi j ] = mat( f ) + mat(g).\\nFor λ ∈ K and j = 1, . . . , m we have\\n(λ f )(v j ) = λ f (v j ) = λ n\\n\u0003 i=1 f i j wi = n\\n\u0003\\n(λ f i j )wi , i=1 and thus mat(λ f ) = [λ f i j ] = λ [ f i j ] = λ mat( f ).\\nIt remains to show that mat is bijective.\\nIf f ∈ ker(mat), i.e., mat( f ) = 0 ∈ K n,m , then f (v j ) = 0 for j = 1, . . . , m.\\nThus, f (v) = 0 for all v ∈ V, so that f = 0\\n(the zero map) and mat is injective (cp.\\n(5) in Lemma 10.7).\\nIf, on the other hand,\\nA = [ai j ] ∈ K n,m is arbitrary, we define the linear map f : V → W via f (v j ) :=\\n\u0002 n i=1 ai j wi , j = 1, . . . , m (cp. the proof of Theorem 10.4).\\nThen mat( f ) = A and hence mat is also surjective (cp.\\n(4) in Lemma 10.7).\\nCorollary 10.12 now shows that dim(L(V, W)) = dim(K n,m ) = n · m (cp. also\\nExample 9.20).\\n\b   \n",
       "136                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    146\\n\\n10 Linear Maps\\n\\nTheorem 10.16 shows, in particular, that f, g ∈ L(V, W) satisfy f = g if and only if [ f ] B1 ,B2 = [g] B1 ,B2 holds for given bases B1 of V and B2 of W. Thus, we can prove the equality of linear maps via the equality of their matrix representations.\\nWe now consider the map from the elements of a finite dimensional vector space to their coordinates with respect to a given basis.\\nLemma 10.17 If B = {v1 , . . . , vn } is a basis of a K -vector space V, then the map\\n⎡\\n\\n\u0003B\\n\\n⎤\\nλ1\\n: V → K n,1 , v = λ1 v1 + . . . + λn vn \u0005→ \u0003 B (v) := ⎣ ... ⎦ ,\\nλn is an isomorphism, called the coordinate map of V with respect to the basis B.\\nProof The linearity of \u0003 B is clear.\\nMoreover, we obviously have \u0003 B (V) = K n,1 , i.e., \u0003 B is surjective.\\nIf v ∈ ker(\u0003 B ), i.e., λ1 = · · · = λn = 0, then v = 0, so that\\n\b ker(\u0003 B ) = {0} and \u0003 B is also injective (cp.\\n(5) in Lemma 10.7).\\nExample 10.18 have\\n\\nIn the vector space K [t]≤n with the basis B = {t 0 , t 1 , . . . , t n } we\\n\\n⎤\\nα0\\n⎢α1 ⎥ n+1\\n⎥\\n\u0003 B (αn t n + αn−1 t n−1 + . . . + α1 t + α0 ) = ⎢\\n⎣ ... ⎦ ∈ K .\\n⎡\\n\\nαn\\n\\nOn the other hand, the basis B = {t n , t n−1 , . . . , t 0 } yields\\n⎤\\nαn\\n⎢αn−1 ⎥ n+1\\n⎥\\n\u0003 B (αn t n + αn−1 t n−1 + . . . + α1 t + α0 ) = ⎢\\n⎣ ... ⎦ ∈ K .\\n⎡\\n\\nα0\\n\\nIf B1 and B2 are bases of the finite dimensional vector spaces V and W, respectively, then we can illustrate the meaning and the construction of the matrix representation [ f ] B1 ,B2 of f ∈ L(V, W) in the following commutative diagram:\\n\\nV\\n\u0003 B1 f\\n\\n/W\\n\\n\u0003 B2\\n[ f ] B1 ,B2 \u000f\\n/\\nK m,1\\nK n,1\\n\u000f\\n\\nWe see that different compositions of maps yield the same result.\\nIn particular, we have\\n(10.3) f = \u0003−1\\nB2 ◦ [ f ] B1 ,B2 ◦ \u0003 B1 ,   \n",
       "137                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               10.2 Linear Maps and Matrices\\n\\n147 where the matrix [ f ] B1 ,B2 ∈ K n,m is interpreted as a linear map from K m,1 to K n,1 , and we use that the coordinate map \u0003 B2 is bijective and hence invertible.\\nIn the same way we obtain\\n\u0003 B2 ◦ f = [ f ] B1 ,B2 ◦ \u0003 B1 , i.e.,\\n\u0003 B2 ( f (v)) = [ f ] B1 ,B2 \u0003 B1 (v) for all v ∈ V.\\n\\n(10.4)\\n\\nIn words, the coordinates of f (v) with respect to the basis B2 of W are given by the product of [ f ] B1 ,B2 and the coordinates of v with respect to the basis B1 of V.\\nWe next show that the consecutive application of linear maps corresponds to the multiplication of their matrix representations.\\nTheorem 10.19 Let V, W and X be K -vector spaces.\\nIf f ∈ L(V, W) and g ∈\\nL(W, X ), then g ◦ f ∈ L(V, X ).\\nMoreover, if V, W and X are finite dimensional with respective bases B1 , B2 and B3 , then\\n[g ◦ f ] B1 ,B3 = [g] B2 ,B3 [ f ] B1 ,B2 .\\nProof Let h := g ◦ f .\\nWe show first that h ∈ L(V, X ).\\nFor u, v ∈ V and λ, μ ∈ K we have h(λu + μv) = g( f (λu + μv)) = g(λ f (u) + μ f (v))\\n= λg( f (u)) + μg( f (v)) = λh(u) + μh(v).\\nNow let B1 = {v1 , . . . , vm }, B2 = {w1 , . . . , wn } and B3 = {x1 , . . . , xs }.\\nIf\\n[ f ] B1 ,B2 = [ f i j ] and [g] B2 ,B3 = [gi j ], then for j = 1, . . . , m we have h(v j ) = g( f (v j )) = g n\\n\u0003 f k j wk\\n\\n= k=1\\n\\n= s n\\n\u0003\\n\u0003 i=1 k=1 f k j gik xi = n\\n\u0003 s\\n\u0003 k=1 n\\n\u0003 i=1 k=1 f k j g(wk ) = k=1 gik f k j\\n\\n%\\n\\n&'\\n=: h i j n\\n\u0003 fk j s\\n\u0003 gik xi i=1 xi .\\n(\\n\\nThus, [h] B1 ,B3 = [h i j ] = [gi j ] [ f i j ] = [g] B2 ,B3 [ f ] B1 ,B2 .\\n\\n\b\\n\\nUsing this theorem we can study how a change of the bases affects the matrix representation of a linear map.   \n",
       "138                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 148\\n\\n10 Linear Maps\\n\\nCorollary 10.20 Let V and W be finite dimensional K -vector spaces with bases\\nB1 , B1 of V and B2 , B2 of W. If f ∈ L(V, W), then\\n[ f ] B1 ,B2 = [IdW ] B2 ,B2 [ f ] B1 , B2 [IdV ] B1 , B1 .\\n\\n(10.5)\\n\\nIn particular, the matrices [ f ] B1 ,B2 and [ f ] B1 , B2 are equivalent.\\nProof Applying Theorem 10.19 twice to the identity f = IdW ◦ f ◦ IdV yields\\n[ f ] B1 ,B2 = [(IdW ◦ f ) ◦ IdV ] B1 ,B2\\n= [IdW ◦ f ] B1 ,B2 [IdV ] B1 , B1\\n= [IdW ] B2 ,B2 [ f ] B1 , B2 [IdV ] B1 , B1 .\\nThe matrices [ f ] B1 ,B2 and [ f ] B1 , B2 are equivalent, since both [IdW ] B2 ,B2 and [IdV ] B1 , B1 are invertible.\\n\b\\nIf V = W, B1 = B2 , and B1 = B2 , then (10.5) becomes\\n[ f ] B1 ,B1 = [IdV ] B1 ,B1 [ f ] B1 , B1 [IdV ] B1 , B1 = ([IdV ] B1 , B1 )−1 [ f ] B1 , B1 [IdV ] B1 , B1 .\\nThus, the matrix representations [ f ] B1 ,B1 and [ f ] B1 , B1 of the endomorphism f ∈\\nL(V, V) are similar (cp.\\nDefinition 8.11).\\nThe following commutative diagram illustrates Corollary 10.20:\\n\\n[ f ] B1 ,B2\\n\\nK m,1 bEE \u0003\\nEE B1\\nEE\\nEE\\n\\n[IdV ] B\\n\\n1 , B1\\n\\n\u000f\\n\\n\u0003B yV y\\n1 yy y y y| y\\n\\nK m,1\\n\\n/\\nK n,1\\n\u0003 B2 yyy< O f /\\nW\\n\\n[ f ]B\\n\\n1 , B2 y yy yy\\n\\n(10.6)\\n\\n[IdW ] B\\n\\nEE \u0003\\nEE B2\\nEE\\nEE\\n\"\\n/ n,1\\n\\n2 ,B2\\n\\nK\\n\\nAnalogously to (10.3) we have\\n−1 f = \u0003−1\\nB2 ◦ [ f ] B1 ,B2 ◦ \u0003 B1 = \u0003 B ◦ [ f ] B1 , B2 ◦ \u0003 B1 .\\n2\\n\\nExample 10.21 For the following bases of the vector space Q2,2 ,\\n\u0013\u0011\\n\\n\u0012\\n10\\nB1 =\\n,\\n00\\n\u0013\u0011 \u0012\\n10\\n,\\nB2 =\\n01\\n\\n\u0011 \u0012\\n01\\n,\\n00\\n\u0011 \u0012\\n10\\n,\\n00\\n\\n\u0011 \u0012\\n00\\n,\\n10\\n\u0011 \u0012\\n11\\n,\\n00\\n\\n\u0011 \u0012\u0014\\n00\\n,\\n01\\n\u0011 \u0012\u0014\\n00\\n,\\n10   \n",
       "139                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            10.2 Linear Maps and Matrices\\n\\n149 we have the coordinate transformation matrices\\n⎡\\n0 0\\n⎢ 1 −1\\n⎢\\n[IdV ] B1 ,B2 = ⎣\\n0 1\\n0 0\\n\\n⎤\\n0 1\\n0 −1 ⎥\\n⎥\\n0 0⎦\\n1 0 and\\n⎡\\n\\n[IdV ] B2 ,B1 = ([IdV ] B1 ,B2 )−1\\n\\n11\\n⎢0 0\\n⎢\\n=⎣\\n00\\n10\\n\\n1\\n1\\n0\\n0\\n\\n⎤\\n0\\n0⎥\\n⎥.\\n1⎦\\n0\\n\\nThe coordinate maps are\\n⎡ ⎤\\n⎡\\n⎤ a11 a22\\n)\u0011\\n\u0012*\\n)\u0011\\n\u0012*\\n⎢a12 ⎥\\n⎢a11 − a12 − a22 ⎥ a11 a12 a11 a12\\n⎥\\n⎥,\\n=⎢\\n=⎢\\n\u0003 B1\\n⎣a21 ⎦ , \u0003 B2 a21 a22\\n⎣\\n⎦ a21 a22 a12 a22 a21 and one can easily verify that\\n\u0003 B2\\n\\n)\u0011\\n)\u0011\\n\u0012*\\n\u0012* a11 a12 a11 a12\\n= ([IdV ] B1 ,B2 ◦ \u0003 B1 )\\n.\\na21 a22 a21 a22\\n\\nTheorem 10.22 Let V and W be K -vector spaces with dim(V) = m and dim(W) = n, respectively.\\nThen there exist bases B1 of V and B2 of W such that\\n\u0011\\n[ f ] B1 ,B2 =\\n\\n\u0012\\nIr 0\\n∈ K n,m ,\\n0 0 where 0 ≤ r = dim(im( f )) ≤ min{n, m}.\\nFurthermore, r = rank(F), where F is the matrix representation of f with respect to arbitrary bases of V and W, and we define rank( f ) := rank(F) = dim(im( f )).\\nProof Let B1 = {v1 , . . . , vm } and B2 = {w1 , . . . , wn } be two arbitrary bases of V and W, respectively.\\nLet r := rank([ f ] B1 , B2 ).\\nThen by Theorem 5.11 there exist invertible matrices Q ∈ K n,n and Z ∈ K m,m with\\n\u0011\\nQ [ f ] B1 , B2 Z =\\n\\n\u0012\\nIr 0\\n,\\n0 0\\n\\n(10.7)   \n",
       "140                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               150\\n\\n10 Linear Maps where r = rank([ f ] B1 , B2 ) ≤ min{n, m}.\\nLet us introduce two new bases B1 =\\n{v1 , . . . , vm } and B2 = {w1 , . . . , wn } of V and W via\\n(v1 , . . . , vm ) := (v1 , . . . , vm )Z ,\\n(w1 , . . . , wn ) := (w1 , . . . , wn )Q −1 , hence (w1 , . . . , wn ) = (w1 , . . . , wn )Q.\\nThen, by construction,\\nZ = [IdV ] B1 , B1 ,\\n\\nQ = [IdW ] B2 ,B2 .\\n\\nFrom (10.7) and Corollary 10.20 we obtain\\n\u0011\\n\\nIr 0\\n0 0\\n\\n\u0012\\n= [IdW ] B2 ,B2 [ f ] B1 , B2 [IdV ] B1 , B1 = [ f ] B1 ,B2 .\\n\\nWe thus have found bases B1 and B2 that yield the desired matrix representation of f .\\nEvery other choice of bases leads, by Corollary 10.20, to an equivalent matrix which therefore also has rank r .\\nIt remains to show that r = dim(im( f )).\\nThe structure of the matrix [ f ] B1 ,B2 shows that\\n+ f (v j ) = w j , 1 ≤ j ≤ r,\\n0, r + 1 ≤ j ≤ m.\\n\\nTherefore, vr +1 , . . . , vm ∈ ker( f ), which implies that dim(ker( f )) ≥ m − r .\\nOn the other hand, w1 , . . . , w j ∈ im( f ) and thus dim(im( f )) ≥ r .\\nTheorem 10.9 yields dim(V) = m = dim(im( f )) + dim(ker( f )), and hence dim(ker( f )) = m − r and dim(im( f )) = r .\\n\\n\b\\n\\nExample 10.23 For A ∈ K n,m and the corresponding map A ∈ L(K m,1 , K n,1 ) from\\n(1) in Examples 10.2 and 10.6, we have im(A) = span{a1 , . . . , am }.\\nThus, rank(A) is equal to the number of linearly independent columns of A. Since rank(A) = rank(A T ) (cp.\\n(4) in Theorem 5.11), this number is equal to the number of linearly independent rows of A.\\nTheorem 10.22 is a first example of a general strategy that we will use several times in the following chapters:\\nBy choosing appropriate bases, the matrix representation should reveal a desired information about a linear map in an efficient way.\\nIn Theorem 10.22 this information is the rank of the linear map f , i.e., the dimension of its image.\\nThe dimension formula for linear maps can be generalized to the composition of maps as follows.   \n",
       "141                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 10.2 Linear Maps and Matrices\\n\\n151\\n\\nTheorem 10.24 If V, W and X are finite dimensional K -vector spaces, f ∈ L(V, W) and g ∈ L(W, X ), then dim(im(g ◦ f )) = dim(im( f )) − dim(im( f ) ∩ ker(g)).\\nProof Let g := g|im( f ) be the restriction of g to the image of f , i.e., the map g ∈ L(im( f ), X ), v \u0005→ g(v).\\nApplying Theorem 10.9 to g yields dim(im( f )) = dim(im(g)) + dim(ker(g)).\\nNow im(g) = {g(v) ∈ X | v ∈ im( f )} = im(g ◦ f ) and ker(g) = {v ∈ im( f ) | g(v) = 0} = im( f ) ∩ ker(g),\\n\b imply the assertion.\\n\\nNote that Theorem 10.22 with V = W, f = IdV , and g ∈ L(V, X ) gives dim(im(g)) = dim(V) − dim(ker(g), which is equivalent to Theorem 10.9.\\nIf we interpret matrices A ∈ K n,m and B ∈ K s,n as linear maps, then Theorem 10.24 implies the equation rank(B A) = rank(A) − dim(im(A) ∩ ker(B)).\\nFor the special case K = R and B = A T we have the following result.\\nCorollary 10.25 If A ∈ Rn,m , then rank(A T A) = rank(A).\\nProof Let w = [ω1 , . . . , ωn ]T ∈ im(A) ∩ ker(A T ).\\nThen w = Ay for a vector y ∈ Rm,1 .\\nMultiplying this equation from the left by A T , and using that w ∈ ker(A T ), we obtain 0 = A T w = A T Ay, which implies\\n0 = y A Ay = w w =\\nT\\n\\nT\\n\\nT n\\n\u0003\\n\\nω 2j .\\nj=1\\n\\nSince this holds only for w = 0, we have im(A) ∩ ker(A T ) = {0}.\\n\\n\b   \n",
       "142                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      152\\n\\n10 Linear Maps\\n\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n\\n⎡\\n⎤\\n201\\n10.1 Consider the linear map on R3,1 given by the matrix A = ⎣2 1 0⎦ ∈ R3,3 .\\n411\\nDetermine ker(A), dim(ker(A)) and dim(im(A)).\\n10.2 Construct a map f ∈ L(V, W) such that for linearly independent vectors v1 , . . . , vr ∈ V the images f (v1 ), . . . , f (vr ) ∈ W are linearly dependent.\\n10.3 The map f : R[t]≤n → R[t]≤n−1 ,\\nαn t n + αn−1 t n−1 + . . . + α1 t + α0 \u0005→ nαn t n−1 + (n − 1)αn−1 t n−2 + . . . + 2α2 t + α1 , is called the derivative of the polynomial p ∈ R[t]≤n with respect to the variable t.\\nShow that⎧⎡ f is⎤linear determine\\n⎡ and\\n⎤ ⎡\\n⎤⎫ ker( f ) and im( f ).\\n\u0013\u0011 \u0012 \u0011 \u0012\u0014\\n0\\n0 ⎬\\n⎨ 1\\n1\\n0\\n10.4 For the bases B1 = ⎣ 0 ⎦ , ⎣ 1 ⎦ , ⎣ 0 ⎦ of R3,1 and B2 =\\n,\\n0\\n1\\n⎩\\n⎭\\n0\\n0\\n1\\n3,1\\n2,1 of R2,1 , let\\n\u0011\\n\u0012 f ∈ L(R , R ) have the matrix representation [ f ] B1 ,B2 =\\n0 23\\n.\\n1 −2 0\\n⎧⎡\\n⎤ ⎡ ⎤ ⎡\\n⎤⎫\\n2\\n1\\n−1 ⎬\\n⎨\\n(a) Determine [ f ] B1 , B2 for the bases B1 = ⎣ 1 ⎦ , ⎣ 0 ⎦ , ⎣ 2 ⎦ of\\n⎩\\n⎭\\n−1\\n3\\n1\\n\u0013\u0011 \u0012 \u0011\\n\u0012\u0014\\n1\\n1\\n, of R2,1 .\\nR3,1 and B2 =\\n1\\n−1\\n(b) Determine the coordinates of f ([4, 1, 3]T ) with respect to the basis B2 .\\n10.5 Construct a map f ∈ L(K [t], K [t]) with the following properties:\\n(1) f ( pq) = ( f ( p))q + p( f (q)) for all p, q ∈ K [t].\\n(2) f (t) = 1.\\nIs this map uniquely determined by these properties or are there further maps with the same properties?\\n10.6 Let α ∈ K and A ∈ K n,n .\\nShow that the maps\\nK [t] → K , p \u0005→ p(α), and\\n\\nK [t] → K m,m , p \u0005→ p(A), are linear and justify the name evaluation homomorphism for this map.\\n10.7 Let S ∈ G L n (K ).\\nShow that the map f : K n,n → K n,n , A \u0005→ S −1 AS is an isomorphism.   \n",
       "143                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          10.2 Linear Maps and Matrices\\n\\n153\\n\\n10.8 Let K be a field with 1 + 1 \u0007= 0 and let A ∈ K n,n .\\nConsider the map f : K n,1 → K , x \u0005→ x T Ax.\\nIs f a linear map?\\nShow that f = 0 if and only if A + A T = 0.\\n10.9 Let V be a Q-vector space with the basis B1 = {v1 , . . . , vn } and let f ∈\\nL(V, V) be defined by\\n+ f (v j ) = v j + v j+1 , j = 1, . . . , n − 1, j = n.\\nv1 + vn ,\\n\\n(a) Determine [ f ] B1 ,B1 .\\n(b) Let B2 = {w1 , . . . , wn } with w j = jvn+1− j , j = 1, . . . , n.\\nShow that\\nB2 is a basis of V. Determine the coordinate transformation matrices\\n[IdV ] B1 ,B2 and [IdV ] B2 ,B1 , as well as the matrix representations [ f ] B1 ,B2 and [ f ] B2 ,B2 .\\n10.10 Can you extend Theorem 10.19 consistently to the case W = {0}?\\nWhat are the properties of the matrices [g ◦ f ] B1 ,B3 , [g] B2 ,B3 and [ f ] B1 ,B2 ?\\n10.11 Consider the map f : R[t]≤n → R[t]≤n+1 ,\\nαn t n + αn−1 t n−1 + . . . + α1 t + α0 \u0005→\\n\\n1\\nαn t n+1 n+1\\n1\\n1\\n+ αn−1 t n + . . . + α1 t 2 + α0 t.\\nn\\n2\\n\\n(a) Show that f is linear.\\nDetermine ker( f ) and im( f ).\\n(b) Choose bases B1 , B2 in the two vector spaces and verify that for your choice rank([ f ] B1 ,B2 ) = dim(im( f )) holds.\\n10.12 Let α1 , . . . , αn ∈ R, n ≥ 2, be pairwise distinct numbers and let n polynomials in R[t] be defined by pj = n )\\n, k=1 k\u0007 = j\\n\\n*\\n1\\n(t − αk ) ,\\nα j − αk j = 1, . . . , n.\\n\\n(a) Show that the set B ={ p1 , . . . , pn } is a basis of R[t]≤n−1 .\\n(This basis is called the Lagrange basis2 of R[t]≤n−1 .)\\n(b) Show that the corresponding coordinate map is given by\\n\\n2 Joseph-Louis de Lagrange (1736–1813).   \n",
       "144                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               154\\n\\n10 Linear Maps\\n\\n⎡\\n\\n\u0003 B : R[t]≤n−1 → Rn,1 ,\\n\\n⎤ p(α1 ) p \u0005→ ⎣ ... ⎦ .\\np(αn )\\n\\n(Hint: You can use Exercise 7.8 (b).)\\n10.13 Verify different paths in the commutative diagram (10.6) for the vector spaces\\n2,2\\n2,2 and bases\\n\u0011 of \u0012Example 10.21 and linear map f : Q → Q , A \u0005→ F A with\\n11\\n.\\nF=\\n−1 1   \n",
       "145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Chapter 11\\n\\nLinear Forms and Bilinear Forms\\n\\nIn this chapter we study different classes of maps between one or two K -vector spaces and the one dimensional K -vector space defined by the field K itself.\\nThese maps play an important role in many areas of Mathematics, including Analysis, Functional\\nAnalysis and the solution of differential equations.\\nThey will also be essential for the further developments in this book: Using bilinear and sesquilinear forms, which are introduced in this chapter, we will define and study Euclidean and unitary vector spaces in Chap.\\n12.\\nLinear forms and dual spaces will be used in the existence proof of the Jordan canonical form in Chap.\\n16.\\n\\n11.1 Linear Forms and Dual Spaces\\nWe start with the set of linear maps from a K -vector space to the vector space K .\\nDefinition 11.1 If V is a K -vector space, then f ∈ L(V, K ) is called a linear form on V. The K -vector space V ∗ := L(V, K ) is called the dual space of V.\\nA linear form is sometimes called a linear functional or a one-form, which stresses that it (linearly) maps into a one dimensional vector space.\\nExample 11.2 If V is the R-vector space of the continuous and real valued functions on the real interval [α, β] and if γ ∈ [α, β], then the two maps f 1 : V → R, g \u0005→ g(γ),\\n\u0002 β f 2 : V → R, g \u0005→ g(x)d x,\\nα are linear forms on V.\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_11\\n\\n155   \n",
       "146                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         156\\n\\n11 Linear Forms and Bilinear Forms\\n\\nIf dim(V) = n, then dim(V ∗ ) = n by Theorem 10.16.\\nLet B1 = {v1 , . . . , vn } be a basis of V and let B2 = {1} be a basis of the K -vector space K .\\nIf f ∈ V ∗ , then f (vi ) = αi for some αi ∈ K , i = 1, . . . , n, and\\n[ f ] B1 ,B2 = [α1 , . . . , αn ] ∈ K 1,n .\\nFor an element v = n\\n\u0003\\n\\nλi vi ∈ V we have i=1 f (v) = f n\\n\u0004\u0005 i=1\\n\\nλi vi\\n\\n\u0006\\n\\n⎡\\n\\n⎤\\nλ1\\n⎢ ⎥\\n=\\nλi f (vi ) =\\nλi αi = [α1 , . . . , αn ] ⎣ ... ⎦\\n\u0007\\n\b i=1 i=1\\nλn\\n∈K 1,n\\n\u0007\b n\\n\u0005 n\\n\u0005\\n\\n∈K n,1\\n\\n= [ f ] B1 ,B2 \u0002 B1 (v), where we have identified the isomorphic vector spaces K and K 1,1 with each other.\\nFor a given basis of a finite dimensional vector space V we will now construct a special, uniquely determined basis of the dual space V ∗ .\\nTheorem 11.3 If V is K -vector space \u0012with the basis B = {v1 , . . . , vn }, then there\\n\u0011 exists a unique basis B ∗ = v1∗ , . . . , vn∗ of V ∗ such that vi∗ (v j ) = δi j , i, j = 1, . . . , n, which is called the dual basis of B.\\nProof By Theorem 10.4, a unique linear map from V to K can be constructed by prescribing its images at the given basis B. Thus, for each i = 1, . . . , n, there exists a unique map vi∗ ∈ L(V, K ) with vi∗ (v j ) = δi j , j = 1, . . . , n.\\nIt remains to show that B ∗ := {v1∗ , . . . , vn∗ } is a basis of V ∗ .\\nIf λ1 , . . . , λn ∈ K are such that n\\n\u0005\\nλi vi∗ = 0V ∗ ∈ V ∗ , i=1 then\\n0 = 0V ∗ (v j ) = n\\n\u0005\\n\\nλi vi∗ (v j ) = λ j , j = 1, . . . , n.\\ni=1\\n\\nThus, v1∗ , . . . , vn∗ are linearly independent, and dim(V ∗ ) = n implies that B ∗ is a\\n\u0006\\n\u0007 basis of V ∗ (cp.\\nExercise 9.6).\\nn,1\\nExample\\n11.4\\n\u0012 Consider V = K with ∗the canonical basis B = {e1 , \u0013. . ∗. \u0014, en }.\\nIf\\n\u0011 ∗\\n∗ e1 , . . . , en is the dual basis of B, then ei (e j ) = δi j , which shows that ei B,{1} = eiT ∈ K 1,n , i = 1, . . . , n.   \n",
       "147                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  11.1 Linear Forms and Dual Spaces\\n\\n157\\n\\nDefinition 11.5 Let V and W be K -vector spaces with their respective dual spaces\\nV ∗ and W ∗ , and let f ∈ L(V, W).\\nThen f ∗ : W ∗ → V ∗ , h \u0005→ f ∗ (h) := h ◦ f, is called the dual map of f .\\nWe next derive some properties of the dual map.\\nLemma 11.6 If V, W and X are K -vector spaces, then the following assertions hold:\\n(1) If f ∈ L(V, W), then the dual map f ∗ is linear, hence f ∗ ∈ L(W ∗ , V ∗ ).\\n(2) If f ∈ L(V, W) and g ∈ L(W, X ), then (g ◦ f )∗ ∈ L(X ∗ , V ∗ ) and (g ◦ f )∗ = f ∗ ◦ g∗ .\\n(3) If f ∈ L(V, W) is bijective, then f ∗ ∈ L(W ∗ , V ∗ ) is bijective and ( f ∗ )−1 =\\n( f −1 )∗ .\\nProof (1) If h 1 , h 2 ∈ W ∗ , λ1 , λ2 ∈ K , then f ∗ (λ1 h 1 + λ2 h 2 ) = (λ1 h 1 + λ2 h 2 ) ◦ f = (λ1 h 1 ) ◦ f + (λ2 h 2 ) ◦ f\\n= λ1 (h 1 ◦ f ) + λ2 (h 2 ◦ f ) = λ1 f ∗ (h 1 ) + λ2 f ∗ (h 2 ).\\n\\n\u0007\\n\u0006\\n(2) and (3) are exercises.\\nAs the following theorem shows, the concepts of the dual map and the transposed matrix are closely related.\\nTheorem 11.7 Let V and W be finite dimensional K -vector spaces with bases\\nB1 and B2 , respectively.\\nLet B1∗ and B2∗ be the corresponding dual bases.\\nIf f ∈ L(V, W), then\\n[ f ∗ ] B2∗ ,B1∗ = ([ f ] B1 ,B2 )T .\\n\u0011\\n\u0012\\nProof \u0011Let B1 = {v\u00121 , . . . , vm }, B2 = {w1 , . . . , wn }, and let B1∗ = v1∗ , . . . , vm∗ ,\\nB2∗ = w1∗ , . . . , wn∗ .\\nLet [ f ] B1 ,B2 = [ai j ] ∈ K n,m , i.e., f (v j ) = n\\n\u0005 ai j wi , j = 1, . . . , m, i=1 and [ f ∗ ] B2∗ ,B1∗ = [bi j ] ∈ K m,n , i.e., m\\n\u0015 \u0016 \u0005 f ∗ w ∗j = bi j vi∗ , i=1 j = 1, . . . , n.   \n",
       "148                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               158\\n\\n11 Linear Forms and Bilinear Forms\\n\\nFor every pair (k, \u0003) with 1 ≤ k ≤ n and 1 ≤ \u0003 ≤ m we then have ak\u0003 = n\\n\u0005 ai\u0003 wk∗ (wi ) = wk∗ n\\n\u0004\u0005 i=1\\n\\n= m\\n\u0004\u0005\\n\\n\u0006\\n\u0015 \u0016 ai\u0003 wi = wk∗ ( f (v\u0003 )) = f ∗ wk∗ (v\u0003 ) i=1 m\\n\u0006\\n\u0005 bik vi∗ (v\u0003 ) = bik vi∗ (v\u0003 ) i=1 i=1\\n\\n= b\u0003k , where we have used the definition of the dual map as well as wk∗ (wi ) = δki and\\n\u0007\\n\u0006 vi∗ (v\u0003 ) = δi\u0003 .\\nBecause of the close relationship between the transposed matrix and the dual map, some authors call the dual map f ∗ the transpose of the linear map f .\\nApplied to matrices, Lemma 11.6 and Theorem 11.7 yield the following rules known from Chap.\\n4:\\n(AB)T = B T A T for A ∈ K n,m and B ∈ K m,\u0003 , and\\n(A−1 )T = (A T )−1 for A ∈ G L n (K ).\\nExample 11.8 For the two bases of R2,1 ,\\n\u0017\\n\u0018 \u0019\\n\u0018 \u0019\u001a\\n1\\n0\\nB1 = v1 =\\n, v2 =\\n,\\n0\\n2\\n\\n\u0017\\n\u0018 \u0019\\n\u0018 \u0019\u001a\\n1\\n1\\nB2 = w1 =\\n, w2 =\\n,\\n0\\n1 the elements of the corresponding dual bases are given by\\n\u0019\\nα1\\n\u0005→ α1 + 0,\\nα2\\n\u0018 \u0019\\nα1\\n\u0005→ α1 − α2 ,\\n→ R,\\nα2 v1∗ : R2,1 → R, w1∗ : R2,1\\n\\n\u0018\\n\\n\u0019\\n1\\nα1\\n\u0005→ 0 + α2 ,\\nα2\\n2\\n\u0018 \u0019\\nα1\\n\u0005→ 0 + α2 .\\n→ R,\\nα2 v2∗ : R2,1 → R, w2∗ : R2,1\\n\\n\u0018\\n\\nThe matrix representations of these maps are\\n\u0013 \u0014\\n\u0013 ∗\u0014 v1 B1 ,{1} = 1 0 ,\\n\u0013 ∗\u0014\\n\u0013 \u0014 w1 B2 ,{1} = 1 0 ,\\n\\n\u0013 ∗\u0014\\n\u0013 \u0014 v2 B1 ,{1} = 0 1 ,\\n\u0013 ∗\u0014\\n\u0013 \u0014 w2 B2 ,{1} = 0 1 .\\n\\nFor the linear map\\n\u0019\\n\u0018\\n\u0019\\nα + α2\\nα1\\n\u0005→ 1\\n,\\nα2\\n3α2\\n\\n\u0018 f : R2,1 → R2,1 ,   \n",
       "149                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         11.1 Linear Forms and Dual Spaces\\n\\n\u0018 we have\\n[ f ] B1 ,B2 =\\n\\n159\\n\\n\u0018\\n\u0019\\n\u0019\\n1 −4\\n10\\n, [ f ∗ ] B2∗ ,B1∗ =\\n.\\n0 6\\n−4 6\\n\\n11.2 Bilinear Forms\\nWe now consider special maps from a pair of K -vector spaces to the K -vector space\\nK.\\nDefinition 11.9 Let V and W be K -vector spaces.\\nA map β : V × W → K is called a bilinear form on V × W, when\\n(1) β(v1 + v2 , w) = β(v1 , w) + β(v2 , w),\\n(2) β(v, w1 + w2 ) = β(v, w1 ) + β(v, w2 ),\\n(3) β(λv, w) = β(v, λw) = λβ(v, w), hold for all v, v1 , v2 ∈ V, w, w1 , w2 ∈ W, and λ ∈ K .\\nA bilinear form β is called non-degenerate in the first variable, if β(v, w) = 0 for all w ∈ W implies that v = 0.\\nAnalogously, it is called non-degenerate in the second variable, if β(v, w) = 0 for all v ∈ V implies that w = 0.\\nIf β is non-degenerate in both variables, then β is called non-degenerate and the spaces V, W are called a dual pair with respect to β.\\nIf V = W, then β is called a bilinear form on V. If additionally β(v, w) =\\nβ(w, v) holds for all v, w ∈ V, then β is called symmetric.\\nOtherwise, β is called nonsymmetric.\\nExample 11.10\\n(1) If A ∈ K n,m , then\\nβ : K m,1 × K n,1 → K , (v, w) \u0005→ w T Av, is a bilinear form on K m,1 × K n,1 that is non-degenerate if and only if n = m and A ∈ G L n (K ), (cp.\\nExercise 11.10).\\n(2) The bilinear form\\nβ : R2,1 × R2,1 → R, (x, y) \u0005→ y T\\n\\n\u0018 \u0019\\n11 x,\\n11 is degenerate in both variables: For \u001b x = [1, −1]T , we have β(\u001b x , y) = 0 for all\\n2,1\\nT y = [1, −1] we have β(x, \u001b y) = 0 for all x ∈ R2,1 .\\nThe set of y ∈ R ; for \u001b all x = [x1 , x2 ]T ∈ R2,1 with β(x, x) = 1 is equal to the solution set of the quadratic equation in two variables x12 + 2x1 x2 + x22 = 1, or (x1 + x2 )2 = 1, for x1 , x2 ∈ R. Geometrically, this set is given by the two straight lines x1 + x2 = 1 and x1 + x2 = −1 in the cartesian coordinate system of R2 .   \n",
       "150                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         160\\n\\n11 Linear Forms and Bilinear Forms\\n\\n(3) If V is a K -vector space, then\\nβ : V × V ∗ → K , (v, f ) \u0005→ f (v), is a bilinear form on V × V ∗ , since\\nβ(v1 + v2 , f ) = f (v1 + v2 ) = f (v1 ) + f (v2 ) = β(v1 , f ) + β(v2 , f ),\\nβ(v, f 1 + f 2 ) = ( f 1 + f 2 )(v) = f 1 (v) + f 2 (v) = β(v, f 1 ) + β(v, f 2 ),\\nβ(λv, f ) = f (λv) = λ f (v) = λβ(v, f ) = (λ f )(v) = β(v, λ f ), hold for all v, v1 , v2 ∈ V, f, f 1 , f 2 ∈ V ∗ and λ ∈ K .\\nThis bilinear form is non-degenerate and thus V, V ∗ are a dual pair with respect to β (cp.\\nExercise\\n11.11 for the case dim(V) ∈ N).\\nDefinition 11.11 Let V and W be K -vector spaces with bases B1 = {v1 , . . . , vm } and B2 = {w1 , . . . , wn }, respectively.\\nIf β is a bilinear form on V × W, then\\n[β] B1 ×B2 = [bi j ] ∈ K n,m , bi j := β(v j , wi ), is called the matrix representation of β with respect to the bases B1 and B2 .\\n\u0003\\n\u0003n\\nIf v = mj=1 λ j v j ∈ V and w = i=1\\nμi wi ∈ W, then\\nβ(v, w) = m \u0005 n\\n\u0005\\n\\nλ j μi β(v j , wi ) = j=1 i=1 n\\n\u0005 i=1\\n\\nμi m\\n\u0005\\n\\n\u0015\\n\u0016T bi j λ j = \u0002 B2 (w) [β] B1 ×B2 \u0002 B1 (v), j=1 where we have used the coordinate map from Lemma 10.17.\\n\u0012\\n\u0012\\n\u0011\\n\u0011\\nExample 11.12 If B1 = e1(m) , . . . , em(m) and B2 = e1(n) , . . . , en(n) are the canonical bases of K m,1 and K n,1 , respectively, and if β is the bilinear form from (1) in\\nExample 11.10 with A = [ai j ] ∈ K n,m , then [β] B1 ×B2 = [bi j ], where\\n\u0015\\n\u0015 \u0016T\\n(n) \u0016\\n= ai j ,\\n= ei(n) Ae(m) bi j = β e(m) j , ei j and hence [β] B1 ×B2 = A.\\nThe following result shows that symmetric bilinear forms have symmetric matrix representations.\\nLemma 11.13 For a bilinear form β on a finite dimensional vector space V the following statements are equivalent:\\n(1) β is symmetric.\\n(2) For every basis B of V the matrix [β] B×B is symmetric.\\n(3) There exists a basis B of V such that [β] B×B is symmetric.   \n",
       "151                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            11.2 Bilinear Forms\\n\\n161\\n\\n\u0007\\n\u0006\\n\\nProof Exercise.\\n\\nWe will now analyze the effect of a basis change on the matrix representation of a bilinear form.\\nTheorem 11.14 Let V and W be finite dimensional K -vector spaces with bases\\nB1 , \n",
       "\\nB1 of V and B2 , \n",
       "\\nB2 of W. If β is a bilinear form on V × W, then\\n\u0015\\n\u0016T\\n[β] B1 ×B2 = [IdW ] B2 , \n",
       "\\n[β] \n",
       "\\nB2\\nB2 [IdV ] B1 , \n",
       "\\nB1 .\\nB1 × \n",
       "\\nProof Let B1 = {v1 , . . . , vm }, \n",
       "\\nB1 = { v1 , . . . , vm }, B2 = {w1 , . . . , wn }, \n",
       "\\nB2 = \n",
       "n }, and\\n{ w1 , . . . , w v1 , . . . , vm )P, where P = [ pi j ] = [IdV ] B1 , \n",
       "\\n(v1 , . . . , vm ) = (\n",
       "\\nB1 ,\\n(w1 , . . . , wn ) = ( w1 , . . . , w \n",
       "n )Q, where Q = [qi j ] = [IdW ] B2 , \n",
       "\\nB2 .\\n\n",
       "\\n\n",
       "\\nWith [β] vj, w \n",
       "i ), we then have\\nB1 × \n",
       "\\nB2 = [bi j ], where bi j = β(\n",
       "\\nβ(v j , wi ) = β m\\n\u0004\u0005 pk j vk , n\\n\u0005 q\u0003i\\n\\n\u0003=1\\n\\n\u0006 q\u0003i w\\n\n",
       "\u0003 =\\n\\n\u0003=1 k=1\\n\\n= n\\n\u0005 m\\n\u0005 n\\n\u0005\\n\u0003=1 q\u0003i m\\n\u0005\\n\\nβ( vk , w\\n\n",
       "\u0003 ) pk j k=1 b\u0003k pk j k=1\\n\\n⎤T\\n⎡\\n⎤ p1 j q1i\\n⎢ . ⎥\\n⎢ ⎥\\n= ⎣ ... ⎦ [β] \n",
       "\\nB2 ⎣ .. ⎦ ,\\nB1 × \n",
       "\\n⎡ qni pm j which implies that [β] B1 ×B2 = Q T [β] \n",
       "\\nB2 P, and hence the assertion follows.\\nB1 × \n",
       "\\n\\n\u0007\\n\u0006\\n\\nIf V = W and B1 , B2 are two bases of V, then we obtain the following special case of Theorem 11.14:\\n\u0016T\\n\u0015\\n[β] B1 ×B1 = [IdV ] B1 ,B2 [β] B2 ×B2 [IdV ] B1 ,B2 .\\nThe two matrix representations [β] B1 ×B1 and [β] B2 ×B2 of β in this case are congruent, which we formally define as follows.\\nDefinition 11.15 If for two matrices A, B ∈ K n,n there exists a matrix Z ∈ G L n (K ) with B = Z T AZ , then A and B are called congruent.\\nLemma 11.16 Congruence is an equivalence relation on the set K n,n .\\nProof Exercise.\\n\\n\u0007\\n\u0006   \n",
       "152                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               162\\n\\n11 Linear Forms and Bilinear Forms\\n\\n11.3 Sesquilinear Forms\\nFor complex vector spaces we introduce another special class of forms.\\nDefinition 11.17 Let V and W be C-vector spaces.\\nA map s : V × W → C is called a sesquilinear form on V × W, when\\n(1)\\n(2)\\n(3)\\n(4) s(v1 + v2 , w) = s(v1 , w) + s(v2 , w), s(λv, w) = λs(v, w), s(v, w1 + w2 ) = s(v, w1 ) + s(v, w2 ), s(v, λw) = λs(v, w), hold for all v, v1 , v2 ∈ V, w, w1 , w2 ∈ W and λ ∈ C.\\nIf V = W, then s is called a sesquilinear form on V. If additionally s(v, w) = s(w, v) holds for all v, w ∈ V, then s is called Hermitian.1\\nThe prefix sesqui is Latin and means “one and a half”.\\nNote that a sesquilinear form is linear in the first variable and semilinear (“half linear”) in the second variable.\\nThe following result characterizes Hermitian sesquilinear forms.\\nLemma 11.18 A sesquilinear form on the C-vector space V is Hermitian if and only if s(v, v) ∈ R for all v ∈ V.\\nProof If s is Hermitian then, in particular, s(v, v) = s(v, v) for all v ∈ V, and thus s(v, v) ∈ R.\\nIf, on the other hand, v, w ∈ V, then by definition s(v + w, v + w) = s(v, v) + s(v, w) + s(w, v) + s(w, w), s(v + iw, v + iw) = s(v, v) + is(w, v) − is(v, w) + s(w, w).\\n\\n(11.1)\\n(11.2)\\n\\nThe first equation implies that s(v, w) + s(w, v) ∈ R, since s(v + w, v + w), s(v, v), s(w, w) ∈ R by assumption.\\nThe second equation implies analogously that is(w, v) − is(v, w) ∈ R. Therefore, s(v, w) + s(w, v) = s(v, w) + s(w, v),\\n−is(v, w) + is(w, v) = is(v, w) − is(w, v).\\nMultiplying the second equation with i and adding the resulting equation to the first\\n\u0007\\n\u0006 we obtain s(v, w) = s(w, v)\\nCorollary 11.19 For a sesquilinear form s on the C-vector space V we have\\n2 s(v, w) = s(v + w, v + w) + is(v + iw, v + iw) − (i + 1) (s(v, v) + s(w, w)).\\nfor all v, w ∈ V.\\n1 Charles\\n\\nHermite (1822–1901).   \n",
       "153                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     11.3 Sesquilinear Forms\\n\\n163\\n\\nProof The result follows from multiplication of (11.2) with i and adding the result to (11.1).\\n\u0007\\n\u0006\\nCorollary 11.19 shows that a sesquilinear form on a C-vector space V is uniquely determined by the values of s(v, v) for all v ∈ V.\\nDefinition 11.20 The Hermitian transpose of A = [ai j ] ∈ Cn,m is the matrix\\nA H := [a i j ]T ∈ Cm,n .\\nIf A = A H , then A is called Hermitian.\\nIf a matrix A has real entries, then obviously A H = A T .\\nThus, a real symmetric matrix is also Hermitian.\\nIf A = [ai j ] ∈ Cn,n is Hermitian, then in particular aii = a ii for i = 1, . . . , n, i.e., Hermitian matrices have real diagonal entries.\\nThe Hermitian transposition satisfies similar rules as the (usual) transposition\\n(cp.\\nLemma 4.6).\\n\u001b ∈ Cn,m , B ∈ Cm,\u0003 and λ ∈ C the following assertions\\nLemma 11.21 For A, A hold:\\n(1)\\n(2)\\n(3)\\n(4)\\n\\n(A H ) H = A.\\n\u001bH .\\n\u001b H = AH + A\\n(A + A)\\n(λ A) H = λ A H .\\n(AB) H = B H A H .\\n\u0007\\n\u0006\\n\\nProof Exercise.\\nExample 11.22 For A ∈ Cn,m the map s : Cm,1 × Cn,1 → C, (v, w) \u0005→ w H Av, is a sesquilinear form.\\n\\nThe matrix representation of a sesquilinear form is defined analogously to the matrix representation of bilinear forms (cp.\\nDefinition 11.11).\\nDefinition 11.23 Let V and W be C-vector spaces with bases B1 = {v1 , . . . , vm } and B2 = {w1 , . . . , wn }, respectively.\\nIf s is a sesquilinear form on V × W, then\\n[s] B1 ×B2 = [bi j ] ∈ Cn,m , bi j := s(v j , wi ), is called the matrix representation of s with respect to the bases B1 and B2 .\\n\u0012\\n\u0012\\n\u0011\\n\u0011\\nExample 11.24 If B1 = e1(m) , . . . , em(m) and B2 = e1(n) , . . . , en(n) are the canonical bases of Cm,1 and Cn,1 , respectively, and s is the sesquilinear form of Example 11.22 with A = [ai j ] ∈ Cn,m , then [s] B1 ×B2 = [bi j ] with   \n",
       "154                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               164\\n\\n11 Linear Forms and Bilinear Forms\\n\\n\u0015\\n\u0015 \u0016T\\n\u0015 \u0016H\\n(n) \u0016 bi j = s e(m)\\n= ei(n) Ae(m)\\n= ai j\\n= ei(n) Ae(m) j , ei j j and, hence, [s] B1 ×B2 = A.\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n11.1.\\nLet V be a finite dimensional K -vector space and v ∈ V. Show that f (v) = 0 for all f ∈ V ∗ if and only if v = 0.\\n11.2.\\nConsider the basis B = {10, t − 1, t 2 − t} of the 3-dimensional vector space\\nR[t]≤2 .\\nCompute the dual basis B ∗ to B.\\n\u0012\\n\u0011\\n11.3.\\nLet V be an n-dimensional K -vector space and let v1∗ , . . . , vn∗ be a basis of V ∗ .\\nProve or disprove: There exists a unique basis {v1 , . . . , vn } of V with vi∗ (v j ) = δi j .\\n11.4.\\nLet V be a finite dimensional K -vector space and let f, g ∈ V ∗ with f = 0.\\nShow that g = λ f for a λ ∈ K \\ {0} holds if and only if ker( f ) = ker(g).\\nIs it possible to omit the assumption f = 0?\\n11.5.\\nLet V be a K -vector space and let U be a subspace of V. The set\\nU 0 := { f ∈ V ∗ | f (u) = 0 for all u ∈ U} is called the annihilator of U. Show the following assertions:\\n(a) U 0 is a subspace of V ∗ .\\n(b) For subspaces U1 , U2 of V we have\\n(U1 + U2 )0 = U10 ∩ U20 , (U1 ∩ U2 )0 = U10 + U20 , and if U1 ⊆ U2 , then U20 ⊆ U10 .\\n(c) If W is a K -vector space and f ∈ L(V, W), then ker( f ∗ ) = (im( f ))0 .\\n11.6.\\nProve Lemma 11.6 (2) and (3).\\n11.7.\\nLet V and W be K -vector spaces.\\nShow that the set of all bilinear forms on\\nV × W with the operations\\n+ : (β1 + β2 )(v, w) := β1 (v, w) + β2 (v, w),\\n· : (λ · β)(v, w) := λ · β(v, w), is a K -vector space.\\n11.8.\\nLet V and W be K -vector spaces with bases {v1 , . . . , vm } and {w1 , . . . , wn } and corresponding dual bases {v1∗ , . . . , vm∗ } and {w1∗ , . . . , wn∗ }, respectively.\\nFor i = 1, . . . , m and j = 1, . . . , n let\\nβi j : V × W → K , (v, w) \u0005→ vi∗ (v)w ∗j (w).\\n(a) Show that βi j is a bilinear form on V × W.   \n",
       "155                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      11.3 Sesquilinear Forms\\n\\n165\\n\\n(b) Show that the set {βi j | i = 1, . . . , m, j = 1, . . . , n} is a basis of the\\nK -vector space of bilinear forms on V × W (cp.\\nExercise 11.7) and determine the dimension of this space.\\n11.9.\\nLet V be the R-vector space of the continuous and real valued functions on the real interval [α, β].\\nShow that\\n\u0002 β f (x)g(x)d x,\\nβ : V × V → R, ( f, g) \u0005→\\nα is a symmetric bilinear form on V. Is β degenerate?\\n11.10.\\nShow that the map β from (1) in Example 11.10 is a bilinear form, and show that it is non-degenerate if and only if n = m and A ∈ G L n (K ).\\n11.11.\\nLet V be a finite dimensional K -vector space.\\nShow that V, V ∗ is a dual pair with respect to the bilinear form β from (3) in Example 11.10, i.e., that the bilinear form β is non-degenerate.\\n11.12.\\nLet V be a finite dimensional K -vector space and let U ⊆ V and W ⊆ V ∗ be subspaces with dim(U) = dim(W) ≥ 1.\\nProve or disprove: The spaces\\nU, W form a dual pair with respect to the bilinear form β : U × W → K ,\\n(v, h) \u0005→ h(v).\\n11.13.\\nLet V and W be finite dimensional K -vector spaces with the bases B1 and\\nB2 , respectively, and let β be a bilinear form on V × W.\\n\\n11.14.\\n11.15.\\n11.16.\\n\\n11.17.\\n\\n(a) Show that the following statements are equivalent:\\n(1) [β] B1 ×B2 is not invertible.\\n(2) β is degenerate in the second variable.\\n(3) β is degenerate in the first variable.\\n(b) Conclude from (a): β is non-degenerate if and only if [β] B1 ×B2 is invertible.\\nProve Lemma 11.16.\\nProve Lemma 11.13.\\nFor a bilinear form β on a K -vector space V, the map qβ : V → K , v \u0005→ β(v, v), is called the quadratic form induced by β.\\nShow the following assertion:\\nIf 1+1 = 0 in K and β is symmetric, then β(v, w) = 21 (qβ (v +w)−qβ (v)− qβ (w)) holds for all v, w ∈ V.\\nShow that a sesquilinear form s on a C-vector space V satisfies the polarization identity s(v, w) =\\n\\n\u0016\\n1\u0015 s(v +w, v +w)−s(v −w, v −w)+is(v +iw, v +iw)−is(v −iw, v −iw)\\n4 for all v, w ∈ V.\\n11.18.\\nConsider the following maps from C3,1 × C3,1 to C:\\n(a) β1 (x, y) = 3x1 x 1 + 3y1 y 1 + x2 y 3 − x3 y 2 ,\\n(b) β2 (x, y) = x1 y 2 + x2 y 3 + x3 y 1 ,\\n(c) β3 (x, y) = x1 y2 + x2 y3 + x3 y1 ,   \n",
       "156                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              166\\n\\n11 Linear Forms and Bilinear Forms\\n\\n(d) β4 (x, y) = 3x1 y 1 + x1 y 2 + x2 y 1 + 2ix2 y 3 − 2ix3 y 2 + x3 y 3 .\\nWhich of these are bilinear forms or sesquilinear forms on C3,1 ?\\nTest whether the bilinear form is symmetric or the sesquilinear form is Hermitian, and derive the corresponding matrix representations with respect to the canonical basis B1 = {e1 , e2 , e3 } and the basis B2 = {e1 , e1 + ie2 , e2 + ie3 }.\\n11.19.\\nProve Lemma 11.21.\\n11.20.\\nLet A ∈ Cn,n be Hermitian.\\nShow that s : Cn,1 × Cn,1 , (v, w) \u0005→ w H Av, is a Hermitian sesquilinear form on Cn,1 .\\n11.21.\\nLet V be a finite dimensional C-vector space with the basis B, and let s be a sesquilinear form on V. Show that s is Hermitian if and only if [s] B×B is\\nHermitian.\\n11.22.\\nShow the following assertions for A, B ∈ Cn,n :\\n(a) If A H = −A, then the eigenvalues of A are purely imaginary.\\n(b) If A H = −A, then trace(A2 ) ≤ 0 and (trace(A))2 ≤ 0.\\n(c) If A H = A and B H = B, then trace((AB)2 ) ≤ trace(A2 B 2 ).   \n",
       "157                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Chapter 12\\n\\nEuclidean and Unitary Vector Spaces\\n\\nIn this chapter we study vector spaces over the fields R and C. Using the definition of bilinear and sesquilinear forms, we introduce scalar products on such vector spaces.\\nScalar products allow the extension of well-known concepts from elementary geometry, such as length and angles, to abstract real and complex vector spaces.\\nThis, in particular, leads to the idea of orthogonality and to orthonormal bases of vector spaces.\\nAs an example for the importance of these concepts in many applications we study least-squares approximations.\\n\\n12.1 Scalar Products and Norms\\nWe start with the definition of a scalar product and the Euclidean or unitary vector spaces.\\nDefinition 12.1 Let V be a K -vector space, where either K = R or K = C. A map\\n\u0002·, ·\u0003 : V × V → K , (v, w) \u0005→ \u0002v, w\u0003, is called a scalar product on V, when the following properties hold:\\n(1) If K = R, then \u0002·, ·\u0003 is a symmetric bilinear form.\\nIf K = C, then \u0002·, ·\u0003 is an Hermitian sesquilinear form.\\n(2) \u0002·, ·\u0003 is positive definite, i.e., \u0002v, v\u0003 ≥ 0 holds for all v ∈ V, with equality if and only if v = 0.\\nAn R-vector space with a scalar product is called a Euclidean vector space1 , and a\\nC-vector space with a scalar product is called a unitary vector space.\\nScalar products are sometimes called inner products.\\nNote that \u0002v, v\u0003 is nonnegative and real also when V is a C-vector space.\\nIt is easy to see that a subspace U of\\n1 Euclid of Alexandria (approx.\\n300 BC).\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_12\\n\\n167   \n",
       "158                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           168\\n\\n12 Euclidean and Unitary Vector Spaces a Euclidean or unitary vector space V is again a Euclidean or unitary vector space, respectively, when the scalar product on the space V is restricted to the subspace U.\\nExample 12.2\\n(1) A scalar product on Rn,1 is given by\\n\u0002v, w\u0003 := w T v.\\nIt is called the standard scalar product of Rn,1 .\\n(2) A scalar product on Cn,1 is given by\\n\u0002v, w\u0003 := w H v.\\nIt is called the standard scalar product of Cn,1 .\\n(3) For both K = R and K = C,\\n\u0002A, B\u0003 := Spur(B H A) is a scalar product on K n,m .\\n(4) A scalar product on the vector space of the continuous and real valued functions on the real interval [α, β] is given by\\n\u0002\\n\u0002 f, g\u0003 :=\\n\\nα\\n\\nβ f (x)g(x)d x.\\n\\nWe will now show how to use the Euclidean or unitary structure of a vector space in order to introduce geometric concepts such as the length of a vector or the angle between vectors.\\nAs a motivation of a general concept of length we have the absolute value of real numbers, i.e., the map | · | : R → R, x \u0005→ |x|.\\nThis map has the following properties:\\n(1) |λx| = |λ| · |x| for all λ, x ∈ R.\\n(2) |x| ≥ 0 for all x ∈ R, with equality if and only if x = 0.\\n(3) |x + y| ≤ |x| + |y| for all x, y ∈ R.\\nThese properties are generalized to real or complex vector spaces as follows.\\nDefinition 12.3 Let V be a K -vector space, where either K = R or K = C. A map\\n·\\n\\n: V → R, v \u0005→ v ,   \n",
       "159                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                12.1 Scalar Products and Norms\\n\\n169 is called a norm on V, when for all v, w ∈ V and λ ∈ K the following properties hold:\\n(1)\\n(2)\\n(3)\\n\\nλv = |λ| · v .\\nv ≥ 0, with equality if and only if v = 0.\\nv + w ≤ v + w (triangle inequality).\\n\\nA K -vector space on which a norm is defined is called a normed space.\\nExample 12.4\\n(1) If \u0002·, ·\u0003 is the standard scalar product on Rn,1 , then v := \u0002v, v\u00031/2 = (v T v)1/2 defines a norm that is called the Euclidean norm of Rn,1 .\\n(2) If \u0002·, ·\u0003 is the standard scalar product on Cn,1 , then v := \u0002v, v\u00031/2 = (v H v)1/2 defines a norm that is called the Euclidean norm of Cn,1 .\\n(This is common terminology, although the space itself is unitary and not Euclidean.)\\n(3) For both K = R and K = C,\\nA\\n\\nF\\n\\n:= (trace(A H A))1/2 = n \u0004 m\\n\u0003\u0004\\n\\n|ai j |2\\n\\n\u00051/2 i=1 j=1 is a norm on K n,m that is called the Frobenius norm2 of K n,m .\\nFor m = 1 the\\nFrobenius norm is equal to the Euclidean norm of K n,1 .\\nMoreover, the Frobenius norm of K n,m is equal to the Euclidean norm of K nm,1 (or K nm ), if we identify these vector spaces via an isomorphism.\\nObviously, we have A F = A T F = A H F for all A ∈ K n,m .\\n(4) If V is the vector space of the continuous and real valued functions on the real interval [α, β], then\\n\u0003\u0002 β\\n\u00051/2\\n( f (x))2 d x f := \u0002 f, f \u00031/2 =\\nα is a norm on V that is called the L 2 -norm.\\n(5) Let K = R or K = C, and let p ∈ R, p ≥ 1 be given.\\nThen for v = [ν1 , . . . , νn ]T ∈ K n,1 the p-norm of K n,1 is defined by v p\\n\\n:= n\\n\u0003\u0004 i=1\\n\\n2 Ferdinand\\n\\nGeorg Frobenius (1849–1917).\\n\\n|νi | p\\n\\n\u00051/ p\\n\\n.\\n\\n(12.1)   \n",
       "160                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       170\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\nFor p = 2 this is the Euclidean norm on K n,1 .\\nFor this norm we typically omit the index 2 and write · instead of · 2 (as in (1) and (2) above).\\nTaking the limit p → ∞ in (12.1), we obtain the ∞-norm of K n,1 , given by v\\n\\n∞\\n\\n= max |νi |.\\n1≤i≤n\\n\\nThe following figures illustrate the unit circle in R2,1 with respect to the p-norm, i.e., the set of all v ∈ R2,1 with v p = 1, for p = 1, p = 2 and p = ∞:\\n\\n(6) For K = R or K = C the p-norm of K n,m is defined by\\nA p\\n\\n:= sup v∈K m,1 \\{0}\\n\\nAv p\\n.\\nv p\\n\\nHere we use the p-norm of K m,1 in the denominator and the p-norm of K n,1 in the numerator.\\nThe notation sup means supremum, i.e., the least upper bound that is known from Analysis.\\nOne can show that the supremum is attained by a vector v, and thus we may write max instead of sup in the definition above.\\nIn particular, for A = [ai j ] ∈ K n,m we have\\nA\\n\\nA\\n\\n1\\n\\n∞\\n\\n= max\\n\\n1≤ j≤m\\n\\n= max\\n\\n1≤i≤n n\\n\u0004 i=1 m\\n\u0004\\n\\n|ai j |,\\n\\n|ai j |.\\nj=1\\n\\nThese norms are called maximum column sum and maximum row sum norm of K n,m , respectively.\\nWe easily see that A 1 = A T ∞ = A H ∞ and\\nA ∞ = A T 1 = A H 1 .\\nHowever, for the matrix\\n\u0006\\nA=\\n\\n1/2 −1/4\\n−1/2 2/3\\n\\n\u0007\\n∈ R2,2 we have A 1 = 1 and A ∞ = 7/6.\\nThus, this matrix A satisfies A 1 < A ∞ and A T ∞ < A T 1 .\\nThe 2-norm of matrices will be considered further in\\nChap.\\n19.   \n",
       "161                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           12.1 Scalar Products and Norms\\n\\n171\\n\\nThe norms in the above examples (1)–(4) have the form v = \u0002v, v\u00031/2 , where\\n\u0002·, ·\u0003 is a given scalar product.\\nWe will show now that the map v \u0005→ \u0002v, v\u00031/2 always defines a norm.\\nOur proof is based on the following theorem.\\nTheorem 12.5 If V is a Euclidean or unitary vector space with the scalar product\\n\u0002·, ·\u0003, then\\n|\u0002v, w\u0003|2 ≤ \u0002v, v\u0003 · \u0002w, w\u0003 for all v, w ∈ V,\\n(12.2) with equality if and only if v, w are linearly dependent.\\nProof The inequality is trivial for w = 0.\\nThus, let w = 0 and let\\nλ :=\\n\\n\u0002v, w\u0003\\n.\\n\u0002w, w\u0003\\n\\nThen\\n0 ≤ \u0002v − λw, v − λw\u0003 = \u0002v, v\u0003 − λ\u0002v, w\u0003 − λ\u0002w, v\u0003 − λ(−λ)\u0002w, w\u0003\\n\u0002v, w\u0003\\n|\u0002v, w\u0003|2\\n\u0002v, w\u0003\\n\u0002v, w\u0003 +\\n\u0002w, w\u0003\\n\u0002v, w\u0003 −\\n\u0002w, w\u0003\\n\u0002w, w\u0003\\n\u0002w, w\u00032\\n|\u0002v, w\u0003|2\\n,\\n= \u0002v, v\u0003 −\\n\u0002w, w\u0003\\n\\n= \u0002v, v\u0003 − which implies (12.2).\\nIf v, w are linearly dependent, then v = λw for a scalar λ, and hence\\n|\u0002v, w\u0003|2 = |\u0002λw, w\u0003|2 = |λ\u0002w, w\u0003|2 = |λ|2 |\u0002w, w\u0003|2 = λλ \u0002w, w\u0003 \u0002w, w\u0003\\n= \u0002λw, λw\u0003 \u0002w, w\u0003 = \u0002v, v\u0003 \u0002w, w\u0003.\\nOn the other hand, let |\u0002v, w\u0003|2 = \u0002v, v\u0003\u0002w, w\u0003.\\nIf w = 0, then v, w are linearly dependent.\\nIf w = 0, then we define λ as above and get\\n\u0002v − λw, v − λw\u0003 = \u0002v, v\u0003 −\\n\\n|\u0002v, w\u0003|2\\n= 0.\\n\u0002w, w\u0003\\n\\nSince the scalar product is positive definite, we have v − λw = 0, and thus v, w are linearly dependent.\\nThe inequality (12.2) is called Cauchy-Schwarz inequality.3 It is an important tool in Analysis, in particular in the estimation of approximation and interpolation errors.\\n\\n3 Augustin\\n\\nLouis Cauchy (1789–1857) and Hermann Amandus Schwarz (1843–1921).   \n",
       "162                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    172\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\nCorollary 12.6 If V is a Euclidean or unitary vector space with the scalar product\\n\u0002·, ·\u0003, then the map\\n: V → R, v \u0005→ v := \u0002v, v\u00031/2 ,\\n\\n· is a norm on V that is called the norm induced by the scalar product.\\nProof We have to prove the three defining properties of the norm.\\nSince \u0002·, ·\u0003 is positive definite, we have v ≥ 0, with equality if and only if v = 0.\\nIf v ∈ V and\\nλ ∈ K (where in the Euclidean case K = R and in the unitary case K = C), then\\nλv\\n\\n2\\n\\n= \u0002λv, λv\u0003 = λλ\u0002v, v\u0003 = |λ|2 \u0002v, v\u0003, and hence λv = |λ| v .\\nIn order to show the triangle inequality, we use the\\nCauchy-Schwarz inequality and the fact that Re(z) ≤ |z| for every complex number z.\\nFor all v, w ∈ V we have v+w\\n\\n2\\n\\n= \u0002v + w, v + w\u0003 = \u0002v, v\u0003 + \u0002v, w\u0003 + \u0002w, v\u0003 + \u0002w, w\u0003\\n= \u0002v, v\u0003 + \u0002v, w\u0003 + \u0002v, w\u0003 + \u0002w, w\u0003\\n= v\\n\\n2\\n\\n+ 2 Re(\u0002v, w\u0003) + w\\n\\n≤ v\\n\\n2\\n\\n+ 2 |\u0002v, w\u0003| + w\\n\\n2\\n\\n≤ v\\n\\n2\\n\\n+2 v\\n\\n2 w + w\\n\\n2\\n\\n= ( v + w )2 , and thus v + w ≤ v + w .\\n\\n12.2 Orthogonality\\nWe will now use the scalar product to introduce angles between vectors.\\nAs motivation we consider the Euclidean vector space R2,1 with the standard scalar product and the induced Euclidean norm v = \u0002v, v\u00031/2 .\\nThe Cauchy-Schwarz inequality shows that\\n−1 ≤\\n\\n\u0002v, w\u0003\\n≤ 1 for all v, w ∈ R2,1 \\ {0}.\\nv w\\n\\nIf v, w ∈ R2,1 \\ {0}, then the angle between v and w is the uniquely determined real number ϕ ∈ [0, π] with cos(ϕ) =\\n\\n\u0002v, w\u0003\\n.\\nv w   \n",
       "163                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          12.2 Orthogonality\\n\\n173\\n\\nThe vectors v, w are orthogonal if ϕ = π/2, so that cos(ϕ) = 0.\\nThus, v, w are orthogonal if and only if \u0002v, w\u0003 = 0.\\nAn elementary calculation now leads to the cosine theorem for triangles: v−w\\n\\n2\\n\\n= \u0002v − w, v − w\u0003 = \u0002v, v\u0003 − 2\u0002v, w\u0003 + \u0002w, w\u0003\\n= v\\n\\n2\\n\\n+ w\\n\\n2\\n\\n−2 v w cos(ϕ).\\n\\nIf v, w are orthogonal, i.e., \u0002v, w\u0003 = 0, then the cosine theorem implies the\\nPythagorean theorem4 : v−w\\n\\n2\\n\\n= v\\n\\n2\\n\\n+ w 2.\\n\\nThe following figures illustrate the cosine theorem and the Pythagorean theorem for vectors in R2,1 :\\n\\nIn the following definition we generalize the ideas of angles and orthogonality.\\nDefinition 12.7 Let V be a Euclidean or unitary vector space with the scalar product\\n\u0002·, ·\u0003.\\n(1) In the Euclidean case, the angle between two vectors v, w ∈ V \\ {0} is the uniquely determined real number ϕ ∈ [0, π] with cos(ϕ) =\\n\\n\u0002v, w\u0003\\n.\\nv w\\n\\n(2) Two vectors v, w ∈ V are called orthogonal, if \u0002v, w\u0003 = 0.\\n(3) A basis {v1 , . . . , vn } of V is called an orthogonal basis, if\\n\u0002vi , v j \u0003 = 0, i, j = 1, . . . , n and i = j.\\nIf, furthermore, vi = 1, i = 1, . . . , n, where v = \u0002v, v\u00031/2 is the norm induced by the scalar product, then\\n{v1 , . . . , vn } is called an orthonormal basis of V. (For an orthonormal basis we therefore have \u0002vi , v j \u0003 = δi j .)\\n4 Pythagoras of Samos (approx.\\n570–500 BC).   \n",
       "164                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   174\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\nNote that the terms in (1)−(3) are defined with respect to the given scalar product.\\nDifferent scalar products yield different angles between vectors.\\nIn particular, the orthogonality of two given vectors may be lost when we consider a different scalar product.\\nExample 12.8 The standard basis vectors e1 , e2 ∈ R2,1 are orthogonal and {e1 , e2 } is an orthonormal basis of R2,1 with respect to the standard scalar product (cp.\\n(1) in\\nExample 12.2).\\nConsider the symmetric and invertible matrix\\n\u0006 \u0007\\n21\\nA=\\n∈ R2,2 ,\\n12 which defines a symmetric and non-degenerate bilinear form on R2,1 by\\n(v, w) \u0005→ w T Av\\n(cp.\\n(1) in Example 11.10).\\nThis bilinear form is positive definite, since for all v =\\n[ν1 , ν2 ]T ∈ R2,1 we have v T Av = ν12 + ν22 + (ν1 + ν2 )2 .\\nThe bilinear form therefore is a scalar product on R2,1 , which we denote by \u0002·, ·\u0003 A .\\nWe denote the induced norm by · A .\\nWith respect to the scalar product \u0002·, ·\u0003 A the vectors e1 , e2 satisfy\\n\u0002e1 , e1 \u0003 A = e1T Ae1 = 2, \u0002e2 , e2 \u0003 A = e2T Ae2 = 2, \u0002e1 , e2 \u0003 A = e2T Ae1 = 1.\\n2,1\\nClearly, {e1 , e2 } is not an\\n√ orthonormal basis of R with respect to \u0002·, ·\u0003 A .\\nAlso note that e1 A = e2 A = 2.\\nOn the other hand, the vectors v1 = [1, 1]T and v2 = [−1, 1]T satisfy\\n\\n\u0002v1 , v1 \u0003 A = v1T Av1 = 6, \u0002v2 , v2 \u0003 A = v2T Av2 = 2, \u0002v1 , v2 \u0003 A = v2T Av1 = 0.\\n√\\n√\\nHence v1 A = 6 and v2 A = 2, so that {6−1/2 v1 , 2−1/2 v2 } is an orthonormal basis of R2,1 with respect to the scalar product \u0002·, ·\u0003 A\\nWe now show that every finite dimensional Euclidean or unitary vector space has an orthonormal basis.\\nTheorem 12.9 Let V be a Euclidean or unitary vector space with the basis\\n{v1 , . . . , vn }.\\nThen there exists an orthonormal basis {u 1 , . . . , u n } of V with span{u 1 , . . . , u k } = span{v1 , . . . , vk }, k = 1, . . . , n.   \n",
       "165                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            12.2 Orthogonality\\n\\n175\\n\\nProof We give the proof by induction on dim(V) = n.\\nIf n = 1, then we set u 1 := v1 −1 v1 .\\nThen u 1 = 1, and {u 1 } is an orthonormal basis of V with span{u 1 } = span{v1 }.\\nLet the assertion hold for an n ≥ 1.\\nLet dim(V) = n + 1 and let {v1 , . . . , vn+1 } be a basis of V. Then Vn := span{v1 , . . . , vn } is an n-dimensional subspace of V. By the induction hypothesis there exists an orthonormal basis {u 1 , . . . , u n } of Vn with span{u 1 , . . . , u k } = span{v1 , . . . , vk } for k = 1, . . . , n.\\nWe define\\n\b u n+1 := vn+1 − n\\n\u0004\\n\u0002vn+1 , u k \u0003u k , u n+1 := ||\b u n+1 ||−1\b u n+1 .\\nk=1\\n\\nSince vn+1 ∈\\n/ Vn = span{u 1 , . . . , u n }, we must have \b u n+1 = 0, and Lemma 9.16 yields span{u 1 , . . . , u n+1 } = span{v1 , . . . , vn+1 }.\\nFor j = 1, . . . , n we have u n+1\\n\u0002u n+1 , u j \u0003 = \u0002 \b\\n\\n−1\\n\\n\b u n+1 , u j \u0003\\n\\n= \b u n+1\\n\\n−1\\n\\n= \b u n+1\\n\\n−1 n\\n\u0004\\n\u0002vn+1 , u k \u0003 \u0002u k , u j \u0003\\n\u0002vn+1 , u j \u0003 − k=1\\n\\n\u0002vn+1 , u j \u0003 − \u0002vn+1 , u j \u0003\\n\\n= 0.\\nu n+1\\nFinally, \u0002u n+1 , u n+1 \u0003 = \b\\n\\n−2\\n\\n\u0002\b u n+1 , \b u n+1 \u0003 = 1 which completes the proof.\\n\\nThe proof of Theorem 12.9 shows how a given basis {v1 , . . . , vn } can be orthonormalized, i.e., transformed into an orthonormal basis {u 1 , . . . , u n } with span{u 1 , . . . , u k } = span{v1 , . . . , vk }, k = 1, . . . , n.\\nThe resulting algorithm is called the Gram-Schmidt method 5 :\\nAlgorithm 12.10 Given a basis {v1 , . . . , vn } of V.\\n(1) Set u 1 := v1 −1 v1 .\\n(2) For j = 1, . . . , n − 1 set\\n\b u j+1 := v j+1 − u j+1 := \b u j+1\\n\\n5 Jørgen j\\n\u0004\\n\u0002v j+1 , u k \u0003u k , k=1\\n−1\\n\\n\b u j+1 .\\n\\nPedersen Gram (1850–1916) and Erhard Schmidt (1876–1959).   \n",
       "166                                                                                                                                                                                                                                                                                                                                   176\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\nA slight reordering and combination of steps in the Gram-Schmidt method yields\\n⎛\\n\\n⎞ v1 \u0002v2 , u 1 \u0003 . . . \u0002vn , u 1 \u0003\\n⎜\\n⎟\\n..\\n..\\n⎜\\n⎟\\n.\\n.\\n\b u2\\n⎟.\\n(v1 , v2 , . . . , vn ) = (u 1 , u 2 , . . . , u n ) ⎜\\n⎜\\n⎟\\n..\\n\u000e\u000f\\n\u0010\\n\u000e\u000f\\n\u0010⎝\\n⎠\\n.\\n\u0002v\\n, u\\n\u0003 n n−1\\n∈V n\\n∈V n\\n\b un\\nThe upper triangular matrix on the right hand side is the coordinate transformation matrix from the basis {v1 , . . . , vn } to the basis {u 1 , . . . , u n } of V (cp.\\nTheorem 9.25 or 10.2).\\nThus, we have shown the following result.\\nTheorem 12.11 If V is a finite dimensional Euclidean or unitary vector space with a given basis B1 , then the Gram-Schmidt method applied to B1 yields an orthonormal basis B2 of V, such that [IdV ] B1 ,B2 is an invertible upper triangular matrix.\\nConsider an m-dimensional subspace of Rn,1 or Cn,1 with the standard scalar product \u0002·, ·\u0003, and write the m vectors of an orthonormal basis {q1 , . . . , qm } as columns of a matrix, Q := [q1 , . . . , qm ].\\nThen we obtain in the real case\\nQ T Q = [qiT q j ] = [\u0002q j , qi \u0003] = [δ ji ] = Im , and analogously in the complex case\\nQ H Q = [qiH q j ] = [\u0002q j , qi \u0003] = [δ ji ] = Im .\\nIf, on the other hand, Q T Q = Im or Q H Q = Im for a matrix Q ∈ Rn,m or Q ∈ Cn,m , respectively, then the m columns of Q form an orthonormal basis (with respect to the standard scalar product) of an m-dimensional subspace of Rn,1 or Cn,1 , respectively.\\nA “matrix version” of Theorem 12.11 can therefore be formulated as follows.\\nCorollary 12.12 Let K = R or K = C and let v1 , . . . , vm ∈ K n,1 be linearly independent.\\nThen there exists a matrix Q ∈ K n,m with its m columns being orthonormal with respect to the standard scalar product of K n,1 , i.e., Q T Q = Im for K = R or\\nQ H Q = Im for K = C, and an upper triangular matrix R ∈ G L m (K ), such that\\n[v1 , . . . , vm ] = Q R.\\n\\n(12.3)\\n\\nThe factorization (12.3) is called a Q R-decomposition of the matrix [v1 , . . . , vm ].\\nThe Q R-decomposition has many applications in Numerical Mathematics (cp.\\nExample 12.16 below).\\nLemma 12.13 Let K = R or K = C and let Q ∈ K n,m be a matrix with orthonormal columns with respect to the standard scalar product of K n,1 .\\nThen v = Qv holds for all v ∈ K m,1 .\\n(Here · is the Euclidean norm of K m,1 and of K n,1 .)   \n",
       "167                                                                                                                                                                                                                                                                                                                                                                                                                                                     12.2 Orthogonality\\n\\n177\\n\\nProof For K = C we have v\\n\\n2\\n\\n= \u0002v, v\u0003 = v H v = v H (Q H Q)v = \u0002Qv, Qv\u0003 = Qv 2 , and the proof for K = R is analogous.\\nWe now introduce two important classes of matrices.\\nDefinition 12.14\\n(1) A matrix Q ∈ Rn,n whose columns form an orthonormal basis with respect to the standard scalar product of Rn,1 is called orthogonal.\\n(2) A matrix Q ∈ Cn,n whose columns form an orthonormal basis with respect to the standard scalar product of Cn,1 is called unitary.\\nA matrix Q = [q1 , . . . , qn ] ∈ Rn,n is therefore orthogonal if and only if\\nQ T Q = [qiT q j ] = [\u0002q j , qi \u0003] = [δ ji ] = In .\\nIn particular, an orthogonal matrix Q is invertible with Q −1 = Q T (cp.\\nCorollary 7.20).\\nThe equation Q Q T = In means that the n rows of Q form an orthonormal basis of R1,n (with respect to the scalar product \u0002v, w\u0003 := wv T ).\\nAnalogously, a unitary matrix Q ∈ Cn,n is invertible with Q −1 = Q H and\\nH\\nQ Q = In = Q Q H .\\nThe n columns of Q form an orthonormal basis of C1,n .\\nLemma 12.15 The sets O(n) of orthogonal and U(n) of unitary n ×n matrices form subgroups of G L n (R) and G L n (C), respectively.\\nProof We consider only O(n); the proof for U(n) is analogous.\\nSince every orthogonal matrix is invertible, we have that O(n) ⊂ G L n (R).\\nThe identity matrix In is orthogonal, and hence In ∈ O(n) = Ø.\\nIf Q ∈ O(n), then also\\nQ T = Q −1 ∈ O(n), since (Q T )T Q T = Q Q T = In .\\nFinally, if Q 1 , Q 2 ∈ O(n), then\\n(Q 1 Q 2 )T (Q 1 Q 2 ) = Q 2T (Q 1T Q 1 )Q 2 = Q 2T Q 2 = In , and thus Q 1 Q 2 ∈ O(n).\\nExample 12.16 In many applications measurements or samples lead to a data set that is represented by tuples (τi , μi ) ∈ R2 , i = 1, . . . , m.\\nHere τ1 < · · · < τm , are the pairwise distinct measurement points and μ1 , . . . , μm are the corresponding measurements.\\nIn order to approximate the given data set by a simple model, one can try to construct a polynomial p of small degree so that the values p(τ1 ), . . . , p(τm ) are as close as possible to the measurements μ1 , . . . , μm .\\nThe simplest case is a real polynomial of degree (at most) 1.\\nGeometrically, this corresponds to the construction of a straight line in R2 that has a minimal distance   \n",
       "168                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         178\\n\\n12 Euclidean and Unitary Vector Spaces to the given points, as shown in the figure below (cp.\\nSect.\\n1.4).\\nThere are many possibilities to measure the distance.\\nIn the following we will describe one of them in more detail and use the Gram-Schmidt method, or the Q R-decomposition, for the construction of the straight line.\\nIn Statistics this method is called linear regression.\\n\\nA real polynomial of degree (at most) 1 has the form p = αt + β and we are looking for coefficients α, β ∈ R with p(τi ) = ατi + β ≈ μi , i = 1, . . . , m.\\nUsing matrices we can write this problem as\\n⎡\\n\\n⎤\\n⎡ ⎤\\n1 \u0006 \u0007\\nμ1\\n\u0006 \u0007\\n.. ⎥ α ≈ ⎢ .. ⎥ or [v , v ] α ≈ y.\\n⎣ . ⎦\\n1 2\\n.⎦ β\\nβ\\nμm\\nτm 1\\n\\nτ1\\n⎢ ..\\n⎣ .\\n\\nAs mentioned above, there are different possibilities for interpreting the symbol “≈”.\\nIn particular, there are different norms in which we can measure the distance between the given values μ1 , . . . , μm and the polynomial values p(τ1 ), . . . , p(τm ).\\nHere we will use the Euclidean norm · and consider the minimization problem\\n\n",
       "\\n\n",
       "\\n\u0006 \u0007\\n\n",
       "\\n\n",
       "\\nα min [v1 , v2 ]\\n− y\n",
       "\\n\n",
       ".\\nβ\\nα,β∈R \n",
       "\\nThe vectors v1 , v2 ∈ Rm,1 are linearly independent, since the entries of v1 are pairwise distinct, while all entries of v2 are equal.\\nLet\\n[v1 , v2 ] = [q1 , q2 ]R be a Q R-decomposition.\\nWe extend the vectors q1 , q2 ∈ Rm,1 to an orthonormal basis {q1 , q2 , q3 , . . . , qm } of Rm,1 .\\nThen Q = [q1 , . . . , qm ] ∈ Rm,m is an orthogonal matrix and   \n",
       "169                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                12.2 Orthogonality\\n\\n179\\n\\n\n",
       "\\n\n",
       "\\n\u0006 \u0007\\n\n",
       "\\n\n",
       "\\nα\\n\n",
       " = min\\n[v\\n− y min \n",
       "\\n, v\\n]\\n1 2\\n\n",
       "\\n\n",
       " α,β∈R\\nβ\\nα,β∈R\\n\\n\n",
       "\\n\n",
       "\\n\u0006 \u0007\\n\n",
       "\\n\n",
       "\\n\n",
       " [q1 , q2 ]R α − y \n",
       "\\n\n",
       "\\n\n",
       "\\nβ\\n\n",
       " \u0006\\n\n",
       "\\n\u0007\u0006 \u0007\\n\n",
       "\\n\n",
       "\\nR\\nα\\n\n",
       "\\n= min \n",
       "\\nQ\\n− y\\n\n",
       "\\n\n",
       "\\n0m−2,2 β\\nα,β∈R\\n\n",
       " \n",
       "\u0006\\n\u0007\u0006 \u0007\\n\u001f\n",
       "\\n\n",
       "\\n\n",
       "\\nR\\nα\\nT\\n\n",
       "\\nQ\\n= min y\\n−\\nQ\\n\n",
       "\\n\n",
       "\\n0m−2,2 β\\nα,β∈R\\n\n",
       "\\n⎡ T ⎤\n",
       "\\n\n",
       "⎡ \u0006 \u0007 ⎤ q1 y \n",
       "\\n\n",
       "\\n\n",
       "\\n⎢ T ⎥\n",
       "\\n\n",
       " R α\\n⎥ ⎢q2 y ⎥\n",
       "\\n\n",
       "⎢\\nβ\\n⎥ ⎢ T ⎥\n",
       "\\n\n",
       "⎢\\n⎥ ⎢\\n\n",
       "⎢\\n⎥\n",
       "\\n= min \n",
       "⎢ 0 ⎥ − ⎢q3 y ⎥\n",
       " .\\nα,β∈R \n",
       "⎢\\n.. ⎥ ⎢ . ⎥\n",
       "\\n\n",
       "⎣ . ⎦ ⎢ . ⎥\n",
       "\\n\n",
       "\\n⎣ . ⎦\n",
       "\\n\n",
       "\\n\n",
       "\\n0 qT y m\\n\\nHere we have used that Q Q T = Im and Qv = v for all v ∈ Rm,1 .\\nThe upper triangular matrix R is invertible and thus the minimization problem is solved by\\n!\\n\"\\n\u0006 \u0007\\nT q y\\nα\\n1\\n.\\n= R −1 T\\nβ q2 y\\nUsing the definition of the Euclidean norm, we can write the minimizing property of the polynomial p := αt + β as\\n\n",
       "\\n\n",
       "2 \u0004\\n\u0006 \u0007 m\\n\n",
       "\\n\n",
       "\\n\n",
       " [v1 , v2 ] α − y \n",
       " =\\n( p(τi ) − μi )2\\n\n",
       "\\n\n",
       "\\nβ i=1\\n\\n= min\\n\\nα,β∈R m\\n\u0003\u0004\\n\\n\u0005\\n((ατi + β) − μi )2 .\\ni=1\\n\\nSince the polynomial p minimizes the sum of squares of the distances between the measurements μi and the polynomial values p(τi ), this polynomial yields a least squares approximation of the measurement values.\\nConsider the example from Sect.\\n1.4.\\nIn the four quarters of a year, a company has profits of 10, 8, 9, 11 million Euros.\\nUnder the assumption that the profits grows linearly, i.e., like a straight line, the goal is to estimate the profit in the last quarter of the following year.\\nThe given data leads to the approximation problem\\n⎡\\n\\n1\\n⎢2\\n⎢\\n⎣3\\n4\\n\\n⎤\\n⎡ ⎤\\n1 \u0006 \u0007\\n10\\n\u0006 \u0007\\n⎢8⎥\\n1⎥\\nα\\nα\\n⎥\\n⎢\\n⎥\\n≈ y.\\n≈ ⎣ ⎦ or [v1 , v2 ]\\n1⎦ β\\nβ\\n9\\n1\\n11   \n",
       "170                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              180\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\nThe numerical computation of a Q R-decomposition of [v1 , v2 ] yields\\n\u0006 \u0007 \u0006√ 1 √ \u0007−1 ! √1\\nα\\n30 3 √30\\n30\\n=\\n√2\\nβ\\n0 13 6\\n6\\n\u000e\u000f\\n\u0010\\n=R −1\\n\\n⎡\\n\\n⎤\\n10\\n\u0006 \u0007\\n√2 √3\\n√4\\n⎢ 8⎥\\n0.4\\n30\\n30\\n30\\n⎢\\n⎥\\n⎣ 9 ⎦ = 8.5 ,\\n√1\\n√1\\n0\\n−\\n6\\n6\\n\u000e\u000f\\n\u0010 11\\n\"\\n\\n=[q1 ,q2 ]T and the resulting profit estimate for the last quarter of the following year is p(8) =\\n11.7, i.e., 11.7 million Euros.\\n\\nMATLAB-Minute.\\nIn Example 12.16 one could imagine that the profit grows quadratically instead of linearly.\\nDetermine, analogously to the procedure in Example 12.16, a polynomial p = αt 2 + βt + γ that solves the least squares problem\\n4\\n\u0004 i=1\\n\\n( p(τi ) − μi )2 = min\\n\\nα,β,γ∈R\\n\\n4 \u0003\\n\u0004\\n(ατi2 + βτi + γ) − μi\\n\\n2\\n\\n.\\ni=1\\n\\nUse the MATLAB command qr for computing a Q R-decomposition, and determine the estimated profit in the last quarter of the following year.\\n\\nWe will now analyze the properties of orthonormal bases in more detail.\\nLemma 12.17 If V is a Euclidean or unitary vector space with the scalar product\\n\u0002·, ·\u0003 and the orthonormal basis {u 1 , . . . , u n }, then v= n\\n\u0004\\n\u0002v, u i \u0003u i i=1 for all v ∈ V.\\nProof#For every v ∈ V there exist uniquely determined coordinates\\n#nλ1 , . . . , λn with n\\nλi u i .\\nFor every j = 1, . . . , n we then have \u0002v, u j \u0003 = i=1\\nλi \u0002u i , u j \u0003 = v = i=1\\nλj.\\nThe coordinates \u0002v, u i \u0003, i = 1, . . . , n, of v with respect to an orthonormal basis\\n6\\n{u 1 , . . . , u n } are often called\\n#n the Fourier coefficients of v with respect to this basis.\\nThe representation v = i=1\\n\u0002v, u i \u0003u i is called the (abstract) Fourier expansion of v in the given orthonormal basis.\\n\\n6 Jean\\n\\nBaptiste Joseph Fourier (1768–1830).   \n",
       "171                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 12.2 Orthogonality\\n\\n181\\n\\nCorollary 12.18 If V is a Euclidean or unitary vector space with the scalar product\\n\u0002·, ·\u0003 and the orthonormal basis {u 1 , . . . , u n }, then the following assertions hold:\\n#n\\n#n\\n(1) \u0002v, w\u0003 = i=1\\n\u0002v, u i \u0003\u0002u i , w\u0003 = i=1\\n\u0002v, u i \u0003\u0002w, u i \u0003 for all v, w ∈ V (Parseval’s identity7 ).# n\\n|\u0002v, u i \u0003|2 for all v ∈ V (Bessel’s identity8 ).\\n(2) \u0002v, v\u0003 = i=1\\nProof\\n(1) We have v =\\n\\n#n\\n\\n\u0002v, w\u0003 = i=1 \u0002v, u i \u0003u i , and thus n n n\\n% \u0004\\n$\u0004\\n\u0004\\n\u0002v, u i \u0003u i , w =\\n\u0002v, u i \u0003\u0002u i , w\u0003 =\\n\u0002v, u i \u0003\u0002w, u i \u0003.\\ni=1 i=1 i=1\\n\\n(2) is a special case of (1) for v = w.\\nBy Bessel’s identity, every vector v ∈ V satisfies v\\n\\n2\\n\\n= \u0002v, v\u0003 = n\\n\u0004\\n\\n|\u0002v, u i \u0003|2 ≥ max |\u0002v, u i \u0003|2 , i=1\\n\\n1≤i≤n where · is the norm induced by the scalar product.\\nThe absolute value of each coordinate of v with respect to an orthonormal basis of V is therefore bounded by the norm of v. This property does not hold for a general basis of V.\\nExample 12.19 Consider V = R2,1 with the standard scalar product and the Euclidean norm, then for every real ε = 0 the set\\n&\u0006 \u0007 \u0006 \u0007'\\n1\\n1\\n,\\n0\\nε is a basis of V. For every vector v = [ν1 , ν2 ]T we then have\\n\u0006 \u0007\\n\u0006 \u0007\\n\u0003\\nν2 \u0005 1\\nν2 1 v = ν1 −\\n+\\n.\\n0\\nε\\nε ε\\nIf |ν1 |, |ν2 | are moderate numbers and if |ε| is (very) small, then |ν1 − ν2 /ε| and\\n|ν2 /ε| are (very) large.\\nIn numerical algorithms such a situation can lead to significant problems (e.g. due to roundoff errors) that are avoided when orthonormal bases are used.\\n\\n7 Marc-Antoine\\n8 Friedrich\\n\\nParseval (1755–1836).\\nWilhelm Bessel (1784–1846).   \n",
       "172                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       182\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\nDefinition 12.20 Let V be a Euclidean or unitary vector space with the scalar product\\n\u0002·, ·\u0003, and let U ⊆ V be a subspace.\\nThen\\nU ⊥ := {v ∈ V | \u0002v, u\u0003 = 0 for all u ∈ U} is called the orthogonal complement of U (in V).\\nLemma 12.21 The orthogonal complement U ⊥ is a subspace of V.\\nProof Exercise.\\nLemma 12.22 If V is an n-dimensional Euclidean or unitary vector space, and if\\nU ⊆ V is an m-dimensional subspace, then dim(U ⊥ ) = n − m and V = U ⊕ U ⊥ .\\nProof We know that m ≤ n (cp.\\nLemma 9.27).\\nIf m = n, then U = V, and thus\\nU ⊥ = V ⊥ = {v ∈ V | \u0002v, u\u0003 = 0 for all u ∈ V} = {0}, so that the assertion is trivial.\\nThus let m < n and let {u 1 , . . . , u m } be an orthonormal basis of U. We extend this basis to a basis of V and apply the Gram-Schmidt method in order to obtain an orthonormal basis {u 1 , . . . , u m , u m+1 , . . . , u n } of V. Then span{u m+1 , . . . , u n } ⊆ U ⊥ and therefore V = U + U ⊥ .\\nIf w ∈ U ∩ U ⊥ , then \u0002w, w\u0003 = 0, and hence w = 0, since the scalar product is positive definite.\\nThus, U ∩ U ⊥ = {0}, which implies that\\nV = U ⊕ U ⊥ and dim(U ⊥ ) = n − m (cp.\\nTheorem 9.29).\\nIn particular, we have\\nU ⊥ = span{u m+1 , . . . , u n }.\\n\\n12.3 The Vector Product in R3,1\\nIn this section we consider a further product on the vector space R3,1 that is frequently used in Physics and Electrical Engineering.\\nDefinition 12.23 The vector product or cross product in R3,1 is the map\\nR3,1 ×R3,1 → R3,1 , (v, w) \u0005 → v×w := [ν2 ω3 − ν3 ω2 , ν3 ω1 − ν1 ω3 , ν1 ω2 − ν2 ω1 ]T , where v = [ν1 , ν2 , ν3 ]T and w = [ω1 , ω2 , ω3 ]T .\\nIn contrast to the scalar product, the vector product of two elements of the vector space R3,1 is not a scalar but again a vector in R3,1 .\\nUsing the canonical basis vectors of R3,1 , e1 = [1, 0, 0]T , e2 = [0, 1, 0]T , e3 = [0, 0, 1]T ,   \n",
       "173                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      12.3 The Vector Product in R3,1\\n\\n183 we can write the vector product as\\n\u0007\u001f\\n\u0007\u001f\\n\u0007\u001f\\n\n",
       "\u0006\\n\n",
       "\u0006\\n\n",
       "\u0006\\nν1 ω 1\\nν1 ω 1\\nν2 ω2 e1 − det e2 + det e3 .\\nv × w = det\\nν3 ω 3\\nν3 ω 3\\nν2 ω 2\\nLemma 12.24 The vector product is linear in both components and for all v, w ∈\\nR3,1 the following properties hold:\\n(1) v × w = −w × v, i.e., the vector product is anti commutative or alternating.\\n(2) v × w 2 = v 2 w 2 − \u0002v, w\u00032 , where \u0002·, ·\u0003 is the standard scalar product and · the Euclidean norm of R3,1 .\\n(3) \u0002v, v × w\u0003 = \u0002w, v × w\u0003 = 0, where \u0002·, ·\u0003 is the standard scalar product of R3,1 .\\nProof Exercise.\\nBy (2) and the Cauchy-Schwarz inequality (12.2), it follows that v × w = 0 holds if and only if v, w are linearly dependent.\\nFrom (3) we obtain\\n\u0002λv + μw, v × w\u0003 = λ\u0002v, v × w\u0003 + μ\u0002w, v × w\u0003 = 0, for arbitrary λ, μ ∈ R. If v, w are linearly independent, then the product v × w is orthogonal to the plane through the origin spanned by v and w in R3,1 , i.e., v × w ∈ {λv + μw | λ, μ ∈ R}⊥ .\\nGeometrically, there are two possibilities:\\n\\nThe positions of the three vectors v, w, v ×w on the left side of this figure correspond to the “right-handed orientation” of the usual coordinate system of R3,1 , where the canonical basis vectors e1 , e2 , e3 are associated with thumb, index finger and middle finger of the right hand.\\nThis motivates the name right-hand rule.\\nIn order to explain this in detail, one needs to introduce the concept of orientation, which we omit here.\\nIf ϕ ∈ [0, π] is the angle between the vectors v, w, then\\n\u0002v, w\u0003 = v w cos(ϕ)   \n",
       "174                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   184\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\n(cp.\\nDefinition 12.7) and we can write (2) in Lemma 12.24 as v×w\\n\\n2\\n\\n= v\\n\\n2 w\\n\\n2\\n\\n− v\\n\\n2 w cos2 (ϕ) = v\\n\\n2\\n\\n2 w\\n\\n2 sin2 (ϕ), so that v×w = v w sin(ϕ).\\n\\nA geometric interpretation of this equation is the following: The norm of the vector product of v and w is equal to the area of the parallelogram spanned by v and w.\\nThis interpretation is illustrated in the following figure:\\n\\nExercises\\n12.1 Let V be a finite dimensional real or complex vector space.\\nShow that there exists a scalar product on V.\\n12.2 Show that the maps defined in Example 12.2 are scalar products on the corresponding vector spaces.\\n12.3 Let \u0002·, ·\u0003 be an arbitrary scalar product on Rn,1 .\\nShow that there exists a matrix\\nA ∈ Rn,n with \u0002v, w\u0003 = w T Av for all v, w ∈ Rn,1 .\\n12.4 Let V be a finite dimensional R- or C-vector space.\\nLet s1 and s2 be scalar products on V with the following property: If v, w ∈ V satisfy s1 (v, w) = 0, then also s2 (v, w) = 0.\\nProve or disprove: There exists a real scalar λ > 0 with s1 (v, w) = λs2 (v, w) for all v, w ∈ V.\\n12.5 Show that the maps defined in Example 12.4 are norms on the corresponding vector spaces.\\n12.6 Show that n m\\n\u0004\\n\u0004\\nA 1 = max\\n|ai j | and A ∞ = max\\n|ai j |\\n1≤ j≤m\\n\\n1≤i≤n i=1 j=1 for all A = [ai j ] ∈ K n,m , where K = R or K = C (cp.\\n(6) in Example 12.4).\\n12.7 Sketch for the matrix A from (6) in Example 12.4 and p ∈ {1, 2, ∞}, the sets\\n{Av | v ∈ R2,1 , v p = 1 } ⊂ R2,1 .\\n12.8 Let V be a Euclidean or unitary vector space and let · be the norm induced by a scalar product on V. Show that · satisfies the parallelogram identity v+w for all v, w ∈ V.\\n\\n2\\n\\n+ v−w\\n\\n2\\n\\n= 2( v\\n\\n2\\n\\n+ w 2)   \n",
       "175                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            12.3 The Vector Product in R3,1\\n\\n185\\n\\n12.9 Let V be a K -vector space (K = R or K = C) with the scalar product \u0002·, ·\u0003 and the induced norm · .\\nShow that v, w ∈ V are orthogonal with respect to \u0002·, ·\u0003 if and only if v + λw = v − λw for all λ ∈ K .\\n12.10 Does there exist a scalar product \u0002·, ·\u0003 on Cn,1 , such that the 1-norm of Cn,1\\n(cp.\\n(5) in Example 12.4) is the induced norm by this scalar product?\\n12.11 Show that the inequality n\\n\u0003\u0004 i=1\\n\\n12.12\\n\\n12.13\\n\\n12.14\\n\\n12.15\\n\\nαi βi\\n\\n\u00052\\n\\n≤ n\\n\u0004\\n\\n(γi αi )2 · i=1 n \u0003\\n\u0004\\nβi \u00052 i=1\\n\\nγi holds for arbitrary real numbers α1 , . . . , αn , β1 , . . . , βn and positive real numbers γ1 , . . . , γn .\\nLet V be a finite dimensional Euclidean or unitary vector space with the scalar product \u0002·, ·\u0003.\\nLet f : V → V be a map with \u0002 f (v), f (w)\u0003 = \u0002v, w\u0003 for all v, w ∈ V. Show that f is an isomorphism.\\nLet V be a unitary vector space and suppose that f ∈ L(V, V) satisfies\\n\u0002 f (v), v\u0003 = 0 for all v ∈ V. Prove or disprove that f = 0.\\nDoes the same statement also hold for Euclidean vector spaces?\\nLet D = diag(d1 , . . . , dn ) ∈ Rn,n with d1 , . . . , dn > 0.\\nShow that \u0002v, w\u0003 = w T Dv is a scalar product on Rn,1 .\\nAnalyze which properties of a scalar product are violated if at least one of the di is zero, or when all di are nonzero but have different signs.\\nOrthonormalize the following basis of the vector space C2,2 with respect to the scalar product \u0002A, B\u0003 = trace(B H A):\\n&\u0006\\n\\n\u0007 \u0006 \u0007 \u0006 \u0007 \u0006 \u0007'\\n10\\n10\\n11\\n11\\n,\\n,\\n,\\n.\\n00\\n01\\n01\\n11\\n\\n12.16 Let Q ∈ Rn,n be an orthogonal or let Q ∈ Cn,n be a unitary matrix.\\nWhat are the possible values of det(Q)?\\n12.17 Let u ∈ Rn,1 \\ {0} and let\\nH (u) = In − 2\\n\\n1 uT u uu T ∈ Rn,n .\\n\\nShow that the n columns of H (u) form an orthonormal basis of Rn,1 with respect to the standard scalar product.\\n(Matrices of this form are called Householder matrices.9 We will study them in more detail in Example 18.15.)\\n12.18 Prove Lemma 12.21.\\n\\n9 Alston\\n\\nScott Householder (1904–1993), pioneer of Numerical Linear Algebra.   \n",
       "176                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     186\\n\\n12 Euclidean and Unitary Vector Spaces\\n\\n⎡\\n\\n12.19 Let\\n[v1 , v2 , v3 ] =\\n\\n√1\\n⎢ √21\\n⎣− 2\\n\\n0\\n\\n⎤\\n0 √12\\n⎥\\n0 √12 ⎦ ∈ R3,3 .\\n0 0\\n\\nAnalyze whether the vectors v1 , v2 , v3 are orthonormal with respect to the standard scalar product and compute the orthogonal complement of span{v1 , v2 , v3 }.\\n12.20 Let V be a Euclidean or unitary vector space with the scalar product \u0002·, ·\u0003, let u 1 , . . . , u k ∈ V and let U = span{u 1 , . . . , u k }.\\nShow that for v ∈ V we have v ∈ U ⊥ if and only if \u0002v, u j \u0003 = 0 for j = 1, . . . , k.\\n12.21 In the unitary vector space C4,1 with the standard scalar product let v1 =\\n[−1, i, 0, 1]T and v2 = [i, 0, 2, 0]T be given.\\nDetermine an orthonormal basis of span{v1 , v2 }⊥ .\\n12.22 Prove Lemma 12.24.   \n",
       "177                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Chapter 13\\n\\nAdjoints of Linear Maps\\n\\nIn this chapter we introduce adjoints of linear maps.\\nIn some sense these represent generalizations of the (Hermitian) transposes of a matrices.\\nA matrix is symmetric\\n(or Hermitian) if it is equal to its (Hermitian) transpose.\\nIn an analogous way, an endomorphism is selfadjoint if it is equal to its adjoint endomorphism.\\nThe sets of symmetric (or Hermitian) matrices and of selfadjoint endomorphisms form certain vector spaces which will play a key role in our proof of the Fundamental Theorem of\\nAlgebra in Chap.\\n15.\\nSpecial properties of selfadjoint endomorphisms will be studied in Chap.\\n18.\\n\\n13.1 Basic Definitions and Properties\\nIn Chap.\\n12 we have considered Euclidean and unitary vector spaces, and hence vector spaces over the fields R and C. Now let V and W be vector spaces over a general field K , and let β be a bilinear form on V × W.\\nFor every fixed vector v ∈ V, the map\\nβv : W → K , w \u0004→ β(v, w), is a linear form on W. Thus, we can assign to every v ∈ V a vector βv ∈ W ∗ , which defines the map\\n(13.1)\\nβ (1) : V → W ∗ , v \u0004→ βv .\\nAnalogously, we define the map\\nβ (2) : W → V ∗ , w \u0004→ βw ,\\n\\n(13.2) where βw : V → K is defined by v \u0004→ β(v, w) for every w ∈ W.\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_13\\n\\n187   \n",
       "178                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         188\\n\\n13 Adjoints of Linear Maps\\n\\nLemma 13.1 The maps β (1) and β (2) defined in (13.1) and (13.2), respectively, are linear, i.e., β (1) ∈ L(V, W ∗ ) and β (2) ∈ L(W, V ∗ ).\\nIf dim(V) = dim(W) ∈ N and\\nβ is non-degenerate (cp.\\nDefinition 11.9), then β (1) and β (2) are bijective and thus isomorphisms.\\nProof We prove the assertion only for the map β (1) ; the proof for β (2) is analogous.\\nWe first show the linearity.\\nLet v1 , v2 ∈ V and λ1 , λ2 ∈ K .\\nFor every w ∈ W we then have\\nβ (1) (λ1 v1 + λ2 v2 )(w) = β(λ1 v1 + λ2 v2 , w)\\n= λ1 β(v1 , w) + λ2 β(v2 , w)\\n= λ1 β (1) (v1 )(w) + λ2 β (1) (v2 )(w)\\n\u0003\\n\u0002\\n= λ1 β (1) (v1 ) + λ2 β (1) (v2 ) (w), and hence β (1) (λ1 v1 +λ2 v2 ) = λ1 β (1) (v1 )+λ2 β (1) (v2 ).\\nTherefore, β (1) ∈ L(V, W ∗ ).\\nLet now dim(V) = dim(W) ∈ N and let β be non-degenerate.\\nWe show that β (1) ∈\\nL(V, W ∗ ) is injective.\\nBy (5) in Lemma 10.7, this holds if and only if ker(β (1) ) = {0}.\\nIf v ∈ ker(β (1) ), then β (1) (v) = βv = 0 ∈ W ∗ , and thus\\nβv (w) = β(v, w) = 0 for all w ∈ W.\\nSince β is non-degenerate, we have v = 0.\\nFinally, dim(V) = dim(W) and dim(W)\\n= dim(W ∗ ) imply that dim(V) = dim(W ∗ ) so that β (1) is bijective (cp.\\nCorollary 10.11).\\n\u0007\\n\u0006\\nWe next discuss the existence of the adjoint map.\\nTheorem 13.2 If V and W are K -vector spaces with dim(V) = dim(W) ∈ N and\\nβ is a non-degenerate bilinear form on V × W, then the following assertions hold:\\n(1) For every f ∈ L(V, V) there exists a uniquely determined g ∈ L(W, W) with\\nβ( f (v), w) = β(v, g(w)) for all v ∈ V and w ∈ W.\\nThe map g is called the right adjoint of f with respect to β.\\n(2) For every h ∈ L(W, W) there exists a uniquely determined k ∈ L(V, V) with\\nβ(v, h(w)) = β(k(v), w) for all v ∈ V and w ∈ W.\\nThe map k is called the left adjoint of h with respect to β.\\nProof We only show (1); the proof of (2) is analogous.\\nLet V ∗ be the dual space of V, let f ∗ ∈ L(V ∗ , V ∗ ) be the dual map of f , and let β (2) ∈ L(W, V ∗ ) be as in (13.2).\\nSince β is non-degenerate, β (2) is bijective by\\nLemma 13.1.\\nDefine g := (β (2) )−1 ◦ f ∗ ◦ β (2) ∈ L(W, W).   \n",
       "179                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  13.1 Basic Definitions and Properties\\n\\n189\\n\\nThen, for all v ∈ V and w ∈ W,\\nβ(v, g(w)) = β(v, ((β (2) )−1 ◦ f ∗ ◦ β (2) )(w))\\n\u0002\\n\u0003\\n= β (2) ((β (2) )−1 ◦ f ∗ ◦ β (2) )(w) (v)\\n\u0002\\n\u0003\\n= β (2) (β (2) )−1 ( f ∗ (β (2) (w))) (v)\\n\u0003\\n\u0002\\n= β (2) ◦ (β (2) )−1 ◦ β (2) (w) ◦ f (v)\\n= β (2) (w)( f (v))\\n= β( f (v), w).\\n(Recall that the dual map satisfies f ∗ (β (2) (w)) = β (2) (w) ◦ f .)\\nIt remains to show the uniqueness of g.\\nLet \u0004 g ∈ L(W, W) with β(v, \u0004 g (w)) =\\nβ( f (v), w) for all v ∈ V and w ∈ W. Then β(v, \u0004 g (w)) = β(v, g(w)), and hence\\nβ(v, (\u0004 g − g)(w)) = 0 for all v ∈ V and w ∈ W.\\nSince β is non-degenerate in the second variable, we have (\u0004 g − g)(w) = 0 for all w ∈ W, so that g = \u0004 g.\\n\u0007\\n\u0006\\nExample 13.3 Let V = W = K n,1 and β(v, w) = w T Bv with a matrix B ∈\\nG L n (K ), so that β is non-degenerate (cp.\\n(1) in Example 11.10).\\nWe consider the linear map f : V → V, v \u0004→ Fv, with a matrix F ∈ K n,n , and the linear map h : W → W, w \u0004→ H w, with a matrix H ∈ K n,n .\\nThen\\nβv : W → K , w \u0004→ w T (Bv),\\nβ (1) : V → W ∗ , v \u0004→ (Bv)T ,\\nβ (2) : W → V ∗ , w \u0004→ w T B, where we have identified the isomorphic vector spaces W ∗ and K 1,n , respectively\\nV ∗ and K 1,n , with each other.\\nIf g ∈ L(W, W) is the right adjoint of f with respect to β, then\\nβ( f (v), w) = w T B f (v) = w T B Fv = β(v, g(w)) = g(w)T Bv for all v ∈ V and w ∈ W. If we represent the linear map g via the multiplication with a matrix G ∈ K n,n , i.e., g(w) = Gw, then w T B Fv = w T G T Bv for all v, w ∈ K n,1 .\\nHence B F = G T B. Since B is invertible, the unique right adjoint is given by G = (B F B −1 )T = B −T F T B T .\\nAnalogously, for the left adjoint k ∈ L(V, V) of h with respect to β we obtain the equation\\nβ(v, h(w)) = (h(w))T Bv = w T H T Bv = β(k(v), w) = w T Bk(v) for all v ∈ V and w ∈ W. With k(v) = Lv for a matrix L ∈ K n,n , we obtain\\nH T B = B L and hence L = B −1 H T B.   \n",
       "180                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        190\\n\\n13 Adjoints of Linear Maps\\n\\nIf V is finite dimensional and β is a non-degenerate bilinear form on V, then by\\nTheorem 13.2 every f ∈ L(V, V) has a unique right adjoint g and a unique left adjoint k, such that\\nβ( f (v), w) = β(v, g(w)) and β(v, f (w)) = β(k(v), w)\\n\\n(13.3) for all v, w ∈ V. If β is symmetric, i.e., if β(v, w) = β(w, v) holds for all v, w ∈ V, then (13.3) yields\\nβ(v, g(w)) = β( f (v), w) = β(w, f (v)) = β(k(w), v) = β(v, k(w)).\\nTherefore, β(v, (g − k)(w)) = 0 for all v, w ∈ V, and hence g = k, since β is non-degenerate.\\nThus, we have proved the following result.\\nCorollary 13.4 If β is a symmetric and non-degenerate bilinear form on a finite dimensional K -vector space V, then for every f ∈ L(V, V) there exists a unique g ∈ L(V, V) with\\nβ( f (v), w) = β(v, g(w)) and β(v, f (w)) = β(g(v), w) for all v, w ∈ V.\\nBy definition, a scalar product on a Euclidean vector space is a symmetric and nondegenerate bilinear form (cp.\\nDefinition 12.1).\\nThis leads to the following corollary.\\nCorollary 13.5 If V is a finite dimensional Euclidean vector space with the scalar product ·, · , then for every f ∈ L(V, V) there exists a unique f ad ∈ L(V, V) with f (v), w = v, f ad (w) and v, f (w) = f ad (v), w\\n\\n(13.4) for all v, w ∈ V. The map f ad is called the adjoint of f (with respect to ·, · ).\\nIn order to determine whether a given map g ∈ L(V, V) is the unique adjoint of f ∈ L(V, V), only one of the two conditions in (13.4) have to be verified: If for f, g ∈ L(V, V) the equation f (v), w = v, g(w) holds for all v, w ∈ V, then also v, f (w) = f (w), v = w, g(v) = g(v), w for all v, w ∈ V, where we have used the symmetry of the scalar product.\\nSimilarly, if v, f (w) = g(v), w holds for all v, w ∈ V, then also f (v), w = v, g(w) for all v, w ∈ V.   \n",
       "181                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          13.1 Basic Definitions and Properties\\n\\n191\\n\\nExample 13.6 Consider the Euclidean vector space R3,1 with the scalar product\\n⎡\\n\\n⎤\\n100 v, w = w T Dv, where D = ⎣0 2 0⎦ ,\\n001 and the linear map\\n⎡ f : R3,1\\n\\n⎤\\n122\\n→ R3,1 , v \u0004→ Fv, where F = ⎣1 0 1⎦ .\\n200\\n\\nFor all v, w ∈ R3,1 we then have f (v), w = w T D Fv = w T D F D −1 Dv = (D −T F T D T w)T Dv = v, f ad (w) ,\\n⎡ and thus f ad : R3,1\\n\\n⎤\\n122\\n→ R3,1 , v \u0004→ D −1 F T Dv = ⎣1 0 0⎦ v,\\n220 where we have used that D is symmetric.\\nWe now show that uniquely determined adjoint maps also exist in the unitary case.\\nHowever, we cannot conclude this directly from Corollary 13.4, since a scalar product on a C-vector space is not a symmetric bilinear form, but a Hermitian sesquilinear form.\\nIn order to show the existence of the adjoint map in the unitary case we construct it explicitly.\\nThis construction works also in the Euclidean case.\\nLet V be a unitary vector space with the scalar product ·, · and let {u 1 , . . . , u n } be an orthonormal basis of V. For a given f ∈ L(V, V) we define the map n v, f (u i ) u i .\\ng : V → V, v \u0004→ i=1\\n\\nIf v, w ∈ V and λ, μ ∈ C, then n g(λv + μw) = n\\n\\nλv + μw, f (u i ) u i = i=1\\n\\n= λg(v) + μg(w), i=1\\n\\n\u0002\\n\u0003\\nλ v, f (u i ) u i + μ v, f (u i ) u i   \n",
       "182                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     192\\n\\n13 Adjoints of Linear Maps and hence g ∈ L(V, V).\\nLet now v = n\\n\\nλi u i ∈ V and w ∈ V, then n i=1 n w, f (u j ) u j =\\n\\nλi u i , v, g(w) = n i=1 j=1 n\\n\\nλi w, f (u i ) = i=1\\n\\nλi f (u i ), w i=1\\n\\n= f (v), w .\\nFurthermore, v, f (w) = f (w), v = w, g(v) = g(v), w for all v, w ∈ V. If \u0004 g ∈ L(V, V) satisfies f (v), w = v, \u0004 g (w) for all v, w ∈ V, then g = \u0004 g , since the scalar product is positive definite.\\nWe can therefore formulate the following result analogously to Corollary 13.5.\\nCorollary 13.7 If V is a finite dimensional unitary vector space with the scalar product ·, · , then for every f ∈ L(V, V) there exists a unique f ad ∈ L(V, V) with f (v), w = v, f ad (w) and v, f (w) = f ad (v), w\\n\\n(13.5) for all v, w ∈ V. The map f ad is called the adjoint of f (with respect to ·, · ).\\nAs in the Euclidean case, again the validity of one of the two equations in (13.5) for all v, w ∈ V implies the validity of the other for all v, w ∈ V.\\nExample 13.8 Consider the unitary vector space C3,1 with the scalar product\\n⎡\\n\\n⎤\\n100 v, w = w H Dv, where D = ⎣0 2 0⎦ ,\\n001 and the linear map\\n⎡ f : C3,1\\n\\n⎤\\n1 2i 2\\n→ C3,1 , v \u0004→ Fv, where F = ⎣ i 0 −i ⎦ .\\n2 0 3i\\n\\nFor all v, w ∈ C3,1 we then have f (v), w = w H D Fv = w H D F D −1 Dv = (D −H F H D H w) H Dv\\n= v, f ad (w) , and thus\\n⎡ f ad : C3,1\\n\\n⎤\\n1 −2i 2\\n→ C3,1 , v \u0004→ D −1 F H Dv = ⎣ −i 0 0 ⎦ v,\\n2 2i −3i   \n",
       "183                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       13.1 Basic Definitions and Properties\\n\\n193 where we have used that D is real and symmetric.\\nWe next investigate the properties of the adjoint map.\\nLemma 13.9 Let V be a finite dimensional Euclidean or unitary vector space.\\n(1) If f 1 , f 2 ∈ L(V, V) and λ1 , λ2 ∈ K (where K = R in the Euclidean and K = C in the unitary case), then\\n(λ1 f 1 + λ2 f 2 )ad = λ1 f 1ad + λ2 f 2ad .\\nIn the Euclidean case the map f \u0004→ f ad is therefore linear, and in the unity case semilinear.\\n(2) We have (IdV )ad = IdV .\\n(3) For every f ∈ L(V, V) we have ( f ad )ad = f .\\n(4) If f 1 , f 2 ∈ L(V, V), then ( f 2 ◦ f 1 )ad = f 1ad ◦ f 2ad .\\nProof\\n(1) If v, w ∈ V and λ1 , λ2 ∈ K , then\\n(λ1 f 1 + λ2 f 2 )(v), w = λ1 f 1 (v), w + λ2 f 2 (v), w\\n\u000e\\n\u000e\\n= λ1 v, f 1ad (w) + λ2 v, f 2ad (w)\\n= v, λ1 f 1ad (w) + λ2 f 2ad (w)\\n\u000f\\n\u0010\\n= v, λ1 f 1ad + λ2 f 2ad (w) , and thus (λ1 f 1 + λ2 f 2 )ad = λ1 f 1ad + λ2 f 2ad .\\n(2) For all v, w ∈ V we have IdV (v), w = v, w = v, IdV (w) , and thus\\n(IdV )ad = IdV .\\n(3) For all v, w ∈ V we have f ad (v), w = v, f (w) , and thus ( f ad )ad = f .\\n(4) For all v, w ∈ V we have\\n\u0002\\n\u000e\\n\u0003\u000e\\n( f 2 ◦ f 1 )(v), w = f 2 ( f 1 (v)), w = f 1 (v), f 2ad (w) = v, f 1ad f 2ad (w)\\n\u0002\\n\u0003\\n\u000e\\n= v, f 1ad ◦ f 2ad (w) , and thus ( f 2 ◦ f 1 )ad = f 1ad ◦ f 2ad .\\n\\n\u0007\\n\u0006\\n\\nThe following result shows relations between the image and kernel of an endomorphism and of its adjoint.\\nTheorem 13.10 If V is a finite dimensional Euclidean or unitary vector space and f ∈ L(V, V), then the following assertions hold:   \n",
       "184                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             194\\n\\n13 Adjoints of Linear Maps\\n\\n(1) ker( f ad ) = im( f )⊥ .\\n(2) ker( f ) = im( f ad )⊥ .\\nProof\\n(1) If w ∈ ker( f ad ), then f ad (w) = 0 and\\n0 = v, f ad (w) = f (v), w for all v ∈ V, hence w ∈ im( f )⊥ .\\nIf, on the other hand, w ∈ im( f )⊥ , then\\n0 = f (v), w = v, f ad (w) for all v ∈ V. Since ·, · is non-degenerate, we have f ad (w) = 0 and, hence, w ∈ ker( f ad ).\\n\u0007\\n\u0006\\n(2) Using ( f ad )ad = f and (1) we get ker( f ) = ker(( f ad )ad ) = im( f ad )⊥ .\\nExample 13.11 Consider the unitary vector space C3,1 with the standard scalar product and the linear map f : C3,1\\n\\n⎡\\n⎤\\n1 i i\\n→ C3,1 , v \u0004→ Fv, with F = ⎣ i 0 0⎦ .\\n100\\n⎡\\n\\nThen f ad : C3,1 → C3,1 , v \u0004→ F H v, with F H\\n\\n⎤\\n1 −i 1\\n= ⎣ −i 0 0 ⎦ .\\n−i 0 0\\n\\nThe matrices F and F H have rank 2.\\nTherefore, dim(ker( f )) = dim(ker( f ad )) = 1.\\nA simple calculation shows that\\n⎧⎡\\n⎧⎡ ⎤⎫\\n⎤⎫\\n0 ⎬\\n⎨\\n⎨ 0 ⎬ ker( f ) = span ⎣ 1 ⎦ and ker( f ad ) = span ⎣1⎦ .\\n⎩\\n⎭\\n⎩\\n⎭\\n−1 i\\nThe dimension formula for linear maps implies that dim(im( f )) = dim(im( f ad )) = 2.\\nFrom the matrices F and F H we can see that\\n⎧⎡ ⎤ ⎡ ⎤⎫\\n⎧⎡ ⎤ ⎡ ⎤⎫\\n1 ⎬\\n1 ⎬\\n⎨ 1\\n⎨ 1 im( f ) = span ⎣ i ⎦ , ⎣0⎦ and im( f ad ) = span ⎣ −i ⎦ , ⎣0⎦ .\\n⎩\\n⎭\\n⎩\\n⎭\\n1\\n0\\n−i\\n0\\nThe equations ker( f ad ) = im( f )⊥ and ker( f ) = im( f ad )⊥ can be verified by direct computation.   \n",
       "185                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     13.2 Adjoint Endomorphisms and Matrices\\n\\n195\\n\\n13.2 Adjoint Endomorphisms and Matrices\\nWe now study the relation between the matrix representations of an endomorphism and its adjoint.\\nLet V be a finite dimensional unitary vector space with the scalar product ·, · and let f ∈ L(V, V).\\nFor an orthonormal basis B = {u 1 , . . . , u n } of V let [ f ] B,B = [ai j ] ∈ Cn,n , i.e., n f (u j ) = ak j u k , j = 1, . . . , n, k=1 and hence n ak j u k , u i = ai j , i, j = 1, . . . , n.\\nf (u j ), u i = k=1\\n\\nIf [ f ad ] B,B = [bi j ] ∈ Cn,n , i.e., n f ad (u j ) = bk j u k , j = 1, . . . , n, k=1 then bi j = f ad (u j ), u i = u j , f (u i ) = f (u i ), u j = a ji .\\nThus, [ f ad ] B,B = ([ f ] B,B ) H .\\nThe same holds for a finite dimensional Euclidean vector space, but then we can omit the complex conjugation.\\nTherefore, we have shown the following result.\\nTheorem 13.12 If V is a finite dimensional Euclidean or unitary vector space with the orthonormal basis B and f ∈ L(V, V), then\\n[ f ad ] B,B = ([ f ] B,B ) H .\\n(In the Euclidean case ([ f ] B,B ) H = ([ f ] B,B )T .)\\nAn important special class are the selfadjoint endomorphisms.\\nDefinition 13.13 Let V be a finite dimensional Euclidean or unitary vector space.\\nAn endomorphism f ∈ L(V, V) is called selfadjoint when f = f ad .\\nTrivial examples of selfadjoint endomorphism in L(V, V) are f = 0 and IdV .\\nCorollary 13.14\\n(1) If V is a finite dimensional Euclidean vector space, f ∈ L(V, V) is selfadjoint and B is an orthonormal basis of V, then [ f ] B,B is a symmetric matrix.   \n",
       "186                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   196\\n\\n13 Adjoints of Linear Maps\\n\\n(2) If V is a finite dimensional unitary vector space, f ∈ L(V, V) is selfadjoint and\\nB is an orthonormal basis of V, then [ f ] B,B is an Hermitian matrix.\\nThe selfadjoint endomorphisms again form a vector space.\\nHowever, one has to be careful to use the appropriate field over which this vector space is defined.\\nIn particular, the set of selfadjoint endomorphisms on a unitary vector space V does not form a C-vector space.\\nIf f = f ad ∈ L(V, V) \\ {0}, then (i f )ad = −i f ad = −i f = i f (cp.\\n(1) in Lemma 13.9).\\nSimilarly, the Hermitian matrices in Cn,n do not form a C-vector space.\\nIf A = A H ∈ Cn,n \\ {0} is Hermitian, then (i A) H = −i A H =\\n−i A = i A.\\nLemma 13.15\\n(1) If V is an n-dimensional Euclidean vector space, then the set of selfadjoint endomorphisms { f ∈ L(V, V) | f = f ad } forms an R-vector space of dimension n(n + 1)/2.\\n(2) If V is an n-dimensional unitary vector space, then the set of selfadjoint endomorphisms { f ∈ L(V, V) | f = f ad } forms an R-vector space of dimension n2.\\nProof Exercise.\\n\u0007\\n\u0006\\nA matrix A ∈ Cn,n with A = A T is called complex symmetric.\\nUnlike the Hermitian matrices, the complex symmetric matrices form a C-vector space.\\nLemma 13.16 The set of complex symmetric matrices in Cn,n forms a C-vector space of dimension n(n + 1)/2.\\nProof Exercise.\\n\u0007\\n\u0006\\nLemmas 13.15 and 13.16 will be used in Chap.\\n15 in our proof of the Fundamental\\nTheorem of Algebra.\\nExercises\\n13.1.\\nLet β(v, w) = w T Bv with B = diag(1, −1) be defined for v, w ∈ R2,1 .\\nConsider the linear maps f : R2,1 → R2,1 , v \u0004→ Fv, and h : R2,1 → R2,1 , w \u0004→ H w, where\\n\u0017 \u0018\\n\u0017 \u0018\\n12\\n10\\nF=\\n∈ R2,2 , H =\\n∈ R2,2 .\\n01\\n11\\nDetermine βv , β (1) and β (2) as in (13.1)–(13.2) as well as the right adjoint of f and the left adjoint of h with respect to β.\\n13.2.\\nLet (V, ·, · V ) and (W, ·, · W ) be two finite dimensional Euclidean vector spaces and let f ∈ L(V, W).\\nShow that there exists a unique g ∈\\nL(W, V) with f (v), w W = v, g(w) V for all v ∈ V and w ∈ W.\\n13.3.\\nLet v, w = w T Bv for all v, w ∈ R2,1 with\\n\u0017\\nB=\\n\\n\u0018\\n21\\n∈ R2,2 .\\n11   \n",
       "187                                                                                                                                                                                                                                                                                                                                                                                               13.2 Adjoint Endomorphisms and Matrices\\n\\n197\\n\\n(a) Show that v, w = w T Bv is a scalar product on R2,1 .\\n(b) Using this scalar product, determine the adjoint map f ad of f : R2,1 →\\nR2,1 , v \u0004→ Fv, with F ∈ R2,2 .\\n(c) Investigate which properties F needs to satisfy so that f is selfadjoint.\\n13.4.\\nLet n ≥ 2 and f : Rn,1 → Rn,1 , [x1 , . . . , xn ]T \u0004→ [0, x1 , . . . , xn−1 ]T .\\n\\n13.5.\\n13.6.\\n\\n13.7.\\n\\n13.8.\\n\\n13.9.\\n13.10.\\n\\n13.11.\\n\\n13.12.\\n\\nDetermine the adjoint f ad of f with respect to the standard scalar product of\\nRn,1 .\\nLet V be a finite dimensional Euclidean or unitary vector space and let f ∈\\nL(V, V).\\nShow that ker( f ad ◦ f ) = ker( f ) and im( f ad ◦ f ) = im( f ad ).\\nLet V be a finite dimensional Euclidean or unitary vector space, let U ⊆ V be a subspace and let f ∈ L(V, V) with f (U) ⊆ U. Show that then f ad (U ⊥ ) ⊆\\nU ⊥.\\nLet V be a finite dimensional Euclidean or unitary vector space, let f ∈\\nL(V, V) and v ∈ V. Show that v ∈ im( f ) if and only if v ∈ ker( f ad )⊥ .\\n“Matrix version”: For A ∈ Cn,n and b ∈ Cn,1 the linear system of equations\\nAx = b has a solution if and only if b ∈ L (A H , 0)⊥ .\\nLet V be a finite dimensional Euclidean or unitary vector space and let f, g ∈\\nL(V, V) be selfadjoint.\\nShow that f ◦ g is selfadjoint if and only if f and g commute, i.e., f ◦ g = g ◦ f .\\nLet V be a finite dimensional unitary vector space and let f ∈ L(V, V).\\nShow that f is selfadjoint if and only if f (v), v ∈ R holds for all v ∈ V.\\nLet V be a finite dimensional Euclidean or unitary vector space and let f ∈\\nL(V, V) be a projection, i.e., f satisfies f 2 = f .\\nShow that f is selfadjoint if and only if ker( f ) ⊥ im( f ), i.e., v, w = 0 holds for all v ∈ ker( f ) and w ∈ im( f ).\\nLet V be a finite dimensional Euclidean or unitary vector space and let f, g ∈\\nL(V, V).\\nShow that if g ad ◦ f = 0 ∈ L(V, V), then v, w = 0 holds for all v ∈ im( f ) and w ∈ im(g).\\nFor two polynomials p, q ∈ R[t]≤n let\\n\u0019 1 p(t)q(t) dt.\\np, q :=\\n−1\\n\\n(a) Show that this defines a scalar product on R[t]≤n .\\n(b) Consider the map n f : R[t]≤n → R[t]≤n , p= n\\n\\nαi t i \u0004→ i=0 iαi t i−1 , i=1 and determine f ad , ker( f ad ), im( f ), ker( f ad )⊥ and im( f )⊥ .\\n13.13.\\nProve Lemma 13.15.\\n13.14.\\nProve Lemma 13.16.   \n",
       "188                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Chapter 14\\n\\nEigenvalues of Endomorphisms\\n\\nIn previous chapters we have already studied eigenvalues and eigenvectors of matrices.\\nIn this chapter we generalize these concepts to endomorphisms, and we investigate when endomorphisms on finite dimensional vector spaces can be represented by diagonal matrices or (upper) triangular matrices.\\nFrom such representations we easily can read off important information about the endomorphism, in particular its eigenvalues.\\n\\n14.1 Basic Definitions and Properties\\nWe first consider an arbitrary vector space and then concentrate on the finite dimensional case.\\nDefinition 14.1 Let V be a K -vector space and f ∈ L(V, V).\\nIf λ ∈ K and v ∈\\nV \\ {0} satisfy f (v) = λv, then λ is called an eigenvalue of f , and v is called an eigenvector of f corresponding to λ.\\nBy definition, v = 0 cannot be an eigenvector, but an eigenvalue λ = 0 may occur\\n(cp. the example following Definition 8.7).\\nThe equation f (v) = λv can be written as\\n0 = λv − f (v) = (λIdV − f )(v).\\nHence, λ ∈ K is an eigenvalue of f if and only if ker(λIdV − f ) \u0003= {0}.\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_14\\n\\n199   \n",
       "189                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        200\\n\\n14 Eigenvalues of Endomorphisms\\n\\nWe already know that the kernel of an endomorphism on V forms a subspace of V\\n(cp.\\nLemma 10.7).\\nThis holds, in particular, for ker(λIdV − f ).\\nDefinition 14.2 If V is a K -vector space and λ ∈ K is an eigenvalue of f ∈ L(V, V), then the subspace\\nV f (λ) := ker(λIdV − f ) is called the eigenspace of f corresponding to λ and g(λ, f ) := dim(V f (λ)) is called the geometric multiplicity of the eigenvalue λ.\\nBy definition, the eigenspace V f (λ) is spanned by all eigenvectors of f corresponding to the eigenvalue λ.\\nIf V f (λ) is finite dimensional, then g(λ, f ) = dim(V f (λ)) is equal to the maximal number of linearly independent eigenvectors of f corresponding to λ.\\nDefinition 14.3 Let V be a K -vector space, let U ⊆ V be a subspace, and let f ∈ L(V, V).\\nIf f (U) ⊆ U, i.e., if f (u) ∈ U holds for all u ∈ U, then U is called an f -invariant subspace of V.\\nAn important example of f -invariant subspaces are the eigenspaces of f .\\nLemma 14.4 If V is a K -vector space and λ ∈ K is an eigenvalue of f ∈ L(V, V), then V f (λ) is an f -invariant subspace of V.\\nProof For every v ∈ V f (λ) we have f (v) = λv ∈ V f (λ).\\n\\n\u0006\\n\u0005\\n\\nWe now consider finite dimensional vector spaces and discuss the relationship between the eigenvalues of f and the eigenvalues of a matrix representation of f with respect to a given basis.\\nLemma 14.5 If V is a finite dimensional K -vector space and f ∈ L(V, V), then the following statements are equivalent:\\n(1) λ ∈ K is an eigenvalue of f .\\n(2) λ ∈ K is an eigenvalue of the matrix [ f ] B,B for every basis B of V.\\nProof Let λ ∈ K be an eigenvalue of f and let B = {v1 , . . . , vn } be an arbitrary basis of V. If v ∈ V is an eigenvector of f corresponding to the eigenvalue λ, then f (v) = λv and\u0002there exist (unique) coordinates μ1 , . . . , μn ∈ K , not all equal to zero, with v = nj=1 μ j v j .\\nUsing (10.4) we obtain\\n⎡\\n\\n⎤\\n⎡ ⎤\\nμ1\\nμ1\\n⎢ .. ⎥\\n⎢ .. ⎥\\n[ f ] B,B ⎣ . ⎦ = \u0002 B ( f (v)) = \u0002 B (λv) = λ\u0002 B (v) = λ ⎣ . ⎦ ,\\nμn\\n\\nμn   \n",
       "190                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           14.1 Basic Definitions and Properties\\n\\n201 and thus λ is an eigenvalue of [ f ] B,B .\\nIf, on the other hand, [ f ] B,B [μ1 , . . . , μn ]T = λ[μ1 , . . . , μn ]T with [μ1 , . . . ,\\n\u0003= 0 for a given (arbitrary) basis B = {v1 , . . . , vn } of V, then we set\\nμn ]T \u0002 v := nj=1 μ j v j .\\nThen v \u0003= 0 and\\n⎤\\n⎡ ⎤\\nμ1\\nμ1\\n⎢ ⎥\\n⎢ ⎥ f (v) =\\nμ j f (v j ) = ( f (v1 ), . . . , f (vn )) ⎣ ... ⎦ = (v1 , . . . , vn )[ f ] B,B ⎣ ... ⎦ j=1\\nμn\\nμn\\n⎛ ⎡ ⎤⎞\\nμ1\\n⎜ ⎢ .. ⎥⎟\\n= (v1 , . . . , vn ) ⎝λ ⎣ . ⎦⎠ = λv,\\n⎡ n\\n\\nμn i.e., λ is an eigenvalue of f .\\n\\n\u0006\\n\u0005\\n\\nLemma 14.5 implies that the eigenvalues of f are the roots of the characteristic polynomial of the matrix [ f ] B,B (cp.\\nTheorem 8.8).\\nThis, however, does not hold\\n\u0012 in general for a matrix representation of the form [ f ] B, \u0012\\nB , where B and B are two different bases of V. In general, the two matrices\\n[ f ] B, \u0012\\nB = [IdV ] B, \u0012\\nB [ f ] B,B and [ f ] B,B do not have the same eigenvalues.\\nExample 14.6 Consider the vector space R2,1 with the bases\\nB=\\n\\n\u0013\u0014 \u0015 \u0014 \u0015\u0016\\n0\\n1\\n,\\n,\\n1\\n0\\n\\n\u0012\\nB=\\n\\n\u0013\u0014\\n\\n\u0015 \u0014 \u0015\u0016\\n1\\n1\\n.\\n,\\n1\\n−1\\n\\nThen the endomorphism\\n\u0014 f : R2,1 → R2,1 , v \b→ Fv, where F =\\n\\n\u0015\\n01\\n,\\n10 has the matrix representations\\n\u0015\\n\u0014\\n\u0015\\n1 −1 1\\n01\\n.\\n, [ f ] B, \u0012\\n=\\nB =\\n11\\n10\\n2\\n\u0014\\n\\n[ f ] B,B\\n\\nWe have det(t I2 − [ f ] B,B ) = t 2 − 1, and thus f has the eigenvalues −1 and 1.\\nOn\\n1\\n2 the other hand, the characteristic polynomial of [ f ] B, \u0012\\nB is t − 2 , so that this matrix\\n√\\n√ has the eigenvalues −1/ 2 and 1/ 2.\\nFor two different bases B and \u0012\\nB of V the matrices [ f ] B,B and [ f ] \u0012\\nB, \u0012\\nB are similar\\n(cp. the discussion following Corollary 10.20).\\nIn Theorem 8.12 we have shown that   \n",
       "191                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               202\\n\\n14 Eigenvalues of Endomorphisms similar matrices have the same characteristic polynomial.\\nThis justifies the following definition.\\nDefinition 14.7 If n ∈ N, V is an n-dimensional K -vector space with the basis B, and f ∈ L(V, V), then\\nP f := det(t In − [ f ] B,B ) ∈ K [t] is called the characteristic polynomial of f .\\nThe characteristic polynomial P f is always a monic polynomial with deg(P f ) = n = dim(V).\\nAs we have discussed before, P f is independent of the choice of the basis of V. A scalar λ ∈ K is an eigenvalue of f if and only if λ is a root of P f , i.e., P f (λ) = 0.\\nAs shown in Example 8.9, in real vector spaces with dimensions at least two, there exist endomorphisms that do not have eigenvalues.\\nIf λ is a root of P f , then P f = (t − λ) · q for a monic polynomial q ∈ K [t], i.e., the linear factor t − λ divides the polynomial P f ; we will show this formally in\\nCorollary 15.5 below.\\nIf also q(λ) = 0, then q = (t − λ) · \u0012 q for a monic polynomial q .\\nWe can continue until P f = (t − λ)d · g for a\\n\u0012 q ∈ K [t], and thus P f = (t − λ)2 · \u0012 g ∈ K [t] with g(λ) \u0003= 0.\\nThis leads to the following definition.\\nDefinition 14.8 Let V be a finite dimensional K -vector space, and let f ∈ L(V, V) have the eigenvalue λ ∈ K .\\nIf the characteristic polynomial of f has the form\\nP f = (t − λ)d · g for some g ∈ K [t] with g(λ) \u0003= 0, then d is called the algebraic multiplicity of the eigenvalue λ of f .\\nIt is denoted by a(λ, f ).\\nIf λ1 , . . . , λk are the pairwise distinct eigenvalues of f with corresponding algebraic multiplicities a(λ1 , f ), . . . , a(λk , f ), and if dim(V) = n, then a(λ1 , f ) + . . . + a(λk , f ) ≤ n, since deg(P f ) = dim(V) = n.\\nExample 14.9 The endomorphism f : R4,1 → R4,1 , v \b→ Fv with\\n⎡\\n\\n1\\n⎢0\\n⎢\\nF =⎣\\n0\\n0\\n\\n⎤\\n2 34\\n1 2 3⎥\\n⎥ ∈ R4,4 ,\\n0 0 1⎦\\n0 −1 0 has the characteristic polynomial P f = (t −1)2 (t 2 +1).\\nThe only real root of P f is 1, and a(λ1 , f ) = 2 < 4 = dim(R4,1 ).   \n",
       "192                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               14.1 Basic Definitions and Properties\\n\\n203\\n\\nLemma 14.10 If V is a finite dimensional K -vector space and f ∈ L(V, V), then g(λ, f ) ≤ a(λ, f ) for every eigenvalue λ of f .\\nProof Let λ ∈ K be an eigenvalue of f with geometric multiplicity m = g(λ, f ).\\nThen there exist m linear independent eigenvectors v1 , . . . , vm ∈ V of f corresponding to the eigenvalue λ.\\nIf m = dim(V), then these m eigenvectors form a basis B of V. If m < dim(V) = n, then we can extend the m eigenvectors to a basis\\nB = {v1 , . . . , vm , vm+1 , . . . , vn } of V.\\nWe have f (v j ) = λv j for j = 1, . . . , m and, therefore,\\n\u0014\\n[ f ] B,B =\\n\\nλIm Z 1\\n0 Z2\\n\\n\u0015 for two matrices Z 1 ∈ K m,n−m and Z 2 ∈ K n−m,n−m .\\nUsing (1) in Lemma 7.10 we obtain\\nP f = det(t In − [ f ] B,B ) = (t − λ)m · det(t In−m − Z 2 ), which implies a(λ, f ) ≥ m = g(λ, f ).\\n\\n\u0006\\n\u0005\\n\\nIn the following we will try to find a basis of V, so that the eigenvalues of a given endomorphism f can be read off easily from its matrix representation.\\nThe easiest forms of matrices in this sense are diagonal and triangular matrices, since their eigenvalues are just their diagonal entries.\\n\\n14.2 Diagonalizability\\nIn this section we will analyze when for a given endomorphism has a diagonal matrix representation.\\nWe formally define this property as follows.\\nDefinition 14.11 Let V be a finite dimensional K -vector space.\\nAn endomorphism f ∈ L(V, V) is called diagonalizable, if there exists a basis B of V, such that [ f ] B,B is a diagonal matrix.\\nAccordingly, a matrix A ∈ K n,n is diagonalizable when there exists a matrix\\nS ∈ G L n (K ) with A = S DS −1 for a diagonal matrix D ∈ K n,n .\\nIn order to analyze the diagonalizablility, we begin with a sufficient condition for the linear independence of eigenvectors.\\nThis condition also holds when V is infinite dimensional.\\nLemma 14.12 Let V be a K -vector space and f ∈ L(V, V).\\nIf λ1 , . . . , λk ∈ K , k ≥ 2, are pairwise distinct eigenvalues of f with corresponding eigenvectors v1 , . . . , vk ∈ V, then v1 , . . . , vk are linearly independent.   \n",
       "193                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  204\\n\\n14 Eigenvalues of Endomorphisms\\n\\nProof We prove the assertion by induction on k.\\nLet k = 2 and let v1 , v2 be eigenvectors of f corresponding to the eigenvalues λ1 \u0003= λ2 .\\nLet μ1 , μ2 ∈ K with\\nμ1 v1 + μ2 v2 = 0.\\nApplying f on both sides of this equation as well as multiplying the equation with λ2 yields the two equations\\nμ1 λ1 v1 + μ2 λ2 v2 = 0,\\nμ1 λ2 v1 + μ2 λ2 v2 = 0.\\nSubtracting the second equation from the first, we get μ1 (λ1 − λ2 )v1 = 0.\\nSince\\nλ1 \u0003= λ2 and v1 \u0003= 0, we have μ1 = 0.\\nThen from μ1 v1 + μ2 v2 = 0 we also obtain\\nμ2 = 0, since v2 \u0003= 0.\\nThus, v1 and v2 are linearly independent.\\nThe proof of the inductive step is analogous.\\nWe assume that the assertion holds for some k ≥ 2.\\nLet λ1 , . . . , λk+1 be pairwise distinct eigenvalues of f with corresponding eigenvectors v1 , . . . , vk+1 , and let μ1 , . . . , μk+1 ∈ K satisfy\\nμ1 v1 + . . . + μk vk + μk+1 vk+1 = 0.\\nApplying f to this equation yields\\nμ1 λ1 v1 + . . . + μk λk vk + μk+1 λk+1 vk+1 = 0, while a multiplication with λk+1 gives\\nμ1 λk+1 v1 + . . . + μk λk+1 vk + μk+1 λk+1 vk+1 = 0.\\nSubtracting this equation from the previous one we get\\nμ1 (λ1 − λk+1 )v1 + . . . + μk (λk − λk+1 )vk = 0.\\nSince λ1 , . . . , λk+1 are pairwise distinct and v1 , . . . , vk are linearly independent by the induction hypothesis, we obtain μ1 = · · · = μk = 0.\\nBut then μk+1 vk+1 = 0\\n\u0006\\n\u0005 implies that also μk+1 = 0, so that v1 , . . . , vk+1 are linearly independent.\\nUsing this result we next show that the sum of eigenspaces corresponding to pairwise distinct eigenvalues is direct (cp.\\nTheorem 9.31).\\nLemma 14.13 Let V be a K -vector space and f ∈ L(V, V).\\nIf λ1 , . . . , λk ∈ K , k ≥ 2, are pairwise distinct eigenvalues of f , then the corresponding eigenspaces satisfy k\\n\\nV f (λi ) ∩\\n\\nV f (λ j ) = {0} j=1 j\u0003 =i for all i = 1, . . . , k.   \n",
       "194                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              14.2 Diagonalizability\\n\\n205\\n\\nProof Let i be fixed and let k v ∈ V f (λi ) ∩\\n\\nV f (λ j ).\\nj=1 j\u0003 =i\\n\\n\u0002\\nIn j\u0003=i v j for some v j ∈ V f (λ j ), j \u0003 = i.\\nThen −v +\\n\u0002 particular, we have v = v\\n=\\n0, and the linear independence of eigenvectors corresponding to pairwise j\u0003=i j distinct eigenvalues (cp.\\nLemma 14.12) implies v = 0.\\n\u0006\\n\u0005\\nThe following theorem gives necessary and sufficient conditions for the diagonalizability of an endomorphism on a finite dimensional vector space.\\nTheorem 14.14 If V is a finite dimensional K -vector space and f ∈ L(V, V), then the following statements are equivalent:\\n(1) f is diagonalizable.\\n(2) There exists a basis of V consisting of eigenvectors of f .\\n(3) The characteristic polynomial P f decomposes into n = dim(V) linear factors over K , i.e.,\\nP f = (t − λ1 ) · . . . · (t − λn ) with the eigenvalues λ1 , . . . , λn ∈ K of f , and for every eigenvalue λ j we have g(λ j , f ) = a(λ j , f ).\\nProof\\n(1) ⇔ (2): If f ∈ L(V, V) is diagonalizable, then there exists a basis B =\\n{v1 , . . . , vn } of V and scalars λ1 , . . . , λn ∈ K with\\n\\n[ f ] B,B\\n\\n⎡\\nλ1\\n⎢ ..\\n=⎣\\n.\\n\\n⎤\\n⎥\\n⎦,\\n\\n(14.1)\\n\\nλn and hence f (v j ) = λ j v j , j = 1, . . . , n.\\nThe scalars λ1 , . . . , λn are thus eigenvalues of f , and the corresponding eigenvectors are v1 , . . . , vn .\\nIf, on the other hand, there exists a basis B = {v1 , . . . , vn } of V consisting of eigenvectors of f , then f (v j ) = λ j v j , j = 1, . . . , n, for scalars λ1 , . . . , λn ∈ K\\n(the corresponding eigenvalues), and hence [ f ] B,B has the form (14.1).\\n(2) ⇒ (3): Let B = {v1 , . . . , vn } be a basis of V consisting of eigenvectors of f , and let λ1 , . . . , λn ∈ K be the corresponding eigenvalues.\\nThen [ f ] B,B has the form (14.1) and hence\\nP f = (t − λ1 ) · . . . · (t − λn ), so that P f decomposes into linear factors over K .   \n",
       "195                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 206\\n\\n14 Eigenvalues of Endomorphisms\\n\\nWe still have to show that g(λ j , f ) = a(λ j , f ) for every eigenvalue λ j .\\nThe eigenvalue λ j has the algebraic multiplicity m j := a(λ j , f ) if and only if λ j occurs m j times on the diagonal of the (diagonal) matrix [ f ] B,B .\\nThis holds if and only if exactly m j vectors of the basis B are eigenvectors of f corresponding to the eigenvalue λ j .\\nEach of these m j linearly independent vectors is a element of the eigenspace V f (λ j ) and, hence, dim(V f (λ j )) = g(λ j , f ) ≥ m j = a(λ j , f ).\\nFrom Lemma 14.10 we know that g(λ j , f ) ≤ a(λ j , f ), and thus g(λ j , f ) = a(λ j , f ).\\nλk be the pairwise distinct eigenvalues of f with correspond(3) ⇒ (2): Let \u0012\\nλ1 , . . . , \u0012\\nλ j , f ), j = 1, . . . , k, ing geometric and algebraic multiplicities g(\u0012\\nλ j , f ) and a(\u0012 respectively.\\nSince P f decomposes into linear factors, we have k a(\u0012\\nλ j , f ) = n = dim(V).\\nj=1\\n\\nNow g(\u0012\\nλ j , f ) = a(\u0012\\nλ j , f ), j = 1, . . . , k, implies that k g(\u0012\\nλ j , f ) = n = dim(V).\\nj=1\\n\\nBy Lemma 14.13 we obtain (cp. also Theorem 9.31)\\nV f (\u0012\\nλ1 ) ⊕ . . . ⊕ V f (\u0012\\nλk ) = V.\\nλ j ), j = 1, . . . , k, then we\\nIf we select bases of the respective eigenspaces V f (\u0012 get a basis of V that consists of eigenvectors of f .\\n\u0006\\n\u0005\\nTheorem 14.14 and Lemma 14.12 imply an important sufficient condition for diagonalizability.\\nCorollary 14.15 If V is an n-dimensional K -vector space and f ∈ L(V, V) has n pairwise distinct eigenvalues, then f is diagonalizable.\\nThe condition of having n = dim(V) pairwise distinct eigenvalues is, however, not necessary for the diagonalizability of an endomorphism.\\nA simple counterexample is the identity IdV , which has the n-fold eigenvalue 1, while [IdV ] B,B = In holds for every basis B of V. On the other hand, there exist endomorphisms with multiple eigenvalues that are not diagonalizable.   \n",
       "196                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         14.2 Diagonalizability\\n\\n207\\n\\nExample 14.16 The endomorphism\\n\u0014 f :R\\n\\n2,1\\n\\n\u0015\\n11\\n→ R , v \b→ Fv with F =\\n,\\n01\\n2,1 has the characteristic polynomial (t − 1)2 and thus only has the eigenvalue 1.\\nWe have ker(V f (1)) = span{[1, 0]T } and thus g(1, f ) = 1 < a(1, f ) = 2.\\nBy Theorem 14.14, f is not diagonalizable.\\n\\n14.3 Triangulation and Schur’s Theorem\\nIf the property g(λ j , f ) = a(λ j , f ) does not hold for every eigenvalue λ j of f , then f is not diagonalizable.\\nHowever, as long as the characteristic polynomial P f decomposes into linear factors, we can find a special basis B such that [ f ] B,B is a triangular matrix.\\nTheorem 14.17 If V is a finite dimensional K -vector space and f ∈ L(V, V), then the following statements are equivalent:\\n(1) The characteristic polynomial P f decomposes into linear factors over K .\\n(2) There exists a basis B of V such that [ f ] B,B is upper triangular, i.e., f can be triangulated.\\nProof\\n(2) ⇒ (1): If n = dim(V) and [ f ] B,B = [ri j ] ∈ K n,n is upper triangular, then\\nP f = (t − r11 ) · . . . · (t − rnn ).\\n(1) ⇒ (2): We show the assertion by induction on n = dim(V).\\nThe case n = 1 is trivial, since then [ f ] B,B ∈ K 1,1 .\\nSuppose that the assertion holds for an n ≥ 1, and let dim(V) = n + 1.\\nBy assumption,\\nP f = (t − λ1 ) · . . . · (t − λn+1 ), where λ1 , . . . , λn+1 ∈ K are the eigenvalues of f .\\nLet v1 ∈ V be an eigenvector corresponding to the eigenvalue λ1 .\\nWe extend this vector to a basis\\nB = {v1 , w2 , . . . , wn+1 } of V. With BW := {w2 , . . . , wn+1 } and W := span BW we have V = span{v1 } ⊕ W and\\n⎡\\n\\n[ f ] B,B\\n\\nλ1 a12\\n⎢ 0 a22\\n⎢\\n=⎢ .\\n..\\n⎣ ..\\n.\\n0 an+1,2\\n\\n⎤\\n· · · a1,n+1\\n. . . a2,n+1 ⎥\\n⎥\\n⎥.\\n..\\n..\\n⎦\\n.\\n.\\n· · · an+1,n+1   \n",
       "197                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       208\\n\\n14 Eigenvalues of Endomorphisms\\n\\nWe define h ∈ L(W, span{v1 }) and g ∈ L(W, W) by n+1 h(w j ) := a1 j v1 and g(w j ) := a k j wk , j = 2, . . . , n + 1.\\nk=2\\n\\nThen f (w) = h(w) + g(w) for all w ∈ W, and\\n\u0014\\n[ f ] B,B =\\n\\n\u0015\\nλ1 [h] BW ,{v1 }\\n.\\n0 [g] BW ,BW\\n\\nConsequently,\\n(t − λ1 )Pg = P f = (t − λ1 ) · . . . · (t − λn+1 ), and hence Pg = (t − λ2 ) · . . . · (t − λn+1 ).\\nNow dim(W) = n and the characteristic polynomial of g ∈ L(W, W) decomposes into linear factors.\\nBy the w2 , . . . , w\\n\u0017n+1 } of W such that induction hypothesis there exists a basis \u0017\\nBW = {\u0017 upper triangular.\\nThus, for the basis\\nB\\n:=\\n{v\\n\u00172 , . . . , w\\n\u0017n+1 } the\\n[g] \u0017\\n1\\n1, w\\nBW\\nBW , \u0017\\n\u0006\\n\u0005 matrix [ f ] B1 ,B1 is upper triangular.\\nA “matrix version” of this theorem reads as follows: The characteristic polynomial\\nPA of A ∈ K n,n decomposes into linear factors over K if and only if A can be triangulated, i.e., there exists a matrix S ∈ G L n (K ) with A = S RS −1 for an upper triangular matrix R ∈ K n,n .\\nCorollary 14.18 Let V be a finite dimensional Euclidian or unitary vector space and f ∈ L(V, V).\\nIf P f decomposes over R (in the Euclidian case case) or C (in the unitary case) into linear factors, then there exists an orthonormal basis B of V, such that [ f ] B,B is upper triangular.\\nProof If P f decomposes into linear factors, then by Theorem 14.17 there exists a basis B1 of V, such that [ f ] B1 ,B1 is upper triangular.\\nApplying the Gram-Schmidt method to the basis B1 , we obtain an orthonormal basis B2 of V, such that [IdV ] B1 ,B2 is upper triangular (cp.\\nTheorem 12.11).\\nThen\\n[ f ] B2 ,B2 = [IdV ] B1 ,B2 [ f ] B1 ,B1 [IdV ] B2 ,B1 = [IdV ]−1\\nB2 ,B1 [ f ] B1 ,B1 [IdV ] B2 ,B1 .\\nThe invertible upper triangular matrices form a group with respect to the matrix multiplication (cp.\\nTheorem 4.13).\\nThus, all matrices in the product on the right\\n\u0006\\n\u0005 hand side are upper triangular, and hence [ f ] B2 ,B2 is upper triangular.\\nExample 14.19 Consider the Euclidian vector space R[t]≤1 with the scalar product\\n\u00181\\n\u0010 p, q\u0011 = 0 p(t)q(t) dt, and the endomorphism   \n",
       "198                                                                                                                                                                                                                                                                                                                                                                                                                                                        14.3 Triangulation and Schur’s Theorem\\n\\n209 f : R[t]≤1 → R[t]≤1 , α1 t + α0 \b→ 2α1 t + α0 .\\nWe have f (1) = 1 and f (t) = 2t, i.e., the polynomials 1 and t are eigenvectors of f corresponding to the (distinct) eigenvalues 1 and 2.\\nThus, \u0017\\nB = {1, t} is a basis\\n\u0017 is a diagonal matrix.\\nNote that\\nB is not an orthonormal basis, of R[t]≤1 , and [ f ] \u0017\\n\u0017\\nB, B since in particular \u00101, t\u0011 \u0003= 0.\\nSince P f decomposes into linear factors, Corollary 14.18 guarantees the existence of an orthonormal basis B for which [ f ] B,B is upper triangular.\\nIn the proof of the implication (1) ⇒ (2) of Theorem 14.17 one chooses any eigenvector of f , and then proceeds inductively in order to obtain the triangulation of f .\\nIn this example, let us use q1 = 1 as the first vector.\\nThis vector is an eigenvector of f with norm\\n1 corresponding to the eigenvalue 1.\\nIf q2 ∈ R[t]≤1 is a vector with norm 1 and\\n\u0010q1 , q2 \u0011 = 0, then B = {q1 , q2 } is an orthonormal basis for which [ f ] B,B is an upper triangular matrix.\\nWe construct the vector q2 by orthogonalizing t against q1 using the Gram-Schmidt method:\\n1\\n\u0017 q2 = t − \u0010t, q1 \u0011q1 = t − ,\\n2\\n\u0019\\n1 \u001a1/2\\n1\\n1\\n=√ ,\\n\u0012\u0017 q2 \u0012 = t − , t −\\n2\\n2\\n12\\n√\\n√\\n−1 q2 = \u0012\u0017 q2 \u0012 \u0017 q2 = 12t − 3.\\nThis leads to the triangulation\\n[ f ] B,B =\\n\\n\u0014 √ \u0015\\n1 3\\n∈ R2,2 .\\n0 2\\n\\n√\\nWe could also choose q1 = 3t, which is an eigenvector of f with norm 1 corresponding to the eigenvalue 2.\\nOrthogonalizing the vector 1 against q1 leads to the second basis vector q2 = −3t + 2.\\nWith the corresponding basis B1 we obtain the triangulation\\n\u0014\\n√ \u0015\\n2− 3\\n∈ R2,2 .\\n[ f ] B1 ,B1 =\\n0 1\\nThis example shows that in the triangulation of f the elements above the diagonal can be different for different orthonormal bases.\\nOnly the diagonal elements are (except for their order) uniquely determined, since they are the eigenvalues of f .\\nA more detailed statement about the uniqueness is given in Lemma 14.22.\\nIn the next chapter we will prove the Fundamental Theorem of Algebra, which states that every non-constant polynomial over C decomposes into linear factors.\\nThis result has the following corollary, which is known as Schur’s theorem.1\\n\\n1 Issai\\n\\nSchur (1875–1941).   \n",
       "199                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            210\\n\\n14 Eigenvalues of Endomorphisms\\n\\nCorollary 14.20 If V is a finite dimensional unitary vector space, then every endomorphism on V can be unitarily triangulated, i.e., for each f ∈ L(V, V) there exists an orthonormal basis B of V, such that [ f ] B,B is upper triangular.\\nThe matrix [ f ] B,B is called a Schur form of f .\\nIf V is the unitary vector space Cn,1 with the standard scalar product, then we obtain the following “matrix version” of Corollary 14.20.\\nCorollary 14.21 If A ∈ Cn,n , then there exists a unitary matrix Q ∈ Cn,n with\\nA = Q R Q H for an upper triangular matrix R ∈ Cn,n .\\nThe matrix R is called a\\nSchur form of A.\\nThe following result shows that a Schur form of a matrix A ∈ Cn,n with n pairwise distinct eigenvalues is “almost unique”.\\nLemma 14.22 Let A ∈ Cn,n have n pairwise distinct eigenvalues, and let R1 , R2 ∈\\nCn,n be two Schur forms of A. If the diagonals of R1 and R2 are equal then R1 =\\nU R2 U H for a unitary diagonal matrix U .\\n\u0006\\n\u0005\\n\\nProof Exercise.\\n\\nA survey of the results on unitary similarity of matrices can be found in the article [Sha91].\\n\\nMATLAB-Minute.\\nConsider for n ≥ 2 the matrix\\n⎤\\n··· n\\n··· n + 1 ⎥\\n⎥\\n··· n + 2 ⎥\\n⎥ ∈ Cn,n .\\n.. ⎥\\n. ⎦\\n1 n + 1 n + 2 . . .\\n2n − 1\\n\\n⎡\\n1\\n⎢1\\n⎢\\n⎢\\nA = ⎢1\\n⎢ ..\\n⎣.\\n\\n2\\n3\\n4\\n..\\n.\\n\\n3\\n4\\n5\\n..\\n.\\n\\nCompute a Schur form of A using the command [U,R] = schur(A) for n =\\n2, 3, 4, . . .\\n10.\\nWhat are the eigenvalues of A?\\nFormulate a conjecture about the rank of A for general n.\\nCan you prove your conjecture?\\n\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n14.1.\\nLet V be a vector space and let f ∈ L(V, V) have the eigenvalue λ.\\nShow that im(λIdV − f ) is an f -invariant subspace.\\n14.2.\\nLet V be a finite dimensional vector space and let f ∈ L(V, V) be bijective.\\nShow that f and f −1 have the same invariant subspaces.   \n",
       "200                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      14.3 Triangulation and Schur’s Theorem\\n\\n211\\n\\n14.3.\\nLet V be an n-dimensional K -vector space, let f ∈ L(V, V), and let U be an m-dimensional f -invariant subspace of V. Show that a basis B of V exists such that\\n\u0015\\n\u0014\\nA1 A2\\n[ f ] B,B =\\n0 A3 for some matrices A1 ∈ K m,m , A2 ∈ K m,n−m and A3 ∈ K n−m,n−m .\\n14.4.\\nLet K ∈ {R, C} and f : K 4,1 → K 4,1 , v \b→ Fv with\\n⎡\\n\\n1\\n⎢0\\nF =⎢\\n⎣0\\n0\\n\\n2 3\\n1 2\\n0 1\\n0 −1\\n\\n⎤\\n4\\n3⎥\\n⎥.\\n1⎦\\n0\\n\\nCompute P f and determine for K = R and K = C the eigenvalues of f with their algebraic and geometric multiplicities, as well as the associated eigenspaces.\\n14.5.\\nConsider the vector space R[t]≤n with the standard basis {1, t, . . . , t n } and the endomorphism n f : R[t]≤n → R[t]≤n , n\\n\\nαi t i \b→ i=0 i(i − 1)αi t i−2 = i=2 d2 p.\\ndt 2\\n\\nCompute P f , the eigenvalues of f with their algebraic and geometric multiplicities, and examine whether f is diagonalizable or not.\\nWhat changes if one considers as map the kth derivative (for k = 3, 4, . . . , n)?\\n14.6.\\nExamine whether the following matrices\\n\u0014\\nA=\\n\\n01\\n−1 0\\n\\n⎡\\n⎤\\n3\\n100\\n⎢0\\n3,3\\nB = ⎣ −1 2 0 ⎦ ∈ Q , C = ⎢\\n⎣2\\n−1 1 1\\n0\\n⎡\\n\\n\u0015\\n∈ Q2,2 ,\\n\\n1\\n2\\n2\\n0\\n\\n⎤\\n0 −2\\n0 0⎥\\n⎥ ∈ Q4,4\\n2 −4 ⎦\\n0 2 are diagonalizable.\\n14.7.\\nIs the set of all diagonalizable and invertible matrices a subgroup of G L n (K )?\\n14.8.\\nLet n ∈ N0 .\\nConsider the R-vector space R[t]≤n and the map f : R[t]≤n → R[t]≤n , p(t) \b→ p(t + 1) − p(t).\\n\\nShow that f is linear.\\nFor which n is f diagonalizable?\\n14.9.\\nLet V be an R-vector space with the basis {v1 , . . . , vn }.\\nExamine whether the following endomorphisms are diagonalizable or not:\\n(a) f (v j ) = v j + v j+1 , j = 1, . . . , n − 1, and f (vn ) = vn ,   \n",
       "201                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        212\\n\\n14 Eigenvalues of Endomorphisms\\n\\n(b) f (v j ) = jv j + v j+1 , j = 1, . . . , n − 1, and f (vn ) = nvn .\\n14.10.\\nLet V be a finite dimensional Euclidian vector space and let f ∈ L(V, V) with f + f ad = 0 ∈ L(V, V).\\nShow that f \u0003= 0 if and only if f is not diagonalizable.\\n14.11.\\nLet V be a C-vector space and let f ∈ L(V, V) with f 2 = −IdV .\\nDetermine all possible eigenvalues of f .\\n14.12.\\nLet V be a finite dimensional vector space and f ∈ L(V, V).\\nShow that\\nP f ( f ) = 0 ∈ L(V, V).\\n14.13.\\nLet V be a finite dimensional K -vector space, let f ∈ L(V, V) and p = (t − μ1 ) · . . . · (t − μm ) ∈ K [t]≤m .\\nShow that p( f ) is bijective if and only if μ1 , . . . , μm are not eigenvalues of f.\\n14.14.\\nDetermine conditions for the entries of the matrices\\n\u0014\\n\u0015\\nαβ\\nA=\\n∈ R2,2 ,\\nγ δ such that A is diagonalizable or can be triangulated.\\n14.15.\\nDetermine an endomorphism on R[t]≤3 that is not diagonalizable and that cannot be triangulated.\\n14.16.\\nLet V be a vector space with dim(V) = n.\\nShow that f ∈ L(V, V) can be triangulated if and only if there exist subspaces V0 , V1 , . . . , Vn of V with\\n(a) V j ⊂ V j+1 for j = 0, 1, . . . , n − 1,\\n(b) dim(V j ) = j for j = 0, 1, . . . , n, and\\n(c) V j is f -invariant for j = 0, 1, . . . , n.\\n14.17.\\nProve Lemma 14.22.   \n",
       "202                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Chapter 15\\n\\nPolynomials and the Fundamental Theorem of Algebra\\n\\nIn this chapter we discuss polynomials in more detail.\\nWe consider the division of polynomials and derive classical results from polynomial algebra, including the factorization into irreducible factors.\\nWe also prove the Fundamental Theorem of\\nAlgebra, which states that every non-constant polynomial over the complex numbers has a least one complex root.\\nThis implies that every complex matrix and every endomorphism on a (finite dimensional) complex vector space has at least one eigenvalue.\\n\\n15.1 Polynomials\\nLet us recall some of the most important terms in the context of polynomials.\\nIf K is a field, then p = α0 + α1 t + . . . + αn t n with n ∈ N0 and α0 , α1 , . . . αn ∈ K is a polynomial over K in the variable t.\\nThe set K [t] of all these polynomials forms a commutative ring with unit (cp.\\nExample 3.17).\\nIf αn \u0003= 0, then deg( p) = n is called the degree of p.\\nIf αn = 1, then p is called monic.\\nIf p = 0, then deg( p) := −∞, and if deg( p) < 1, then p is called constant.\\nLemma 15.1 For two polynomials p, q ∈ K [t] the following assertions hold:\\n(1) deg( p + q) ≤ max{deg( p), deg(q)}.\\n(2) deg( p · q) = deg( p) + deg(q).\\n\u0007\\n\u0006\\n\\nProof Exercise.\\nWe now introduce some concepts associated with the division of polynomials.\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_15\\n\\n213   \n",
       "203                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     214\\n\\n15 Polynomials and the Fundamental Theorem of Algebra\\n\\nDefinition 15.2 Let K be a field.\\n(1) If for two polynomials p, s ∈ K [t] there exists a polynomial q ∈ K [t] with p = s · q, then s is called a divisor of p and we write s| p (read this as “s divides p”).\\n(2) Two polynomials p, s ∈ K [t] are called coprime, if q| p and q|s for some q ∈ K [t] always imply that q is constant.\\n(3) A non-constant polynomial p ∈ K [t] is called irreducible (over K ), if p = s · q for two polynomials s, q ∈ K [t] implies that s or q are constant.\\nIf there exist two non-constant polynomials s, q ∈ K [t] with p = s · q, then p is called reducible (over K ).\\nNote that the property of irreducibility is only defined for polynomials of degree at least 1.\\nA polynomial of degree 1 is always irreducible.\\nWhether a polynomial of degree at least 2 is irreducible may depend on the underlying field.\\nExample 15.3 The polynomial 2 − t 2 ∈ Q[t] is irreducible, but the factorization\\n2 − t2 =\\n\\n\u0002√\\n\\n\u0003 \u0002√\\n\u0003\\n2−t · 2+t shows that 2 − t 2 ∈ R[t] is reducible.\\nThe polynomial 1 + t 2 ∈ R[t] is irreducible, but using the imaginary unit i we have\\n1 + t 2 = (−i + t) · (i + t), so that 1 + t 2 ∈ C[t] is reducible.\\nThe next result concerns the division with remainder of polynomials.\\nTheorem 15.4 If p ∈ K [t] and s ∈ K [t] \\ {0}, then there exist uniquely defined polynomials q, r ∈ K [t] with p = s · q + r and deg(r ) < deg(s).\\n\\n(15.1)\\n\\nProof We show first the existence of polynomials q, r ∈ K [t] such that (15.1) holds.\\nIf deg(s) = 0, then s = s0 for an s0 ∈ K \\ {0} and (15.1) follows with q := s0−1 · p and r := 0, where deg(r ) < deg(s).\\nWe now assume that deg(s) ≥ 1.\\nIf deg( p) < deg(s), then we set q := 0 and r := p.\\nThen p = s · q + r with deg(r ) < deg(s).\\nLet n := deg( p) ≥ m := deg(s) ≥ 1.\\nWe prove (15.1) by induction on n.\\nIf n = 1, then m = 1.\\nHence p = p1 · t + p0 with p1 \u0003= 0 and s = s1 · t + s0 with s1 \u0003= 0.\\nTherefore, p = s · q + r for q := p1 s1−1 , r := p0 − p1 s1−1 s0 , where deg(r ) < deg(s).   \n",
       "204                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            15.1 Polynomials\\n\\n215\\n\\nSuppose that the assertion holds for an n ≥ 1.\\nLet two polynomials p and s with n + 1 = deg( p) ≥ deg(s) = m be given, and let pn+1 (\u0003= 0) and sm (\u0003= 0) be the highest coefficients of p and s.\\nIf h := p − pn+1 sm−1 s · t n+1−m ∈ K [t], then deg(h) < deg( p) = n + 1.\\nBy the induction hypothesis there exist polynomials\\n\u0004 q , r ∈ K [t] with h = s ·\u0004 q + r and deg(r ) < deg(s).\\nIt then follows that p = s · q + r with q := \u0004 q + pn+1 sm−1 t n+1−m , where deg(r ) < deg(s).\\nIt remains to show the uniqueness.\\nSuppose that (15.1) holds and that there exist polynomials \u0005 q ,\u0005 r ∈ K [t] with p = s · \u0005 q +\u0005 r and deg(\u0005 r ) < deg(s).\\nThen r −\u0005 r = s · (\u0005 q − q).\\nIf \u0005 r − r \u0003= 0, then \u0005 q − q \u0003= 0 and thus deg(r − \u0005 r ) = deg(s · (\u0005 q − q)) = deg(s) + deg(\u0005 q − q) ≥ deg(s).\\nOn the other hand, we also have deg(r − \u0005 r ) ≤ max{deg(r ), deg(\u0005 r )} < deg(s).\\nThis is a contradiction, which shows that indeed r = \u0005 r and q = \u0005 q.\\n\\n\u0007\\n\u0006\\n\\nThis theorem has some important consequences for the roots of polynomials.\\nThe first of these is known as the Theorem of Ruffini.1\\nCorollary 15.5 If λ ∈ K is a root of p ∈ K [t], i.e., p(λ) = 0, then there exists a uniquely determined polynomial q ∈ K [t] with p = (t − λ) · q.\\nProof When we apply Theorem 15.4 to the polynomials p and s = t − λ \u0003= 0, then we get uniquely determined polynomials q and r with deg(r ) < deg(s) = 1 and p = (t − λ) · q + r.\\nThe polynomial r is constant and evaluating it at λ gives\\n0 = p(λ) = (λ − λ) · q(λ) + r (λ) = r (λ),\\n1 Paolo\\n\\nRuffini (1765–1822).   \n",
       "205                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             216\\n\\n15 Polynomials and the Fundamental Theorem of Algebra which yields r = 0 and p = (t − λ) · q.\\n\\n\u0007\\n\u0006\\n\\nIf a polynomial p ∈ K [t] has at least degree 2 and a root λ ∈ K , then the linear factor t − λ is a divisor of p and, in particular, p is reducible.\\nThe converse of this statement does not hold.\\nFor instance the polynomial 4−4t 2 +t 4 = (2−t 2 )·(2−t 2 ) ∈\\nQ[t] is reducible, but it does not have a root in Q.\\nCorollary 15.5 motivates the following definition.\\nDefinition 15.6 If λ ∈ K is a root of p ∈ K [t] \\ {0}, then its multiplicity is the uniquely determined nonnegative integer m, such that p = (t − λ)m · q for a polynomial q ∈ K [t] with q(λ) \u0003= 0.\\nRecursive application of Corollary 15.5 to a given polynomial p ∈ K [t] leads to the following result.\\nCorollary 15.7 If λ1 . . . , λk ∈ K are pairwise distinct roots of p ∈ K [t] \\ {0} with the corresponding multiplicities m 1 , . . . , m k , then there exists a unique polynomial q ∈ K [t] with p = (t − λ1 )m 1 · . . . · (t − λk )m k · q and q(λ j ) \u0003= 0 for j = 1, . . . , k.\\nIn particular, the sum of the multiplicities of all pairwise distinct roots of p is at most deg( p).\\nThe next result is known as the Lemma of Bézout.2\\nLemma 15.8 If p, s ∈ K [t] \\ {0} are coprime, then there exist polynomials q1 , q2 ∈\\nK [t] with p · q1 + s · q2 = 1.\\nProof We may assume without loss of generality that deg( p) ≥ deg(s) (≥ 0), and we proceed by induction on deg(s).\\nIf deg(s) = 0, then s = s0 for an s0 ∈ K \\ {0}, and thus p · q1 + s · q2 = 1 with q1 := 0, q2 := s0−1 .\\nSuppose that the assertion holds for all polynomials p, s ∈ K [t] \\ {0} with deg(s) = n for an n ≥ 0.\\nLet p, s ∈ K [t] \\ {0} with deg( p) ≥ deg(s) = n + 1 be given.\\nBy Theorem 15.4 there exist polynomials q and r with p = s · q + r and deg(r ) < deg(s).\\nHere we have r \u0003= 0, since by assumption p and s are coprime.\\nSuppose that there exists a non-constant polynomial h ∈ K [t] that divides both s and r .\\nThen h also divides p, in contradiction to the assumption that p and s are coprime.\\nThus, the polynomials s and r are coprime.\\nSince deg(r ) < deg(s), we can\\n2 Étienne\\n\\nBézout (1730–1783).   \n",
       "206                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   15.1 Polynomials\\n\\n217 apply the induction hypothesis to the polynomials s, r ∈ K [t] \\ {0}.\\nHence there q2 ∈ K [t] with exist polynomials \u0004 q1 , \u0004 q2 = 1.\\ns ·\u0004 q1 + r · \u0004\\nFrom r = p − s · q we then get q2 = p · \u0004 q2 + s · (\u0004 q1 − q · \u0004 q2 ),\\n1 = s ·\u0004 q1 + ( p − s · q) · \u0004\\n\u0007\\n\u0006 which completes the proof.\\nUsing the Lemma of Bézout we can easily prove the following result.\\n\\nLemma 15.9 If p ∈ K [t] is irreducible and a divisor of the product s · h of two polynomials s, h ∈ K [t], then p divides at least one of the factors, i.e., p|s or p|h.\\nProof If s = 0, then p|s, because every polynomial is a divisor of the zero polynomial.\\nIf s \u0003= 0 and p is not a divisor of s, then p and s are coprime, since p is irreducible.\\nBy Lemma 15.8 there exist polynomials q1 , q2 ∈ K [t] with p · q1 + s · q2 = 1, and hence h = h · 1 = (q1 · h) · p + q2 · (s · h).\\nThe polynomial p divides both terms on the right hand side, and thus also p|h.\\n\\n\u0007\\n\u0006\\n\\nBy recursive application of Lemma 15.9 we obtain the Euclidean theorem, which describes a prime factor decomposition in the ring of polynomials.\\nTheorem 15.10 Every polynomial p = α0 + α1 t + . . . + αn t n ∈ K [t] \\ {0} has a unique (up to the ordering of the factors) decomposition p = μ · p1 · . . . · pk with μ ∈ K and monic irreducible polynomials p1 , . . . , pk ∈ K [t].\\nProof If deg( p) = 0, and thus p = α0 , then the assertion holds with k = 0 and\\nμ = α0 .\\nLet deg( p) ≥ 1.\\nIf p is irreducible, then the assertion holds with p1 = μ−1 p and μ = αn .\\nIf p is reducible, then p = p1 · p2 for two non-constant polynomials p1 and p2 .\\nThese are either irreducible, or we can decompose them further.\\nEvery multiplicative decomposition of p that is obtained in this way has at most deg( p) = n non-constant factors.\\nSuppose that p = μ · p1 · . . . · pk = β · q 1 · . . . · q \u0002\\n\\n(15.2) for some k, \u0002, where 1 ≤ \u0002 ≤ k ≤ n, μ, β ∈ K , as well as monic irreducible polynomials p1 , . . . , pk , q1 , . . . , q\u0002 ∈ K [t].\\nThen p1 | p and hence p1 |q j for some j.\\nSince the polynomials p1 and q j are irreducible, we must have p1 = q j .   \n",
       "207                                                                                                                                                                                                                                                                                         218\\n\\n15 Polynomials and the Fundamental Theorem of Algebra\\n\\nWe may assume without loss of generality that j = 1 and cancel the polynomial p1 = q1 in the identity (15.2), which gives\\nμ · p2 · . . . · pk = β · q 2 · . . . · q \u0002 .\\nProceeding analogously for the polynomials p2 , . . . , pk , we finally obtain k = \u0002,\\n\u0006\\n\u0007\\nμ = β and p j = q j for j = 1, . . . , k.\\n\\n15.2 The Fundamental Theorem of Algebra\\nWe have seen above that the existence of roots of a polynomial depends on the field over which it is considered.\\nThe field C is special in this sense, since here the\\nFundamental Theorem of Algebra3 guarantees that every non-constant polynomial has a root.\\nIn order to use this theorem in our context, we first present an equivalent formulation in the language of Linear Algebra.\\nTheorem 15.11 The following statements are equivalent:\\n(1) Every non-constant polynomial p ∈ C[t] has a root in C.\\n(2) If V \u0003= {0} is a finite dimensional C-vector space, then every endomorphism f ∈ L(V, V) has an eigenvector.\\nProof\\n(1) ⇒ (2): If V \u0003= {0} and f ∈ L(V, V), then the characteristic polynomial P f ∈\\nC[t] is non-constant, since deg(P f ) = dim(V) > 0.\\nThus, P f has a root in C, which is an eigenvalue of f , so that f indeed has an eigenvector.\\n(2) ⇒ (1): Let p = α0 + α1 t + . . . + αn t n ∈ C[t] be a non-constant polynomial with αn \u0003= 0.\\nThe roots of p are equal to the roots of the monic polynomial p , then PA = \u0005 p (cp.\\n\u0005 p := αn−1 p.\\nLet A ∈ Cn,n be the companion matrix of \u0005\\nLemma 8.4).\\nIf V is an n-dimensional C-vector space and B is an arbitrary basis of V, then there exists a uniquely determined f ∈ L(V, V) with [ f ] B,B = A (cp.\\nTheorem 10.16).\\nBy assumption, f has an eigenvector and hence also an eigenvalue,\\n\u0007\\n\u0006 so that \u0005 p = PA has a root.\\nThe Fundamental Theorem of Algebra cannot be proven without tools from Analysis.\\nIn particular, one needs that polynomials are continuous.\\nWe will use the following standard result, which is based on the continuity of polynomials.\\nLemma 15.12 Every polynomial p ∈ R[t] with odd degree has a (real) root.\\n3 Numerous proofs of this important result exist.\\nCarl Friedrich Gauß (1777–1855) alone gave four different proofs, starting with the one in his dissertation from 1799, which contained however a gap.\\nThe history of this result is described in detail in the book [Ebb91].   \n",
       "208                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            15.2 The Fundamental Theorem of Algebra\\n\\n219\\n\\nProof Let the highest coefficient of p be positive.\\nThen lim p(t) = +∞, t→∞ lim p(t) = −∞.\\nt→−∞\\n\\nSince the real function p(t) is continuous, the Intermediate Value Theorem from\\nAnalysis implies the existence of a root of p.\\nThe argument in the case of a negative leading coefficient is analogous.\\n\u0007\\n\u0006\\nOur proof of the Fundamental Theorem of Algebra below follows the presentation in the article [Der03].\\nThe proof is by induction on the dimension of V. However, we do not use the usual consecutive order, i.e., dim(V) = 1, 2, 3, . . . , but an order that is based on the sets\\nM j := {2m · \u0002 | 0 ≤ m ≤ j − 1, \u0002 odd} ⊂ N, j = 1, 2, 3, . . . .\\n\\nFor instance,\\nM1 = {\u0002 | \u0002 odd} = {1, 3, 5, 7, . . . },\\n\\nM2 = M1 ∪ {2, 6, 10, 14, . . . }.\\n\\nLemma 15.13\\n(1) If V is an R-vector space and if dim(V) is odd, i.e., dim(V) ∈ M1 , then every f ∈ L(V, V) has an eigenvector.\\n(2) Let K be a field and j ∈ N. If for every K -vector space V with dim(V) ∈ M j every f ∈ L(V, V) has an eigenvector, then two commuting f 1 , f 2 ∈ L(V, V) have a common eigenvector.\\nThat is, if f 1 ◦ f 2 = f 2 ◦ f 1 , then there exists a vector v ∈ V \\ {0} and two scalars λ1 , λ2 ∈ K with f 1 (v) = λ1 v and f 2 (v) = λ2 v.\\n(3) If V is an R-vector space and if dim(V) is odd, then two commuting f 1 , f 2 ∈\\nL(V, V) have a common eigenvector.\\nProof\\n(1) For every f ∈ L(V, V) the degree of P f ∈ R[t] is odd.\\nHence Lemma 15.12 implies that P f has a root, and therefore f has an eigenvector.\\n(2) We proceed by induction on dim(V), where dim(V) runs through the elements of\\nM j in increasing order.\\nThe set M j is a proper subset of N consisting of natural numbers that are not divisible by 2 j and, in particular, 1 is the smallest element of M j .\\nIf dim(V) = 1 ∈ M j , then by assumption two arbitrary f 1 , f 2 ∈ L(V, V) each have an eigenvector, i.e., f 1 (v1 ) = λ1 v1 , f 2 (v2 ) = λ2 v2 .\\n\\nSince dim(V) = 1, we have v1 = αv2 for an α ∈ K \\ {0}.\\nThus, f 2 (v1 ) = f 2 (αv2 ) = α f 2 (v2 ) = λ2 (αv2 ) = λ2 v1 ,   \n",
       "209                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                220\\n\\n15 Polynomials and the Fundamental Theorem of Algebra i.e., v1 is a common eigenvector of f 1 and f 2 .\\nLet now dim(V) ∈ M j , and let the assertion be proven for all K -vector spaces whose dimensions is an element of M j that is smaller than dim(V).\\nLet f 1 , f 2 ∈\\nL(V, V) with f 1 ◦ f 2 = f 2 ◦ f 1 .\\nBy assumption, f 1 has an eigenvector v1 with corresponding eigenvalue λ1 .\\nLet\\nU := im(λ1 IdV − f 1 ),\\n\\nW := V f1 (λ1 ) = ker(λ1 IdV − f 1 ).\\n\\nThe subspaces U and W of V are f 1 -invariant, i.e., f 1 (U) ⊆ U and f 1 (W) ⊆ W.\\nFor the space W we have shown this in Lemma 14.4 and for the space U this can be easily shown as well (cp.\\nExercise 14.1).\\nThe subspaces U and W are also f 2 -invariant:\\nIf u ∈ U, then u = (λ1 IdV − f 1 )(v) for a v ∈ V. Since f 1 and f 2 commute, we have f 2 (u) = ( f 2 ◦ (λ1 IdV − f 1 ))(v) = ((λ1 IdV − f 1 ) ◦ f 2 )(v)\\n= (λ1 IdV − f 1 )( f 2 (v)) ∈ U.\\nIf w ∈ W, then\\n(λ1 IdV − f 1 )( f 2 (w)) = ((λ1 IdV − f 1 ) ◦ f 2 )(w) = ( f 2 ◦ (λ1 IdV − f 1 ))(w)\\n= f 2 ((λ1 IdV − f 1 )(w)) = f 2 (0) = 0, hence f 2 (w) ∈ W.\\nWe have dim(V) = dim(U) + dim(W) and since dim(V) is not divisible by 2 j , either dim(U) or dim(W) is not divisible by 2 j .\\nHence either dim(U) ∈ M j or dim(W) ∈ M j .\\nIf the corresponding subspace is a proper subspace of V, then its dimension is an element of M j that is smaller than dim(V).\\nBy the induction hypothesis then f 1 and f 2 have a common eigenvector in this subspace.\\nThus, f 1 and f 2 have a common eigenvector in V.\\nIf the corresponding subspace is equal to V, then this must be the subspace W, since dim(W) ≥ 1.\\nBut if V = W, then every vector in V \\ {0} is an eigenvector of f 1 .\\nBy assumption also f 2 has an eigenvector, so that there exists at least one common eigenvector of f 1 and f 2 .\\n(3) By (1) it follows that the assumption of (2) holds for K = R and j = 1, which means that (3) holds as well.\\n\u0007\\n\u0006\\nWe will now prove the Fundamental Theorem of Algebra in the formulation (2) of Theorem 15.11.\\nTheorem 15.14 If V \u0003= {0} is a finite dimensional C-vector space, then every f ∈\\nL(V, V) has an eigenvector.   \n",
       "210                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        15.2 The Fundamental Theorem of Algebra\\n\\n221\\n\\nProof We prove the assertion by induction on j = 1, 2, 3, . . . and dim(V) ∈ M j .\\nWe start with j = 1 and thus by showing the assertion for all C-vector spaces of odd dimension.\\nLet V be an arbitrary C-vector space with n := dim(V) ∈ M1 .\\nLet f ∈ L(V, V) and consider an arbitrary scalar product on V (such a scalar product always exists; cp.\\nExercise 12.1), as well as the set of self-adjoint maps with respect to this scalar product,\\nH := {g ∈ L(V, V) | g = g ad }.\\nBy Lemma 13.15 the set H forms an R-vector space of dimension n 2 .\\nIf we define h 1 , h 2 ∈ L(H, H) by h 1 (g) :=\\n\\n1\\n1\\n( f ◦ g + g ◦ f ad ), h 2 (g) := ( f ◦ g − g ◦ f ad )\\n2\\n2i for all g ∈ H, then h 1 ◦ h 2 = h 2 ◦ h 1 (cp.\\nExercise 15.8).\\nSince n is odd, also n 2 is odd.\\nBy (3) in Lemma 15.13, h 1 and h 2 have a common eigenvector.\\nHence, there exists a \u0004 g ∈ H \\ {0} with g ) = λ1\u0004 g , h 2 (\u0004 g ) = λ2 \u0004 g for some λ1 , λ2 ∈ R.\\nh 1 (\u0004\\nWe have (h 1 + ih 2 )(g) = f ◦ g for all g ∈ H and therefore, in particular, g ) = (λ1 + iλ2 )\u0004 g.\\nf ◦\u0004 g = (h 1 + ih 2 )(\u0004\\nSince \u0004 g \u0003= 0, there exists a v ∈ V with \u0004 g (v) \u0003= 0.\\nThen g (v)), f (\u0004 g (v)) = (λ1 + iλ2 ) (\u0004 which shows that \u0004 g (v) ∈ V is an eigenvector of f , so that the proof for j = 1 is complete.\\nAssume now that for some j ≥ 1 and every C-vector space V with dim(V) ∈ M j , every f ∈ L(V, V) has an eigenvector.\\nThen (2) in Lemma 15.13 implies that every two commuting f 1 , f 2 ∈ L(V, V) have a common eigenvector.\\nWe have to show that for every C-vector space V with dim(V) ∈ M j+1 , every f ∈ L(V, V) has an eigenvector.\\nSince\\nM j+1 = M j ∪ {2 j q | q odd}, we only have to prove this for C-vector spaces V with n := dim(V) = 2 j q for odd q.\\nLet V be such a vector space and let f ∈ L(V, V) be given.\\nWe choose an arbitrary basis of V and denote the matrix representation of f with respect to this basis by\\nA ∈ Cn,n .\\nLet   \n",
       "211                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     222\\n\\n15 Polynomials and the Fundamental Theorem of Algebra\\n\\nS := {B ∈ Cn,n | B = B T } be the set of complex symmetric n × n matrices.\\nIf we define h 1 , h 2 ∈ L(S, S) by h 1 (B) := AB + B A T , h 2 (B) := AB A T for all B ∈ S, then h 1 ◦ h 2 = h 2 ◦ h 1 (cp.\\nExercise 15.9).\\nBy Lemma 13.16 the set\\nS forms a C-vector space of dimension n(n + 1)/2.\\nWe have n = 2 j q for an odd natural number q.\\nThus, n(n + 1)\\n2 j q (2 j q + 1)\\n=\\n= 2 j−1 q · (2 j q + 1) ∈ M j .\\n2\\n2\\nBy the induction hypothesis, the commuting endomorphisms h 1 and h 2 have a common eigenvector.\\nHence there exists a \u0004\\nB ∈ S \\ {0} with\\nB) = λ1 \u0004\\nB) = λ2 \u0004\\nB, h 2 ( \u0004\\nB for some λ1 , λ2 ∈ C.\\nh1( \u0004\\nIn particular, we have λ1 \u0004\\nB = A\u0004\\nB+\u0004\\nB A T .\\nMultiplying this equation from the left with A yields\\nB = A2 \u0004\\nB) = A2 \u0004\\nB + A\u0004\\nB A T = A2 \u0004\\nB + h2( \u0004\\nB + λ2 \u0004\\nB,\\nλ1 A \u0004 so that\\n\\n\u0002\\n\\n\u0003\\nB = 0.\\nA 2 − λ1 A + λ2 I n \u0004\\n\\nWe now factorize t 2 − λ1 t + λ2 = (t − α)(t − β) with\\nα=\\n\\nλ1 +\\n\\n\u0006\\nλ21 − 4λ2\\n2\\n\\n, β=\\n\\nλ1 −\\n\\n\u0006\\nλ21 − 4λ2\\n2\\n\\n, where we have used that every complex number has a square root.\\nThen\\n(A − αIn )(A − β In ) \u0004\\nB = 0.\\nBv \u0003= 0.\\nIf (A − β In ) \u0004\\nBv = 0, then \u0004\\nBv is\\nSince \u0004\\nB \u0003= 0, there exists a v ∈ Cn,1 with \u0004\\nBv \u0003= 0, then an eigenvector of A corresponding to the eigenvalue β.\\nIf (A − β In ) \u0004\\nBv is an eigenvector of A corresponding to the eigenvalue α.\\nSince A has\\n(A − β In ) \u0004 an eigenvector, also f has an eigenvector.\\n\u0007\\n\u0006   \n",
       "212                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               15.2 The Fundamental Theorem of Algebra\\n\\n223\\n\\nMATLAB-Minute.\\nCompute the eigenvalues of the matrix\\n⎡\\n\\n1\\n⎢1\\n⎢\\nA=⎢\\n⎢2\\n⎣5\\n4\\n\\n2\\n2\\n3\\n1\\n2\\n\\n3\\n4\\n4\\n4\\n3\\n\\n4\\n3\\n1\\n2\\n1\\n\\n⎤\\n5\\n5⎥\\n⎥\\n5,5\\n5⎥\\n⎥∈R\\n⎦\\n3\\n5 using the command eig(A).\\nBy definition a real matrix A can only have real eigenvalues.\\nThe reason for the occurrence of complex eigenvalues is that MATLAB interprets every matrix as a complex matrix.\\nThis means that within MATLAB every matrix can be unitarily triangulated, since every complex polynomial (of degree at least 1) decomposes into linear factors.\\nAs a direct corollary of the Fundamental Theorem of Algebra and (2) in\\nLemma 15.13 we have the following result.\\nCorollary 15.15 If V \u0003= {0} is a finite dimensional C-vector space, then two commuting f 1 , f 2 ∈ L(V, V) have a common eigenvector.\\nExample 15.16 The two complex 2 × 2 matrices\\n\u000e\\n\u000e i 1\\n2i 1\\nA= and B =\\n1 i\\n1 2i commute.\\nThe eigenvalues of A are ±1 + i and those of B are ±2 + i.\\nHence A and B do not have a common eigenvalue, while [1, 1]T and [−1, 1]T are common eigenvectors of A and B.\\nUsing Corollary 15.15, Schur’s theorem (Corollary 14.20) can be generalized as follows.\\nTheorem 15.17 If V \u0003= {0} is a finite dimensional unitary vector space and f 1 , f 2 ∈\\nL(V, V) commute, then f 1 and f 2 can be simultaneously unitarily triangulated, i.e., there exists an orthonormal basis B of V, such that [ f 1 ] B,B and [ f 2 ] B,B are both upper triangular.\\nProof Exercise.\\n\\n\u0007\\n\u0006   \n",
       "213                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                224\\n\\n15 Polynomials and the Fundamental Theorem of Algebra\\n\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n15.1.\\nProve Lemma 15.1.\\n15.2.\\nShow the following assertions for p1 , p2 , p3 ∈ K [t]:\\n(a)\\n(b)\\n(c)\\n(d) p1 |( p1 p2 ).\\np1 | p2 and p2 | p3 imply that p1 | p3 .\\np1 | p2 and p1 | p3 imply that p1 |( p2 + p3 ).\\nIf p1 | p2 and p2 | p1 , then there exists a c ∈ K \\ {0} with p1 = cp2 .\\n\\n15.3.\\nExamine whether the following polynomials are irreducible: p1 = t 3 − t 2 + t − 1 ∈ Q[t], p4 = t 3 − t 2 + t − 1 ∈ R[t], p2 = t 3 − t 2 + t − 1 ∈ C[t], p5 = 4t 3 − 4t 2 − t + 1 ∈ Q[t], p3 = 4t 3 − 4t 2 − t + 1 ∈ R[t], p6 = t 3 − 4t 2 − t + 1 ∈ C[t].\\n\\nDetermine the decompositions into irreducible factors.\\n15.4.\\nDecompose the polynomials p1 = t 2 − 2, p2 = t 2 + 2, p3 = t 4 − 1 and p4 = t 2 + t + 1 into irreducible factors over the fields K = Q, K = R and\\nK = C.\\n15.5.\\nShow the following assertions for p ∈ K [t]:\\n(a) If deg( p) = 1, then p is irreducible.\\n(b) If deg( p) ≥ 2 and p has a root, then p is not irreducible.\\n(c) If deg( p) ∈ {2, 3}, then p is irreducible if and only if p does not have a root.\\n15.6.\\nLet A ∈ G L n (C), n ≥ 2, and let adj(A) ∈ Cn,n be the adjunct of A. Show that there exist n − 1 matrices A j ∈ Cn,n with det(−A j ) = det(A), j =\\n1, . . . , n − 1, and n−1\\n\u000f\\nAj.\\nadj(A) = j=1\\n\\n(Hint: Use PA to construct a polynomial p ∈ C[t]≤n−1 with adj(A) = p(A) and express p as product of linear factors.)\\n15.7.\\nShow that two polynomials p, q ∈ C[t] \\ {0} have a common root if and only if there exist polynomials r1 , r2 ∈ C[t] with 0 ≤ deg(r1 ) < deg( p) such that\\n0 ≤ deg(r2 ) < deg(q) and p · r2 + q · r1 = 0.\\n15.8.\\nLet V be a finite dimensional unitary vector space, f ∈ L(V, V), H = {g ∈\\nL(V, V) | g = g ad } and let h 1 : H → L(V, V), g \u0010→\\n\\n1\\n( f ◦ g + g ◦ f ad ),\\n2   \n",
       "214                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                15.2 The Fundamental Theorem of Algebra\\n\\n225 h 2 : H → L(V, V), g \u0010→\\n\\n1\\n( f ◦ g − g ◦ f ad ).\\n2i\\n\\nShow that h 1 , h 2 ∈ L(H, H) and h 1 ◦ h 2 = h 2 ◦ h 1 .\\n15.9.\\nLet A ∈ Cn,n , S = {B ∈ Cn,n | B = B T } and let h 1 : S → Cn,n ,\\n\\nB \u0010→ AB + B A T , h 2 : S → Cn,n ,\\n\\nB \u0010→ AB A T .\\n\\nShow that h 1 , h 2 ∈ L(S, S) and h 1 ◦ h 2 = h 2 ◦ h 1 .\\n15.10.\\nLet V be a C-vector space, f ∈ L(V, V) and let U \u0003= {0} be a finite dimensional f -invariant subspace of V. Show that U contains at least one eigenvector of f .\\n15.11.\\nLet V \u0003= {0} be a K -vector space and let f ∈ L(V, V).\\nShow the following statements:\\n(a) If K = C, then there exists an f -invariant subspace U of V with dim(U) = 1.\\n(b) If K = R, then there exists an f -invariant subspace U of V with dim(U) ∈\\n{1, 2}.\\n15.12.\\nProve Theorem 15.17.\\n15.13.\\nConstruct an example showing that the condition f ◦ g = g ◦ f in Theorem 15.17 is sufficient but not necessary for the simultaneous unitary triangulation of f and g.\\n15.14.\\nLet A ∈ K n,n be a diagonal matrix with pairwise distinct diagonal entries and B ∈ K n,n with AB = B A. Show that in this case B is a diagonal matrix.\\nWhat can you say about B, when the diagonal entries of A are not all pairwise distinct?\\n15.15.\\nShow that the matrices\\n\u000e\\n\u000e\\n01\\n−1 1\\n, B=\\nA=\\n10\\n1 −1 commute and determine a unitary matrix Q such that Q H AQ and Q H B Q are upper triangular.\\n15.16.\\nShow the following statements for p ∈ K [t]:\\n(a) For all A ∈ K n,n and S ∈ G L n (K ) we have p(S AS −1 ) = Sp(A)S −1 .\\n(b) For all A, B, C ∈ K n,n with AB = C A we have Ap(B) = p(C)A.\\n(c) If K = C and A ∈ Cn,n , then there exists a unitary matrix Q, such that\\nQ H AQ and Q H p(A)Q are upper triangular.\\n15.17.\\nLet V be a finite dimensional unitary vector space.\\nLet f ∈ L(V, V) be normal, i.e., f satisfies f ◦ f ad = f ad ◦ f .   \n",
       "215                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            226\\n\\n15 Polynomials and the Fundamental Theorem of Algebra\\n\\n(a) Show that if λ ∈ C is an eigenvalue of f , then V f (λ)⊥ is an f -invariant subspace.\\n(b) Show (using (a)) that f is diagonalizable.\\n(Hint: Show by induction on dim(V), that V is the direct sum of the eigenspaces of f .)\\n(c) Show (using(a) or (b)), that f is even unitarily diagonalizable, i.e., there exists an orthonormal basis B of V such that [ f ] B,B is a diagonal matrix.\\n(d) Let g ∈ L(V, V) be unitarily diagonalizable.\\nShow that g is normal.\\n(This shows that an endomorphism on a finite dimensional unitary vector space is normal if and only if it is unitarily diagonalizable.\\nWe will give a different proof of this result in Theorem 18.2.)\\n15.18.\\nLet V be a finite dimensional K -vector space, f ∈ L(V, V) and V = U1 ⊕U2 , where U1 , U2 are f -invariant subspaces of V. Let, furthermore, f j := f |U j ∈\\nL(U j , U j ), j = 1, 2.\\n(a) For every v ∈ V there exist unique u 1 ∈ U1 and u 2 ∈ U2 with v = u 1 +u 2 .\\nShow that then also f (v) = f (u 1 ) + f (u 2 ) = f 1 (u 1 ) + f 2 (u 2 ).\\n(We write this as f = f 1 ⊕ f 2 and call f the direct sum of f 1 and f 2 with respect to the decomposition V = U1 ⊕ U2 .)\\n(b) Show that rank( f ) = rank( f 1 ) + rank( f 2 ) and P f = P f1 · P f2 .\\n(c) Show that a(λ, f ) = a(λ, f 1 ) + a(λ, f 2 ) for all λ ∈ K .\\n(Here we set a(λ, h) = 0, if λ is not an eigenvalue of h ∈ L(V, V).)\\n(d) Show that g(λ, f ) = g(λ, f 1 ) + g(λ, f 2 ) for all λ ∈ K .\\n(Here we set g(λ, h) = dim(ker(λIdV −h)) even if λ is not an eigenvalue of h ∈ L(V, V).)\\n(e) Show that p( f ) = p( f 1 ) ⊕ p( f 2 ) for all p ∈ K [t].   \n",
       "216                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Chapter 16\\n\\nCyclic Subspaces, Duality and the Jordan\\nCanonical Form\\n\\nIn this chapter we use the duality theory to analyze the properties of an endomorphism f on a finite dimensional vector space V in detail.\\nWe are particularly interested in the algebraic and geometric multiplicities of the eigenvalues of f and the characterization of the corresponding eigenspaces.\\nOur strategy in this analysis is to decompose the vector space V into a direct sum of f -invariant subspaces so that, with appropriately chosen bases, the essential properties of f will be obvious from its matrix representation.\\nThe matrix representation that we derive is called the Jordan canonical form of f .\\nBecause of its great importance there have been many different derivations of this form using different mathematical tools.\\nOur approach using duality theory is based on an article by Vlastimil Pták (1925–1999) from 1956 [Pta56].\\n\\n16.1 Cyclic f -invariant Subspaces and Duality\\nLet V be a finite dimensional K -vector space.\\nIf f ∈ L(V, V) and v0 ∈ V \\ {0}, then there exists a uniquely defined smallest number m ∈ N, such that the vectors v0 , f (v0 ), . . . , f m−1 (v0 ) are linearly independent and the vectors v0 , f (v0 ), . . . , f m−1 (v0 ), f m (v0 ) are linearly dependent.\\nObviously m ≤ dim(V), since at most dim(V) vectors of V can be linearly independent.\\nThe number m is called the grade of v0 with respect to f .\\nWe denote this grade by m( f, v0 ).\\nThe vector v0 = 0 is linearly dependent, and thus its grade is 0 (with respect to any f ).\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_16\\n\\n227   \n",
       "217                                                                                                                                                                                                                                                                                               228\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\nFor v0 \u0004= 0 we have m( f, v0 ) = 1 if and only if the vectors v0 , f (v0 ) are linearly dependent.\\nThis holds if and only if v0 is an eigenvector of f .\\nIf v0 \u0004= 0 is not an eigenvector of f , then m( f, v0 ) ≥ 2.\\nFor every j ∈ N we define the subspace\\nK j ( f, v0 ) := span{v0 , f (v0 ), . . . , f j−1\\n\\n(v0 )} ⊆ V.\\n\\nThe space K j ( f, v0 ) is called the jth Krylov subspace1 of f and v0 .\\nLemma 16.1 If V is a finite dimensional K -vector space, f ∈ L(V, V), and v0 ∈ V, then the following assertions hold:\\n(1) If m = m( f, v0 ), then Km ( f, v0 ) is an f -invariant subspace of V, and span{v0 } = K1 ( f, v0 ) ⊂ K2 ( f, v0 ) ⊂ · · · ⊂ Km ( f, v0 ) = Km+ j ( f, v0 ) for all j ∈ N.\\n(2) If m = m( f, v0 ) and U ⊆ V is an f -invariant subspace that contains the vector v0 , then Km ( f, v0 ) ⊆ U. Thus, among all f -invariant subspaces of V that contain the vector v0 , the Krylov subspace Km ( f, v0 ) is the one of smallest dimension.\\n(3) If f m−1 (v0 ) \u0004= 0 and f m (v0 ) = 0 for an m ∈ N, then dim(K j ( f, v0 )) = j for j = 1, . . . , m.\\nProof\\n(1) Exercise.\\n(2) The assertion is trivial if v0 = 0.\\nThus, let v0 \u0004= 0 with m = d( f, v0 ) ≥ 1 and let U ⊆ V be an f -invariant subspace that contains v0 .\\nThen U also contains the vectors f (v0 ), . . . , f m−1 (v0 ), so that Km ( f, v0 ) ⊆ U and, in particular, dim(U) ≥ m = dim(Km ( f, v0 ).\\n(3) Let γ0 , . . . , γm−1 ∈ K with\\n0 = γ0 v0 + . . . + γm−1 f m−1 (v0 ).\\nIf we apply f m−1 to both sides, then 0 = γ0 f m−1 (v0 ) and thus γ0 = 0, since f m−1 (v0 ) \u0004= 0.\\nIf m > 1, then we apply inductively f m−k for k = 2, . . . , m and obtain γ1 = · · · = γm−1 = 0.\\nThus, the vectors v0 , . . . , f m−1 (v0 ) are linearly\\n\b independent, which implies that dim(K j ( f, v0 )) = j for j = 1, . . . , m.\\nThe vectors v0 , f (v0 ), . . . , f m−1 (v0 ) form, by construction, a basis of the Krylov subspace Km ( f, v0 ).\\nThe application of f to a vector f k (v0 ) of this basis yields the next basis vector f k+1 (v0 ), k = 0, 1, . . . , m − 2, and the application of f to the last vector f m−1 (v0 ) yields a linear combination of all basis vectors, since f m (v0 ) ∈ Km ( f, v0 ).\\nDue to this special structure, the subspace Km ( f, v0 ) is called a cyclic f -invariant subspace.\\n1 Aleksey\\n\\nNikolaevich Krylov (1863–1945).   \n",
       "218                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       16.1 Cyclic f -invariant Subspaces and Duality\\n\\n229\\n\\nDefinition 16.2 Let V \u0004= {0} be a K -vector space.\\nAn endomorphism f ∈ L(V, V) is called nilpotent, if f m = 0 holds for an m ∈ N. If at the same time f m−1 \u0004= 0, then f is called nilpotent of index m.\\nThe zero map f = 0 is the only nilpotent endomorphism of index m = 1.\\nIf\\nV = {0}, then the zero map is the only endomorphism on V. This map is nilpotent of index m = 1, where in this case we omit the requirement f m−1 = f 0 \u0004= 0.\\nIf f is nilpotent of index m and v \u0004= 0 is any vector with f m−1 (v) \u0004= 0, then f ( f m−1 )(v) = f m (v) = 0 = 0 · f m−1 (v).\\nHence f m−1 (v) is an eigenvector of f corresponding to the eigenvalue 0.\\nOur construction in Sect.\\n16.2 will show that 0 is the only eigenvalue of a nilpotent endomorphism (also cp.\\nExercise 8.3).\\nLemma 16.3 If V \u0004= {0} is a K -vector space and if f ∈ L(V, V) is nilpotent of index m, then m ≤ dim(V).\\nProof If f is nilpotent of index m, then there exists a v0 ∈ V with f m−1 (v0 ) \u0004= 0 and f m (v0 ) = 0.\\nBy (3) in Lemma 16.1 the m vectors v0 , . . . , f m−1 (v0 ) are linearly independent, which implies that m ≤ dim(V).\\n\b\\nExample 16.4 In the vector space K 3,1 the endomorphism f : K 3,1\\n\\n⎡ ⎤\\n⎡ ⎤\\nν1\\n0\\n→ K 3,1 , ⎣ν2 ⎦ → ⎣ν1 ⎦ ,\\nν3\\nν2 is nilpotent of index 3, since f \u0004= 0, f 2 \u0004= 0 and f 3 = 0.\\nIf U is an f -invariant subspace of V, then f |U ∈ L(U, U), where f |U : U → U, u → f (u), is the restriction of f to the subspace U (cp.\\nDefinition 2.12).\\nTheorem 16.5 Let V be a finite dimensional K -vector space and f ∈ L(V, V).\\nThen there exist f -invariant subspaces U1 ⊆ V and U2 ⊆ V with V = U1 ⊕ U2 , such that f |U1 ∈ L(U1 , U1 ) is bijective and f |U2 ∈ L(U2 , U2 ) is nilpotent.\\nProof If v ∈ ker( f ), then f 2 (v) = f ( f (v)) = f (0) = 0.\\nThus, v ∈ ker( f 2 ) and therefore ker( f ) ⊆ ker( f 2 ).\\nProceeding inductively we see that\\n{0} ⊆ ker( f ) ⊆ ker( f 2 ) ⊆ ker( f 3 ) ⊆ · · · .\\nSince V is finite dimensional, there exists a smallest number m ∈ N0 with ker( f m ) = ker( f m+ j ) for all j ∈ N. For this number m let\\nU1 := im( f m ), U2 := ker( f m ).   \n",
       "219                                                                                                                                                                                                                                                                                                                                                                                         230\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\n(If f is bijective, then m = 0, U1 = V and U2 = {0}.)\\nWe now show that the spaces\\nU1 and U2 satisfy the assertion.\\nFirst observe that U1 and U2 are both f -invariant: If v ∈ U1 , then v = f m (w) for some w ∈ V, and therefore f (v) = f ( f m (w)) = f m ( f (w)) ∈ U1 .\\nIf v ∈ U2 , then f m ( f (v)) = f ( f m (v)) = f (0) = 0, and therefore f (v) ∈ U2 .\\nWe have U1 + U2 ⊆ V. An application of the dimension formula for linear maps\\n(cp.\\nTheorem 10.9) to f m gives dim(V) = dim(U1 ) + dim(U2 ).\\nIf v ∈ U1 ∩ U2 , then v = f m (w) for some w ∈ V (since v ∈ U1 ) and hence\\n0 = f m (v) = f m ( f m (w)) = f 2m (w).\\nThe first equation holds since v ∈ U2 .\\nBy the definition of m we have ker( f m ) = ker( f 2m ), which implies f m (w) = 0, and therefore v = f m (w) = 0.\\nFrom U1 ∩U2 =\\n{0} we obtain V = U1 ⊕ U2 .\\nLet now v ∈ ker( f |U1 ) ⊆ U1 be given.\\nSince v ∈ U1 , there exists a vector w ∈ V with v = f m (w), which implies 0 = f (v) = f ( f m (w)) = f m+1 (w).\\nBy the definition of m we have ker( f m ) = ker( f m+1 ), thus w ∈ ker( f m ), and therefore v = f m (w) = 0.\\nThis implies that ker( f |U1 ) = {0}, i.e., f |U1 is injective and thus also bijective (cp.\\nCorollary 10.11).\\nIf, on the other hand, v ∈ U2 , then by definition 0 = f m (v) = ( f |U2 )m (v), and\\n\b thus ( f |U2 )m is the zero map in L(U2 , U2 ), so that f |U2 is nilpotent.\\nFor the further development we recall some terms and results from Chap.\\n11.\\nLet\\nV be a finite dimensional K -vector space and let V ∗ be the dual space of V. If U ⊆ V and W ⊆ V ∗ are two subspaces and if the bilinear form\\nβ : U × W → K , (v, h) → h(v),\\n\\n(16.1) is non-degenerate, then U, W is called a dual pair with respect to β.\\nThis requires that dim(U) = dim(W).\\nFor f ∈ L(U, U) the dual map f ∗ ∈ L(U ∗ , U ∗ ) is defined by f ∗ : U ∗ → U ∗ , h → h ◦ f.\\nFor all v ∈ U and h ∈ U ∗ we have ( f ∗ (h))(v) = h( f (v)).\\nFurthermore, ( f k )∗ =\\n( f ∗ )k for all k ∈ N0 .\\nThe set\\nU 0 := {h ∈ V ∗ | h(u) = 0 for all u ∈ U} is called the annihilator of U. This set is a subspace of V ∗ (cp.\\nExercise 11.5).\\nAnalogously, the set\\nW 0 := {v ∈ V | h(v) = 0 for all h ∈ W} is called the annihilator of W. This set is a subspace of V.   \n",
       "220                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           16.1 Cyclic f -invariant Subspaces and Duality\\n\\n231\\n\\nLemma 16.6 Let V be a finite dimensional K -vector space, f ∈ L(V, V), V ∗ the dual space of V, f ∗ ∈ L(V ∗ , V ∗ ) the dual map of f , and let U ⊆ V and W ⊆ V ∗ be two subspaces.\\nThen the following assertions hold:\\n(1) dim(V) = dim(W) + dim(W 0 ) = dim(U) + dim(U 0 ).\\n(2) If f is nilpotent of index m, then f ∗ is nilpotent of index m.\\n(3) If W ⊆ V ∗ is an f ∗ -invariant subspace, then W 0 ⊆ V is an f -invariant subspace.\\n(4) If U, W are a dual pair with respect to the bilinear form defined in (16.1), then\\nV = U ⊕ W 0.\\nProof\\n(1) Exercise.\\n(2) For all v ∈ V we have f m (v) = 0 and hence,\\n0 = h( f m (v)) = (( f m )∗ (h))(v) = (( f ∗ )m (h))(v) for every h ∈ V ∗ and v ∈ V, so that f ∗ is nilpotent of index at most m.\\nIf ( f ∗ )m−1 = 0, then ( f ∗ )m−1 (h) = 0 for all h ∈ V ∗ , and therefore 0 =\\n(( f ∗ )m−1 (h))(v) = h( f m−1 (v)) for all v ∈ V. This implies that f m−1 = 0, in contradiction to the assumption that f is nilpotent of index m.\\nThus, f ∗ is nilpotent of index m.\\n(3) Let w ∈ W 0 .\\nFor every h ∈ W, we have f ∗ (h) ∈ W, and thus 0 = f ∗ (h)(w) = h( f (w)).\\nHence f (w) ∈ W 0 .\\n(4) If u ∈ U ∩ W 0 , then h(u) = 0 for all h ∈ W, since u ∈ W 0 .\\nSince U, W is a dual pair with respect to the bilinear form defined in (16.1), we have u = 0.\\nMoreover, dim(U) = dim(W) and using (1) we obtain dim(V) = dim(W) + dim(W 0 ) = dim(U) + dim(W 0 ).\\nFrom U ∩ W 0 = {0} we obtain V = U ⊕ W 0 .\\n\\n\b\\n\\nExample 16.7 We consider the vector space V = R2,1 with the canonical basis\\nB = {e1 , e2 }.\\nFor the subspaces\\n\u0006\u0007 \b\\n0\\n⊂ V,\\nU = span\\n1\\nW = h ∈ V ∗ [h] B,{1} = [α, α] for an α ∈ R ⊂ V ∗ , we have\\nU 0 = h ∈ V ∗ [h] B,{1} = [α, 0] for an α ∈ R ⊂ V ∗ ,\\n\u0006\u0007\\n\b\\n1\\n0\\nW = span\\n⊂ V.\\n−1   \n",
       "221                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 232\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\nIn this example, we easily see that dim(V) = dim(W) + dim(W 0 ) = dim(U) + dim(U 0 ), and that U, W form a dual pair with respect to the bilinear form defined in\\n(16.1) with K = R. Moreover, V = U ⊕ W 0 .\\nThe following theorem presents, for a given nilpotent f , a decomposition of V into f -invariant subspaces.\\nThe idea of the decomposition is to construct a dual pair of subspaces U ⊆ V and W ⊆ V ∗ , where U is f -invariant and W is f ∗ -invariant.\\nBy (3) in Lemma 16.6 then W 0 is f -invariant and with (4) in Lemma 16.6 it follows that V = U ⊕ W 0 .\\nTheorem 16.8 Let V be a finite dimensional K -vector space and let f ∈ L(V, V) be nilpotent of index m.\\nLet v0 ∈ V satisfy f m−1 (v0 ) \u0004= 0 and let h 0 ∈ V ∗ satisfy h 0 ( f m−1 (v0 )) \u0004= 0.\\nThen m( f, v0 ) = m( f ∗ , h 0 ) = m, and the f - and f ∗ -invariant subspaces Km ( f, v0 )\\n⊆ V and Km ( f ∗ , h 0 ) ⊆ V ∗ , respectively, are a dual pair with respect to the bilinear form defined in (16.1).\\nFurthermore,\\nV = Km ( f, v0 ) ⊕ (Km ( f ∗ , h 0 ))0 , where (Km ( f ∗ , h 0 ))0 is an f -invariant subspace of V.\\nProof Let v0 ∈ V be a vector with f m−1 (v0 ) \u0004= 0.\\nSince f m (v0 ) = 0, the space\\nKm ( f, v0 ) is an m-dimensional f -invariant subspace of V (cp.\\n(3) in Lemma 16.1).\\nLet h 0 ∈ V ∗ be a vector with\\n0 \u0004= h 0 ( f m−1 (v0 )) = (( f ∗ )m−1 (h 0 ))(v0 ).\\nThen, in particular, 0 \u0004= ( f ∗ )m−1 (h 0 ) ∈ L(V ∗ , V ∗ ).\\nSince f is nilpotent of index m, also f ∗ is nilpotent of index m (cp.\\n(2) in Lemma 16.6), so that\\n( f ∗ )m (h 0 ) = 0 ∈ L(V ∗ , V ∗ ).\\nTherefore, Km ( f ∗ , h 0 ) is an m-dimensional f ∗ -invariant subspace of V ∗ (cp.\\n(3) in\\nLemma 16.1).\\nIt remains to show that Km ( f, v0 ), Km ( f ∗ , h 0 ) are a dual pair.\\nLet m−1 v1 =\\n\\nγ j f j (v0 ) ∈ Km ( f, v0 ) j=0 be a vector with h(v1 ) = β(v1 , h) = 0 for all h ∈ Km ( f ∗ , h 0 ).\\nWe show inductively that then γ0 = · · · = γm−1 = 0, and thus v1 = 0.   \n",
       "222                                                                                                                                                 16.1 Cyclic f -invariant Subspaces and Duality\\n\\n233\\n\\nUsing ( f ∗ )m−1 (h 0 ) ∈ Km ( f ∗ , h 0 ) our assumption on the vector v1 yields m−1\\n∗ m−1\\n\\n0 = (( f )\\n\\n(h 0 ))(v1 ) = h 0 ( f m−1\\n\\n(v1 )) =\\n\\nγ j h 0 ( f m−1+ j (v0 )) j=0\\n\\n= γ0 h 0 ( f m−1 (v0 )).\\nThe last equation holds, since f m−1+ j (v0 ) = 0 for j = 1, . . . , m − 1 (because f m = 0).\\nFrom h 0 ( f m−1 (v0 )) \u0004= 0 we obtain γ0 = 0.\\nSuppose now that γ0 = · · · = γk−1 = 0 for a k, 1 ≤ k ≤ m − 2.\\nUsing\\n( f ∗ )m−1−k (h 0 ) ∈ Km ( f ∗ , h 0 ) our assumption on the vector v1 yields m−1\\n∗ m−1−k\\n\\n0 = (( f )\\n\\n(h 0 ))(v1 ) = h 0 ( f m−1−k\\n\\n(v1 )) =\\n\\nγ j h 0 ( f m−1+ j−k (v0 )) j=0\\n\\n= γk h 0 ( f m−1 (v0 )).\\nThe last equation holds, since γ j = 0 for j = 0, . . . , k − 1 and f m−1+ j−k (v0 ) = 0 for j = k + 1, . . . , m − 1.\\nWe have v1 = 0 as asserted, and therefore the bilinear form defined in (16.1) for the spaces Km ( f, v0 ), Km ( f ∗ , h 0 ) is non-degenerate in the first variable.\\nAnalogously, the bilinear form is non-degenerate in the second variable, and hence\\nKm ( f, v0 ), Km ( f ∗ , h 0 ) are a dual pair.\\nUsing (4) in Lemma 16.6 we now have V = Km ( f, v0 ) ⊕ (Km ( f ∗ , h 0 ))0 , where the space (Km ( f ∗ , h 0 ))0 , is by (3) in Lemma 16.6 an f -invariant subspace of V. \b\\n\\n16.2 The Jordan Canonical Form\\nLet V be a finite dimensional K -vector space and f ∈ L(V, V).\\nIf there exists a basis B of V consisting of eigenvectors of f , then [ f ] B,B is a diagonal matrix, i.e., f is diagonalizable.\\nA necessary and sufficient condition for this is that the characteristic polynomial P f decomposes into linear factors over K and that in addition g( f, λ j ) = a( f, λ j ) for every eigenvalue λ j (cp.\\nTheorem 14.14).\\nIf P f decomposes into linear factors but g( f, λ j ) < a( f, λ j ) holds for at least one eigenvalue λ j , then f is not diagonalizable but can still be triangulated, i.e., there exists a basis B of V, such that [ f ] B,B is an upper triangular matrix\\n(cp.\\nTheorem 14.17).\\nFrom this triangular matrix we can read off the algebraic, but usually not the geometric multiplicities of the eigenvalues.\\nThe goal of the following construction is to determine a basis B of V, so that [ f ] B,B is upper triangular and in addition to the algebraic also reveals the geometric multiplicities of the eigenvalues.\\nUnder the assumption that P f decomposes into linear factors over K , we will construct a basis B of V for which [ f ] B,B is a block diagonal matrix of the form   \n",
       "223                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               234\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\n⎡\\n[ f ] B,B = ⎣\\n\\nJd1 (λ1 )\\n\\n⎤\\n...\\n\\n⎦\\nJdm (λm ), where each diagonal block has the form\\n⎡\\n⎤\\nλj 1\\n⎢ ... ... ⎥\\n⎢\\n⎥\\nJd j (λ j ) := ⎢\\n∈ K d j ,d j\\n... ⎥\\n⎣\\n1⎦\\nλj\\n\\n(16.2) for some λ j ∈ K and d j ∈ N, j = 1, . . . , m.\\nA matrix of the form (16.2) is called a\\nJordan block of size d j corresponding to the eigenvalue λ j .\\nIn the following construction we first do not assume that P f decomposes into linear factors.\\nWe only assume the existence of a single eigenvalue λ1 ∈ K of f .\\nUsing this eigenvalue, we define the endomorphism g := f − λ1 IdV ∈ L(V, V).\\nBy Theorem 16.5 there exist g-invariant subspaces U ⊆ V and W ⊆ V with\\nV = U ⊕ W, such that g1 := g|U is nilpotent and g|W is bijective.\\nThen U \u0004= {0}, since otherwise W = V and g|W = g|V = g would be bijective, which contradicts the assumption that λ1 is an eigenvalue of f .\\nLet g1 be nilpotent of index d1 .\\nThen by construction 1 ≤ d1 ≤ dim(U).\\nLet w1 ∈ U be a vector with g1d1 −1 (w1 ) \u0004= 0.\\nSince g1d1 (w1 ) = 0, the vector g1d1 −1 (w1 ) is a eigenvector of g1 corresponding to the eigenvalue 0.\\nBy (3) in Lemma 16.1, the d1 vectors w1 , g1 (w1 ), . . . , g1d1 −1 (w1 ) are linearly independent and U1 := Kd1 (g1 , w1 ) is a d1 -dimensional g1 -invariant subspace of U.\\nConsider the basis\\n\u0011\\n\u0010\\nB1 := g1d1 −1 (w1 ), . . . , g1 (w1 ), w1 of U1 .\\nThen the matrix representation g1 |U1 with respect to the basis B1 is given by\\n[g1 |U1 ] B1 ,B1 = Jd1 (0) ∈ K d1 ,d1 .   \n",
       "224                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         16.2 The Jordan Canonical Form\\n\\n235\\n\\nThis shows, in particular, that the characteristic polynomial of g1 |U1 is given by the monomial t d1 , and hence 0 is the only eigenvalue of g1 |U1 .\\nMoreover, by construction\\n[g1 |U1 ] B1 ,B1 = [g|U1 ] B1 ,B1 .\\nIf d1 = dim(U), then our construction is complete for the moment.\\nIf, on the other hand, d1 < dim(U), then applying Theorem 16.8 to g1 ∈ L(U, U) shows that there\\n\u0012 \u0004= {0} with U = U1 ⊕ U,\\n\u0012 and we consider exists a g1 -invariant subspace U g2 := g1 |U\u0012.\\nThis map is nilpotent of index d2 , where 1 ≤ d2 ≤ d1 .\\nWe now carry out the same construction as before:\\n\u0012 with g d2 −1 (w2 ) \u0004= 0.\\nThen g d2 −1 (w2 ) is an\\nWe determine a vector w2 ∈ U\\n2\\n2 eigenvector of g2 , U2 := Kd2 (g2 , w2 ) is a d2 -dimensional g2 -invariant subspace of\\n\u0012 ⊂ U and for the basis\\nU\\n\u0011\\n\u0010\\nB2 := g2d2 −1 (w2 ), . . . , g2 (w2 ), w2 of U2 we have\\n\\n[g2 |U2 ] B2 ,B2 = Jd2 (0) ∈ K d2 ,d2 , where again [g2 |U2 ] B2 ,B2 = [g|U2 ] B2 ,B2 by construction.\\nAfter k ≤ dim(U) steps this procedure terminates.\\nWe then have found a decomposition of U of the form\\nU = Kd1 (g1 , w1 ) ⊕ . . . ⊕ Kdk (gk , wk ) = Kd1 (g, w1 ) ⊕ . . . ⊕ Kdk (g, wk ).\\nIn the second equation we have used that Kd j (g j , w j ) = Kd j (g, wk ) for j = 1, . . . , k.\\nIf we combine the constructed bases B1 , . . . , Bk to a basis B of U, then\\n[g|U ] B,B\\n\\n⎡\\n[g|U1 ] B1 ,B1\\n...\\n⎣\\n=\\n\\n⎤\\n\\n⎡\\n\\n⎦ = ⎣\\n\\nJd1 (0)\\n\\n[g|Uk ] Bk ,Bk\\n\\n⎤\\n...\\n\\n⎦.\\nJdk (0)\\n\\nThus, the nilpotent endomorphism g1 = g|U has the characteristic polynomial t d1 +...+dk , and its only eigenvalue is 0.\\nWe now transfer these results to f = g + λ1 IdV .\\nEvery g-invariant subspace is f -invariant and one observes easily that\\nKd j ( f, w j ) = Kd j (g, w j ), j = 1, . . . , k   \n",
       "225                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     236\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\n(cp.\\nExercise 16.3).\\nHence, it follows that\\nU = Kd1 ( f, w1 ) ⊕ . . . ⊕ Kdk ( f, wk ).\\nFor every j = 1, . . . , k and 0 ≤ \u0002 ≤ d j − 1 we have\\n\u0014\\n\u0013\\n\u0014\\n\u0013 f g \u0002 (w j ) = g g \u0002 (w j ) + λ1 g \u0002 (w j ) = λ1 g \u0002 (w j ) + g \u0002+1 (w j ),\\n\\n(16.3) where g d j (w j ) = 0.\\nThe matrix representation of f |U with respect to the basis B of\\nU is therefore given by\\n[ f |U ] B,B\\n\\n⎡\\n[ f |U1 ] B1 ,B1\\n...\\n=⎣\\n\\n⎤\\n\\n⎡\\n\\n⎦=⎣\\n\\nJd1 (λ1 )\\n\\n[ f |Uk ] Bk ,Bk\\n\\n⎤\\n...\\n\\n⎦.\\n\\n(16.4)\\n\\nJdk (λ1 )\\n\\nThe map g|W = f |W − λ1 IdW is bijective by construction, i.e., λ1 is not an eigenvalue of f |W .\\nTherefore, a( f, λ1 ) = dim(U) = d1 + . . . + dk .\\nIn order to determine g( f, λ1 ), let v ∈ U be an arbitrary vector.\\nThen there exist scalars α j,\u0002 ∈ K with d j −1 k\\n\\nα j,\u0002 g \u0002 (w j ).\\nv= j=1 \u0002=0\\n\\nUsing (16.3) we obtain k d j −1 f (v) = k d j −1 k d j −1\\n\u0015\\n\u0016\\nα j,\u0002 f g \u0002 (w j ) =\\nα j,\u0002 λ1 g \u0002 (w j ) +\\nα j,\u0002 g \u0002+1 (w j ) j=1 \u0002=0 j=1 \u0002=0 k d j −2\\n\\n= λ1 v + j=1 \u0002=0\\n\\nα j,\u0002 g \u0002+1 (w j ).\\nj=1 \u0002=0\\n\\nThe vectors in the last sum are linearly independent.\\nHence, f (v) = λ1 v if and only if α j,\u0002 = 0 for j = 1, . . . , k and \u0002 = 0, 1, . . . , d j − 2.\\nThis shows that every eigenvector of f corresponding to the eigenvalue λ1 has the form k\\n\\nα j g d j −1 (w j ), v= j=1 where at least one α j is nonzero, so that we have\\nV f (λ1 ) = span{g d1 −1 (w1 ), . . . , g dk −1 (wk )}.   \n",
       "226                                                                                                                                                                                                                           16.2 The Jordan Canonical Form\\n\\n237\\n\\nSince g d1 −1 (w1 ), . . . , g dk −1 (wk ) are linearly independent, it follows that g( f, λ1 ) = k.\\nThe geometric multiplicity of the eigenvalue λ1 therefore is equal to the number of\\nJordan blocks corresponding to the eigenvalue λ1 in the matrix representation (16.4).\\nFurthermore, we observe that in every subspace Kd j ( f, w j ), the endomorphism f has exactly one (linear independent) eigenvector corresponding to the eigenvalue λ1 .\\nWe summarize these results in the following theorem.\\nTheorem 16.9 Let V be a finite dimensional K -vector space and let f ∈ L(V, V).\\nIf λ1 ∈ K is an eigenvalue of f , then the following assertions hold:\\n(1) There exist f -invariant subspaces {0} \u0004= U ⊆ V and W ⊂ V with V = U ⊕ W.\\nThe map f |U − λ1 IdU is nilpotent and the map f |W − λ1 IdW is bijective.\\nIn particular, λ1 is not an eigenvalue of f |W .\\n(2) The subspace U from (1) can be written as\\nU = Kd1 ( f, w1 ) ⊕ . . . ⊕ Kdk ( f, wk ) for some vectors w1 , . . . , wk ∈ U, where Kd j ( f, w j ) is a d j -dimensional f invariant subspace of V, j = 1, . . . , k.\\nThis is called a cyclic decomposition of U.\\n(3) There exists a basis B of U with\\n⎡\\n[ f |U ] B,B = ⎣\\n\\nJd1 (λ1 )\\n\\n⎤\\n...\\n\\n⎦.\\nJdk (λ1 )\\n\\n(4) We have a( f, λ1 ) = d1 + . . . + dk and g( f, λ1 ) = k.\\nIf f has a further eigenvalue λ2 \u0004= λ1 , then it is an eigenvalue of the restriction f |W ∈ L(W, W) and we can apply Theorem 16.9 to f |W .\\nThe vector space W then is the direct sum of the form W = X ⊕ Y, where f |X − λ2 IdX is nilpotent and f |Y − λ2 IdY is bijective.\\nThe space X has a cyclic decomposition analogous to (2) in Theorem 16.9, and there exists a matrix representation of f |X analogous to (3).\\nThis construction can be carried out for all eigenvalues of f .\\nIf the characteristic polynomial P f decomposes into linear factors over K , then we finally obtain a cyclic decomposition of the entire space V, which gives the following theorem.\\nTheorem 16.10 Let V be a finite dimensional K -vector space and let f ∈ L(V, V).\\nIf the characteristic polynomial P f decomposes into linear factors over K , then there exists a basis B of V, such that\\n⎡\\n[ f ] B,B = ⎣\\n\\nJd1 (λ1 )\\n\\n⎤\\n...\\n\\n⎦,\\n\\n(16.5)\\n\\nJdm (λm ) where λ1 , . . . , λm ∈ K are the (not necessarily pairwise distinct) eigenvalues of f .\\nFor every eigenvalue λ j of f then a( f, λ j ) is equal to the sum of the sizes of all   \n",
       "227                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       238\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\nJordan blocks corresponding to λ j in (16.5), and g( f, λ j ) is equal to the number of Jordan blocks corresponding to λ j in (16.5).\\nA matrix representation of the form\\n(16.5) is called a Jordan canonical form2 of f .\\nFrom Theorem 14.14 we know that f ∈ L(V, V) is diagonalizable if and only if P f decomposes into linear factors over K and g( f, λ j ) = a( f, λ j ) holds for every eigenvalue λ j of f .\\nIf P f decomposes into linear factors, then the Jordan canonical form (16.5) shows that g( f, λ j ) = a( f, λ j ) if and only if every Jordan block corresponding to λ j is of size 1.\\nThe Fundamental Theorem of Algebra yields the following corollary of Theorem 16.10.\\nCorollary 16.11 If V is a finite dimensional C-vector space, then every f ∈ L(V, V) has a Jordan canonical form.\\nThe following uniqueness result justifies the name canonical form.\\nTheorem 16.12 Let V be a finite dimensional K -vector space.\\nIf f ∈ L(V, V) has a Jordan canonical form, then it is unique up to the order of the Jordan blocks on the diagonal.\\nProof Let dim(V) = n and let B1 , B2 be two bases of V with\\n⎡\\nA1 = [ f ] B1 ,B1 = ⎣ as well as\\n\\nJd1 (λ1 )\\n\\n⎡\\nA2 = [ f ] B2 ,B2 = ⎣\\n\\nJc1 (μ1 )\\n\\n⎤\\n...\\n\\n⎦ ∈ K n,n ,\\nJdm (λm )\\n⎤\\n\\n...\\n\\n⎦ ∈ K n,n .\\nJck (μk )\\n\\nFor a given eigenvalue λ j , 1 ≤ j ≤ m, we define\\n\u0014\\n\u0013 rs(1) (λ j ) := rank (A1 − λ j In )s , s = 0, 1, 2, . . . .\\nThen\\n\\n(1)\\n(λ j ) − rs(1) (λ j ), s = 1, 2, . . . , ds(1) (λ j ) := rs−1 is equal to the number of Jordan blocks J\u0002 (λ j ) ∈ K \u0002,\u0002 on the diagonal of A1 with\\n\u0002 ≥ s.\\nThe number of Jordan blocks corresponding to the eigenvalue λ j with exact size s therefore is given by\\n(1)\\n(1)\\n(1)\\n(λ j ) = rs−1\\n(λ j ) − 2rs(1) (λ j ) + rs+1\\n(λ j ) ds(1) (λ j ) − ds+1\\n\\n2 Marie\\n\\n(16.6)\\n\\nEnnemond Camille Jordan (1838–1922) derived this form 1870.\\nTwo years earlier, Karl\\nWeierstraß (1815–1897) proved a result that implies the Jordan canonical form.   \n",
       "228                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 16.2 The Jordan Canonical Form\\n\\n239\\n\\n(cp.\\nExample 16.13).\\nThe matrices A1 and A2 are similar and, therefore, have the same eigenvalues, i.e.,\\n{λ1 , . . . , λm } = {μ1 , . . . , μk }.\\nFurthermore, rank\\n\\n\u0013\u0013\\n\\nA1 − αIn\\n\\n\u0014m \u0014\\n\\n= rank\\n\\n\u0013\u0013\\n\\nA2 − αIn\\n\\n\u0014m \u0014 for all α ∈ K and m ∈ N0 .\\nIn particular, for every λ j there exists μi ∈ {μ1 , . . . , μk } with μi = λ j and for this μi and the matrix A2 we get rs(2) (μi ) := rank\\n\\n\u0013\u0013\\n\\nA2 − μi In\\n\\n\u0014s \u0014\\n\\n= rs(1) (λ j ), s = 0, 1, 2, . . . .\\n\\nNow (16.6) shows that the matrix A2 has, up to reordering, the same Jordan blocks\\n\b on the diagonal as the matrix A1 .\\nExample 16.13 This example illustrates the construction in the proof of Theorem 16.12.\\nIf\\n⎤\\n⎡\\n11\\n⎡\\n⎤\\n⎥\\n⎢ 1\\nJ2 (1)\\n⎥\\n⎢\\n⎥ ∈ R5,5 ,\\n⎢\\n⎣\\n⎦\\n1\\nJ1 (1)\\nA=\\n(16.7)\\n=⎢\\n⎥\\n⎣\\nJ2 (0)\\n0 1⎦\\n0 then (A − 1 · I5 )0 = I5 ,\\n⎡\\n\\n01\\n⎢ 0\\n⎢\\n0\\nA − 1 · I5 = ⎢\\n⎢\\n⎣\\n−1\\n\\n⎤\\n\\n⎡\\n\\n⎤\\n00\\n⎢ 0\\n⎥\\n⎥\\n⎢\\n⎥\\n⎥\\n⎥ , (A − 1 · I5 )2 = ⎢\\n⎥,\\n0\\n⎢\\n⎥\\n⎥\\n⎣\\n⎦\\n1\\n1 −2 ⎦\\n−1\\n1 and we get r0 (1) = 5, r1 (1) = 3, rs (1) = 2, s ≥ 2, d1 (1) = 2, d2 (1) = 1, ds (1) = 0, s ≥ 3, d1 (1) − d2 (1) = 1, d2 (1) − d3 (1) = 1, ds (1) − ds+1 (1) = 0, s ≥ 3.   \n",
       "229                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        240\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\nWe now consider the powers of a Jordan block Jd (λ) ∈ K d,d .\\nSince Id and Jd (0) commute,\\n\u0017 \u0018 k\\nλk− j (Jd (0)) j = j j=0 k\\n\\n(Jd (λ))k = (λId + Jd (0))k = k j=0 p ( j) (λ)\\n(Jd (0)) j , j!\\nfor every k ∈ N0 , where p ( j) is the jth derivative of the polynomial p = t k with respect to t, p (0) = (t k )(0) = t k , p ( j) = (t k )( j) = k(k −1)·. . .·(k − j +1) t k− j , j = 1, . . . , k.\\n\\nWe can now easily show the following result.\\nLemma 16.14 If p ∈ K [t] is a polynomial of degree k ≥ 0, then k p (Jd (λ)) = j=0 p ( j) (λ)\\n(Jd (0)) j .\\nj!\\n\\n(16.8)\\n\b\\n\\nProof Exercise.\\n\\nConsidered as a linear map from K d,1 to K d,1 , the matrix Jd (0) represents an\\n“upshift”, since\\n⎤ ⎡ ⎤\\nα2\\nα1\\n⎢α2 ⎥ ⎢ ... ⎥\\n⎥ ⎢ ⎥\\nJd (0) ⎢\\n⎣ ... ⎦ = ⎣α ⎦ for all d\\n⎡\\n\\nαd\\n\\nClearly,\\n\\n0\\n\\n⎡\\n\\n⎤\\nα1\\n⎢α2 ⎥\\n⎢ . ⎥ ∈ K d,1 .\\n⎣ .. ⎦\\nαd\\n\\n(Jd (0))\u0002 \u0004= 0, \u0002 = 0, 1, . . . , d − 1, (Jd (0))d = 0, and hence the linear map Jd (0) is nilpotent of index d.\\nThe sum on the right hand side of (16.8) therefore has at most d terms, even when deg( p) > d.\\nMoreover, the right hand side of (16.8) shows that p (Jd (λ)) is an upper triangular matrix with constant entries on its diagonals.\\nA matrix with constant diagonals is called a Toeplitz matrix.3 In particular, on the main diagonal we have the entry p(λ).\\nFrom (16.8) we see that p(Jd (λ)) = 0 holds if and only if p(λ) = p \u0010 (λ) = · · · = p (d−1) (λ) = 0.\\nThus we have shown the following result.\\n\\n3 Otto\\n\\nToeplitz (1881–1940).   \n",
       "230                                                                                                                                  16.2 The Jordan Canonical Form\\n\\n241\\n\\nLemma 16.15 Let p ∈ K [t] be a polynomial and Jd (λ) ∈ K d,d be a Jordan block.\\n(1) The matrix p(Jd (λ)) is invertible if and only if λ is not a root of p.\\n(2) We have p(Jd (λ)) = 0 ∈ K d,d if and only if λ is a d-fold root of p, i.e., if the linear factor (t − λ)d is a divisor of p.\\nLet V be a finite dimensional K -vector space and let f ∈ L(V, V), where we do not assume that P f decomposes into linear factors.\\nFrom the Cayley-Hamilton theorem (Theorem 8.6) we know that P f ( f ) = 0 ∈ L(V, V), i.e., there exists a monic polynomial of degree at most dim(V), which annihilates the endomorphism f .\\nLet p1 , p2 ∈ K [t] be two monic polynomials of smallest possible degree with p1 ( f ) = p2 ( f ) = 0.\\nThen ( p1 − p2 )( f ) = 0, and since p1 and p2 are monic, p1 − p2 ∈ K [t] is a polynomial with deg( p1 − p2 ) < deg( p1 ) = deg( p2 ).\\nThe minimality assumption on deg( p1 ) and deg( p2 ) implies that p1 − p2 = 0, i.e., p1 = p2 .\\nThus, for every f ∈ L(V, V) there exists a uniquely determined monic polynomial of minimal degree which annihilates f .\\nThis justifies the following definition.\\nDefinition 16.16 If V is finite dimensional K -vector space and f ∈ L(V, V), then the uniquely determined monic polynomial of minimal degree that annihilates f is called the minimal polynomial of f .\\nWe denote this polynomial by M f .\\nBy construction we always have deg(M f ) ≤ deg(P f ) = dim(V).\\nLemma 16.17 If V is a finite dimensional K -vector space and f ∈ L(V, V), then the minimal polynomial M f divides every polynomial that annihilates f and is, in particular, a divisor of the characteristic polynomial P f .\\nProof For p = 0 we have p( f ) = 0 and M f divides p.\\nIf p ∈ K [t] \\ {0} is a polynomial with p( f ) = 0, then deg(M f ) ≤ deg( p).\\nUsing division with remainder\\n(cp.\\nTheorem 15.4), there exist uniquely determined polynomials q, r ∈ K [t] with p = q · M f + r and deg(r ) < deg(M f ).\\nThus,\\n0 = p( f ) = q( f )M f ( f ) + r ( f ) = r ( f ).\\nThe minimality of deg(M f ) implies that r = 0, and hence M f divides p.\\n\\n\b\\n\\nIf P f decomposes into linear factors, then we can explicitly construct M f using the Jordan canonical form of f .\\nLemma 16.18 Let V be a finite dimensional K -vector space.\\nIf f ∈ L(V, V) has a\\nJordan canonical form with pairwise distinct eigenvalues \u0012\\nλ1 , . . . , \u0012\\nλk and if d\u00121 , . . . , d\u0012k are the respective maximal sizes of the corresponding Jordan blocks, then\\nMf = k\\n\u0019 j=1\\n\\n\u0012\\n\\n(t − \u0012\\nλ j )d j .   \n",
       "231                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   242\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\nProof We know from Lemma 16.17 that M f is a divisor of P f .\\nTherefore,\\nMf = k\\n\u0019\\n\\n(t − \u0012\\nλ j )\u0002 j j=1 for some exponents \u00021 , . . . , \u0002k .\\nIf\\n⎡\\nA=⎣\\n\\nJd1 (λ1 )\\n\\n⎤\\n...\\n\\n⎦\\nJdm (λm ) is a Jordan canonical form of f , then M f ( f ) = 0 ∈ L(V, V) is equivalent to\\nM f (A) = 0 ∈ K n,n , where n = dim(V).\\nWe have M f (A) = 0 if and only if\\nM f (Jd j (λ j )) = 0 for j = 1, . . . , m.\\nFor this it is necessary and sufficient that\\nM f (Jd\u0012j (\u0012\\nλ j )) = 0 for j = 1, . . . , k.\\nBy Lemma 16.15 this holds if and only if every\\n\u0012 of the linear factors (t − \u0012\\nλ j )d j , j = 1, . . . , k, is a divisor of M f .\\nTherefore, M f has the desired form.\\n\b\\nExample 16.19 If f is an endomorphism with the Jordan canonical form A in (16.7), then\\nP f = (t − 1)3 t 2 , M f = (t − 1)2 t 2 and\\n\\n⎡\\n\\n⎤\\n00\\n⎢ 0\\n⎥\\n⎢\\n⎥\\n2 2\\n⎢\\n⎥\\n0\\nM f (A) = (A − 1 · I5 ) A = ⎢\\n⎥\\n⎣\\n1 −2 ⎦\\n1\\n\\n⎡\\n\\n12\\n⎢ 1\\n⎢\\n⎢\\n1\\n⎢\\n⎣\\n0\\n\\n⎤\\n⎥\\n⎥\\n⎥,\\n⎥\\n0⎦\\n0 which shows that M f (A) = 0 ∈ R5,5 and M f ( f ) = 0 ∈ L(V, V).\\nThe Jordan canonical form is of great importance in theoretical Linear Algebra.\\nIn practical applications, however, where usually matrices over K = R or K = C are considered, it is not so relevant, since there is no numerically stable method for computing the Jordan canonical form of a general matrix in finite precision arithmetic.\\nThe reason for the lack of such a method is that the entries of the Jordan canonical form do not depend continuously on the entries of the given matrix.\\nExample 16.20 Consider the matrix\\n\b\\nε1\\n, ε ∈ R.\\nA(ε) =\\n00\\n\u0007   \n",
       "232                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               16.2 The Jordan Canonical Form\\n\\n243\\n\\nFor every given ε \u0004= 0, the matrix A(ε) has the two distinct eigenvalues ε and 0, and hence the diagonal matrix\\n\u0007 \b\\nε0\\nJ (ε) =\\n00 is a Jordan canonical form of A(ε).\\nHowever, for ε → 0, we obtain\\nA(ε) →\\n\\n\u0007 \b\\n01\\n,\\n00\\n\\nJ (ε) →\\n\\n\u0007 \b\\n00\\n.\\n00\\n\\nThus, J (ε) does not converge to the Jordan canonical form of A(0) for ε → 0.\\nA similar example is given by the matrices in Exercise 8.5: While A(0) is a\\nJordan block of size n corresponding to the eigenvalue 1, for every ε \u0004= 0 we obtain a diagonalizable matrix A(ε) ∈ Cn,n with n pairwise distinct eigenvalues.\\n\\nMATLAB-Minute.\\nLet\\nA = T −1\\n\\n\u0007\\n\\n\b\\n10\\nT ∈ C2,2 ,\\n11 where T ∈ C2,2 is a random matrix constructed with the command T= rand(2).\\nConstruct several such matrices and always compute the eigenvalues using the command eig(A).\\nDisplay the eigenvalues in format long.\\nOne observes that the two eigenvalues are real or complex conjugates, and that they always have an error starting from the 8th digit after the decimal point, i.e., an error on the order of 10−8 .\\nThis does not happen by chance, but is due to the behavior of the eigenvalues under perturbations, which arise from rounding errors in the computer.\\n\\n16.3 Computation of the Jordan Canonical Form\\nWe now derive a method for the computation of the Jordan canonical form of an endomorphism f on a finite dimensional K -vector space V. We assume that P f decomposes into linear factors over K , and that the roots of P f , i.e., the eigenvalues of f , are known.\\nThe construction follows the important steps in the existence proof of the Jordan canonical form in Sect.\\n16.2.\\nSuppose that λ is an eigenvalue of f and that f has a corresponding Jordan block of size s.\\nThen there exist s linearly independent vectors t1 , . . . , ts with [ f ] \u001a\\nB, \u001a\\nB = Js (λ)   \n",
       "233                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     244\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form for \u001a\\nB = {t1 , . . . , ts }.\\nWith t0 := 0 and writing Id instead of IdV for simplicity of notation, we then have\\n( f − λId)(t1 ) = t0 ,\\n( f − λId)(t2 ) = t1 ,\\n..\\n.\\n( f − λId)(ts ) = ts−1 , hence ts− j = ( f − λId) j (ts ) for j = 0, 1, . . . , s.\\nThe vectors ts , ts−1 , . . . , t1 form a sequence as the one we have constructed in the context of the Krylov subspaces, and span{ts , ts−1 , . . . , t1 } = Ks ( f − λId, ts ).\\nThe reverse sequence t1 , t 2 , . . . , t s is called a Jordan chain of f corresponding to the eigenvalue λ.\\nThe vector t1 is an eigenvector of f corresponding to λ.\\nFor the vector t2 we then have ( f −λId)(t2 ) \u0004= 0 and\\n( f − λId)2 (t2 ) = ( f − λId)(t1 ) = 0.\\nHence t2 ∈ ker(( f − λId)2 ) \\ ker( f − λId), and in general t j ∈ ker(( f − λId) j ) \\ ker(( f − λId) j−1 ), j = 1, . . . , s.\\n\\nThis motivates the following definition.\\nDefinition 16.21 Let V be a finite dimensional K -vector space, let f ∈ L(V, V) have the eigenvalue λ ∈ K , and let k ∈ N. A vector v ∈ V with v ∈ ker(( f − λId)k ) \\ ker(( f − λId)k−1 ) is called a principal vector of level k of f corresponding to the eigenvalue λ.\\nPrincipal vectors of level one are eigenvectors.\\nPrincipal vectors of higher levels can be considered generalizations of eigenvectors, and they are therefore sometimes called generalized eigenvectors.\\nFor the computation of the Jordan canonical form of f , we thus need to know the number and lengths of the Jordan chains corresponding to the different eigenvalues of f .\\nThese correspond to the number and sizes of the Jordan blocks of f .\\nIf F is a matrix representation of f with respect to an arbitrary basis, then (cp. the proof of\\nTheorem 16.12)   \n",
       "234                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 16.3 Computation of the Jordan Canonical Form\\n\\n245 ds (λ) := rank((F − λI )s−1 ) − rank((F − λI )s )\\n= dim(im(( f − λId)s−1 )) − dim(im(( f − λId)s ))\\n= dim(V) − dim(ker(( f − λId)s−1 )) − (dim(V) − dim(ker(( f − λId)s )))\\n= dim(ker(( f − λId)s )) − dim(ker(( f − λId)s−1 )) is the number of Jordan blocks corresponding to λ of size at least s.\\nThis implies, in particular, that ds (λ) ≥ ds+1 (λ) ≥ 0, s = 1, 2, . . . , and ds (λ) − ds+1 (λ) is the number of Jordan blocks of exact size s corresponding to λ.\\nThere exists a smallest number m ∈ N with\\n{0} = ker(( f − λId)0 ) ⊂ ker(( f − λId)1 ) ⊂ · · · ⊂ ker(( f − λId)m ) = ker(( f − λId)m+1 ).\\n\\nHence ds (λ) = 0 for all s ≥ m + 1, so that there is no Jordan block corresponding to λ of size m + 1 or larger.\\nIn order to compute the Jordan canonical form, we therefore proceed as follows:\\n(1) Determine the eigenvalues of f .\\n(2) For every eigenvalue λ of f carry out the following steps:\\n(a) Determine the smallest number m ∈ N with ker(( f −λId)0 ) ⊂ ker(( f −λId)1 ) ⊂ · · · ⊂ ker(( f −λId)m ) = ker(( f −λId)m+1 ).\\n\\nThen dim(ker(( f − λId)m )) = a(λ, f ).\\n(b) For s = 1, . . . , m determine ds (λ) = dim(ker(( f − λId)s )) − dim(ker(( f − λId)s−1 )) > 0.\\nIf s ≥ m + 1, then ds (λ) = 0, and d1 (λ) = dim(ker( f − λId)) = g(λ, f ) is the number of Jordan blocks corresponding to λ.\\n(c) To simplify notation, we write ds := ds (λ) and determine the Jordan chains as follows:\\n(i) Since dm − dm+1 = dm , there exist dm Jordan blocks of size m.\\nFor each of these blocks we determine a Jordan chain of dm principal vectors of level m, i.e., vectors t1,m , t2,m , . . . , tdm ,m ∈ ker(( f − λId)m ) \\ ker(( f − λId)m−1 )   \n",
       "235                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                246\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form with the following property: dm\\n\u001b\\nIf α1 , . . . , αdm ∈ K with\\nαi ti,m ∈ ker(( f − λId)m−1 ), then α1 = i=1\\n\\n· · · = αdm = 0.\\nHere the first index in ti, j indicates the number of the chain, and the second indicates the level of the principal vector (from ker(( f − λId) j ) and not ker(( f − λId) j−1 )).\\n(ii) For j = m, m − 1, . . . , 2 we proceed as follows:\\nWhen we have determined d j principal vectors of level j, say t1, j , t2, j ,\\n. . . , td j , j , we apply f − λId to each of these vectors, hence ti, j−1 := ( f − λId)(ti, j ), 1 ≤ i ≤ d j , in order to determine the principal vectors of level j − 1.\\ndj\\n\u001b\\nαi ti, j−1 ∈ ker(( f − λId) j−2 ), then\\nIf α1 , . . . , αd j ∈ K with i=1\\n\\n⎛ dj\\n\\n0 = ( f − λId) j−2 ⎝ dj\\n\u001b\\n\\n⎛\\n\\n⎞ dj\\n\\nαi ti, j−1 ⎠ = ( f − λId) j−1 ⎝ i=1 and thus\\n\\n⎞\\n\\nαi ti, j ⎠ , i=1\\n\\nαi ti, j ∈ ker(( f − λId) j−1 ) giving α1 = · · · = αd j = 0.\\ni=1 d j−1 > d j , then there exist d j − d j−1 Jordan blocks of size j − 1.\\nFor\\nIf these we need the Jordan chains of length j − 1.\\nThus we extend the already computed t1, j−1 , t2, j−1 , . . . , td j , j−1 ∈ ker(( f − λId) j−1 ) \\ ker(( f − λId) j−2 ) to d j−1 principal vectors of level ( j − 1) (but only if d j−1 > d j ) via t1, j−1 , t2, j−1 , . . . , td j−1 , j−1 ∈ ker(( f − λId) j−1 ) \\ ker(( f − λId) j−2 ), where the following must hold: If α1 , . . . , αd j−1 ∈ K with d\u001b j−1\\n\\nαi ti, j−1 ∈ i=1 ker(( f − λId) j−2 ), then α1 = · · · = αd j−1 = 0.\\nAfter completing the step for j = 2, we have obtained (linearly independent) vectors t1,1 , t2,1 , . . . , td1 ,1 ∈ ker( f − λId).\\nSince dim(ker( f − λId)) = d1 , we have found a basis of ker( f − λId).\\nIn this way we have determined d1 different Jordan chains that we combine as follows:\\nTλ := t1,1 , t1,2 , . . . , t1,m ; t2,1 , t2,2 , . . . , t2,∗ ; . . . ; td1 ,1 , . . . , td1 ,∗ .   \n",
       "236                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     16.3 Computation of the Jordan Canonical Form\\n\\n247\\n\\nEach chain begins with an eigenvector, followed by principal vectors of increasing levels.\\nHere we use the convention that the chains are ordered decreasingly according to their length.\\n(3) Jordan chains are linearly independent, if their first vectors (the eigenvectors) are linearly independent.\\n(Show this as an exercise.)\\nThus, if λ1 , . . . , λ\u0002 are the pairwise distinct eigenvalues of f , then\\nT = Tλ1 , . . . , Tλ\u0002 is a basis, for which [ f ]T,T is in Jordan canonical form.\\nExample 16.22 We interpret the matrix\\n⎡\\n\\n50\\n⎢ 01\\n⎢\\nF =⎢\\n⎢ −1 0\\n⎣ 00\\n00\\n\\n10\\n00\\n30\\n01\\n00\\n\\n⎤\\n0\\n0⎥\\n⎥\\n5,5\\n0⎥\\n⎥∈R\\n⎦\\n0\\n4 as endomorphism on R5,1 .\\n(1) The eigenvalues of F are the roots of PF = (t − 1)2 (t − 4)3 .\\nIn particular PF decomposes into linear factors and F has a Jordan canonical form.\\n(2) We now consider the different eigenvalues of F:\\n(a) For the eigenvalue λ1 = 1 we obtain\\n⎛⎡\\n\\n4\\n⎜⎢ 0\\n⎜⎢\\n⎢ ker(F − I ) = ker ⎜\\n⎜⎢ −1\\n⎝⎣ 0\\n0\\n\\n0\\n0\\n0\\n0\\n0\\n\\n10\\n00\\n20\\n00\\n00\\n\\n⎤⎞\\n0\\n⎟\\n0⎥\\n⎥⎟\\n⎥\\n0 ⎥⎟\\n⎟ = span{e2 , e4 }.\\n0 ⎦⎠\\n3\\n\\nHere dim(ker(F − I )) = 2 = a(1, F).\\nFor the eigenvalue λ2 = 4 we obtain\\n⎤⎞\\n1 0 1 00\\n⎜⎢ 0 −3 0 0 0 ⎥⎟\\n⎥⎟\\n⎜⎢\\n⎢\\n⎥⎟ ker(F − 4 I ) = ker ⎜\\n⎜⎢ −1 0 −1 0 0 ⎥⎟ = span{e1 − e3 , e5 },\\n⎝⎣ 0 0 0 −3 0 ⎦⎠\\n0 0 0 00\\n⎛⎡   \n",
       "237                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              248\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\n⎛⎡\\n\\n0\\n⎜⎢ 0\\n⎜⎢\\n⎢ ker((F − 4 I )2 ) = ker ⎜\\n⎜⎢ 0\\n⎝⎣ 0\\n0\\n\\n00\\n90\\n00\\n00\\n00\\n\\n0\\n0\\n0\\n9\\n0\\n\\n⎤⎞\\n0\\n⎟\\n0⎥\\n⎥⎟\\n⎥\\n0 ⎥⎟\\n⎟ = span{e1 , e3 , e5 }.\\n0 ⎦⎠\\n0\\n\\nHere dim(ker((F − 4 I )2 )) = 3 = a(4, F).\\n(b) For λ1 = 1 we have d1 (1) = dim(ker(F − I )) = 2.\\nFor λ2 = 4 we have d1 (4) = dim(ker(F − 4 I )) = 2 and d2 (4) = dim(ker((F − 4 I )2 )) − dim(ker(F − 4 I )) = 3 − 2 = 1.\\n(c) Computation of the Jordan chains:\\n• For λ1 = 1 we have m = 1.\\nAs principal vectors of level one we choose t1,1 = e2 and t2,1 = e4 .\\nThese form a basis of ker(F − I ): If α1 , α2 ∈ R with α1 e2 + α2 e4 = 0, then α1 = α2 = 0.\\nFor λ1 = 1 we are finished.\\n• For λ2 = 4 we have m = 2, and we choose a principal vector of level two, say t1,2 = e1 .\\nFor this vector we have: If α1 ∈ R with α1 e1 ∈ span{e1 − e3 , e5 }, then α1 = 0.\\nWe compute t1,1 := (F − 4 I )t1,2 = e1 − e3 .\\nSince d1 (4) = 2 > 1 = d2 (4), we have to add to t1,1 another principal vector of level one, and we choose t2,1 = e5 .\\nSince the vectors are linearly independent, α1 t1,1 + α2 t2,1 ∈ ker((F − 4 I )0 ) = {0} implies that α1 =\\nα2 = 0.\\nIn this way we get\\n⎤\\n⎤\\n⎡\\n00\\n110\\n⎢1 0⎥\\n⎢ 0 0 0⎥\\n⎥\\n⎥\\n⎢\\n⎢\\n⎥\\n⎥\\n⎢\\nTλ1 = ⎢ 0 0 ⎥ and Tλ2 = ⎢\\n⎢ −1 0 0 ⎥ .\\n⎣0 1⎦\\n⎣ 0 0 0⎦\\n00\\n001\\n⎡\\n\\n(3) The coordinate transformation matrix is T = [Tλ1 Tλ2 ], and the Jordan canonical form of F is\\n⎤\\n⎡\\n⎤\\n⎡\\n1\\n01 000\\n⎢0 0 0 1 0⎥\\n⎥\\n⎢ 1\\n⎥\\n⎢\\n⎥\\n⎢\\n−1\\n−1\\n⎥\\n⎥\\n⎢\\n4 1 ⎥ = T F T, where T = ⎢\\n⎢ 0 0 −1 0 0 ⎥ .\\n⎢\\n⎣\\n⎦\\n⎣\\n1 0 1 0 0⎦\\n4\\n00 001\\n4   \n",
       "238                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    16.3 Computation of the Jordan Canonical Form\\n\\n249\\n\\nExercises\\n(In the following exercises K is an arbitrary field.)\\n16.1.\\nProve Lemma 16.1 (1).\\n16.2.\\nProve Lemma 16.6 (1).\\n16.3.\\nLet V be a K -vector space, f ∈ L(V, V) and λ ∈ K .\\nProve or disprove: A subspace U ⊆ V is f -invariant, if it is ( f − λIdV )-invariant.\\n16.4.\\nLet V be a finite dimensional K -vector space, f ∈ L(V, V), v ∈ V and\\nλ ∈ K .\\nShow that K j ( f, v) = K j ( f − λIdV , v) for all j ∈ N. Conclude that the grade of v with respect to f is equal to the grade of v with respect to f − λIdV .\\n16.5.\\nProve Lemma 16.14.\\n16.6.\\nLet V be a finite dimensional Euclidean or unitary vector space and let f ∈\\nL(V, V) be selfadjoint and nilpotent.\\nShow that then f = 0.\\n16.7.\\nLet V \u0004= {0} be a finite dimensional K -vector space, let f ∈ L(V, V) be nilpotent of index m and suppose that P f decomposes into linear factors.\\nShow the following assertions:\\n(a) P f = t n with n = dim(V).\\n(b) M f = t m .\\n(c) There exists a vector v ∈ V of grade m with respect f .\\n(d) For every λ ∈ K we have M f −λIdV = (t + λ)m .\\n16.8.\\nLet V be a finite dimensional K -vector space and f ∈ L(V, V).\\nShow the following assertions:\\n(a) ker( f j ) ⊆ ker( f j+1 ) for all j ≥ 0 and there exists an m ≥ 0 with ker( f m ) = ker( f m+1 ).\\nFor this m we have ker( f m ) = ker( f m+ j ) for all j ≥ 1.\\n(b) im( f j ) ⊇ im( f j+1 ) for all j ≥ 0 and there exists an \u0002 ≥ 0 with im( f \u0002 ) = im( f \u0002+1 ).\\nFor this \u0002 we have im( f \u0002 ) = im( f \u0002+ j ) for all j ≥ 1.\\n(c) If m, \u0002 ≥ 0 are minimal with ker( f m ) = ker( f m+1 ) and im( f \u0002 ) = im( f \u0002+1 ), then m = \u0002.\\n(Theorem 16.5 now implies that V = ker( f m ) ⊕ im( f m ) is a decomposition of V into f -invariant subspaces.)\\n16.9.\\nLet V be a finite dimensional K -vector space and let f ∈ L(V, V) be a projection (cp.\\nExercise 13.10).\\nShow the following assertions:\\n(a) v ∈ im( f ) implies that f (v) = v.\\n(b) V = im( f ) ⊕ ker( f ).\\n(c) There exists a basis B of V with\\n\u0007\\n[ f ] B,B =\\n\\n\b\\n\\nIk\\n0n−k\\n\\n, where k = dim(im( f )) and n = dim(V).\\nIn particular, P f = (t −1)k t n−k and λ ∈ {0, 1} for every eigenvalue λ of f .   \n",
       "239                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   250\\n\\n16 Cyclic Subspaces, Duality and the Jordan Canonical Form\\n\\n(d) The map g = IdV − f is a projection with ker(g) = im( f ) and im(g) = ker( f ).\\n16.10.\\nLet V be a finite dimensional K -vector space and let U, W ⊆ V be two subspaces with V = U ⊕ W. Show that there exists a uniquely determined projection f ∈ L(V, V) with im( f ) = U and ker( f ) = W.\\n16.11.\\nDetermine the Jordan canonical form of the matrices\\n⎡\\n⎤\\n⎡\\n⎤\\n2 10 00\\n1 −1 0 0\\n⎢ −1 1 1 0 0 ⎥\\n⎢\\n⎥\\n⎢ 1 −1 0 0 ⎥\\n4,4\\n5,5\\n⎥\\n⎢\\n⎥\\nA=⎣\\n∈R , B=⎢\\n⎢ −1 0 3 0 0 ⎥ ∈ R\\n3 0 3 −3 ⎦\\n⎣ −1 −1 0 1 1 ⎦\\n4 −1 3 −3\\n−2 −1 1 −1 3 using the method presented in Sect.\\n16.3.\\nDetermine also the minimal polynomial.\\n16.12.\\nDetermine the Jordan canonical form and the minimal polynomial of the linear map f : C≤3 [t] → C≤3 [t], α0 + α1 t + α2 t 2 + α3 t 3 → α1 + α2 t + α3 t 3 .\\n16.13.\\nDetermine (up to the order of blocks) all matrices J in Jordan canonical form with PJ = (t + 1)3 (t − 1)3 and M J = (t + 1)2 (t − 1)2 .\\n16.14.\\nLet V \u0004= {0} be a finite dimensional K -vector space, f ∈ L(V, V), and suppose that P f decomposes into linear factors.\\nShow the following assertions:\\n(a) P f = M f holds if and only if g(λ, f ) = 1 for all eigenvalues λ of f .\\n(b) f is diagonalizable if and only if M f has only simple roots, i.e., roots with multiplicity one.\\n(c) A root of λ ∈ K of M f is simple if and only if ker( f − λIdV ) = ker(( f − λIdV )2 ).\\n16.15.\\nLet V be a K -vector space of dimension 2 or 3 and let f ∈ L(V, V) with P f decomposing into linear factors.\\nShow that the Jordan canonical form of f is uniquely determined by P f and M f .\\nWhy does this not hold any longer if dim(V) ≥ 4?\\n16.16.\\nLet A ∈ K n,n be a matrix for which the characteristic polynomial decomposes into linear factors.\\nShow that there exists a diagonalizable matrix D and a nilpotent matrix N with A = D + N and D N = N D.\\n16.17.\\nLet A ∈ K n,n be a matrix that has a Jordan canonical form.\\nWe define\\n⎤\\n⎡\\nλ\\n⎤\\n⎡\\n1\\n⎢\\n#\\n\"\\n. . .\\n1⎥\\n⎥\\n⎢\\nInR := δi,n+1− j = ⎣ . . . ⎦ , JnR (λ) := ⎢\\n⎥ ∈ K n,n .\\n⎣ ... ... ⎦\\n1\\nλ 1   \n",
       "240                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        16.3 Computation of the Jordan Canonical Form\\n\\nShow the following assertions:\\n(a)\\n(b)\\n(c)\\n(d)\\n\\nInR Jn (λ)InR = Jn (λ)T .\\nA and A T are similar.\\nJn (λ) = InR JnR (λ).\\nA can be written as a product of two symmetric matrices.\\n\\n16.18.\\nDetermine for the matrix\\n⎡\\n\\n⎤\\n511\\nA = ⎣0 5 1⎦ ∈ R3,3\\n004 two symmetric matrices S1 , S2 ∈ R3,3 with A = S1 S2 .\\n\\n251   \n",
       "241                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Chapter 17\\n\\nMatrix Functions and Systems of Differential Equations\\n\\nIn this chapter we give an introduction to the area of matrix functions.\\nWe first define general matrix functions and derive their most important properties.\\nUsing the examples of network analysis and chemical reactions, we illustrate how matrix functions arise naturally in applications.\\nThe network analysis example involves the exponential function of matrices, and we study the properties of this important function in detail.\\nThe analysis of chemical reaction kinetics leads to a system of ordinary differential equations, whose solution again is based on the matrix exponential function.\\n\\n17.1 Matrix Functions and the Matrix Exponential\\nFunction\\nIn the following we will study functions that yield for a given n × n matrix again an n × n matrix.\\nA possible definition of such a function is given by the entrywise application one\\n\u0002\\n\u0003 could define for\\n\u0002 \u0003 of scalar functions to the matrix.\\nFor instance,\\nA = ai j ∈ Cn,n the function sin(A) by sin(A) := sin(ai j ) .\\nHowever, such a definition\\n\u0004 \u0005is not compatible with the matrix multiplication, since in general already\\n2\\nA \u0003= ai2j .\\nThe following definition of the primary matrix function from [Hig08, Definition 1.1–1.2] will turn out to be consistent with the matrix multiplication.\\nSince this definition is based on the Jordan canonical form, we assume for simplicity that\\nA ∈ Cn,n .\\nOur considerations also apply to square matrices over R, as long as they have a Jordan canonical form.\\nDefinition 17.1 Let A ∈ Cn,n have the Jordan canonical form\\nJ = diag(Jd1 (λ1 ), . . . , Jdm (λm )) = S −1 AS,\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_17\\n\\n253   \n",
       "242                                                                                                                                                                                                                                                                                                                                                                                                  254\\n\\n17 Matrix Functions and Systems of Differential Equations and let \u0002 ⊂ C be such that {λ1 , . . . , λm } ⊆ \u0002.\\nA function f : \u0002 → C is said to be defined on the spectrum of A, if the values f ( j) (λi ) for i = 1, . . . , m and j = 0, 1 . . . , di − 1\\n\\n(17.1) exist.\\nHere f ( j) (λi ), j = 1, . . . , di − 1, is the jth derivative of the function f (λ) with respect to λ evaluated at λi .\\nIf λi ∈ R, then this is the real derivative, and for\\nλi ∈ C \\ R it is the complex derivative.\\nMoreover, we assume that equal eigenvalues that occur in different Jordan blocks are mapped to the same values in (17.1).\\nIf f is defined on the spectrum of A then the primary matrix function f (A) is defined by f (A) := S f (J )S −1 where f (J ) := diag( f (Jd1 (λ1 )), . . . , f (Jdm (λm ))) (17.2) and\\n⎡\\n⎢\\n⎢\\n⎢\\n⎢\\n\u0007\\n\u0006 f Jdi (λi ) := ⎢\\n⎢\\n⎢\\n⎢\\n⎣\\n\\n⎤\\n(di −1)\\n(λi )\\n. . . f (di −1)!\\n⎥\\n..\\n.\\n⎥\\n⎥ f (λi ) f \u0007 (λi ) . .\\n.\\n⎥\\n..\\n..\\nf \u0007\u0007 (λi ) ⎥\\n⎥ for i = 1, . . . , m.\\n(17.3)\\n.\\n.\\n2!\\n⎥\\n⎥\\n..\\n. f \u0007 (λi ) ⎦ f (λi ) f (λi ) f \u0007 (λi ) f \u0007\u0007 (λi )\\n2!\\n\\nNote that for the definition of f (A) in (17.2)–(17.3) only the existence of the values in (17.1) is required.\\n√\\nExample 17.2 Let √\\nA = I2 ∈ C2,2 and let f (z)\\n√ = z (the square root function).\\nIf we set f (1) = 1 = +1, then f (A) = A = I2 by Definition\\n√ 17.1.\\nIf we\\n1 = −1, then choose the other branch of the square root function, i.e., f\\n(1)\\n=\\n√ f (A) = A = −I2 .\\nThe matrices I2 and −I2 are primary square roots of A = I2 .\\nTaking different branches of a function for different Jordan blocks corresponding to the same eigenvalue is incompatible with Definition 17.1.\\nFor instance, the matrices\\n\u000e\\nX1 =\\n\\n1 0\\n0 −1\\n\\n\u000e\\n\\n\u000f and X 2 =\\n\\n−1 0\\n01\\n\\n\u000f are incompatible with Definition 17.1, despite the fact that X 12 = I2 and X 22 = I2 .\\nAll solutions X ∈ Cn,n of the matrix equation X 2 = A are called square roots of the matrix A ∈ Cn,n .\\nBut as Example 17.2 shows, some of these may not be primary square roots according to Definition 17.1.\\nIn the following, by f (A) we will always mean a primary matrix function according to Definition 17.1, and will usually omit the term “primary”.\\nIn (16.8) we have shown that for each polynomial p ∈ C[t] of degree k ≥ 0 we have   \n",
       "243                                                                                                                                                                                                                                                                                                                                                                                                                          17.1 Matrix Functions and the Matrix Exponential Function p(Jdi (λi )) =\\n\\n255 k\\n\u0010 p ( j) (λi )\\n(Jdi (0)) j .\\nj!\\nj=0\\n\\n(17.4)\\n\\nA simple comparison shows that this formula agrees with (17.3) for f = p.\\nThis means that the computation of p(Jdi (λi )) with (17.4) leads to the same result as the definition of p(Jdi (λi )) by (17.3).\\nMore generally, the following result holds.\\nLemma 17.3 Let A ∈ Cn,n and p = αk t k + . . . + α1 t + α0 ∈ C[t].\\nThen (17.2)–\\n(17.3) with f = p yields a matrix function f (A) that satisfies f (A) = αk Ak + . . . +\\nα1 A + α0 In .\\nProof Exercise.\\nIf we consider, in particular, the polynomial f = t 2 in (17.2)–(17.3), then the resulting f (A) is equal to the product A ∗ A. This shows that the definition of the primary matrix function f (A) is consistent with the matrix multiplication.\\nThe following theorem, which is of great practical and theoretical importance, shows that the matrix f (A) can always be written as a polynomial in A.\\nTheorem 17.4 Let A ∈ Cn,n have the minimal polynomial M A , and let f (A) be as in Definition 17.1.\\nThen there exists a uniquely determined polynomial p ∈ C[t] of degree at most deg(M A ) − 1 with f (A) = p(A).\\nIn particular, A f (A) = f (A)A, f (A T ) = f (A)T as well as f (V AV −1 ) = V f (A)V −1 for all V ∈ G L n (C).\\nProof We will not present the proof here since it requires advanced results from interpolation theory.\\nDetails can be found in [Hig08, Chap.\\n1].\\nUsing Theorem 17.4 we can show that the primary matrix function f (A) in\\nDefinition 17.1 is independent of the choice of the Jordan canonical form of A. We already know from Theorem 16.12, that the Jordan canonical form of A is unique up to the order of the Jordan blocks.\\nIf\\nJ = diag(Jd1 (λ1 ), . . . , Jdm (λm )) = S −1 AS,\\nλ1 ), . . . , Jd\u0011 (\u0011\\nλm )) = \u0011\\nS −1 A\u0011\\nS\\nJ\u0011 = diag(Jd\u0011 (\u0011\\n1 m are two Jordan canonical forms of A, then J\u0011 = P T J P for a permutation matrix\\nP ∈ Rn,n , where the matrices J and J\u0011 are the same up to the order of diagonal blocks.\\nHence f (J ) = diag( f (Jd1 (λ1 )), . . . , f (Jdm (λm )))\\n\u0006\\n\u0007\\n= P P T diag( f (Jd1 (λ1 )), . . . , f (Jdm (λm )))P P T\\n\u0006\\n\u0007\\n= P diag( f (Jd\u0011 (\u0011\\nλ1 )), . . . , f (Jd\u0011 (\u0011\\nλm ))) P T\\n1\\n\\n= P f ( J\u0011)P T .\\nm   \n",
       "244                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            256\\n\\n17 Matrix Functions and Systems of Differential Equations\\n\\nTheorem 17.4 applied to the matrix J yields the existence of a polynomial p with f (J ) = p(J ).\\nThus, we get f (A) = S f (J )S −1 = Sp(J )S −1 = p(A) = p(\u0011\\nS J\u0011\u0011\\nS −1 ) = \u0011\\nS P T p(J )P \u0011\\nS −1 = \u0011\\nS P T f (J )P \u0011\\nS −1\\n=\u0011\\nS f ( J\u0011)\u0011\\nS −1 .\\n\\nLet us now consider the exponential function f (z) = e z that is infinitely often complex differentiable throughout C. In particular, e z is defined (in the sense of\\nDefinition 17.1) on the spectrum of every given matrix\\nA = Sdiag(Jd1 (λ1 ), . . . , Jdm (λm ))S −1 ∈ Cn,n .\\nIf t ∈ C is arbitrary (but fixed), then the derivatives of the function et z with respect to the variable z are given by d j tz e = t j et z , dz j j = 0, 1, 2, . . . .\\n\\nWe will use the notation exp(M) instead of e M for the exponential function of a matrix\\nM. For every Jordan block Jd (λ) of A we then have, by (17.3) with f (z) = e z ,\\n⎡\\n2\\n1 t t2! . . .\\n⎢\\n⎢ 1 t ...\\n⎢\\n⎢\\n.. ..\\nexp(t Jd (λ)) = etλ ⎢\\n. .\\n⎢\\n⎢\\n..\\n⎣\\n.\\nt d−1 ⎤\\n(d−1)!\\n\\n.. ⎥\\n. ⎥ d−1\\n⎥\\n\u0010\\n1\\n⎥ tλ\\n(t Jd (0))k , t2 ⎥ = e\\n⎥ k!\\n2!\\nk=0\\n⎥ t ⎦\\n1\\n\\n(17.5) and the matrix exponential function exp(t A) is given by exp(t A) = Sdiag(exp(t Jd1 (λ1 )), . . . , exp(t Jdm (λm )))S −1 .\\n\\n(17.6)\\n\\nThe parameter t will be used in the next section in the context of linear differential equations.\\nIn Analysis it is shown that for every z ∈ C the function e z can be represented by the absolutely convergent series ez =\\n\\n∞\\n\u0010 zj\\n.\\nj!\\nj=0\\n\\nUsing this series and the equation (Jd (0))\u0003 = 0 for all \u0003 ≥ d, we obtain   \n",
       "245                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    17.1 Matrix Functions and the Matrix Exponential Function\\n\\n257\\n\\n⎛\\n⎞ \u0016\\n\u0017\\n∞\\n∞ d−1 j\\n\u0010\\n\u0010\\n\u0010\\n1\\n(tλ)\\n1\\n\u0003\\n\u0003 tλ\\n⎝\\n⎠ exp(t Jd (λ)) = e\\n·\\n(t Jd (0)) =\\n(t Jd (0))\\n\u0003!\\nj!\\n\u0003!\\nj=0\\n\u0003=0\\n\u0003=0\\n\u0016 j\\n\u0017\\n∞\\n\u0010\\n\u0010 (tλ) j−\u0003 1\\n\u0003\\n=\\n· (t Jd (0))\\n( j − \u0003)! \u0003!\\nj=0\\n\u0003=0\\n\u0017\\n\u0016 j \u0018 \u0019\\n∞\\n\u0010 tj \u0010 j j−\u0003\\n=\\nλi (Jd (0))\u0003\\n\u0003 j!\\nj=0\\n\u0003=0\\n∞\\n\u0010 tj\\n=\\n(λId + Jd (0)) j j!\\nj=0\\n\\n=\\n\\n∞\\n\u0010\\n1\\n(t Jd (λ)) j .\\nj!\\nj=0\\n\\n(17.7)\\n\\nIn this derivation we have used the absolute convergence of the exponential series and the finiteness of the series with the matrix Jd (0).\\nThis allows the application of the Cauchy product formula1 for absolutely convergent series, which is also proven in Analysis.\\nLemma 17.5 If A ∈ Cn,n , t ∈ C and exp(t A) is the matrix exponential function in\\n(17.5)–(17.6), then\\n∞\\n\u0010\\n1\\n(t A) j .\\nexp(t A) = j!\\nj=0\\nProof In (17.7) we have shown this already for Jordan blocks.\\nThe assertion then follows from\\n⎛\\n⎞\\n∞\\n∞\\n\u0010\\n\u0010\\n1\\n1\\n(t S J S −1 ) j = S ⎝\\n(t J ) j ⎠ S −1 j!\\nj!\\nj=0 j=0 and the representation (17.6) of the matrix exponential function.\\nWe immediately see from Lemma 17.5 that for a matrix A ∈ Rn,n and every real t the matrix exponential function exp(t A) is a real matrix.\\nThe following result presents further important properties of the matrix exponential function.\\nLemma 17.6 If the two matrices A, B ∈ Cn,n commute, then exp(A + B) = exp(A) exp(B).\\nFor every matrix A ∈ Cn,n we have exp(A) ∈ G L n (C) with\\n(exp(A))−1 = exp(−A).\\n\\n1 Augustin\\n\\nLouis Cauchy (1789–1857).   \n",
       "246                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               258\\n\\n17 Matrix Functions and Systems of Differential Equations\\n\\nProof If A and B commute, then the Cauchy product formula yields\\n⎛\\n\\n⎞\u0016\\n\u0017\\n\u0016 j\\n\u0017\\n∞\\n∞\\n∞\\n\u0010\\n\u0010\\n\u0010\\n\u0010 1\\n1\\n1\\n1 j\\n\u0003\\n\u0003 j−\u0003 exp(A) exp(B) = ⎝\\nA ⎠\\nB =\\nA\\nB j!\\n\u0003!\\n\u0003!\\n( j − \u0003)!\\nj=0 j=0 \u0003=0\\n\u0003=0\\n\u0016\\n\u0017 j \u0018 \u0019\\n∞\\n∞\\n\u0010\\n\u0010\\n1 \u0010 j\\n1\\n\u0003 j−\u0003\\nA B\\n=\\n=\\n(A + B) j\\n\u0003 j!\\nj!\\nj=0 j=0\\n\u0003=0\\n= exp(A + B).\\nHere we have used the binomial formula for commuting matrices (cp.\\nExercise 4.10).\\nSince A and −A commute, we have exp(A) exp(−A) = exp(A − A) = exp(0) =\\n\\n∞\\n\u0010\\n1 j\\n0 = In , j!\\nj=0 and hence exp(A) ∈ G L n (C) with (exp(A))−1 = exp(−A).\\nFor non-commuting matrices the statements in Lemma 17.6 in general do not hold\\n(cp.\\nExercise 17.9).\\n\\nMATLAB-Minute.\\nCompute the matrix exponential function exp(A) for the matrix\\n⎤\\n1 −1 3 4 5\\n⎢ −1 −2 4 3 5 ⎥\\n⎥\\n⎢\\n5,5\\n⎥\\nA=⎢\\n⎢ 2 0 −3 1 5 ⎥ ∈ R\\n⎣ 3 0 0 −2 −3 ⎦\\n4 0 0 −3 −5\\n⎡ using the command E1=expm(A).\\n(Look at help expm.)\\nAlso compute the diagonalization of A using the command [S,D]=eig(A), and form the matrix exponential function exp(A) as E2=S∗expm(D)/S.\\nCompare the matrices E1 and E2 and compute the relative error norm(E1E2)/norm(E2).\\n(Look at help norm.)\\n\\nExample 17.7 Let A = [ai j ] ∈ Cn,n be a symmetric matrix with aii = 0 and ai j ∈\\n{0, 1} for all i, j = 1, . . . , n.\\nWe identify the matrix A with a graph G A = (V A , E A ) consisting of a set of n vertices V A = {1, . . . , n} and a set of edges E A ⊆ V A × V A .\\nFor i = 1, . . . , n the row i of A is identified with the vertex i ∈ E A , and every entry ai j = 1 is identified with an edge (i, j) ∈ E A .\\nDue to the symmetry of A, we have ai j = 1 if and only if a ji = 1.\\nWe therefore consider in the following the elements   \n",
       "247                                                                                                                                                                                                                                                                                                                                                                                                                                                                   17.1 Matrix Functions and the Matrix Exponential Function\\n\\n259 of E A as unordered pairs, i.e., (i, j) = ( j, i).\\nThe following example illustrates this identification:\\n⎡\\n⎤\\n01110\\n⎢1 0 0 1 1⎥\\n⎢\\n⎥\\n⎥\\nA=⎢\\n⎢1 0 0 0 1⎥\\n⎣1 1 0 0 0⎦\\n01100 is identified with G A = (V A , E A ), where\\nE A = {1, 2, 3, 4, 5}, V A = {(1, 2), (1, 3), (1, 4), (2, 4), (2, 5), (3, 5)}, and the graph G A can be displayed as follows:\\n\\nA path of length m from the vertex k1 to the vertex km+1 is an ordered list of vertices k1 , k2 , . . . , km+1 , where (ki , ki+1 ) ∈ V A for i = 1, . . . , m.\\nIf k1 = km+1 , then this is a closed path of length m.\\nIn the above example, paths from 1 to 4 are given by 1, 2, 4 and 1, 2, 5, 3, 1, 2, 4; these have the lengths 2 and 6, respectively.\\nIn the mathematical field of Graph Theory one usually assumes that the vertices in a path are pairwise distinct.\\nOur deviation from this convention is motivated by the following interpretation of a matrix A and its powers:\\nAn entry ai j = 1 in the matrix A means that there exists a path of length 1 from vertex i to vertex j, i.e., the vertices i and j are adjacent.\\nIf ai j = 0, then no such path exists.\\nThe matrix A is therefore called the adjacency matrix of the graph G A .\\nIf we square the adjacency matrix, then the entry in the (i, j) position is given by\\n(A2 )i j = n\\n\u0010 ai\u0003 a\u0003j .\\n\\n\u0003=1\\n\\nIn the sum on the right hand side, we obtain for a given \u0003 a 1 if and only if (i, \u0003) ∈ E A and (\u0003, j) ∈ E A .\\nThe sum on the right had side therefore is equal to the number of vertices that are adjacent to both i and j.\\nHence the (i, j) entry of A2 is equal to the number of pairwise distinct paths from i to j (i \u0003= j), or the pairwise distinct closed paths from i to i of length 2 in G A .\\nMore generally, one can show the following (cp.\\nExercise 17.10):\\nLet A = [ai j ] ∈ Cn,n be a symmetric adjacency matrix, i.e., A = A T with aii = 0 and ai j ∈ {0, 1} for all i, j = 1, . . . , n, and let G A be the graph identified with A.\\nThen for each m ∈ N the (i, j) entry of Am is equal to the number of pairwise distinct paths from i to j (i \u0003= j) or the pairwise distinct closed paths from i to i of length m in G A .   \n",
       "248                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              260\\n\\n17 Matrix Functions and Systems of Differential Equations\\n\\nFor the above matrix A we obtain\\n⎡\\n⎡\\n⎤\\n31012\\n2\\n⎢1 3 2 1 0⎥\\n⎢6\\n⎢\\n⎢\\n⎥\\n3\\n⎢\\n⎥\\nA2 = ⎢\\n⎢ 0 2 2 1 0 ⎥ and A = ⎢ 5\\n⎣1 1 1 2 1⎦\\n⎣4\\n20012\\n1\\n\\n6\\n2\\n1\\n4\\n5\\n\\n5\\n1\\n0\\n2\\n4\\n\\n4\\n4\\n2\\n2\\n2\\n\\n⎤\\n1\\n5⎥\\n⎥\\n4⎥\\n⎥.\\n2⎦\\n0\\n\\nThe 3 pairwise distinct closed paths of length 2 from 1 to 1 are\\n1, 2, 1, 1, 3, 1, 1, 4, 1 and the 4 pairwise distinct paths of length 3 from 1 to 4 are\\n1, 2, 1, 4, 1, 3, 1, 4, 1, 4, 1, 4, 1, 4, 2, 4.\\nNumerous real world applications involve networks that can be modeled mathematically using graphs.\\nExamples include social, biological, telecommunication or airline networks.\\nThe properties of such networks are studied in the interdisciplinary area of Network Science.\\nAn important task is to identify participants in the network that are central in the sense that their functionality has a significant impact on the entire network.\\nIf the network has been modeled by a graph, then we can study the centrality of the vertices.\\nFor example, a vertex can be considered central if it is connected to a large part of the graph via many short closed paths.\\nLonger connections are usually less important, and thus paths should be scaled down according to their length.\\nIf we use the scaling factor 1/m! for a path of length m, then for the vertex i in the graph G A with the adjacency matrix A we obtain a centrality measure of the form\\n\u0018\\n\u0019\\n1\\n1 2\\n1 3\\nA + A + A + ...\\n.\\n1!\\n2!\\n3!\\nii\\nThe relative ordering of the vertices according to this formula is not changed when we add the constant 1.\\nWe then obtain the centrality of the vertex i as\\n\u0018\\n\\n1\\n1\\nI + A + A2 + A3 + . . .\\n2\\n3!\\n\\n\u0019\\n= (exp(A))ii .\\nii\\n\\nAnother important quantity is the so-called communicability between the vertices i and j for i \u0003= j, which is given by the weighted sum of the pairwise distinct paths from i to j, i.e., by\\n\u0018\\nI + A+\\n\\n1 2\\n1\\nA + A3 + . . .\\n2\\n3!\\n\\n\u0019\\n= (exp(A))i j .\\nij\\n\\nFor the above matrix A the MATLAB function expm yields   \n",
       "249                                                                                                                                                                                                                                                                                                                                   17.1 Matrix Functions and the Matrix Exponential Function\\n\\n⎡\\n\\n3.7630\\n⎢ 3.1953\\n⎢ exp(A) = ⎢\\n⎢ 2.2500\\n⎣ 2.7927\\n1.8176\\n\\n3.1953\\n3.7630\\n1.8176\\n2.7927\\n2.2500\\n\\n2.2500\\n1.8176\\n2.4881\\n1.2749\\n1.9204\\n\\n2.7927\\n2.7927\\n1.2749\\n2.8907\\n1.2749\\n\\n261\\n\\n⎤\\n1.8176\\n2.2500 ⎥\\n⎥\\n1.9204 ⎥\\n⎥.\\n1.2749 ⎦\\n2.4881\\n\\nThe vertices 1 and 2 have the largest centrality, followed by 4, 3 and 5.\\nIf we would define the centrality of a vertex as the number of adjacent vertices, then in this example we could not distinguish between the vertices 3, 4 and 5.\\nThe largest communicability in this example exists between the vertices 1 and 2.\\nFurther information concerning the analysis of networks using adjacency matrices and matrix functions can be found in the article [EstH10].\\n\\n17.2 Systems of Linear Ordinary Differential Equations\\nA differential equation describes a relationship between a desired function and its derivatives.\\nSuch equations are used in all areas of science and engineering for modeling physical phenomena.\\nOrdinary differential equations involve a function of one variable and its derivatives, while partial differential equations involve functions of several variables and their partial derivatives.\\nIn this section we focus on ordinary differential equations of first order, i.e., those in which only the function and its first derivative occur.\\nA simple example for the modeling with ordinary differential equations of first order is the increase or decrease of a biological population, such as bacteria in a petri dish.\\nLet y = y(t) be the size of the population at time t.\\nIf there is enough food and if the external conditions (e.g. temperature or pressure) are constant, then the population grows with a (real) rate k > 0, that is proportional to the current number of individuals.\\nThis can be described by the equation ẏ := d y = ky.\\ndt\\n\\n(17.8)\\n\\nClearly, one can also take k < 0, and then the population shrinks.\\nWe are then looking for a function y : D ⊂ R → R that satisfies (17.8).\\nThe general solution of (17.8) is given by the exponential function y = cetk , where c ∈ R is an arbitrary constant.\\nFor a unique solution of (17.8) we need to know the size of the population at a given initial time t0 .\\nIn this way we obtain the initial value problem ẏ = ky, y(t0 ) = y0 ,   \n",
       "250                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              262\\n\\n17 Matrix Functions and Systems of Differential Equations which, as we will show below, is solved uniquely by the function y = e(t−t0 )k y0 .\\nExample 17.8 In a chemical reaction certain initial substances (called educts or reactants) are transformed into other substances (called products).\\nReactions can be distinguished concerning their order.\\nHere we only discuss reactions of first order, where the reaction rate is determined by only one educt.\\nIn reactions of second and higher order one typically obtains nonlinear differential equations, which are beyond our focus in this chapter.\\nIf, for example, the educt A1 is transformed into the product A2 with the rate\\n−k1 < 0, then we write this reaction symbolically as\\nA1 k1\\n\\n/ A2 , and we model it mathematically by the ordinary differential equation ẏ1 = −k1 y1 .\\nHere the value y1 (t) is the concentration of the substance A1 at time t.\\nFor the concentration of the product A2 , which grows with the rate k1 > 0, we have the corresponding equation ẏ2 = k1 y1 .\\nIt may happen that a reaction of first order develops in both directions.\\nIf A1 transforms into A2 with the rate −k1 , and A2 transforms into A1 with the rate −k2 , i.e., k1\\n/\\nA1 o\\nA2 , k2 then we can model this reaction mathematically by the system of linear ordinary differential equations ẏ1 = −k1 y1 + k2 y2 , ẏ2 = k1 y1 − k2 y2 .\\nCombining the functions y1 and y2 in a vector valued function y = [y1 , y2 ]T , we can write this system as\\n\u000f\\n−k1 k2\\n.\\nẏ = Ay, where A = k1 −k2\\n\u000e   \n",
       "251                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            17.2 Systems of Linear Ordinary Differential Equations\\n\\n263\\n\\nThe derivative of the function y(t) is always considered entrywise,\\n\u000e \u000f ẏ ẏ = 1 .\\nẏ2\\nReactions can also have several steps.\\nFor example, a reaction of the form\\nA1 k1\\n\\n/ A2 o k2\\n\\n/\\n\\nA3 k4\\n\\n/ A4 k3 leads to the differential equations ẏ1 = −k1 y1 , ẏ2 = k1 y1 − k2 y2 + k3 y3 , ẏ3 = k2 y2 − (k3 + k4 )y3 , ẏ4 = k4 y3 , and thus to the system\\n⎤\\n⎡\\n0\\n0\\n−k1 0\\n⎢ k1 −k2 k3\\n0⎥\\n⎥ ẏ = Ay, where A = ⎢\\n⎣ 0 k2 −(k3 + k4 ) 0⎦ .\\n0\\n0\\n0 k4\\nThe sum of the entries in each column of A is equal to zero, since for every decrease in a substance with a certain rate other substances increase with the same rate.\\nIn summary, a chemical reaction of first order leads to a system of linear ordinary differential equations of first order that can be written as ẏ = Ay with a (real) square matrix A.\\nWe now derive the general theory for systems of linear (real or complex) ordinary differential equations of first order of the form ẏ = Ay + g, t ∈ [0, a].\\n\\n(17.9)\\n\\nHere A ∈ K n,n is a given matrix, a is a given positive real number, g : [0, a] → K n,1 is a given function, y : [0, a] → K n,1 is the desired solution, and we assume that\\nK = R or K = C. If g(t) = 0 ∈ K n,1 for all t ∈ [0, a], then the system (17.9) is called homogeneous, otherwise it is called non-homogeneous.\\nFor a given system of the form (17.9), the system ẏ = Ay, t ∈ [0, a],\\n(17.10) is called the associated homogeneous system.   \n",
       "252                                                                                                                                                                                                                                                                                                                                                                                                                                                                               264\\n\\n17 Matrix Functions and Systems of Differential Equations\\n\\nLemma 17.9 The solutions of the homogeneous system (17.10) form a subspace of the (infinite dimensional) K -vector space of the continuously differentiable functions from the interval [0, a] to K n,1 .\\nProof We will show the required properties according to Lemma 9.5.\\nThe function w = 0 is continuously differentiable on [0, a] and solves the homogeneous system\\n(17.10).\\nThus, the solution set of this system is not empty.\\nIf w1 , w2 : [0, a] → K n,1 are continuously differentiable solutions and if α1 , α2 ∈ K , then w = α1 w1 + α2 w2 is continuously differentiable on [0, a], and ẇ = α1 ẇ1 + α2 ẇ2 = α1 Aw1 + α2 Aw2 = Aw, i.e., the function w is a solution of the homogeneous system.\\nThe following characterization of the solutions of the non-homogeneous system\\n(17.9) is analogous to the characterization of the solution set of a non-homogeneous linear system of equations in Lemma 6.2 (also cp.\\n(8) in Lemma 10.7 ).\\nLemma 17.10 If w1 : [0, a] → K n,1 is a solution of the non-homogeneous system\\n(17.9), then every other solution y can be written as y = w1 + w2 , where w2 is a solution of the associated homogeneous system (17.10).\\nProof If w1 and y are solutions of (17.9), then ẏ − ẇ1 = (Ay + g) − (Aw1 + g) =\\nA(y − w1 ).\\nThe difference w2 := y − w1 thus is a solution of the associated homogeneous system and y = w1 + w2 .\\nIn order to describe the solutions of systems of ordinary differential equations, we consider for a given matrix A ∈ K n,n the matrix exponential function exp(t A) from\\nLemma 17.5 or (17.5)–(17.6), where we now consider t ∈ [0, a] as real variable.\\nThe power series of the matrix exponential function in Lemma 17.5 converges, and it can be differentiated termwise with respect to the variable t, where again the derivative of a matrix with respect to the variable t is considered entrywise.\\nThis yields d d exp(t A) = dt dt\\n\\n\u0018\\n\\n1\\n1\\nI + (t A) + (t A)2 + (t A)3 + . . .\\n2\\n6\\n1\\n= A + t A2 + t 2 A3 + . . .\\n2\\n= A exp(t A).\\n\\n\u0019\\n\\nThe same result is obtained by the entrywise differentiation of the matrix exp(t A) in\\n(17.5)–(17.6) with respect to t.\\nWith   \n",
       "253                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    17.2 Systems of Linear Ordinary Differential Equations\\n\\n⎡\\n2\\n1 t t2! . . .\\n⎢\\n⎢ 1 t ...\\n⎢\\n⎢\\n.. ..\\nM(t) := ⎢\\n. .\\n⎢\\n⎢\\n..\\n⎣\\n.\\n\\n265 t d−1 ⎤\\n(d−1)!\\n\\n.. ⎥\\n. ⎥\\n⎥\\n⎥ t2 ⎥\\n2! ⎥\\n⎥ t ⎦\\n1 we obtain\\n\u0007 d d \u0006 tλ exp(t Jd (λ)) = e M(t) dt dt\\n= λetλ M(t) + etλ Ṁ(t)\\n= λetλ M(t) + etλ Jd (0)M(t)\\n= (λId + Jd (0)) etλ M(t)\\n= Jd (λ) exp(t Jd (λ)), which also gives d dt exp(t A) = A exp(t A).\\n\\nTheorem 17.11\\n(1) The unique solution of the homogeneous differential equation system (17.10) for a given initial condition y(0) = y0 ∈ K n,1 is given by the function y = exp(t A)y0 .\\n(2) The set of all solutions of the homogeneous differential equation system (17.10) forms an n-dimensional K -vector space with the basis {exp(t A)e1 , . . . , exp(t A)en }.\\nProof\\n(1) If y = exp(t A)y0 , then\\n\u0019\\n\u0018 d d\\n(exp(t A)y0 ) = exp(t A) y0 = (A exp(t A))y0 dt dt\\n= A(exp(t A)y0 ) = Ay, ẏ = and y(0) = exp(0)y0 = In y0 = y0 .\\nHence y is a solution of (17.10) that satisfies the initial condition.\\nIf w is another such solution and u := exp(−t A)w, then d\\n(exp(−t A)w) = −A exp(−t A)w + exp(−t A)ẇ dt\\n= exp(−t A) (ẇ − Aw) = 0 ∈ K n,1 , u̇ = which shows that the function u has constant entries.\\nIn particular, we then have u = u(0) = w(0) = y0 = y(0) and w = exp(t A)y0 , where we have used that exp(−t A) = (exp(t A))−1 (cp.\\nLemma 17.6).   \n",
       "254                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          266\\n\\n17 Matrix Functions and Systems of Differential Equations\\n\\n(2) Each of the functions exp(t A)e j , . . . , exp(t A)en : [0, a] → K n,1 , j = 1, . . . , n, solves the homogeneous system ẏ = Ay.\\nSince the matrix exp(t A) ∈ K n,n is invertible for every t ∈ [0, a] (cp.\\nLemma 17.6), these functions are linearly independent.\\nIf \u0011 y is an arbitrary solution of ẏ = Ay, then \u0011 y(0) = y0 for some y0 ∈ K n,1 .\\nBy\\n(1) then \u0011 y is the unique solution of the initial value problem with y(0) = y0 , so y is a linear combination of the functions that \u0011 y = exp(t A)y0 .\\nAs a consequence, \u0011 exp(t A)e1 , . . . , exp(t A)en .\\nTo describe the solution of the non-homogeneous system (17.9), we need the integral of functions of the form\\n⎤ w1\\n⎢ ⎥ w = ⎣ ... ⎦ : [0, a] → K n,1 .\\n⎡ wn\\n\\nFor every fixed t ∈ [0, a] we define\\n\u001a\\n0\\n\\n⎡\u001b t t\\n\\n⎢ w(s)ds := ⎣\\n\\n0\\n\\n\u001bt\\n0\\n\\n⎤ w1 (s)ds\\n⎥\\n..\\nn,1\\n⎦ ∈ K ,\\n.\\nwn (s)ds i.e., we apply the integral entrywise to the function w.\\nBy this definition we have d dt\\n\\n\u0018\u001a t\\n\\n\u0019 w(s)ds\\n\\n= w(t)\\n\\n0 for all t ∈ [0, a].\\nWe can now determine an explicit solution formula for systems of linear differential equations based on the so-called Duhamel integral.2\\nTheorem 17.12 The unique solution of the non-homogeneous differential equation system (17.9) with the initial condition y(0) = y0 ∈ K n,1 is given by\\n\u001a y = exp(t A)y0 + exp(t A) t exp(−s A)g(s)ds.\\n\\n(17.11)\\n\\n0\\n\\nProof The derivative of the function y defined in (17.11) is\\n\u0018\\n\u0019\\n\u001a t exp(t A) exp(−s A)g(s)ds\\n0\\n\u001a t exp(−s A)g(s)ds + exp(t A) exp(−t A)g\\n= A exp(t A)y0 + A exp(t A) d d ẏ =\\n(exp(t A)y0 ) + dt dt\\n\\n0\\n2 Jean-Marie\\n\\nConstant Duhamel (1797–1872).   \n",
       "255                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     17.2 Systems of Linear Ordinary Differential Equations\\n\\n\u001a t\\n\\n= A exp(t A)y0 + A exp(t A)\\n\\n267 exp(−s A)g(s)ds + g\\n\\n0\\n\\n= Ay + g.\\nFurthermore, we have\\n\u001a\\n\\n0 y(0) = exp(0)y0 + exp(0) exp(−s A)g(s)ds = y0 ,\\n\\n0 so that y also satisfies the initial condition.\\nLet now \u0011 y be another solution of (17.9) that satisfies the initial condition.\\nBy\\nLemma 17.10 we then have \u0011 y = y + w, where w solves the homogeneous system\\n(17.10).\\nTherefore, w = exp(t A)c for some c ∈ K n,1 (cp.\\n(2) in Theorem 17.11).\\ny = y.\\nFor t = 0 we obtain y0 = y0 + c, where c = 0 and hence \u0011\\nIn the above theorems we have shown that for the explicit solution of systems of linear ordinary differential equations of first order, we have to compute the matrix exponential function.\\nWhile we have introduced this function using the Jordan canonical form of the given matrix, numerical computations based on the Jordan canonical form are not advisable (cp.\\nExample 16.20).\\nBecause of its significant practical relevance, numerous different algorithms for computing the matrix exponential function have been proposed.\\nBut, as shown in the article [MolV03], no existing algorithm is completely satisfactory.\\nExample 17.13 The example from circuit simulation presented in Sect.\\n1.5 lead to the system of ordinary differential equations\\nR\\n1\\n1 d\\nI = − I − VC + VS , dt\\nL\\nL\\nL d\\n1\\nVC = − I.\\ndt\\nC\\nUsing (17.11) and the initial values I (0) = I 0 and VC (0) = VC0 , we obtain the solution\\n\u000e \u000f\\n\u0018 \u000e\\n\u000f\u0019 \u000e 0 \u000f\\nI\\n−R/L −1/L\\nI\\n= exp t\\nVC0\\nVC\\n−1/C 0\\n\u0018\\n\u000e\\n\u000f\\n\u000f\u0019 \u000e\\n\u001a t\\n−R/L −1/L\\nVS (s)\\n+ exp (t − s) ds.\\n0\\n−1/C 0\\n0\\nExample 17.14 Let us also consider an example from Mechanics.\\nA weight with mass m > 0 is attached to a spring with the spring constant μ > 0.\\nLet x0 > 0 be the distance of the weight from its equilibrium position, as illustrated in the following figure:   \n",
       "256                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       268\\n\\n17 Matrix Functions and Systems of Differential Equations\\n\\nWe want to determine the position x(t) of the weight at time t ≥ 0, where x(0) = x0 .\\nThe extension of the spring is described by Hooke’s law.3 The corresponding ordinary differential equation of second order is ẍ =\\n\\nμ d2 x = − x,\\n2 dt m with initial conditions x(0) = x0 and ẋ(0) = v0 , where v0 > 0 is the initial velocity of the weight.\\nWe can write this differential equation of second order for x as a system of first order by introducing the velocity v as new variable.\\nThe velocity is given by the derivative of the position with respect to time, i.e., v = ẋ, and thus for the acceleration we have v̇ = ẍ, which yields the system\\n\u000e ẏ = Ay, where A =\\n\\n0 1\\n− mμ 0\\n\\n\u000f and y =\\n\\n\u000e \u000f x\\n.\\nv\\n\\nThe initial condition then is y(0) = y0 = [x0 , v0 ]T .\\nBy Theorem 17.11, the unique solution of this homogeneous initial value problem is given by the function y = exp(t A)y0 .\\nWe consider A as an element of C2,2 .\\nThe eigenvalues of A\n",
       "are the two complex (non-real) numbers λ1 = iρ and λ2 = −iρ =\\nλ1 , where ρ := mμ .\\nCorresponding eigenvectors are s1 =\\n\\n\u000f\\n\u000e \u000f\\n\u000e\\n1\\n1\\n∈ C2,1 , s2 =\\n∈ C2,1 iρ\\n−iρ and thus exp(t A)y0 = S\\n\\n3 Sir\\n\\n\u000f\\n\u000e itρ\\n\u000e\\n\u000f\\n0 e\\n1 1\\n−1\\nS y\\n,\\nS\\n=\\n∈ C2,2 .\\n0\\n0 e−itρ iρ −iρ\\n\\nRobert Hooke (1635–1703).   \n",
       "257                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      17.2 Systems of Linear Ordinary Differential Equations\\n\\n269\\n\\nExercises\\n\\n\u0004 \u0005\\n17.1 Construct a matrix A = [ai j ] ∈ C2,2 with A3 \u0003= ai3j .\\n17.2 Determine all solutions X ∈ C2,2 of the matrix equation X 2 = I2 , and classify which of these solutions are primary square roots of I2 .\\n17.3 Determine a matrix X ∈ C2,2 with real entries and X 2 = −I2 .\\n17.4 Prove Lemma 17.3.\\n17.5 Prove the following assertions for A ∈ Cn,n :\\n(a) det(exp(A)) = exp(trace(A)).\\n(b) If A H = −A, then exp(A) is unitary.\\n(c) If A2 = I , then exp(A) = 21 (e + 1e )I + 21 (e − 1e )A.\\n17.6 Let A = S diag(Jd1 (λ1 ), . . . , Jdm (λm )) S −1 ∈ Cn,n with rank(A) = n.\\nDetermine the primary matrix function f (A) for f (z) = z −1 .\\nDoes this function also exist if rank(A) < n?\\n17.7 Let log : {z = r eiϕ | r > 0, −π < ϕ < π} → C, r eiϕ \u000e→ ln(r ) + iϕ, be the principle branch of the complex logarithm (where ln denotes the real natural logarithm).\\nShow that this function is defined on the spectrum of\\n\u000e\\n\\n\u000f\\n01\\nA=\\n∈ C2,2 ,\\n−1 0 and compute log(A) as well as exp(log(A)).\\n17.8 Compute\\n\u0018\u000e exp\\n\\n01\\n−1 0\\n\\n\u000f\u0019\\n\\n\u0018\u000e\\n, exp\\n\\n−1 1\\n−1 −3\\n\\n\u000f\u0019\\n\\n⎛⎡\\n⎤⎞\\nπ1 1\\n, sin ⎝⎣ 0 π 1 ⎦⎠ .\\n0 0π\\n\\n17.9 Construct two matrices A, B ∈ C2,2 with exp(A + B) \u0003= exp(A) exp(B).\\n17.10 Prove the assertion on the entries of Ad in Example 17.7.\\n17.11 Let\\n⎡\\n⎤\\n511\\nA = ⎣0 5 1⎦ ∈ R3,3 .\\n004\\nCompute exp(t A) for t ∈ R and solve the homogeneous system of differential equations ẏ = Ay with the initial condition y(0) = [1, 1, 1]T .\\n17.12 Compute the matrix exp(t A) from Example 17.14 explicitly and thus show that exp(t A) ∈ R2,2 (for t ∈ R), despite the fact that the eigenvalues and eigenvectors of A are not real.   \n",
       "258                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Chapter 18\\n\\nSpecial Classes of Endomorphisms\\n\\nIn this chapter we discuss some classes of endomorphisms (or square matrices) whose eigenvalues and eigenvectors have special properties.\\nSuch properties only exist under further assumptions, and in this chapter our assumptions concern the relationship between the given endomorphism and its adjoint endomorphism.\\nThus, we focus on Euclidean or unitary vector spaces.\\nThis leads to the classes of normal, orthogonal, unitary and selfadjoint endomorphisms.\\nEach of these classes has a natural counterpart in the set of square (real or complex) matrices.\\n\\n18.1 Normal Endomorphisms\\nWe start with the definition of a normal1 endomorphism or matrix.\\nDefinition 18.1 Let V be a finite dimensional Euclidean or unitary vector space.\\nAn endomorphism f ∈ L(V, V) is called normal if f ◦ f ad = f ad ◦ f .\\nA matrix A ∈ Rn,n or A ∈ Cn,n is called normal if A T A = A A T or A H A = A A H , respectively.\\nFor all z ∈ C we have zz = |z|2 = zz.\\nThe property of normality can therefore be interpreted as a generalization of this property of complex numbers.\\nWe will first study the properties of normal endomorphisms on a finite dimensional unitary vector space V. Recall the following results:\\n(1) If B is an orthonormal basis of V and if f ∈ L(V, V), then ([ f ] B,B ) H = [ f ad ] B,B\\n(cp.\\nTheorem 13.12).\\n(2) Every f ∈ L(V, V) can be unitarily triangulated (cp.\\nCorollary 14.20, Schur’s theorem).\\nThis does not hold in general in the Euclidean case, since not every real polynomial decomposes into linear factors over R.\\n1 This term was introduced by Otto Toeplitz (1881–1940) in 1918 in the context of bilinear forms.\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_18\\n\\n271   \n",
       "259                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  272\\n\\n18 Special Classes of Endomorphisms\\n\\nUsing these results we obtain the following characterization of normal endomorphisms on a unitary vector space.\\nTheorem 18.2 If V is a finite dimensional unitary vector space, then f ∈ L(V, V) is normal if and only if there exists an orthonormal basis B of V such that [ f ] B,B is a diagonal matrix, i.e., f is unitarily diagonalizable.\\nProof Let f ∈ L(V, V) be normal and let B be an orthonormal basis of V such that R := [ f ] B,B is an upper triangular matrix.\\nThen R H = [ f ad ] B,B , and from f ◦ f ad = f ad ◦ f we obtain\\nR R H = [ f ◦ f ad ] B,B = [ f ad ◦ f ] B,B = R H R.\\nWe now show by induction on n = dim(V) that R is diagonal.\\nThis is obvious for n = 1.\\nLet the assertion hold for an n ≥ 1, and let R ∈ Cn+1,n+1 be upper triangular with\\nR R H = R H R. We write R as\\n\u0003\\n\u0002\\nR1 r 1\\n,\\nR=\\n0 α1 where R1 ∈ Cn,n is upper triangular, r1 ∈ Cn,1 , and α1 ∈ C. Then\\n\u0002\\n\\nR1 R1H + r1r1H α1r1\\nα1r1H\\n|α1 |2\\n\\n\u0003\\n\\n\u0002\\n= RRH = RH R =\\n\\n\u0003\\nR1H r1\\nR1H R1\\n.\\nr1H R1 r1H r1 + |α1 |2\\n\\nFrom |α1 |2 = r1H r1 + |α1 |2 we obtain r1H r1 = 0, hence r1 = 0 and R1 R1H = R1H R1 .\\nBy the induction hypothesis, R1 ∈ Cn,n is diagonal, and therefore\\n\u0002\\nR=\\n\\nR1 0\\n0 α1\\n\\n\u0003 is diagonal as well.\\nConversely, suppose that there exists orthonormal basis B of V such that [ f ] B,B is diagonal.\\nThen [ f ad ] B,B = ([ f ] B,B ) H is diagonal and, since diagonal matrices commute, we have\\n[ f ◦ f ad ] B,B = [ f ] B,B [ f ad ] B,B = [ f ad ] B,B [ f ] B,B = [ f ad ◦ f ] B,B , which implies f ◦ f ad = f ad ◦ f , and hence f is normal.\\n\\n\u0006\\n\u0005\\n\\nThe application of this theorem to the unitary vector space V = Cn,1 with the standard scalar product and a matrix A ∈ Cn,n viewed as element of L(V, V) yields the following “matrix version”.\\nCorollary 18.3 A matrix A ∈ Cn,n is normal if and only if there exists an orthonormal basis of Cn,1 consisting of eigenvectors of A, i.e., A is unitarily diagonalizable.   \n",
       "260                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        18.1 Normal Endomorphisms\\n\\n273\\n\\nThe following theorem presents another characterization of normal endomorphisms on a unitary vector space.\\nTheorem 18.4 If V is a finite dimensional unitary vector space, then f ∈ L(V, V) is normal if and only if there exists a polynomial p ∈ C[t] with p( f ) = f ad .\\nProof If p( f ) = f ad for a polynomial p ∈ C[t], then f ◦ f ad = f ◦ p( f ) = p( f ) ◦ f = f ad ◦ f, and hence f is normal.\\nConversely, if f is normal, then there exists an orthonormal basis B of V, such that [ f ] B,B = diag(λ1 , . . . , λn ).\\nFurthermore,\\n\u0004\\n\u0005\\n[ f ad ] B,B = ([ f ] B,B ) H = diag λ1 , . . . , λn .\\nLet p ∈ C[t] be a polynomial with p(λ j ) = λ j for j = 1, . . . , n.\\nSuch a polynomial can be explicitly constructed using the Lagrange basis of C[t]≤n−1 (cp.\\nExercise 10.12).\\nThen\\n\u0005\\n\u0004\\n\u0004\\n\u0005\\n[ f ad ] B,B = diag λ1 , . . . , λn = diag p(λ1 ), . . . , p(λn ) = p(diag(λ1 , . . . , λn ))\\n\u0004\\n\u0005\\n= p [ f ] B,B = [ p( f )] B,B , and hence also f ad = p( f ).\\n\\n\u0006\\n\u0005\\n\\nSeveral other characterizations of normal endomorphisms on a finite dimensional unitary vector space and of normal matrices A ∈ Cn,n can be found in the article [HorJ12] (see also Exercise 18.8).\\nWe now consider the Euclidean case, where we focus on real square matrices.\\nAll the results can be formulated analogously for normal endomorphisms on a finite dimensional Euclidean vector space.\\nLet A ∈ Rn,n be normal, i.e., A T A = A A T .\\nThen A also satisfies A H A = A A H and when A is considered as an element of Cn,n , it is unitarily diagonalizable, i.e.,\\nA = S DS H holds for a unitary matrix S ∈ Cn,n and a diagonal matrix D ∈ Cn,n .\\nDespite the fact that A has real entries, neither S nor D will be real in general, since\\nA as an element of Rn,n may not be diagonalizable.\\nFor instance,\\n\u0002\\nA=\\n\\n\u0003\\n12\\n∈ R2,2\\n−2 1 is a normal matrix that is not diagonalizable (over R).\\nConsidered as element of C2,2 , it has the eigenvalues 1 + 2i and 1 − 2i and it is unitarily diagonalizable.\\nTo discuss the case of real normal matrices in more detail, we first prove a “real version” of Schur’s theorem.   \n",
       "261                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      274\\n\\n18 Special Classes of Endomorphisms\\n\\nTheorem 18.5 For every matrix A ∈ Rn,n there exists an orthogonal matrix\\nU ∈ Rn,n with\\n⎤\\n⎡\\nR11 . . .\\nR1m\\n⎢\\n. ⎥ n,n\\n..\\nU T AU = R = ⎣\\n. .. ⎦ ∈ R ,\\nRmm where for every j = 1, . . . , m either R j j ∈ R1,1 or\\nRjj =\\n\\n( j)\\n\\n( j) r1 r2\\n( j)\\n2,2 with r3 \b= 0.\\n( j) ( j) ∈ R r3 r4\\n\\nIn the second case R j j has, considered as complex matrix, a pair of complex conjugate eigenvalues of the form α j ± iβ j with α j ∈ R and β j ∈ R \\ {0}.\\nThe matrix R is called a real Schur form of A.\\nProof We proceed via induction on n.\\nFor n = 1 we have A = [a11 ] = R and\\nU = [1].\\nSuppose that the assertion holds for some n ≥ 1 and let A ∈ Rn+1,n+1 be given.\\nWe consider A as an element of Cn+1,n+1 .\\nThen A has an eigenvalue λ = α+iβ ∈ C,\\nα, β ∈ R, corresponding to the eigenvector v = x + iy ∈ Cn+1,1 , x, y ∈ Rn+1,1 , and we have Av = λv.\\nDividing this equation into its real and imaginary parts, we obtain the two real equations\\nAx = αx − β y and Ay = βx + αy.\\n\\n(18.1)\\n\\nWe have two cases:\\nCase 1: β = 0.\\nThen the two equations in (18.1) are Ax = αx and Ay =\\nαy.\\nThus at least one of the real vectors x or y is an eigenvector corresponding to the real eigenvalue α of A. Without loss of generality we assume that this is the vector x and that x 2 = 1.\\nWe extend x by the vectors w2 , . . . , wn+1 to an orthonormal basis of Rn+1,1 with respect to the standard scalar product.\\nThe matrix\\nU1 := [x, w2 , . . . , wn+1 ] ∈ Rn+1,n+1 then is orthogonal and satisfies\\n\u0002\\nU1T AU1 =\\n\\nα \u0002\\n0 A1\\n\\n\u0003 for a matrix A1 ∈ Rn,n .\\nBy the induction hypothesis there exists an orthogonal matrix\\nU2 ∈ Rn,n such that R1 := U2T A1 U2 has the desired form.\\nThe matrix\\n\u0002\\nU := U1 is orthogonal and satisfies\\n\\n1 0\\n0 U2\\n\\n\u0003   \n",
       "262                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              18.1 Normal Endomorphisms\\n\\n\u0002\\nU T AU =\\n\\n275\\n\\n1 0\\n0 U2T\\n\\n\u0003\\n\\n\u0002\\nU1T AU1\\n\\n1 0\\n0 U2\\n\\n\u0003\\n\\n\u0002\\n=\\n\\nα \u0002\\n0 R1\\n\\n\u0003\\n=: R, where R has the desired form.\\nCase 2: β \b= 0.\\nWe first show that x, y are linearly independent.\\nIf x = 0, then using β \b= 0 in the first equation in (18.1) implies that also y = 0.\\nThis is not possible, since the eigenvector v = x + iy must be nonzero.\\nThus, x \b= 0, and using β \b= 0 in the second equation in (18.1) implies that also y \b= 0.\\nIf x, y ∈ Rn,1 \\ {0} are linearly dependent, then there exists a μ ∈ R \\ {0} with x = μy.\\nThe two equations in (18.1) then can be written as\\nAx = (α − βμ)x and Ax =\\n\\n1\\n(β + αμ)x,\\nμ which implies that β(1 + μ2 ) = 0.\\nSince 1 + μ2 \b= 0 for all μ ∈ R, this implies\\nβ = 0, which contradicts the assumption that β \b= 0.\\nConsequently, x, y are linearly independent.\\nWe can combine the two equations in (18.1) to the system\\n\u0002\\n\u0003\\nαβ\\nA[x, y] = [x, y]\\n,\\n−β α where rank([x, y]) = 2.\\nApplying the Gram-Schmidt method with respect to the standard scalar product of Rn+1,1 to the matrix [x, y] ∈ Rn+1,2 yields\\n[x, y] = [q1 , q2 ]\\n\\n\u0003\\n\u0002 r11 r12\\n=: Q R1 ,\\n0 r22 with Q T Q = I2 and R1 ∈ G L 2 (R).\\nIt then follows that\\nAQ = A[x, y]R1−1 = [x, y]\\n\\n\u0002\\n\\n\u0002\\n\u0003\\n\u0003\\nαβ\\nαβ\\nR1−1 .\\nR1−1 = Q R1\\n−β α\\n−β α\\n\u0002\\n\\nThe real matrix\\nR2 := R1\\n\\n\u0003\\nαβ\\nR1−1\\n−β α has, considered as element of C2,2 , the pair of complex conjugate eigenvalues α ± iβ with β \b= 0.\\nIn particular, the (2, 1)-entry of R2 is nonzero, since otherwise R2 would have two real eigenvalues.\\nWe again extend q1 , q2 by vectors w3 , . . . , wn+1 to an orthonormal basis of Rn+1,1 with respect to the standard scalar product.\\n(For n = 1 the list w3 , . . . , wn+1 is empty.)\\nThen U1 := [Q, w3 , . . . , wn+1 ] ∈ Rn+1,n+1 is orthogonal and we have\\n\u000f\\n\u000f\\n\u000e\\n\u000e\\nU1T AU1 = U1T AQ, A[w3 , . . . , wn+1 ] = U1T Q R2 , A[w3 , . . . , wn+1 ] =\\n\\n\u0002\\n\\nR2 \u0002\\n0 A1\\n\\n\u0003   \n",
       "263                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          276\\n\\n18 Special Classes of Endomorphisms for a matrix A1 ∈ Rn−1,n−1 .\\nAnalogously to the first case, an application of the induction hypothesis to this matrix yields the desired matrices R and U .\\n\u0005\\n\u0006\\nTheorem 18.5 implies the following result for real normal matrices.\\nCorollary 18.6 A matrix A ∈ Rn,n is normal if and only if there exists an orthogonal matrix U ∈ Rn,n with\\nU T AU = diag(R1 , . . . , Rm ), where, for every j = 1, . . . , m either R j ∈ R1,1 or\\n\u0002\\nRj =\\n\\nαj βj\\n−β j α j\\n\\n\u0003\\n∈ R2,2 with β j \b= 0.\\n\\nIn the second case the matrix R j has, considered as complex matrix, a pair of complex conjugate eigenvalues of the form α j ± iβ j .\\n\u0006\\n\u0005\\n\\nProof Exercise.\\nExample 18.7 The matrix\\n√ ⎤\\n√\\n⎡\\n0 2− 2\\n1 ⎣ √\\nA=\\n−√2 1\\n1 ⎦ ∈ R3,3\\n2\\n2 1\\n1 has, considered as a complex matrix, the eigenvalues 1, i, −i.\\nIt is therefore neither diagonalizable nor can it be triangulated over R. For the orthogonal matrix\\n⎤\\n0 2 √0\\n√\\n1\\nU = ⎣ −√2 0 √2 ⎦ ∈ R3,3\\n2\\n20 2\\n⎡ the transformed matrix\\n\\n⎡\\n\\n⎤\\n010\\nU T AU = ⎣ −1 0 0 ⎦\\n001 is in real Schur form.\\n\\n18.2 Orthogonal and Unitary Endomorphisms\\nIn this section we extend the concept of orthogonal and unitary matrices to endomorphisms.   \n",
       "264                                                                                                                                                                                                                                                                                                                                                                              18.2 Orthogonal and Unitary Endomorphisms\\n\\n277\\n\\nDefinition 18.8 Let V be a finite dimensional Euclidean or unitary vector space.\\nAn endomorphism f ∈ L(V, V) is called orthogonal or unitary, respectively, if f ad ◦ f = IdV .\\nIf f ad ◦ f = IdV , then f ad ◦ f is bijective and hence f is injective (cp.\\nExercise 2.7).\\nCorollary 10.11 implies that f is bijective.\\nHence f ad is the unique inverse of f , and we also have f ◦ f ad = IdV (cp. our remarks following Definition 2.21).\\nNote that an orthogonal or unitary endomorphism f is normal, and therefore all results from the previous section also apply to f .\\nLemma 18.9 Let V be a finite dimensional Euclidean or unitary vector space and let f ∈ L(V, V) be orthogonal or unitary, respectively.\\nIf B is an orthonormal basis of V, then [ f ] B,B is an orthogonal or unitary matrix, respectively.\\nProof Let dim(V) = n.\\nFor every orthonormal basis B of V we have\\nIn = [IdV ] B,B = [ f ad ◦ f ] B,B = [ f ad ] B,B [ f ] B,B = ([ f ] B,B ) H [ f ] B,B , and thus [ f ] B,B is orthogonal or unitary, respectively.\\n(In the Euclidean case\\n\u0006\\n\u0005\\n([ f ] B,B ) H = ([ f ] B,B )T .)\\nIn the following theorem we show that an orthogonal or unitary endomorphism is characterized by the fact that it does not change the scalar product of arbitrary vectors.\\nLemma 18.10 Let V be a finite dimensional Euclidean or unitary vector space with the scalar product ·, · .\\nThen f ∈ L(V, V) is orthogonal or unitary, respectively, if and only if f (v), f (w) = v, w for all v, w ∈ V.\\nProof If f is orthogonal or unitary and if v, w ∈ V, then\\n\u0010\\n\u0011 v, w = IdV (v), w = ( f ad ◦ f )(v), w = f (v), f (w) .\\nOn the other hand, suppose that v, w = f (v), f (w) for all v, w ∈ V. Then\\n\u0011\\n\u0010\\n0 = v, w − f (v), f (w) = v, w − v, ( f ad ◦ f )(w)\\n\u0010\\n\u0011\\n= v, (IdV − f ad ◦ f )(w) .\\nSince the scalar product is non-degenerate and v can be chosen arbitrarily, we have\\n\u0006\\n\u0005\\n(IdV − f ad ◦ f )(w) = 0 for all w ∈ V, and hence IdV = f ad ◦ f .\\nWe have the following corollary (cp.\\nLemma 12.13).\\nCorollary 18.11 If V is a finite dimensional Euclidean or unitary vector space with the scalar product ·, · , f ∈ L(V, V) is orthogonal or unitary, respectively, and\\n· = ·, · 1/2 is the norm induced by the scalar product, then f (v) = v for all v ∈ V.   \n",
       "265                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        278\\n\\n18 Special Classes of Endomorphisms\\n\\nFor the vector space V = Cn,1 with the standard scalar product and induced norm v 2 = (v H v)1/2 as well as a unitary matrix A ∈ Cn,n , we have Av 2 = v 2 for all v ∈ Cn,1 .\\nThus,\\nAv 2\\nA 2 = sup\\n=1 v 2 v∈Cn,1 \\{0}\\n(cp.\\n(6) in Example 12.4).\\nThis holds analogously for orthogonal matrices A ∈ Rn,n .\\nWe now study the eigenvalues and eigenvectors of orthogonal and unitary endomorphisms.\\nLemma 18.12 Let V be a finite dimensional Euclidean or unitary vector space and let f ∈ L(V, V) be orthogonal or unitary, respectively.\\nIf λ is an eigenvalue of f , then |λ| = 1.\\nProof Let ·, · be the scalar product on V. If f (v) = λv with v \b= 0, then v, v = IdV (v), v = ( f ad ◦ f )(v), v = f (v), f (v) = λv, λv = |λ|2 v, v , and v, v \b= 0 implies that |λ| = 1.\\n\\n\u0006\\n\u0005\\n\\nThe statement of Lemma 18.12 holds, in particular, for unitary and orthogonal matrices.\\nHowever, one should keep in mind that an orthogonal matrix (or an orthogonal endomorphism) may not have an eigenvalue.\\nFor example, the orthogonal matrix\\n\u0002\\nA=\\n\\n\u0003\\n0 −1\\n∈ R2,2\\n1 0 has the characteristic polynomial PA = t 2 + 1, which has no real roots.\\nIf considered as an element of C2,2 , the matrix A has the eigenvalues i and −i.\\nTheorem 18.13\\n(1) If A ∈ Cn,n is unitary, then there exists a unitary matrix U ∈ Cn,n with\\nU H AU = diag(λ1 , . . . , λn ) and |λ j | = 1 for j = 1, . . . , n.\\n(2) If A ∈ Rn,n is orthogonal, then there exists an orthogonal matrix U ∈ Rn,n with\\nU T AU = diag(R1 , . . . , Rm ), where for every j = 1, . . . , m either R j = [λ j ] ∈ R1,1 with λ j = ±1 or\\n\u0002 cj sj\\nRj =\\n−s j c j\\n\\n\u0003\\n∈ R2,2 with s j \b= 0 and c2j + s 2j = 1.   \n",
       "266                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                18.2 Orthogonal and Unitary Endomorphisms\\n\\n279\\n\\nProof\\n(1) A unitary matrix A ∈ Cn,n is normal and hence unitarily diagonalizable (cp.\\nCorollary 18.3).\\nBy Lemma 18.12, all eigenvalues of A have absolute value 1.\\n(2) An orthogonal matrix A is normal and hence by Corollary 18.6 there exists an orthogonal matrix U ∈ Rn,n with U T AU = diag(R1 , . . . , Rm ), where either\\nR j ∈ R1,1 or\\n\u0003\\n\u0002\\nαj βj\\n∈ R2,2\\nRj =\\n−β j α j with β j \b= 0.\\nIn the first case then R j = [λ j ] with |λ j | = 1 by Lemma 18.12.\\nSince A and U are orthogonal, also U T AU is orthogonal, and hence every diagonal block R j is orthogonal as well.\\nFrom R Tj R j = I2 we obtain α2j +β 2j = 1,\\n\u0006\\n\u0005 so that R j has the desired form.\\nWe now study two important classes of orthogonal matrices.\\nExample 18.14 Let i, j, n ∈ N with 1 ≤ i < j ≤ n and let α ∈ R. We define\\n⎡\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\nRi j (α) := ⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎣\\n\\n⎤\\n\\n1\\n..\\n\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥ ←i\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥ ← j\\n⎥\\n⎥\\n⎥\\n⎥\\n⎦\\n\\n.\\n1\\n− sin(α) cos(α)\\n1\\n..\\n\\n.\\n1 sin(α) cos(α)\\n1\\n..\\n\\n.\\n1\\n\\n↑ i\\n\\n.\\n\\n↑ j\\n\\nThe matrix Ri j (α) = [ri j ] ∈ Rn,n is equal to the identity matrix In except for its entries rii = cos(α), ri j = − sin(α), r ji = sin(α), r j j = cos(α).\\nFor n = 2 we have the matrix\\n\u0002\\nR12 (α) =\\n\\n\u0003 cos(α) − sin(α)\\n, sin(α) cos(α)   \n",
       "267                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   280\\n\\n18 Special Classes of Endomorphisms which satisfies\\n\u0002\\nR12 (α)T R12 (α) =\\n= cos(α) sin(α)\\n− sin(α) cos(α)\\n\\n\u0003\u0002 cos(α) − sin(α) sin(α) cos(α)\\n\\n\u0003\\n\\n\u0003\\n\u0002 2\\n0 cos (α) + sin2 (α)\\n0 cos2 (α) + sin2 (α)\\n\\n= I2 = R12 (α)R12 (α)T .\\nOne easily sees that each of the matrices Ri j (α) ∈ Rn,n is orthogonal.\\nThe multiplication of a vector v ∈ Rn,1 with the matrix Ri j (α) results in a (counterclockwise) rotation of v by the angle α in the (i, j)-coordinate plane.\\nIn Numerical Mathematics, the matrices Ri j (α) are called Givens rotations.2 This is illustrated in the figure below for the vector v = [1.0, 0.75]T ∈ R2,1 and the matrices R12 (π/2) and\\n), which represent rotations by 90 and 120 degrees, respectively.\\nR12 ( 2π\\n3\\n\\nExample 18.15 For u ∈ Rn,1 \\ {0} we define the Householder matrix\\nH (u) := In −\\n\\n2 uT u uu T ∈ Rn,n ,\\n\\n(18.2) and for u = 0 we set H (0) := In .\\nFor every u ∈ Rn,1 then H (u) is an orthogonal matrix (cp.\\nExercise 12.17).\\nThe multiplication of a vector v ∈ Rn,1 with the matrix\\nH (u) describes a reflection of v at the hyperplane\\n(span{u})⊥ =\\n\\n\u0012\\n\\n\u0013 y ∈ Rn,1 | u T y = 0 , i.e., the hyperplane of vectors that are orthogonal to u with respect to the standard scalar product.\\nThis is illustrated in the figure below for the vector v = [1.75, 0.5]T ∈\\nR2,1 and the Householder matrix\\n\u0002 \u0003\\n01\\nH (u) =\\n,\\n10 which corresponds to u = [−1, 1]T ∈ R2,1 .\\n2 Wallace\\n\\nGivens (1910–1993), pioneer of Numerical Linear Algebra.   \n",
       "268                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     18.2 Orthogonal and Unitary Endomorphisms\\n\\n281\\n\\nMATLAB-Minute.\\nLet u = [5, 3, 1]T ∈ R3,1 .\\nApply the command norm(u) to compute the Euclidean norm of u and form the Householder matrix H=eye(3)(2/(u’∗u))∗(u∗u’).\\nCheck the orthogonality of H via the computation of norm(H’∗H-eye(3)).\\nForm the vector v=H∗u and compare the Euclidean norms of u and v.\\n\\n18.3 Selfadjoint Endomorphisms\\nWe have already studied selfadjoint endomorphisms f on a finite dimensional Euclidean or unitary vector space.\\nThe defining property for this class of endomorphisms is f = f ad (cp.\\nDefinition 13.13).\\nObviously, selfadjoint endomorphisms are normal and hence the results of\\nSect.\\n18.1 hold.\\nWe now strengthen some of these results.\\nLemma 18.16 For a finite dimensional Euclidean or unitary vector space V and f ∈ L(V, V), the following statements are equivalent:\\n(1) f is selfadjoint.\\n(2) For every orthonormal basis B of V we have [ f ] B,B = ([ f ] B,B ) H .\\n(3) There exists an orthonormal basis B of V with [ f ] B,B = ([ f ] B,B ) H .\\n(In the Euclidean case ([ f ] B,B ) H = ([ f ] B,B )T .)\\nProof In Corollary 13.14 we have already shown that (1) implies (2), and obviously (2) implies (3).\\nIf (3) holds, then [ f ] B,B = ([ f ] B,B ) H = [ f ad ] B,B (cp.\\nTheorem 13.12), and hence f = f ad , so that (1) holds.\\n\u0006\\n\u0005\\nWe have the following strong result on the diagonalizability of selfadjoint endomorphisms in both the Euclidean and the unitary case.\\nTheorem 18.17 If V is a finite dimensional Euclidean or unitary vector space and f ∈ L(V, V) is selfadjoint, then there exists an orthonormal basis B of V such that\\n[ f ] B,B is a real diagonal matrix.   \n",
       "269                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             282\\n\\n18 Special Classes of Endomorphisms\\n\\nProof Consider first the unitary case.\\nIf f is selfadjoint, then f is normal and hence unitarily diagonalizable (cp.\\nTheorem 18.2).\\nLet B be an orthonormal basis of V so that [ f ] B,B is a diagonal matrix.\\nThen [ f ] B,B = [ f ad ] B,B = ([ f ] B,B ) H implies that the diagonal entries of [ f ] B,B , which are the eigenvalues of f , are real.\\nLet V be an n-dimensional Euclidean vector space.\\nIf \u0014\\nB = {v1 , . . . , vn } is an is symmetric and in particular normal.\\nBy Corolorthonormal basis of V, then [ f ] \u0014\\n\u0014\\nB, B lary 18.6, there exists an orthogonal matrix U = [u i j ] ∈ Rn,n with\\nU T [ f ]\u0014\\nB, \u0014\\nB U = diag(R1 , . . . , Rm ), where for j = 1, . . . , m either R j ∈ R1,1 or\\n\u0002\\n\\nαj βj\\nRj =\\n−β j α j\\n\\n\u0003\\n∈ R2,2 with β j \b= 0.\\n\\nSince U T [ f ] \u0014\\nB, \u0014\\nB U is symmetric, a 2 × 2 block R j with β j \b = 0 cannot occur.\\nThus,\\nU is a real diagonal matrix.\\nU T [ f ]\u0014\\nB, \u0014\\nB\\nWe define the basis B = {w1 , . . . , wn } of V by\\n(w1 , . . . , wn ) = (v1 , . . . , vn )U.\\nT\\n−1\\n= [IdV ] \u0014\\nThen, by construction, U = [IdV ] B, \u0014\\nB and hence U = U\\nB,B .\\nTherefore,\\nT\\nU\\n=\\n[ f\\n]\\n.\\nIf\\n·,\\n· is the scalar product on\\nV, then vi , v j = δi j ,\\nU [ f ]\u0014\\n\u0014\\nB,B\\nB, B i, j = 1, . . . , n.\\nWith U T U = In we get wi , w j = n\\n\u0015\u0016 k=1 u ki vk , n\\n\u0016 n n \u0016 n\\n\u0017 \u0016\\n\u0016 u \u0003j v\u0003 = u ki u \u0003j vk , v\u0003 = u ki u k j = δi j .\\n\\n\u0003=1 k=1 \u0003=1\\n\\nHence B is an orthonormal basis of V.\\nk=1\\n\\n\u0006\\n\u0005\\n\\nThis theorem has the following “matrix version”.\\nCorollary 18.18\\n(1) If A ∈ Rn,n is symmetric, then there exist an orthogonal matrix U ∈ Rn,n and a diagonal matrix D ∈ Rn,n with A = U DU T .\\n(2) If A ∈ Cn,n is Hermitian, then there exist a unitary matrix U ∈ Cn,n and a diagonal matrix D ∈ Rn,n with A = U DU H .\\nThe statement (1) in this corollary is known as the principal axes transformation.\\nWe will briefly discuss the background of this name from the theory of bilinear forms and their applications in geometry.\\nA symmetric matrix A = [ai j ] ∈ Rn,n defines a symmetric bilinear form on Rn,1 via   \n",
       "270                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             18.3 Selfadjoint Endomorphisms\\n\\n283\\n\\nβ A : Rn,1 × Rn,1 → R, (x, y) \u0012→ y T Ax = n n \u0016\\n\u0016 ai j xi y j .\\ni=1 j=1\\n\\nThe map q A : Rn,1 → R, x \u0012→ β A (x, x) = x T Ax, is called the quadratic form associated with this symmetric bilinear form.\\nSince A is symmetric, there exists an orthogonal matrix U = [u 1 , . . . , u n ] such that U T AU = D is a real diagonal matrix.\\nIf B1 = {e1 , . . . , en }, then [β A ] B1 ×B1 = A.\\nThe set B2 = {u 1 , . . . , u n } forms an orthonormal basis of Rn,1 with respect to the standard scalar product, and [u 1 , . . . , u n ] = [e1 , . . . , en ]U , hence U = [IdRn,1 ] B2 ,B1 .\\nFor the change of bases from of B1 to B2 we obtain\\n\u0005T\\n\u0004\\n[β A ] B2 ×B2 = [IdRn,1 ] B2 ,B1 [β A ] B1 ×B1 [IdRn,1 ] B2 ,B1 = U T AU = D\\n(cp.\\nTheorem 11.14).\\nThus, the real diagonal matrix D represents the bilinear form\\nβ A defined by A with respect to the basis B2 .\\nThe quadratic form q A associated with β A is also transformed to a simpler form by this change of bases, since analogously\\n⎡ q A (x) = x T Ax = x T U DU T x = y T Dy = n\\n\u0016\\n\\nλi yi2 = q D (y), i=1\\n\\n⎤ y1\\n⎢ ⎥ y = ⎣ ... ⎦ := U T x.\\nyn\\n\\nThus, the quadratic form q A is turned into a “sum of squares”, defined by the quadratic form q D .\\nThe principal axes transformation is given by the change of bases from the canonical basis of Rn,1 to the basis given by the pairwise orthonormal eigenvectors of A in\\nRn,1 .\\nThe n pairwise orthogonal subspaces span{u j }, j = 1, . . . , n, form the n principal axes.\\nThe geometric interpretation of this term is illustrated in the following example.\\nExample 18.19 For the symmetric matrix\\nA=\\n\u0002 we have\\nU T AU =\\n\\n\u0002 \u0003\\n41\\n∈ R2,2\\n12\\n\\n√\\n\u0003\\n0√\\n3+ 2\\n= D,\\n0\\n3− 2   \n",
       "271                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             284\\n\\n18 Special Classes of Endomorphisms with the orthogonal matrix U = [u 1 , u 2 ] ∈ R2,2 and\\n\u0002 \u0003\\n\u0002\\n\u0003 c\\n−s\\n, u2 =\\n, where u1 = s c\\n√\\n1+ 2\\n1 c= \u0018\\n= 0.9239, s = \u0018\\n= 0.3827.\\n√\\n√\\n(1 + 2)2 + 1\\n(1 + 2)2 + 1\\n(The numbers here are rounded to the fourth significant digit.)\\nWith the associated quadratic form q A (x) = 4x12 + 2x1 x2 + 2x22 , we define the set\\nE A = {x ∈ R2,1 | q A (x) − 1 = 0}.\\nAs described above, the principal axes transformation consists in the transformation from the canonical coordinate system to a coordinate system given by an orthonormal basis of eigenvectors of A. If we carry out this transformation and replace q A by the quadratic form q D , we get the set\\n\u0012\\n\\nED = y ∈ R\\n\\n2,1\\n\\n\u0019\\n| q D (y) − 1 = 0 = [y1 , y2 ]T ∈ R2,1\\n\u0013 where β1 =\\n\\n\n",
       "\\n1\\n√ = 0.4760, β2 =\\n3+ 2\\n\\n\u001a 2\\n\u001b\\n\u001a y1 y22\\n\u001a\\n\u001a β2 + β2 − 1 = 0 ,\\n1\\n2\\n\\n1\\n√ = 0.7941.\\n3− 2\\n\\nThis set forms the ellipse centered at the origin of the two dimensional cartesian coordinate system (spanned by the canonical basis vectors e1 , e2 ) with axes of lengths\\nβ1 and β2 , which is illustrated on the left part of the following figure:\\n\\nThe elements x ∈ E A are given by x = U y for y ∈ E D .\\nThe orthogonal matrix\\n\u0002\\nU= c −s s c\\n\\n\u0003   \n",
       "272                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   18.3 Selfadjoint Endomorphisms\\n\\n285 is a Givens rotation that rotates the ellipse E D counterclockwise by the angle cos−1 (c) = 0.3926 (approximately 22.5 degrees).\\nHence E A is just a “rotated version” of E D .\\nThe right part of the figure above shows the ellipse E A in the cartesian coordinate system.\\nThe dashed lines indicate the respective spans of the vectors u 1 and u 2 , which are the eigenvectors of A and the principal axes of the ellipse E A .\\nLet A ∈ Rn,n be symmetric.\\nFor a given vector v ∈ Rn,1 and a scalar α ∈ R,\\nQ(x) = x T Ax + v T x + α, x ∈ Rn,1 is a quadratic function in n variables (the entries of the vector x).\\nThe set of zeros of this function, i.e., the set {x ∈ Rn,1 | Q(x) = 0}, is called a hypersurface of degree\\n2 or a quadric.\\nIn Example 18.19 we have already seen quadrics in the case n = 2 and with v = 0.\\nWe next give some further examples.\\nExample 18.20\\n(1) Let n = 3, A = I3 , v = [0, 0, 0]T and α = −1.\\nThe corresponding quadric\\n\u0012\\n\\n[x1 , x2 , x3 ]T ∈ R3,1 | x12 + x22 + x32 − 1 = 0\\n\\n\u0013 is the surface of the ball with radius 1 around the origin:\\n\\n\u0003\\n10\\n, v = [0, 2]T and α = 0.\\nThe corresponding quadric\\n00\\n\\n\u0002\\n(2) Let n = 2, A =\\n\\n\u0012\\n\u0013\\n[x1 , x2 ]T ∈ R2,1 | x12 + 2x2 = 0 is a parabola:   \n",
       "273                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  286\\n\\n18 Special Classes of Endomorphisms\\n\\n⎡\\n\\n⎤\\n100\\n(3) Let n = 3, A = ⎣0 0 0⎦, v = [0, 2, 0]T and α = 0.\\nThe corresponding quadric\\n000\\n\u0012\\n\\n[x1 , x2 , x3 ]T ∈ R3,1 | x12 + 2x2 = 0\\n\\n\u0013 is a parabolic cylinder:\\n\\nCorollary 18.18 motivates the following definition.\\nDefinition 18.21 If A ∈ Rn,n is symmetric or A ∈ Cn,n is Hermitian with n + positive, n − negative and n 0 zero eigenvalues (counted with their corresponding multiplicities), then the triple (n + , n − , n 0 ) is called the inertia of A.\\nLet us first consider, for simplicity, only the case of real symmetric matrices.\\nLemma 18.22 If A ∈ Rn,n symmetric has the inertia (n + , n − , n 0 ), then A and\\nS A = diag(In + , −In − , 0n 0 ) are congruent.\\nProof Let A ∈ Rn,n be symmetric and let A = U \u0004U T with an orthogonal matrix\\nU ∈ Rn,n and \u0004 = diag(λ1 , . . . , λn ) ∈ Rn,n .\\nIf A has the inertia (n + , n − , n 0 ), then we can assume without loss of generality that\\n⎡\\n\u0004n +\\n\u0004n −\\n\u0004=⎣\\n\\n⎤\\n⎦ = diag(\u0004n + , \u0004n − , 0n 0 ),\\n0n 0 where the diagonal matrices \u0004n + and \u0004n − contain the positive and negative eigenvalues of A, respectively, and 0n 0 ∈ Rn 0 ,n 0 .\\nWe have \u0004 = \u0005S A \u0005, where\\nS A := diag(In + , −In − , 0n 0 ) ∈ Rn,n ,\\n\u0005 := diag((\u0004n + )1/2 , (−\u0004n − )1/2 , In 0 ) ∈ G L n (R).   \n",
       "274                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    18.3 Selfadjoint Endomorphisms\\n\\n287\\n\\n√\\n√\\nHere (diag(μ1 , . . . , μm ))1/2 = diag( μ1 , . . . , μm ) and thus\\nA = U \u0004U T = U \u0005S A \u0005U T = (U \u0005)S A (U \u0005)T .\\n\\n\u0006\\n\u0005\\n\\nThis result will be used in the proof of Sylvester’s law of inertia.3\\nTheorem 18.23 The inertia of a symmetric matrix A ∈ Rn,n is invariant under congruence, i.e., for every matrix G ∈ G L n (R) the matrices A and G T AG have the same inertia.\\nProof The assertion is trivial for A = 0.\\nLet A \b= 0 have the inertia (n + , n − , n 0 ), then not both n + and n − can be equal to zero.\\nWe assume without loss of generality that n + > 0.\\n(If n + = 0, then the following argument can be applied for n − > 0.)\\nBy Lemma 18.22 there exist G 1 ∈ G L n (R) and S A = diag(In + , −In − , 0n 0 ) with\\nA = G 1T S A G 1 .\\nLet G 2 ∈ G L n (R) be arbitrary and set B := G 2T AG 2 .\\nThen B n − ,\u0014 n 0 ).\\nTherefore, B = G 3T S B G 3 for S B = is symmetric and has an inertia (\u0014 n + ,\u0014 n + and diag(I\u0014n + , −I\u0014n − , 0\u0014n 0 ) and a matrix G 3 ∈ G L n (R).\\nIf we show that n + = \u0014 n 0 , then also n − = \u0014 n−.\\nn0 = \u0014\\nWe have\\n\u0005T\\n\u0004 −1 \u0005T T\\n\u0004\\n−1\\nT\\nBG −1\\nG 3 S B G 3 G −1\\nA = G −1\\n2\\n2 = G2\\n2 = G 4 S B G 4 , G 4 := G 3 G 2 , n0.\\nand G 4 ∈ G L n (R) implies that rank(A) = rank(S B ) = rank(B), hence n 0 = \u0014\\nWe set\\nG −1\\n1 = [u 1 , . . . , u n + , v1 , . . . , vn − , w1 , . . . , wn 0 ] and u1, . . . , \u0014 u\u0014n + ,\u0014 v1 , . . . ,\u0014 v\u0014n − , w\\n\u00141 , . . . , w\\n\u0014n 0 ].\\nG −1\\n4 = [\u0014 v1 , . . . ,\u0014 v\u0014n − , w\\n\u00141 , . . . , w\\n\u0014n 0 }.\\nSince n + >\\nLet V1 := span{u 1 , . . . , u n + } and V2 := span{\u0014\\n0, we have dim(V1 ) ≥ 1.\\nIf x ∈ V1 \\ {0}, then x= n+\\n\u0016\\n\\nT\\nα j u j = G −1\\n1 [α1 , . . . , αn + , 0, . . . , 0] j=1 for some α1 , . . . , αn + ∈ R that are not all zero.\\nThis implies x T Ax = n+\\n\u0016\\n\\nα2j > 0.\\nj=1\\n\\n3 James\\n\\nJoseph Sylvester (1814–1897) proved this result for quadratic forms in 1852.\\nHe also coined the name law of inertia which according to him is “expressing the fact of the existence of an invariable number inseparably attached to such [bilinear] forms”.   \n",
       "275                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 288\\n\\n18 Special Classes of Endomorphisms\\n\\nIf, on the other hand, x ∈ V2 , then an analogous argument shows that x T Ax ≤ 0.\\nHence V1 ∩ V2 = {0}, and the dimension formula for subspaces (cp.\\nTheorem 9.29) yields dim(V ) + dim(V2 ) − dim(V1 ∩ V2 ) = dim(V1 + V2 ) ≤ dim(Rn,1 ) = n,\\n\n",
       " \n",
       "\u001f 1\\n\n",
       " \n",
       "\u001f\\n\n",
       "\\n\n",
       "\u001f\\n=n +\\n\\n=n−\u0014 n+\\n\\n=0 n + .\\nIf we repeat the same construction by interchanging the roles of and thus n + ≤ \u0014 n + , then \u0014 n + ≤ n + .\\nThus, n + = \u0014 n + and the proof is complete.\\n\u0006\\n\u0005 n + and \u0014\\nIn the following result we transfer Lemma 18.22 and Theorem 18.23 to complex\\nHermitian matrices.\\nTheorem 18.24 Let A ∈ Cn,n be Hermitian with the inertia (n + , n − , n 0 ).\\nThen there exists a matrix G ∈ G L n (C) with\\nA = G H diag(In + , In − , 0n 0 ) G.\\nMoreover, for every matrix G ∈ G L n (C) the matrices A and G H AG have the same inertia.\\nProof Exercise.\\n\\n\u0006\\n\u0005\\n\\nFinally, we discuss a special class of symmetric and Hermitian matrices.\\nDefinition 18.25 A real symmetric or complex Hermitian n × n matrix A is called\\n(1) positive semidefinite, if v H Av ≥ 0 for all v ∈ Rn,1 resp. v ∈ Cn,1 ,\\n(2) positive definite, if v H Av > 0 for all v ∈ Rn,1 \\ {0} resp. v ∈ Cn,1 \\ {0}.\\nIf in (1) or (2) the reverse inequality holds, then the corresponding matrices are called negative semidefinite or negative definite, respectively.\\nFor selfadjoint endomorphisms we define analogously: If V is a finite dimensional\\nEuclidean or unitary vector space with the scalar product ·, · and if f ∈ L(V, V) is selfadjoint, then f is called positive semidefinite or positive definite, if f (v), v ≥ 0 for all v ∈ V resp. f (v), v > 0 for all v ∈ V \\ {0}.\\nThe following theorem characterizes symmetric positive definite matrices; see\\nExercise 18.19 and Exercise 18.20 for the transfer of the results to positive semidefinite matrices resp. positive definite endomorphisms.\\nTheorem 18.26 If A ∈ Rn,n is symmetric, then the following statements are equivalent:\\n(1) A is positive definite.\\n(2) All eigenvalues of A are real and positive.\\n(3) There exists a lower triangular matrix L ∈ G L n (R) with A = L L T .   \n",
       "276                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         18.3 Selfadjoint Endomorphisms\\n\\n289\\n\\nProof\\n(1) ⇒ (2): The symmetric matrix A is diagonalizable with real eigenvalues (cp.\\n(1) in Corollary 18.18).\\nIf λ is an eigenvalue with associated eigenvector v, i.e.,\\nAv = λv, then λv T v = v T Av > 0 and v T v > 0 implies that λ > 0.\\n(2) ⇒ (1): Let A = U T diag(λ1 , . . . , λn ) U be a diagonalization A with an orthogonal matrix U ∈ Rn,n (cp.\\n(1) in Corollary 18.18) and λ j > 0, j = 1, . . . , n.\\nLet v ∈ Rn,1 \\ {0} be arbitrary and let w := U v. Then w \b= 0 and v = U T w, so that v T Av = (U T w)T U T diag(λ1 , . . . , λn ) U (U T w) = w T diag(λ1 , . . . , λn ) w\\n= n\\n\u0016\\n\\nλ j w 2j > 0.\\nj=1\\n\\n(3) ⇒ (1): If A = L L T with L ∈ G L n (R), then for every v ∈ Cn,1 \\ {0} we have v T Av = v T L L T v = L T v\\n\\n2\\n2\\n\\n> 0, since L T is invertible.\\n(Note that here we do not need that L is lower triangular.)\\n(1) ⇒ (3): Let A = U T diag(λ1 , . . . , λn ) U be a diagonalization of A with an orthogonal matrix U ∈ Rn,n (cp.\\n(1) in Corollary 18.18).\\nSince A is positive definite, we know from (2) that λ j > 0, j = 1, . . . , n.\\nWe set\\n!\\n!\\n\u00041/2 := diag( λ1 , . . . , λn ), and then have A = (U \u00041/2 )(\u00041/2 U T ) =: B T B. Let B = Q R be a Q Rdecomposition of the invertible matrix B (cp.\\nCorollary 12.12), where Q ∈ Rn,n is orthogonal and R ∈ Rn,n is an invertible upper triangular matrix.\\nThen A =\\n\u0006\\n\u0005\\nB T B = (Q R)T (Q R) = L L T , where L := R T .\\nOne easily sees that an analogous result holds for complex Hermitian matrices\\nA ∈ Cn,n .\\nIn this case in assertion (3) the lower triangular matrix is L ∈ G L n (C) with A = L L H .\\nThe factorization A = L L T in (3) is called a Cholesky factorization4 of A. It is special case of the LU -decomposition in Theorem 5.4.\\nIn fact, Theorem 18.26 shows that an LU -decomposition of a (real) symmetric positive definite matrix can be computed without row permutations.\\nIn order to compute the Cholesky factorization of the symmetric positive definite matrix A = [ai j ] ∈ Rn,n , we consider the equation\\n\\n4 André-Louis\\n\\nCholesky (1875–1918).   \n",
       "277                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     290\\n\\n18 Special Classes of Endomorphisms\\n\\n⎤⎡\\n⎤\\n⎡ l11 · · · ln1 l11\\n⎥⎢\\n⎥\\n⎢\\nA = L L T = ⎣ ... . . . ⎦ ⎣ . . . ... ⎦ .\\nlnn ln1 · · · lnn\\nFor the first row of A we obtain\\n2\\n=⇒ l11 = a11 = l11 a1 j = l11l j1 =⇒ l j1\\n\\n√ a11 , a1 j\\n=\\n, l11\\n\\n(18.3) j = 2, . . . , n.\\n\\n(18.4)\\n\\nAnalogously, for the rows i = 2, . . . , n of A we obtain aii = i\\n\u0016 i−1\\n#1/2\\n\"\\n\u0016 li j li j =⇒ lii = aii − li2j\\n, j=1 ai j = n\\n\u0016\\n\\n(18.5) j=1 lik l jk = k=1 i\\n\u0016 lik l jk = k=1 i−1\\n\u0016 lik l jk + lii l ji k=1\\n\\n#\\n\u0016\\n1\" ai j − lik l jk , for j > i.\\nlii k=1 i−1\\n\\n=⇒ l ji =\\n\\n(18.6)\\n\\nThe symmetric or Hermitian positive definite matrices are closely related to the positive definite bilinear forms on Euclidian or unitary vector spaces.\\nTheorem 18.27 If V is a finite dimensional Euclidian or unitary vector space and if β is a symmetric or Hermitian bilinear form on V, respectively, then the following statements are equivalent:\\n(1) β is positive definite, i.e., β(v, v) > 0 for all v ∈ V \\ {0}.\\n(2) For every basis B of V the matrix representation [β] B×B is (symmetric or Hermitian) positive definite.\\n(3) There exists a basis B of V such that the matrix representation [β] B×B is (symmetric or Hermitian) positive definite.\\n\u0006\\n\u0005\\n\\nProof Exercise.\\nExercises\\n\\n18.1 Let A ∈ Rn,n be normal.\\nShow that α A for every α ∈ R, Ak for every k ∈ N0 , and p(A) for every p ∈ R[t] are normal.\\n18.2 Let A, B ∈ Rn,n be normal.\\nAre A + B and AB then normal as well?\\n18.3 Let A ∈ R2,2 be normal but not symmetric.\\nShow that then\\n\u0002\\n\\nαβ\\nA=\\n−β α\\n\\n\u0003   \n",
       "278                                                                                                                                                                                                                                                                                                   18.3 Selfadjoint Endomorphisms\\n\\n291 for some α ∈ R and β ∈ R \\ {0}.\\n18.4 Prove Corollary 18.6 using Theorem 18.5.\\n18.5 Show that real skew-symmetric matrices (i. e., matrices with A = −A T ∈\\nRn,n ) and complex skew-Hermitian matrices (i. e., matrices with A = −A H ∈\\nCn,n ) are normal.\\n18.6 Let V be a finite dimensional unitary vector space and let f ∈ L(V, V) be normal.\\nShow the following assertions:\\n(a) If f = f 2 , then f is selfadjoint.\\n(b) If f 2 = f 3 , then f = f 2 .\\n(c) If f is nilpotent, then f = 0.\\n18.7 Let V be a finite dimensional real or complex vector space and let f ∈ L(V, V) be diagonalizable.\\nShow that there exists a scalar product on V such that f is normal with respect to this scalar products.\\n18.8 Let A ∈ Cn,n .\\nShow the following assertions:\\n(a) A is normal if and only if there exists a normal matrix B with n distinct eigenvalues that commutes with A.\\n(b) A is normal if and only if A + a I is normal for every a ∈ C.\\n(c) Let H (A) := 21 (A + A H ) be the Hermitian and S(A) := 21 (A − A H ) the skew-Hermitian part of A. Show that A = H (A)+S(A), H (A) H = H (A) and S(A) H = −S(A).\\nShow, furthermore, that A is normal if and only if\\nH (A) and S(A) commute.\\n18.9 Show that if A ∈ Cn,n is normal and if f (z) = az+b with ad − bc \b= 0 is cz+d defined on the spectrum of A, then f (A) = (a A + bI )(c A + d I )−1 .\\n(The map f (z) is called a Möbius transformation.5 Such transformations play an important role in Function Theory and in many other areas of Mathematics.)\\n18.10 Let V be a finite dimensional Euclidian or unitary vector space and let f ∈\\nL(V, V) be orthogonal or unitary, respectively.\\nShow that f −1 exists and is again orthogonal or unitary, respectively.\\n18.11 Let u ∈ Rn,1 and let the Householder matrix H (u) be defined as in (18.2).\\nShow the following assertions:\\n(a) For u \b= 0 the matrices H (u) and [−e1 , e2 , . . . , en ] are orthogonally similar, i.e., there exists an orthogonal matrix Q ∈ Rn,n with\\nQ T H (u)Q = [−e1 , e2 , . . . , en ].\\n(This implies that H (u) only has the eigenvalues 1 and −1 with the algebraic multiplicities n − 1 and 1, respectively.)\\n(b) Every orthogonal matrix A ∈ Rn,n can be written as product of n Householder matrices, i.e., there exist u 1 , . . . , u n ∈ Rn,1 with A = H (u 1 ) . . .\\nH (u n ).\\n5 August\\n\\nFerdinand Möbius (1790–1868).   \n",
       "279                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        292\\n\\n18 Special Classes of Endomorphisms\\n\\n18.12 Let v ∈ Rn,1 satisfy v T v = 1.\\nShow that there exists an orthogonal matrix\\nU ∈ Rn,n with U v = e1 .\\n18.13 Transfer the proofs of Lemma 18.22 and Theorem 18.23 to complex Hermitian matrices and thus show Theorem 18.24.\\n18.14 Determine for the symmetric matrix\\n\u0002\\n\u0003\\n10 6\\nA=\\n∈ R2,2\\n6 10 an orthogonal matrix U ∈ R2,2 such that U T AU is diagonal.\\nIs A positive\\n(semi-)definite?\\n18.15 Let K ∈ {R, C} and let {v1 , . . . , vn } be a basis of K n,1 .\\nProve or disprove: A matrix A = A H ∈ K n,n is positive definite if and only if v Hj Av j > 0 for all j = 1, . . . , n.\\n18.16 Use Definition 18.25 to test whether the symmetric matrices\\n\u0002 \u0003 \u0002 \u0003 \u0002 \u0003\\n11\\n12\\n21\\n,\\n,\\n∈ R2,2\\n11\\n21\\n12 are positive (semi-)definite.\\nDetermine in all cases the inertia.\\n18.17 Let\\n\u0003\\n\u0002\\nA11 A12\\n∈ Rn,n\\nA=\\nT\\nA12\\nA22\\nT\\nT with A11 = A11\\n∈ G L m (R), A12 ∈ Rm,n−m and A22 = A22\\n∈ Rn−m,n−m .\\nThe\\n−1\\nT m,m is called the Schur complement 6 of matrix S := A22 − A12 A11 A12 ∈ R\\nA11 in A. Show that A is positive definite if A11 and S are positive definite.\\n(For the Schur complement, see also Exercise 4.17.)\\n18.18 Show that A ∈ Cn,n is Hermitian positive definite if and only if x, y = y H Ax defines a scalar product on Cn,1 .\\n18.19 Prove the following version of Theorem 18.26 for positive semidefinite matrices.\\n\\nIf A ∈ Rn,n is symmetric, then the following statements are equivalent:\\n(1) A is positive semidefinite.\\n(2) All eigenvalues of A are real and nonnegative.\\n(3) There exists an upper triangular matrix L ∈ Rn,n with A = L L T .\\n18.20 Let V be a finite dimensional Euclidian or unitary vector space and let f ∈\\nL(V, V) be selfadjoint.\\nShow that f is positive definite if and only if all eigenvalues of f are real and positive.\\n18.21 Let A ∈ Rn,n .\\nA matrix X ∈ Rn,n with X 2 = A is called a square root of A\\n(cp.\\nSect.\\n17.1).\\n6 Issai\\n\\nSchur (1875–1941).   \n",
       "280                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           18.3 Selfadjoint Endomorphisms\\n\\n293\\n\\n(a) Show that a symmetric positive definite matrix A ∈ Rn,n has a symmetric positive definite square root.\\n(b) Show that the matrix\\n⎡\\n\\n⎤\\n33\\n6\\n6\\nA = ⎣ 6 24 −12 ⎦\\n6 −12 24 is symmetric positive definite and compute a symmetric positive definite square root of A.\\n(c) Show that the matrix A = Jn (0), n ≥ 2, does not have a square root.\\n18.22 Show that the matrix\\n\\n⎡\\n⎤\\n210\\nA = ⎣1 2 1⎦ ∈ R3,3\\n012 is positive definite and compute a Cholesky factorization of A using (18.3)–\\n(18.6).\\n18.23 Let A, B ∈ Cn,n be Hermitian and let B be furthermore positive definite.\\nShow that the polynomial det(t B − A) ∈ C[t]≤n has exactly n real roots.\\n18.24 Prove Theorem 18.27.   \n",
       "281                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Chapter 19\\n\\nThe Singular Value Decomposition\\n\\nThe matrix decomposition introduced in this chapter is very important in many practical applications, since it yields the best possible approximation (in a certain sense) of a given matrix by a matrix of low rank.\\nA low rank approximation can be considered a “compression” of the data represented by the given matrix.\\nWe illustrate this below with an example from image processing.\\nWe first prove the existence of the decomposition.\\nTheorem 19.1 Let A ∈ Cn,m with n ≥ m be given.\\nThen there exist unitary matrices\\nV ∈ Cn,n and W ∈ Cm,m such that\\n\u0003\\n\u0002\\n\u0002r 0r,m−r\\nH\\n∈ Rn,m , \u0002r = diag(σ1 , . . . , σr ), with \u0002 =\\nA = V \u0002W\\n0n−r,r 0n−r,m−r\\n(19.1) where σ1 ≥ σ2 ≥ · · · ≥ σr > 0 and r = rank(A).\\nProof If A = 0, then we set V = In , \u0002 = 0 ∈ Cn,m , \u0002r = [ ], W = Im , and we are finished.\\nLet A \u0004= 0 and r := rank(A).\\nSince n ≥ m, we have 1 ≤ r ≤ m, and since\\nA H A ∈ Cm,m is Hermitian, there exists a unitary matrix W = [w1 , . . . , wm ] ∈ Cm,m with\\nW H (A H A)W = diag(λ1 , . . . , λm ) ∈ Rm,m\\n(cp.\\n(2) in Corollary 18.18).\\nWithout loss of generality we assume that λ1 ≥ λ2 ≥\\n· · · ≥ λm .\\nFor every j = 1, . . . , m then A H Aw j = λ j w j , and hence\\nλ j w Hj w j = w Hj A H Aw j = \u0006Aw j \u000622 ≥ 0, i.e., λ j ≥ 0 for j = 1, . . . , m.\\nThen rank(A H A) = rank(A) = r (to see this, modify the proof of Lemma 10.25 for the complex case).\\nTherefore, the matrix A H A has exactly r positive eigenvalues λ1 , . . . , λr and m − r times the eigenvalue 0.\\nWe then\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_19\\n\\n295   \n",
       "282                                                                                                                                                                                                                                                                                                                                                                                                                                     296\\n\\n19 The Singular Value Decomposition\\n1/2 define σ j := λ j , j = 1, . . . , r , and have σ1 ≥ σ2 ≥ · · · ≥ σr .\\nLet \u0002r be as in\\n(19.1),\\nD :=\\n\\n\u0003\\n\u0002\\n\u0002r 0\\n∈ G L m (R),\\n0 Im−r\\n\\nX = [x1 , . . . , xm ] := AW D −1 ,\\n\\nVr := [x1 , . . . , xr ], and Z := [xr +1 , . . . , xm ].\\nThen\\n\u0002\\n\\n\u0003 \u0002 H\u0003\\n\u0003\\n\u0002\\nVr\\nVrH Vr VrH Z\\nIr 0\\nH\\n−1\\nH H\\n−1\\n=\\n,\\n[V\\n,\\nZ\\n]\\n=\\nX\\nX\\n=\\nD\\nW\\nA\\nAW\\nD\\n= r\\nZH\\nZ H Vr Z H Z\\n0 0 which implies, in particular, that Z = 0 and VrH Vr = Ir .\\nWe extend the vectors xr +1 , . . . , \u0004 xn } of Cn,1 with respect to x1 , . . . , xr to an orthonormal basis {x1 , . . . , xr , \u0004 the standard scalar product.\\nThen the matrix xr +1 , . . . , \u0004 xn ] ∈ Cn,n\\nV := [Vr , \u0004 is unitary.\\nFrom X = AW D −1 and X = [Vr , Z ] = [Vr , 0] we finally obtain\\n\b\\n\u0007\\nA = [Vr , 0]DW H and A = V \u0002W H with \u0002 as in (19.1).\\nAs the proof shows, Theorem 19.1 can be formulated analogously for real matrices\\nA ∈ Rn,m with n ≥ m.\\nIn this case the two matrices V and W are orthogonal.\\nIf n < m we can apply the theorem to A H (resp.\\nA T in the real case).\\nDefinition 19.2 A decomposition of the form (19.1) is called a singular value decomposition or short SVD1 of the matrix A. The diagonal entries of the matrix\\n\u0002r are called singular values and the columns of V resp.\\nW are called left resp. right singular vectors of A.\\nFrom (19.1) we obtain the unitary diagonalizations of the matrices A H A and\\nA AH ,\\n\u0002 2 \u0003\\n\u0002 2 \u0003\\n\u0002r 0\\n\u0002r 0\\nH\\nH\\nH\\nA A=W and A A = V\\nW\\nV H.\\n0 0\\n0 0\\nThe singular values of A are therefore uniquely determined as the positive square roots of the positive eigenvalues of A H A or A A H .\\nThe unitary matrices V and W in the singular value decomposition, however, are (as the eigenvectors in general) not uniquely determined.\\n\\n1 In the development of this decomposition from special cases in the middle of the 19th century to its current general form many important players of the history of Linear Algebra played a role.\\nIn the historical notes concerning the singular value decomposition in [HorJ91] one finds contributions of Jordan (1873), Sylvester (1889/1890) and Schmidt (1907).\\nThe current form was shown in 1939 by Carl Henry Eckart (1902–1973) and Gale Young.   \n",
       "283                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              19 The Singular Value Decomposition\\n\\n297\\n\\nIf we write the SVD of A in the form\\n\u0003\\n\u0006\u0005 \u0002\\n\u0003\\n\u0006\\n\u0005 \u0002\\n\u0002r 0\\nIm\\nWH\\nW\\nW H =: U P,\\nA = V \u0002W H = V\\n0n−m,m\\n0 0m−r then U ∈ Cn,m has orthonormal columns, i.e., U H U = Im , and P = P H ∈ Cm,m is positive semidefinite with the inertia (r, 0, m − r ).\\nThe factorization A = U P is called a polar decomposition of A. It can be viewed as a generalization of the polar representation of complex numbers, z = eiϕ |z|.\\nLemma 19.3 Suppose that the matrix A ∈ Cn,m with rank(A) = r has an SVD of the form (19.1) with V = [v1 , . . . , vn ] and W = [w1 , . . . , wm ].\\nConsidering\\nA as an element of L(Cm,1 , Cn,1 ), we then have im(A) = span{v1 , . . . , vr } and ker(A) = span{wr +1 , . . . , wm }.\\nProof For j = 1, . . . , r we have Aw j = V \u0002W H w j = V \u0002e j = σ j v j \u0004= 0, since\\nσ j \u0004= 0.\\nHence these r linear independent vectors satisfy v1 , . . . , vr ∈ im(A).\\nNow r = rank(A) = dim(im(A)) implies that im(A) = span{v1 , . . . , vr }.\\nFor j = r +1, . . . , m we have Aw j = 0, and hence these m −r linear independent vectors satisfy wr +1 , . . . , wm ∈ ker(A).\\nThen dim(ker(A)) = m − dim(im(A)) =\\n\b\\n\u0007 m − r implies that ker(A) = span{wr +1 , . . . , wm }.\\nAn SVD of the form (19.1) can be written as\\nA= r\\n\u0007\\n\\nσ j v j w Hj .\\nj=1\\n\\nThus, A can be written as a sum of r matrices of the form σ j v j w Hj , where\\n\b rank σ j v j w Hj = 1.\\nLet\\nAk := k\\n\u0007\\n\\nσ j v j w Hj for some k, 1 ≤ k ≤ r.\\n\\n(19.2) j=1\\n\\nThen rank(Ak ) = k and, using that the matrix 2-norm is unitarily invariant (cp.\\nExercise 19.1), we get\\n\u0006A − Ak \u00062 = \u0006diag(σk+1 , . . . , σr )\u00062 = σk+1 .\\n\\n(19.3)\\n\\nHence A is approximated by the matrix Ak , where the rank of the approximating matrix and the approximation error in the matrix 2-norm are explicitly known.\\nThe singular value decomposition, furthermore, yields the best possible approximation of A by a matrix of rank k with respect to the matrix 2-norm.\\nTheorem 19.4 With Ak as in (19.2), we have \u0006A − Ak \u00062 ≤ \u0006A − B\u00062 for every matrix B ∈ Cn,m with rank(B) = k.   \n",
       "284                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              298\\n\\n19 The Singular Value Decomposition\\n\\nProof The assertion is clear for k = rank(A), since then Ak = A and \u0006A− Ak \u00062 = 0.\\nLet k < rank(A) ≤ m.\\nLet B ∈ Cn,m with rank(B) = k be given, then dim(ker(B)) = m − k, where we consider B as an element of L(Cm,1 , Cn,1 ).\\nIf w1 , . . . , wm are the right singular vectors of A from (19.1), then U := span{w1 , . . . , wk+1 } has the dimension k + 1.\\nSince ker(B) and U are subspaces of Cm,1 with dim(ker(B)) + dim(U) = m + 1, we have ker(B) ∩ U \u0004= {0}.\\nLet v ∈ ker(B) ∩ U with \u0006v\u00062 = 1 be given.\\nThen there exist α1 , . . . , αk+1 ∈ C k+1\\n2\\n2 with v = k+1 j=1 α j w j and j=1 |α j | = \u0006v\u00062 =1.\\nHence\\n(A − B)v = Av − Bv\u000e =\\n=0 k+1\\n\u0007 j=1\\n\\nα j Aw j = k+1\\n\u0007\\n\\nαjσjvj j=1 and, therefore, k+1\\n\u000f\u0007\\n\u000f\\nα j σ j v j \u000f2\\n\u0006A − B\u00062 = max \u0006(A − B)y\u00062 ≥ \u0006(A − B)v\u00062 = \u000f\\n\u0006y\u00062 =1\\n\\n= k+1\\n\b\u0007 j=1\\n\\n|α j σ j |2\\n\\n1/2\\n\\n(since v1 , . . . , vk+1 are pairwise orthonormal) j=1\\n\\n≥ σk+1 k+1\\n\b\u0007\\n\\n|α j |2\\n\\n1/2\\n\\n(since σ1 ≥ · · · ≥ σk+1 ) j=1\\n\\n= σk+1 = \u0006A − Ak \u00062 , which completes the proof.\\n\\n\b\\n\u0007\\n\\nMATLAB-Minute.\\nThe command A=magic(n) generates for n ≥ 3 an n × n matrix A with entries from 1 to n 2 , so that all row, column and diagonal sums of A are equal.\\nThe entries of A therefore from a “magic square”.\\nCompute the SVD of A=magic(10) using the command [V,S,W]=svd(A).\\nWhat can be said about the singular values of A and what is rank(A)?\\nForm\\nAk for k = 1, 2, . . . , rank(A) as in (19.2) and verify numerically the equation\\n(19.3).\\n\\nThe SVD is one of the most important and practical mathematical tools in almost all areas of science, engineering and social sciences, in medicine and even in psychology.\\nIts great importance is due to the fact that the SVD allows to distinguish between\\n“important” and “non-important” information in a given data.\\nIn practice, the latter   \n",
       "285                                                                                                                                                                                                            19 The Singular Value Decomposition\\n\\n299 corresponds, e.g., to measurement errors, noise in the transmission of data, or fine details in a signal or an image that do not play an important role.\\nOften, the “important” information corresponds to the large singular values, and the “non-important” information to the small ones.\\nIn many applications one sees, furthermore, that the singular values of a given matrix decay rapidly, so that there exist only few large and many small singular values.\\nIf this is the case, then the matrix can be approximated well by a matrix with low rank, since already for a small k the approximation error \u0006A − Ak \u00062 = σk+1 is small.\\nA low rank approximation Ak requires little storage capacity in the computer; only k scalars and 2k vectors have to be stored.\\nThis makes the SVD a powerful tool in all applications where data compression is of interest.\\nExample 19.5 We illustrate the use of the SVD in image compression with a picture that we obtained from the research center Matheon: Mathematics for Key Technologies2 .\\nThe greyscale picture is shown on the left of the figure below.\\nIt consists of 286 × 152 pixels, where each of the pixels is given by a value between 0 and 64.\\nThese values are stored in a real 286 × 152 matrix A which has (full) rank 152.\\n\\nWe compute an SVD A = V \u0002W T using the command [V,S,W]=svd(A) in MATLAB.\\nThe diagonal entries of the matrix S, i.e., the singular values of A, are ordered decreasingly by MATLAB (as in Theorem 19.1).\\nFor k = 100, 20, 10 we now compute matrices Ak with rank k as in (19.2) using the command Ak=V(:,1:k)∗\\nS(1:k,1:k)∗W(:,1:k)’.\\nThese matrices represent approximations of the original picture based on the k largest singular values and the corresponding singular vectors.\\nThe three approximations are shown next to the original picture above.\\nThe quality of the approximation decreases with decreasing k, but even the approximation for k = 10 shows the essential features of the “Matheon bear”.\\nAnother important application of the SVD arises in the solution of linear systems of equations.\\nIf A ∈ Cn,m has an SVD of the form (19.1), we define the matrix\\nA := W \u0002 V\\n†\\n\\n†\\n\\nH\\n\\n∈C m,n\\n\\n\u0002 −1 \u0003\\n\u0002r 0\\n, where \u0002 :=\\n∈ Rm,n .\\n0 0\\n†\\n\\n(19.4)\\n\\n2 We thank Falk Ebert for his help.\\nThe original bear can be seen in front of the Mathematics building of the TU Berlin.\\nMore information on MATHEON can be found at www.matheon.de.   \n",
       "286                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  300\\n\\n19 The Singular Value Decomposition\\n\\nOne easily sees that\\n\\n\u0002\\n\\n\u0003\\nIr 0\\nA A=W\\nW H ∈ Rm,m .\\n0 0\\n†\\n\\nIf r = m = n, then A is invertible and the right hand side of the above equation is equal to the identity matrix In .\\nIn this case we have A† = A−1 .\\nThe matrix A† can therefore be viewed as a generalized inverse, that in the case of an invertible matrix\\nA is equal to the inverse of A.\\nDefinition 19.6 The matrix A† in (19.4) is called Moore-Penrose inverse3 or pseudoinverse of A.\\nLet A ∈ Cn,m and b ∈ Cn,1 be given.\\nIf the linear system of equations Ax = b has x is “as close as possible” no solution, then we can try to find an \u0010 x ∈ Cm,1 such that A\u0010 to b.\\nUsing the Moore-Penrose inverse we obtain the best possible approximation with respect to the Euclidean norm.\\nTheorem 19.7 Let A ∈ Cn,m with n ≥ m and b ∈ Cn,1 be given.\\nIf A = V \u0002W H is x = A† b satisfies an SVD, and A† is as in (19.4), then \u0010\\n\u0006b − A\u0010 x \u00062 ≤ \u0006b − Ay\u00062 for all y ∈ Cm,1 , and\\n\\n⎛\\n\u0013\\n\u0013 ⎞1/2 r \u0013 H \u00132\\n\u0007 v b\\n\u0013 j \u0013 ⎠\\n\u0006\u0010 x \u00062 = ⎝\\n≤ \u0006y\u00062\\n\u0013\\n\u0013\\n\u0013 σj \u0013 j=1 for all y ∈ Cm,1 with \u0006b − A\u0010 x \u00062 = \u0006b − Ay\u00062 .\\nProof Let y ∈ Cm,1 be given and let z = [ξ1 , . . . , ξm ]T := W H y.\\nThen\\n\u0006b − Ay\u000622 = \u0006b − V \u0002W Hy\u000622 = \u0006V (V H b − \u0002z)\u000622 = \u0006V H b − \u0002z\u000622\\n= r n\\n\u0007\\n\u0007\\n\u0013 H\\n\u0013\\n\u0013 H \u00132\\n\u0013v b − σ j ξ j \u00132 +\\n\u0013v b\u0013 j j j=r +1 j=1 n\\n\u0007\\n\u0013 H \u00132\\n\u0013v b \u0013 .\\n≥ j\\n\\n(19.5) j=r +1\\n\\n\b\\nEquality holds if and only if ξ j = v Hj b /σ j for all j = 1, . . . , r .\\nThis is satisfied if z = W H y = \u0002 † V H b.\\nThe last equation holds if and only if x.\\ny = W \u0002 † V H b = A† b = \u0010\\n3 Eliakim\\n\\nHastings Moore (1862–1932) and Sir Roger Penrose (1931–).   \n",
       "287                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        19 The Singular Value Decomposition\\n\\n301\\n\\nThe vector \u0010 x therefore attains the lower bound (19.5).\\nThe equation\\n⎛\\n\u0013\\n\u0013 ⎞1/2 r \u0013 H \u00132\\n\u0007 vj b\u0013\\n\u0013\\n\u0006\u0010 x \u00062 = ⎝\\n\u0013\\n\u0013 ⎠\\n\u0013 σj \u0013 j=1 is easily checked.\\nEvery vector y ∈ Cm,1 that attains the lower bound (19.5) must have the form\\n\u0003T\\n\u0002 H vH b v1 b\\n, . . . , r , yr +1 , . . . , ym y=W\\nσ1\\nσr for some yr +1 , . . . , ym ∈ C, which implies that \u0006y\u00062 ≥ \u0006\u0010 x \u00062 .\\n\\n\b\\n\u0007\\n\\nThe minimization problem for the vector \u0010 x can be written as\\n\u0006b − A\u0010 x \u00062 = min \u0006b − Ay\u00062 .\\ny∈Cm,1\\n\\n⎡\\n\\nIf\\n\\nτ1\\n⎢ ..\\nA=⎣ .\\n\\n⎤\\n1\\n.. ⎥ ∈ Rm,2\\n.⎦\\n\\nτm 1 for (pairwise distinct) τ1 , . . . , τm ∈ R, then this minimization problem corresponds to the problem of linear regression and the least squares approximation in Example 12.16, that we have solved with the Q R-decomposition of A. If A = Q R is this decomposition, then A† = (A H A)−1 A H (cp.\\nExercise 19.5) and we have\\nA† = (R H Q H Q R)−1 R H Q H = R −1 (R H )−1 R H Q H = R −1 Q H .\\nThus, the solution of the least-squares approximation in Example 12.16 is identical to the solution of the above minimization problem using the SVD of A.\\nExercises\\n19.1 Show that the Frobenius norm and the matrix 2-norm are unitarily invariant, i.e., that \u0006P AQ\u0006 F = \u0006A\u0006 F and \u0006P AQ\u00062 = \u0006A\u00062 for all A ∈ Cn,m and unitary matrices P ∈ Cn,n , Q ∈ Cm,m .\\n(Hint: For the Frobenius norm one can use that \u0006A\u00062F = trace(A H A).)\\n\n",
       "1/2\\n\n",
       "\\n19.2 Use the result of Exercise 19.1 to show that \u0006A\u0006 F = σ12 + . . . + σr2 and\\n\u0006A\u00062 = σ1 , where σ1 ≥ · · · ≥ σr > 0 are the singular values of A ∈ Cn,m .\\n19.3 Show that \u0006A\u00062 = \u0006A H \u00062 and \u0006A\u000622 = \u0006A H A\u00062 for all A ∈ Cn,m .\\n19.4 Show that \u0006A\u000622 ≤ \u0006A\u00061 \u0006A\u0006∞ for all A ∈ Cn,m .   \n",
       "288                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   302\\n\\n19 The Singular Value Decomposition\\n\\n19.5 Let A ∈ Cn,m and let A† be the Moore-Penrose inverse of A. Show the following assertions:\\n(a) If rank(A) = m, then A† = (A H A)−1 A H .\\n(b) The matrix X = A† is the uniquely determined matrix that satisfies the following four matrix equations:\\n(1) AX A = A,\\n(2) X AX = X ,\\n(3) (AX ) H = AX ,\\n(4) (X A) H = X A.\\n19.6 Let\\n⎡\\n\\n⎤\\n⎡\\n⎤\\n2 1\\n5\\nA = ⎣ 0 3 ⎦ ∈ R3,2 , b = ⎣ 2 ⎦ ∈ R3,1 .\\n1 −2\\n−5\\nCompute the Moore-Penrose inverse of A and a vector \u0010 x ∈ R2,1 such that\\n(a) \u0006b − A\u0010 x \u00062 ≤ \u0006b − Ay\u00062 for all y ∈ R2,1 , and x \u00062 .\\n(b) \u0006\u0010 x \u00062 ≤ \u0006y\u00062 for all y ∈ R2,1 with \u0006b − Ay\u00062 = \u0006b − A\u0010\\n19.7 Prove the following theorem:\\nLet A ∈ Cn,m and B ∈ C\u0003,m with m ≤ n ≤ \u0003.\\nThen A H A = B H B if and only if B = U A for a matrix U ∈ C\u0003,n with U H U = In .\\nIf A and B are real, then\\nU can also be chosen to be real.\\n(Hint: One direction is trivial.\\nFor the other direction consider the unitary diagonalization of A H A = B H B. This yields the matrix W in the SVD of A and of B. Show the assertion using these two decompositions.\\nThis theorem and its applications can be found in the article [HorO96].)   \n",
       "289                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Chapter 20\\n\\nThe Kronecker Product and Linear Matrix\\nEquations\\n\\nMany applications, in particular the stability analysis of differential equations, lead to linear matrix equations, such as AX + X B = C. Here the matrices A, B, C are given and the goal is to determine a matrix X that solves the equation (we will give a formal definition below).\\nIn the description of the solutions of such equations, the Kronecker product,1 another product of matrices, is useful.\\nIn this chapter we develop the most important properties of this products and we study its application in the context of linear matrix equations.\\nMany more results on this topic can be found in the books [HorJ91, LanT85].\\nDefinition 20.1 If K is a field, A = [ai j ] ∈ K m,m and B ∈ K n,n , then\\n⎤ a11 B · · · a1m B\\n⎢\\n.. ⎥ ,\\nA ⊗ B := [ai j B] = ⎣ ...\\n. ⎦ am1 B · · · amm B\\n⎡ is called the Kronecker product of A and B.\\nThe Kronecker product is sometimes called the tensor product of matrices.\\nThis product defines a map from K m,m × K n,n to K mn,mn .\\nThe definition can be extended to non-square matrices, but for simplicity we consider here only the case of square matrices.\\nThe following lemma presents the basic computational rules of the Kronecker product.\\nLemma 20.2 For all square matrices A, B, C over K , the following computational rules hold:\\n(1) A ⊗ (B ⊗ C) = (A ⊗ B) ⊗ C.\\n1 Leopold\\n\\nKronecker (1832–1891) is said to have used this product in his lectures in Berlin in the\\n1880s.\\nIt was defined formally for the first time in 1858 by Johann Georg Zehfuss (1832–1901).\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7_20\\n\\n303   \n",
       "290                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  304\\n\\n20 The Kronecker Product and Linear Matrix Equations\\n\\n(2)\\n(3)\\n(4)\\n(5)\\n\\n(μA) ⊗ B = A ⊗ (μB) = μ(A ⊗ B) for all μ ∈ K .\\n(A + B) ⊗ C = (A ⊗ C) + (B ⊗ C), whenever A + B is defined.\\nA ⊗ (B + C) = (A ⊗ B) + (A ⊗ C), whenever B + C is defined.\\n(A ⊗ B)T = A T ⊗ B T , and therefore the Kronecker product of two symmetric matrices is symmetric.\\n\u0005\\n\u0004\\n\\nProof Exercise.\\n\\nIn particular, in contrast to the standard matrix multiplication, the order of the factors in the Kronecker product does not change under transposition.\\nThe following result describes the matrix multiplication of two Kronecker products.\\nLemma 20.3 For A, C ∈ K m,m and B, D ∈ K n,n we have\\n(A ⊗ B)(C ⊗ D) = (AC) ⊗ (B D).\\nHence, in particular,\\n(1) A ⊗ B = (A ⊗ In )(Im ⊗ B) = (Im ⊗ B)(A ⊗ In ),\\n(2) (A ⊗ B)−1 = A−1 ⊗ B −1 , if A and B are invertible.\\nProof Since A ⊗ B = [ai j B] and C ⊗ D = [ci j D], the block Fi j ∈ K n,n in the block matrix [Fi j ] = (A ⊗ B)(C ⊗ D) is given by\\nFi j = m m m\\n\b\\n\b\\n\b\\n(aik B)(ck j D) = aik ck j B D = aik ck j B D.\\nk=1 k=1 k=1\\n\\nFor the block matrix [G i j ] = (AC) ⊗ (B D) with G i j ∈ K n,n we obtain\\nG i j = gi j B D, where gi j = m\\n\b aik ck j , k=1 which shows (A ⊗ B)(C ⊗ D) = (AC) ⊗ (B D).\\nNow (1) and (2) easily follow from this equation.\\n\u0005\\n\u0004\\nIn general the Kronecker product is non-commutative (cp.\\nExercise 20.2), but we have the following relationship between A ⊗ B and B ⊗ A.\\nLemma 20.4 For A ∈ K m,m and B ∈ K n,n there exists a permutation matrix\\nP ∈ K mn,mn with\\nP T (A ⊗ B)P = B ⊗ A.\\nProof Exercise.\\n\\n\u0005\\n\u0004\\n\\nFor the computation of the determinant, trace and rank of a Kronecker product there exist simple formulas.   \n",
       "291                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        20 The Kronecker Product and Linear Matrix Equations\\n\\n305\\n\\nTheorem 20.5 For A ∈ K m,m and B ∈ K n,n the following rules hold:\\n(1) det(A ⊗ B) = (det A)n (det B)m = det(B ⊗ A).\\n(2) trace(A ⊗ B) = trace(A) trace(B) = trace(B ⊗ A).\\n(3) rank(A ⊗ B) = rank(A) rank(B) = rank(B ⊗ A).\\nProof (1) From (1) in Lemma 20.3 and the multiplication theorem for determinants\\n(cp.\\nTheorem 7.15) we get det(A ⊗ B) = det ((A ⊗ In ) (Im ⊗ B)) = det(A ⊗ In ) det(Im ⊗ B).\\nBy Lemma 20.4 there exists a permutation matrix P with A⊗ In = P(In ⊗ A)P T .\\nThis implies that det(A ⊗ In ) = det P(In ⊗ A)P T = det(In ⊗ A) = (det A)n .\\nSince det(Im ⊗ B) = (det B)m , it then follows that det(A ⊗ B) = (det A)n\\n(det B)m , and therefore also det(A ⊗ B) = det(B ⊗ A).\\n(2) From (A ⊗ B) = [ai j B] we obtain trace(A ⊗ B) = m \b n\\n\b i=1 j=1 aii b j j = m\\n\b i=1 aii n\\n\b b j j = trace(A) trace(B) j=1\\n\\n= trace(B) trace(A) = trace(B ⊗ A).\\n\u0005\\n\u0004\\n\\n(3) Exercise.\\n\\nFor a matrix A = [a1 , . . . , an ] ∈ K m,n with columns a j ∈ K m,1 , j = 1, . . . , n, we define\\n⎡ ⎤ a1\\n⎢ a2 ⎥\\n⎢ ⎥ vec(A) := ⎢ . ⎥ ∈ K mn,1 .\\n⎣ .. ⎦ an\\nThe application of vec turns the matrix A into a “column vector” and thus “vectorizes”\\nA.\\nLemma 20.6 The map vec : K m,n → K mn,1 is an isomorphism.\\nIn particular,\\nA1 , . . . , Ak ∈ K m,n are linearly independent if and only if vec(A1 ), . . . , vec(Ak ) ∈\\nK mn,1 are linearly independent.\\nProof Exercise.\\n\\n\u0005\\n\u0004\\n\\nWe now consider the relationship between the Kronecker product and the vec map.   \n",
       "292                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      306\\n\\n20 The Kronecker Product and Linear Matrix Equations\\n\\nTheorem 20.7 For A ∈ K m,m , B ∈ K n,n and C ∈ K m,n we have vec(AC B) = (B T ⊗ A)vec(C).\\nHence, in particular,\\n(1) vec(AC) = (In ⊗ A)vec(C) and vec(C B) = (B T ⊗ Im )vec(C),\\n(2) vec(AC + C B) = (In ⊗ A) + (B T ⊗ Im ) vec(C).\\nProof For j = 1, . . . , n, the jth column of AC B is given by\\n(AC B)e j = (AC)(Be j ) = n\\n\b bk j (AC)ek = n\\n\b k=1\\n\\n(bk j A)(Cek ) k=1\\n\\n= [ b1 j A, b2 j A, . . . , bn j A ] vec(C), which implies that vec(AC B) = (B T ⊗ A)vec(C).\\nWith B = In resp.\\nA = Im we obtain (1), while (1) and the linearity of vec yield (2).\\n\u0005\\n\u0004\\nIn order to study the relationship between the eigenvalues of the matrices A, B and those of the Kronecker product A⊗B, we use bivariate polynomials, i.e., polynomials in two variables (cp.\\nExercise 9.10).\\nIf p(t1 , t2 ) = l\\n\b j\\n\\nαi j t1i t2 ∈ K [t1 , t2 ] i, j=0 is such a polynomial, then for A ∈ K m,m and B ∈ K n,n we define the matrix p(A, B) := l\\n\b\\n\\nαi j Ai ⊗ B j .\\n\\n(20.1) i, j=0\\n\\nHere we have to be careful with the order of the factors, since in general Ai ⊗ B j \u0007=\\nB j ⊗ Ai (cp.\\nExercise 20.2).\\nExample 20.8 For A ∈ Rm,m , B ∈ Rn,n and p(t1 , t2 ) = 2t1 +3t1 t22 = 2t11 t20 +3t11 t22 ∈\\nR[t1 , t2 ] we get the matrix p(A, B) = 2 A ⊗ In + 3A ⊗ B 2 .\\nThe following result is known as Stephanos’ theorem.2\\n\\n2 Named after Cyparissos Stephanos (1857–1917) who in 1900 showed besides this result also the assertion of Lemma 20.3.   \n",
       "293                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       20 The Kronecker Product and Linear Matrix Equations\\n\\n307\\n\\nTheorem 20.9 Let A ∈ K m,m and B ∈ K n,n be two matrices that have Jordan normal forms and the eigenvalues λ1 , . . . , λm ∈ K and μ1 , . . . , μn ∈ K , respectively.\\nIf p(A, B) is defined as in (20.1), then the following assertions hold:\\n(1) The eigenvalues of p(A, B) are p(λk , μ\u0002 ) for k = 1, . . . , m and \u0002 = 1, . . . , n.\\n(2) The eigenvalues of A ⊗ B are λk · μ\u0002 for k = 1, . . . , m and \u0002 = 1, . . . , n.\\n(3) The eigenvalues of A⊗In +Im ⊗B are λk +μ\u0002 for k = 1, . . . , m and \u0002 = 1, . . . , n.\\nProof Let S ∈ G L m (K ) and T ∈ G L n (K ) be such that S −1 AS = J A and T −1 BT =\\nJ B are in Jordan canonical form.\\nThe matrices J A and J B are upper triangular.\\nThus, j j for all i, j ∈ N0 the matrices J Ai , J B and J Ai ⊗ J B are upper triangular.\\nThe eigenvalues j j j i i i of J A and J B are λ1 , . . . , λm and μ1 , . . . , μn , respectively.\\nThus, p(λk , μ\u0002 ), k =\\n1, . . . , m, \u0002 = 1, . . . , n, are the diagonal entries of the matrix p(J A , J B ).\\nUsing\\nLemma 20.3 we obtain p(A, B) = l\\n\b\\n\\nαi j (S J A S −1 )i ⊗ (T J B T −1 ) j = i, j=0\\n\\n= l\\n\b l\\n\b\\n\\nαi j (S J Ai S −1 ) ⊗ (T J B T −1 ) j i, j=0\\n\\nαi j (S J Ai ) ⊗ (T J B ) (S −1 ⊗ T −1 ) j i, j=0\\n\\n= l\\n\b i, j=0\\n\\nαi j (S ⊗ T )(J Ai ⊗ J B )(S ⊗ T )−1 j\\n\\n⎛\\n\\n= (S ⊗ T ) ⎝ l\\n\b\\n\\n⎞\\nαi j (J Ai ⊗ J B )⎠ (S ⊗ T )−1 j i, j=0\\n\\n= (S ⊗ T ) p(J A , J B )(S ⊗ T )−1 , which implies (1).\\nThe assertions (2) and (3) follow from (1) with p(t1 , t2 ) = t1 t2 and p(t1 , t2 ) =\\n\u0005\\n\u0004 t1 + t2 , respectively.\\nThe following result on the matrix exponential function of a Kronecker product is helpful in applications that involve systems of linear differential equations.\\nLemma 20.10 For A ∈ Cm,m , B ∈ Cn,n and C := (A ⊗ In ) + (Im ⊗ B) we have exp(C) = exp(A) ⊗ exp(B).\\nProof From Lemma 20.3 we know that the matrices A ⊗ In and Im ⊗ B commute.\\nUsing Lemma 17.6 we obtain   \n",
       "294                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      308\\n\\n20 The Kronecker Product and Linear Matrix Equations exp(C) = exp(A ⊗ In + Im ⊗ B) = exp(A ⊗ In ) exp(Im ⊗ B)\\n⎛\\n⎞\u0011\\n\u0012\\n∞\\n∞\\n\b\\n\b\\n1\\n1 j i\\n(A ⊗ In ) ⎠\\n(Im ⊗ B)\\n=⎝ j!\\ni!\\nj=0 i=0\\n=\\n\\n∞\\n∞\\n\b\\n1 \b1\\n(A ⊗ In ) j (Im ⊗ B)i j!\\ni!\\nj=0 i=0\\n\\n=\\n\\n∞\\n∞\\n\b\\n1 \b1 j\\n(A ⊗ B i ) j!\\ni!\\nj=0 i=0\\n\\n= exp(A) ⊗ exp(B), where we have used the properties of the matrix exponential series\\n(cp.\\nSect.\\n17.1).\\n\u0005\\n\u0004\\nFor given matrices A j ∈ K m,m , B j ∈ K n,n , j = 1, . . . , q, and C ∈ K m,n an equation of the form\\nA1 X B1 + A2 X B2 + . . . + Aq X Bq = C\\n\\n(20.2) is called a linear matrix equation for the unknown matrix X ∈ K m,n .\\nTheorem 20.11 A matrix \u0013\\nX ∈ K m,n solves (20.2) if and only if \u0013 x := vec( \u0013\\nX) ∈ mn,1 solves the linear system of equations\\nK\\nGx = vec(C), where G := q\\n\b\\n\\nB Tj ⊗ A j .\\nj=1\\n\\n\u0005\\n\u0004\\n\\nProof Exercise.\\nWe now consider two special cases of (20.2).\\nTheorem 20.12 For A ∈ Cm,m , B ∈ Cn,n and C ∈ Cm,n the Sylvester equation3\\nAX + X B = C\\n\\n(20.3) has a unique solution if and only if A and −B have no common eigenvalue.\\nIf all eigenvalues of A and B have negative real parts, then the unique solution of (20.3) is given by\\n\u0014∞\\n\u0013\\nX = − exp(t A)C exp(t B)dt.\\n0\\n\\n(As in Sect.\\n17.2 the integral is defined entrywise.)\\n3 James\\n\\nJoseph Sylvester (1814–1897).   \n",
       "295                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             20 The Kronecker Product and Linear Matrix Equations\\n\\n309\\n\\nProof Analogous to the representation in Theorem 20.11, we can write the Sylvester equation (20.3) as\\n(In ⊗ A + B T ⊗ Im )x = vec(C).\\nIf A and B have the eigenvalues λ1 , . . . , λm and μ1 , . . . , μn , respectively, then G =\\nIn ⊗ A + B T ⊗ Im by (3) in Theorem 20.9 has the eigenvalues λk + μ\u0002 , k = 1, . . . , m,\\n\u0002 = 1, . . . , n.\\nThus, G is invertible, and the Sylvester equation is uniquely solvable, if and only if λk + μ\u0002 \u0007= 0 for all k = 1, . . . , m and \u0002 = 1, . . . , n.\\nLet A and B be matrices with eigenvalues that have negative real parts.\\nThen A and\\n−B have no common eigenvalues and (20.3) has a unique solution.\\nLet J A = S −1 AS and J B = T −1 BT be Jordan canonical forms of A and B. We consider the linear differential equation dZ\\n= AZ + Z B, Z (0) = C,\\n(20.4) dt that is solved by the function\\nZ : [0, ∞) → Cm,n ,\\n\\nZ (t) := exp(t A)C exp(t B)\\n\\n(cp.\\nExercise 20.10).\\nThis function satisfies lim Z (t) = lim exp(t A)C exp(t B) t→∞ t→∞\\n\\n= lim S exp(t J A ) \u0015S −1\u0016\u0017C T\u0018 exp(t J B ) T −1 = 0.\\nt→∞\\n\u0015 \u0016\u0017 \u0018\\n\u0015 \u0016\u0017 \u0018 constant\\n\\n→0\\n\\n→0\\n\\nIntegration of equation (20.4) from t = 0 to t = ∞ yields\\n\u0014∞\\n−C = − Z (0) = lim (Z (t) − Z (0)) = A t→∞\\n\\n⎛∞\\n⎞\\n\u0014\\nZ (t)dt + ⎝ Z (t)dt ⎠ B.\\n\\n0\\n\\n0\\n\\n(Here we use without proof the existence of the infinite integrals.)\\nThis implies that\\n\u0013\\nX := −\\n\\n\u0014∞\\n\\n\u0014∞\\nZ (t)dt = −\\n\\n0 exp(t A)C exp(t B)dt\\n0\\n\\n\u0005\\n\u0004 is the unique solution of (20.3).\\nTheorem 20.12 also gives the solution of another important matrix equation.\\nCorollary 20.13 For A, C ∈ Cn,n the Lyapunov equation4\\nAX + X A H = −C\\n4 Alexandr\\n\\nMikhailovich Lyapunov (also Ljapunov or Liapunov; 1857–1918).\\n\\n(20.5)   \n",
       "296                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              310\\n\\n20 The Kronecker Product and Linear Matrix Equations has a unique solution \u0013\\nX ∈ Cn,n if the eigenvalues of A have negative real parts.\\nIf, furthermore, C is Hermitian positive definite, then also \u0013\\nX is Hermitian positive definite.\\nProof Since by assumption A and −A H have no common eigenvalues, the unique solvability of (20.5) follows from Theorem 20.12, and the solution is given by the matrix\\n\u0013\\nX =−\\n\\n\u0014∞\\n\\n\u0014∞ exp(t A)(−C) exp t A\\n\\nH dt =\\n\\n0 exp(t A)C exp t A H dt.\\n0\\n\\nIf C is Hermitian positive definite, then \u0013\\nX is Hermitian and for x ∈ Cn,1 \\ {0} we have\\n⎞\\n⎛∞\\n\u0014\\n\u0014∞\\nH\u0013\\nH⎝\\nH x Xx = x exp(t A)C exp t A dt ⎠ x = x H exp(t A)C exp t A H x dt > 0.\\n\u0016\u0017\\n\u0018\\n\u0015\\n0\\n\\n0\\n\\n>0\\n\\nThe last inequality follows from the monotonicity of the integral and the fact that for\\n\u0005\\n\u0004 x \u0007= 0 also exp(t A H )x \u0007= 0, since exp t A H is invertible for every real t.\\nExercises\\n20.1\\n20.2\\n20.3\\n20.4\\n20.5\\n20.6\\n20.7\\n20.8\\n20.9\\n20.10\\n\\nProve Lemma 20.2.\\nConstruct two square matrices A, B with A ⊗ B \u0007= B ⊗ A.\\nProve Lemma 20.4.\\nProve Theorem 20.5 (3).\\nProve Lemma 20.6.\\nShow that A ⊗ B is normal if A ∈ Cm,m and B ∈ Cn,n are normal.\\nIs it true that if A ⊗ B is unitary, then A and B are unitary?\\nUse the singular value decompositions of A = V A \u0003 A W AH ∈ Cm,m and B =\\nVB \u0003 B W BH ∈ Cn,n to derive the singular value decomposition of A ⊗ B.\\nShow that for A ∈ Cm,m and B ∈ Cn,n and the matrix 2-norm, the equation\\nA ⊗ B 2 = A 2 B 2 holds.\\nProve Theorem 20.11.\\nLet A ∈ Cm,m , B ∈ Cn,n and C ∈ Cm,n .\\nShow that Z (t) = exp(t A)C exp(t B) is the solution of the matrix differential equation ddtZ = AZ + Z B with the initial condition Z (0) = C.   \n",
       "297                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Appendix A\\n\\nA Short Introduction to MATLAB\\n\\nMATLAB1 is an interactive software system for numerical computations, simulations and visualizations.\\nIt contains a large number of predefined functions and allows users to implement their programs in so-called m-files.\\nThe name MATLAB originates from MATrix LABoratory, which indicates the matrix orientation of the software.\\nIndeed, matrices are the major objects in MATLAB.\\nDue to the simple and intuitive use of matrices, we consider MATLAB well suited for teaching in the field of Linear Algebra.\\nIn this short introduction we explain the most important ways to enter and operate with matrices in MATLAB.\\nOne can learn the essential matrix operations as well as important algorithms and concepts in the context of matrices (and Linear Algebra in general) by actively using the MATLAB-Minutes in this book.\\nThese only use predefined functions.\\nA matrix in MATLAB can be entered in form of a list of entries enclosed by square brackets.\\nThe entries in the list are ordered by rows in the natural order of the indices, i.e., from “top to bottom” and “left to right”).\\nA new row starts after every semicolon.\\nFor example, the matrix\\n⎡\\n\\n⎤\\n123\\nA = ⎣4 5 6⎦ is entered in MATLAB by typing A=[1 2 3;4 5 6;7 8 9];\\n789\\n\\nA semicolon after the matrix A suppresses the output in MATLAB.\\nIf it is omitted then MATLAB writes out all the entered or computed quantities.\\nFor example, after entering\\nA=[1 2 3;4 5 6;7 8 9]\\n\\n1 MATLAB® is a registered trademark of The MathWorks Inc.\\n\\n© Springer International Publishing Switzerland 2015\\nJ. Liesen and V. Mehrmann, Linear Algebra, Springer Undergraduate\\nMathematics Series, DOI 10.1007/978-3-319-24346-7\\n\\n311   \n",
       "298                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  312\\n\\nAppendix A: A Short Introduction to MATLAB\\n\\nMATLAB gives the output\\nA\\n\\n=\\n1\\n4\\n7\\n\\n2\\n5\\n8\\n\\n3\\n6\\n9\\n\\nOne can access parts of matrices by the corresponding indices.\\nThe list of indices from k to m is abbreviated by k:m .\\nA colon : means all rows for given column indices, or all columns for given row indices.\\nIf A is as above, then for example\\nA(2,1) is the matrix\\n\\nA(3,1:2) is the matrix\\n\\nA(:,2:3) is the matrix\\n\\n[4],\\n[7 8],\\n\u0006\\n\u0007\\n2\\n5\\n8\\n\\n3\\n6 .\\n9\\n\\nThere are several predefined functions that produce matrices.\\nIn particular, for given positive integers n and m, eye(n) the identity matrix In , zeros(n,m) ones(n,m) an n × m matrix with all zeros, an n × m matrix with all ones, rand(n,m) an n × m “random matrix”.\\n\\nSeveral matrices (of appropriate sizes) be combined to a new matrix.\\nFor example, the commands\\nA=eye(2);\\n\\nB=[4;3];\\n\\nC=[2 -1];\\n\\nD=[-5]; E=[A B;C D] lead to\\nE\\n\\n=\\n1\\n0\\n2\\n\\n0\\n1\\n-1\\n\\n4\\n3\\n-5\\n\\nThe help function in MATLAB is started with the command help.\\nIn order to get information about specific functions one adds the name of the function.\\nFor example:   \n",
       "299                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Appendix A: A Short Introduction to MATLAB\\n\\nInput: help ops\\n\\nInformation on: operations and operators in MATLAB\\n(in particular addition, multiplication, transposition) help matfun MATLAB functions that operate with matrices help gallery collection of example matrices help det determinant help expm matrix exponential function\\n\\n313   \n",
       "\n",
       "     page_number real_page_num  \\\n",
       "0           12.0        <FEFF>   \n",
       "1           13.0            13   \n",
       "2           14.0            14   \n",
       "3           15.0            15   \n",
       "4           16.0            16   \n",
       "5           17.0            17   \n",
       "6           18.0            18   \n",
       "7           19.0        <FEFF>   \n",
       "8           20.0            20   \n",
       "9           21.0            21   \n",
       "10          22.0            22   \n",
       "11          23.0            23   \n",
       "12          24.0            24   \n",
       "13          25.0            25   \n",
       "14          26.0            26   \n",
       "15          27.0            27   \n",
       "16          28.0            28   \n",
       "17          29.0            29   \n",
       "18          30.0            30   \n",
       "19          31.0            31   \n",
       "20          32.0        <FEFF>   \n",
       "21          33.0            33   \n",
       "22          34.0            34   \n",
       "23          35.0            35   \n",
       "24          36.0            36   \n",
       "25          37.0            37   \n",
       "26          38.0            38   \n",
       "27          39.0            39   \n",
       "28          40.0            40   \n",
       "29          41.0            41   \n",
       "30          42.0            42   \n",
       "31          43.0            43   \n",
       "32          44.0            44   \n",
       "33          45.0        <FEFF>   \n",
       "34          46.0            46   \n",
       "35          47.0            47   \n",
       "36          48.0            48   \n",
       "37          49.0            49   \n",
       "38          50.0            50   \n",
       "39          51.0            51   \n",
       "40          52.0            52   \n",
       "41          53.0            53   \n",
       "42          54.0            54   \n",
       "43          55.0            55   \n",
       "44          56.0            56   \n",
       "45          57.0            57   \n",
       "46          58.0            58   \n",
       "47          59.0            59   \n",
       "48          60.0            60   \n",
       "49          61.0            61   \n",
       "50          62.0        <FEFF>   \n",
       "51          63.0            63   \n",
       "52          64.0            64   \n",
       "53          65.0            65   \n",
       "54          66.0            66   \n",
       "55          67.0            67   \n",
       "56          68.0            68   \n",
       "57          69.0            69   \n",
       "58          70.0            70   \n",
       "59          71.0            71   \n",
       "60          72.0            72   \n",
       "61          73.0            73   \n",
       "62          74.0            74   \n",
       "63          75.0            75   \n",
       "64          76.0            76   \n",
       "65          77.0            77   \n",
       "66          78.0            78   \n",
       "67          79.0        <FEFF>   \n",
       "68          80.0            80   \n",
       "69          81.0            81   \n",
       "70          82.0            82   \n",
       "71          83.0            83   \n",
       "72          84.0            84   \n",
       "73          85.0            85   \n",
       "74          86.0        <FEFF>   \n",
       "75          87.0            87   \n",
       "76          88.0            88   \n",
       "77          89.0            89   \n",
       "78          90.0            90   \n",
       "79          91.0            91   \n",
       "80          92.0            92   \n",
       "81          93.0            93   \n",
       "82          94.0            94   \n",
       "83          95.0            95   \n",
       "84          96.0            96   \n",
       "85          97.0            97   \n",
       "86          98.0            98   \n",
       "87          99.0            99   \n",
       "88         100.0           100   \n",
       "89         101.0           101   \n",
       "90         102.0           102   \n",
       "91         103.0           103   \n",
       "92         104.0           104   \n",
       "93         105.0        <FEFF>   \n",
       "94         106.0           106   \n",
       "95         107.0           107   \n",
       "96         108.0           108   \n",
       "97         109.0           109   \n",
       "98         110.0           110   \n",
       "99         111.0           111   \n",
       "100        112.0           112   \n",
       "101        113.0           113   \n",
       "102        114.0           114   \n",
       "103        115.0           115   \n",
       "104        116.0           116   \n",
       "105        117.0           117   \n",
       "106        118.0        <FEFF>   \n",
       "107        119.0           119   \n",
       "108        120.0           120   \n",
       "109        121.0           121   \n",
       "110        122.0           122   \n",
       "111        123.0           123   \n",
       "112        124.0           124   \n",
       "113        125.0           125   \n",
       "114        126.0           126   \n",
       "115        127.0           127   \n",
       "116        128.0           128   \n",
       "117        129.0           129   \n",
       "118        130.0           130   \n",
       "119        131.0           131   \n",
       "120        132.0           132   \n",
       "121        133.0           133   \n",
       "122        134.0           134   \n",
       "123        135.0           135   \n",
       "124        136.0           136   \n",
       "125        137.0        <FEFF>   \n",
       "126        138.0           138   \n",
       "127        139.0           139   \n",
       "128        140.0           140   \n",
       "129        141.0           141   \n",
       "130        142.0           142   \n",
       "131        143.0           143   \n",
       "132        144.0           144   \n",
       "133        145.0           145   \n",
       "134        146.0           146   \n",
       "135        147.0           147   \n",
       "136        148.0           148   \n",
       "137        149.0           149   \n",
       "138        150.0           150   \n",
       "139        151.0           151   \n",
       "140        152.0           152   \n",
       "141        153.0           153   \n",
       "142        154.0           154   \n",
       "143        155.0           155   \n",
       "144        156.0           156   \n",
       "145        157.0           157   \n",
       "146        158.0           158   \n",
       "147        159.0           159   \n",
       "148        160.0           160   \n",
       "149        161.0           161   \n",
       "150        162.0           162   \n",
       "151        163.0           163   \n",
       "152        164.0           164   \n",
       "153        165.0           165   \n",
       "154        166.0           166   \n",
       "155        167.0           167   \n",
       "156        168.0           168   \n",
       "157        169.0           169   \n",
       "158        170.0           170   \n",
       "159        171.0           171   \n",
       "160        172.0           172   \n",
       "161        173.0           173   \n",
       "162        174.0           174   \n",
       "163        175.0           175   \n",
       "164        176.0           176   \n",
       "165        177.0           177   \n",
       "166        178.0           178   \n",
       "167        179.0           179   \n",
       "168        180.0           180   \n",
       "169        181.0           181   \n",
       "170        182.0           182   \n",
       "171        183.0           183   \n",
       "172        184.0           184   \n",
       "173        185.0           185   \n",
       "174        186.0           186   \n",
       "175        187.0           187   \n",
       "176        188.0           188   \n",
       "177        189.0           189   \n",
       "178        190.0           190   \n",
       "179        191.0           191   \n",
       "180        192.0           192   \n",
       "181        193.0           193   \n",
       "182        194.0           194   \n",
       "183        195.0           195   \n",
       "184        196.0           196   \n",
       "185        197.0           197   \n",
       "186        198.0           198   \n",
       "187        199.0           199   \n",
       "188        200.0        <FEFF>   \n",
       "189        201.0           201   \n",
       "190        202.0           202   \n",
       "191        203.0           203   \n",
       "192        204.0           204   \n",
       "193        205.0           205   \n",
       "194        206.0           206   \n",
       "195        207.0           207   \n",
       "196        208.0           208   \n",
       "197        209.0           209   \n",
       "198        210.0           210   \n",
       "199        211.0           211   \n",
       "200        212.0           212   \n",
       "201        213.0           213   \n",
       "202        214.0           214   \n",
       "203        215.0           215   \n",
       "204        216.0           216   \n",
       "205        217.0           217   \n",
       "206        218.0           218   \n",
       "207        219.0           219   \n",
       "208        220.0           220   \n",
       "209        221.0           221   \n",
       "210        222.0           222   \n",
       "211        223.0           223   \n",
       "212        224.0           224   \n",
       "213        225.0           225   \n",
       "214        226.0           226   \n",
       "215        227.0           227   \n",
       "216        228.0           228   \n",
       "217        229.0           229   \n",
       "218        230.0           230   \n",
       "219        231.0           231   \n",
       "220        232.0           232   \n",
       "221        233.0           233   \n",
       "222        234.0           234   \n",
       "223        235.0           235   \n",
       "224        236.0           236   \n",
       "225        237.0           237   \n",
       "226        238.0           238   \n",
       "227        239.0           239   \n",
       "228        240.0           240   \n",
       "229        241.0           241   \n",
       "230        242.0           242   \n",
       "231        243.0           243   \n",
       "232        244.0           244   \n",
       "233        245.0           245   \n",
       "234        246.0           246   \n",
       "235        247.0           247   \n",
       "236        248.0           248   \n",
       "237        249.0           249   \n",
       "238        250.0           250   \n",
       "239        251.0           251   \n",
       "240        252.0           252   \n",
       "241        253.0        <FEFF>   \n",
       "242        254.0           254   \n",
       "243        255.0           255   \n",
       "244        256.0           256   \n",
       "245        257.0           257   \n",
       "246        258.0           258   \n",
       "247        259.0           259   \n",
       "248        260.0           260   \n",
       "249        261.0           261   \n",
       "250        262.0           262   \n",
       "251        263.0           263   \n",
       "252        264.0           264   \n",
       "253        265.0           265   \n",
       "254        266.0           266   \n",
       "255        267.0           267   \n",
       "256        268.0           268   \n",
       "257        269.0           269   \n",
       "258        270.0        <FEFF>   \n",
       "259        271.0           271   \n",
       "260        272.0           272   \n",
       "261        273.0           273   \n",
       "262        274.0           274   \n",
       "263        275.0           275   \n",
       "264        276.0           276   \n",
       "265        277.0           277   \n",
       "266        278.0           278   \n",
       "267        279.0           279   \n",
       "268        280.0           280   \n",
       "269        281.0           281   \n",
       "270        282.0           282   \n",
       "271        283.0           283   \n",
       "272        284.0           284   \n",
       "273        285.0           285   \n",
       "274        286.0           286   \n",
       "275        287.0           287   \n",
       "276        288.0           288   \n",
       "277        289.0           289   \n",
       "278        290.0           290   \n",
       "279        291.0           291   \n",
       "280        292.0           292   \n",
       "281        293.0        <FEFF>   \n",
       "282        294.0           294   \n",
       "283        295.0           295   \n",
       "284        296.0           296   \n",
       "285        297.0           297   \n",
       "286        298.0           298   \n",
       "287        299.0           299   \n",
       "288        300.0           300   \n",
       "289        301.0           301   \n",
       "290        302.0           302   \n",
       "291        303.0           303   \n",
       "292        304.0           304   \n",
       "293        305.0           305   \n",
       "294        306.0           306   \n",
       "295        307.0           307   \n",
       "296        308.0           308   \n",
       "297        309.0           309   \n",
       "298        310.0           310   \n",
       "299        311.0           311   \n",
       "\n",
       "                                                  section_level_1  \\\n",
       "0                              1 Linear Algebra in Every Day Life   \n",
       "1                              1 Linear Algebra in Every Day Life   \n",
       "2                              1 Linear Algebra in Every Day Life   \n",
       "3                              1 Linear Algebra in Every Day Life   \n",
       "4                              1 Linear Algebra in Every Day Life   \n",
       "5                              1 Linear Algebra in Every Day Life   \n",
       "6                              1 Linear Algebra in Every Day Life   \n",
       "7                                   2 Basic Mathematical Concepts   \n",
       "8                                   2 Basic Mathematical Concepts   \n",
       "9                                   2 Basic Mathematical Concepts   \n",
       "10                                  2 Basic Mathematical Concepts   \n",
       "11                                  2 Basic Mathematical Concepts   \n",
       "12                                  2 Basic Mathematical Concepts   \n",
       "13                                  2 Basic Mathematical Concepts   \n",
       "14                                  2 Basic Mathematical Concepts   \n",
       "15                                  2 Basic Mathematical Concepts   \n",
       "16                                  2 Basic Mathematical Concepts   \n",
       "17                                  2 Basic Mathematical Concepts   \n",
       "18                                  2 Basic Mathematical Concepts   \n",
       "19                                  2 Basic Mathematical Concepts   \n",
       "20                                         3 Algebraic Structures   \n",
       "21                                         3 Algebraic Structures   \n",
       "22                                         3 Algebraic Structures   \n",
       "23                                         3 Algebraic Structures   \n",
       "24                                         3 Algebraic Structures   \n",
       "25                                         3 Algebraic Structures   \n",
       "26                                         3 Algebraic Structures   \n",
       "27                                         3 Algebraic Structures   \n",
       "28                                         3 Algebraic Structures   \n",
       "29                                         3 Algebraic Structures   \n",
       "30                                         3 Algebraic Structures   \n",
       "31                                         3 Algebraic Structures   \n",
       "32                                         3 Algebraic Structures   \n",
       "33                                                     4 Matrices   \n",
       "34                                                     4 Matrices   \n",
       "35                                                     4 Matrices   \n",
       "36                                                     4 Matrices   \n",
       "37                                                     4 Matrices   \n",
       "38                                                     4 Matrices   \n",
       "39                                                     4 Matrices   \n",
       "40                                                     4 Matrices   \n",
       "41                                                     4 Matrices   \n",
       "42                                                     4 Matrices   \n",
       "43                                                     4 Matrices   \n",
       "44                                                     4 Matrices   \n",
       "45                                                     4 Matrices   \n",
       "46                                                     4 Matrices   \n",
       "47                                                     4 Matrices   \n",
       "48                                                     4 Matrices   \n",
       "49                                                     4 Matrices   \n",
       "50                    5 The Echelon Form and the Rank of Matrices   \n",
       "51                    5 The Echelon Form and the Rank of Matrices   \n",
       "52                    5 The Echelon Form and the Rank of Matrices   \n",
       "53                    5 The Echelon Form and the Rank of Matrices   \n",
       "54                    5 The Echelon Form and the Rank of Matrices   \n",
       "55                    5 The Echelon Form and the Rank of Matrices   \n",
       "56                    5 The Echelon Form and the Rank of Matrices   \n",
       "57                    5 The Echelon Form and the Rank of Matrices   \n",
       "58                    5 The Echelon Form and the Rank of Matrices   \n",
       "59                    5 The Echelon Form and the Rank of Matrices   \n",
       "60                    5 The Echelon Form and the Rank of Matrices   \n",
       "61                    5 The Echelon Form and the Rank of Matrices   \n",
       "62                    5 The Echelon Form and the Rank of Matrices   \n",
       "63                    5 The Echelon Form and the Rank of Matrices   \n",
       "64                    5 The Echelon Form and the Rank of Matrices   \n",
       "65                    5 The Echelon Form and the Rank of Matrices   \n",
       "66                    5 The Echelon Form and the Rank of Matrices   \n",
       "67                                  6 Linear Systems of Equations   \n",
       "68                                  6 Linear Systems of Equations   \n",
       "69                                  6 Linear Systems of Equations   \n",
       "70                                  6 Linear Systems of Equations   \n",
       "71                                  6 Linear Systems of Equations   \n",
       "72                                  6 Linear Systems of Equations   \n",
       "73                                  6 Linear Systems of Equations   \n",
       "74                                     7 Determinants of Matrices   \n",
       "75                                     7 Determinants of Matrices   \n",
       "76                                     7 Determinants of Matrices   \n",
       "77                                     7 Determinants of Matrices   \n",
       "78                                     7 Determinants of Matrices   \n",
       "79                                     7 Determinants of Matrices   \n",
       "80                                     7 Determinants of Matrices   \n",
       "81                                     7 Determinants of Matrices   \n",
       "82                                     7 Determinants of Matrices   \n",
       "83                                     7 Determinants of Matrices   \n",
       "84                                     7 Determinants of Matrices   \n",
       "85                                     7 Determinants of Matrices   \n",
       "86                                     7 Determinants of Matrices   \n",
       "87                                     7 Determinants of Matrices   \n",
       "88                                     7 Determinants of Matrices   \n",
       "89                                     7 Determinants of Matrices   \n",
       "90                                     7 Determinants of Matrices   \n",
       "91                                     7 Determinants of Matrices   \n",
       "92                                     7 Determinants of Matrices   \n",
       "93   8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "94   8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "95   8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "96   8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "97   8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "98   8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "99   8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "100  8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "101  8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "102  8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "103  8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "104  8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "105  8 The Characteristic Polynomial  and Eigenvalues of Matrices   \n",
       "106                                               9 Vector Spaces   \n",
       "107                                               9 Vector Spaces   \n",
       "108                                               9 Vector Spaces   \n",
       "109                                               9 Vector Spaces   \n",
       "110                                               9 Vector Spaces   \n",
       "111                                               9 Vector Spaces   \n",
       "112                                               9 Vector Spaces   \n",
       "113                                               9 Vector Spaces   \n",
       "114                                               9 Vector Spaces   \n",
       "115                                               9 Vector Spaces   \n",
       "116                                               9 Vector Spaces   \n",
       "117                                               9 Vector Spaces   \n",
       "118                                               9 Vector Spaces   \n",
       "119                                               9 Vector Spaces   \n",
       "120                                               9 Vector Spaces   \n",
       "121                                               9 Vector Spaces   \n",
       "122                                               9 Vector Spaces   \n",
       "123                                               9 Vector Spaces   \n",
       "124                                               9 Vector Spaces   \n",
       "125                                                10 Linear Maps   \n",
       "126                                                10 Linear Maps   \n",
       "127                                                10 Linear Maps   \n",
       "128                                                10 Linear Maps   \n",
       "129                                                10 Linear Maps   \n",
       "130                                                10 Linear Maps   \n",
       "131                                                10 Linear Maps   \n",
       "132                                                10 Linear Maps   \n",
       "133                                                10 Linear Maps   \n",
       "134                                                10 Linear Maps   \n",
       "135                                                10 Linear Maps   \n",
       "136                                                10 Linear Maps   \n",
       "137                                                10 Linear Maps   \n",
       "138                                                10 Linear Maps   \n",
       "139                                                10 Linear Maps   \n",
       "140                                                10 Linear Maps   \n",
       "141                                                10 Linear Maps   \n",
       "142                                                10 Linear Maps   \n",
       "143                                                10 Linear Maps   \n",
       "144                                                10 Linear Maps   \n",
       "145                            11 Linear Forms and Bilinear Forms   \n",
       "146                            11 Linear Forms and Bilinear Forms   \n",
       "147                            11 Linear Forms and Bilinear Forms   \n",
       "148                            11 Linear Forms and Bilinear Forms   \n",
       "149                            11 Linear Forms and Bilinear Forms   \n",
       "150                            11 Linear Forms and Bilinear Forms   \n",
       "151                            11 Linear Forms and Bilinear Forms   \n",
       "152                            11 Linear Forms and Bilinear Forms   \n",
       "153                            11 Linear Forms and Bilinear Forms   \n",
       "154                            11 Linear Forms and Bilinear Forms   \n",
       "155                            11 Linear Forms and Bilinear Forms   \n",
       "156                            11 Linear Forms and Bilinear Forms   \n",
       "157                        12 Euclidean and Unitary Vector Spaces   \n",
       "158                        12 Euclidean and Unitary Vector Spaces   \n",
       "159                        12 Euclidean and Unitary Vector Spaces   \n",
       "160                        12 Euclidean and Unitary Vector Spaces   \n",
       "161                        12 Euclidean and Unitary Vector Spaces   \n",
       "162                        12 Euclidean and Unitary Vector Spaces   \n",
       "163                        12 Euclidean and Unitary Vector Spaces   \n",
       "164                        12 Euclidean and Unitary Vector Spaces   \n",
       "165                        12 Euclidean and Unitary Vector Spaces   \n",
       "166                        12 Euclidean and Unitary Vector Spaces   \n",
       "167                        12 Euclidean and Unitary Vector Spaces   \n",
       "168                        12 Euclidean and Unitary Vector Spaces   \n",
       "169                        12 Euclidean and Unitary Vector Spaces   \n",
       "170                        12 Euclidean and Unitary Vector Spaces   \n",
       "171                        12 Euclidean and Unitary Vector Spaces   \n",
       "172                        12 Euclidean and Unitary Vector Spaces   \n",
       "173                        12 Euclidean and Unitary Vector Spaces   \n",
       "174                        12 Euclidean and Unitary Vector Spaces   \n",
       "175                        12 Euclidean and Unitary Vector Spaces   \n",
       "176                        12 Euclidean and Unitary Vector Spaces   \n",
       "177                                    13 Adjoints of Linear Maps   \n",
       "178                                    13 Adjoints of Linear Maps   \n",
       "179                                    13 Adjoints of Linear Maps   \n",
       "180                                    13 Adjoints of Linear Maps   \n",
       "181                                    13 Adjoints of Linear Maps   \n",
       "182                                    13 Adjoints of Linear Maps   \n",
       "183                                    13 Adjoints of Linear Maps   \n",
       "184                                    13 Adjoints of Linear Maps   \n",
       "185                                    13 Adjoints of Linear Maps   \n",
       "186                                    13 Adjoints of Linear Maps   \n",
       "187                                    13 Adjoints of Linear Maps   \n",
       "188                               14 Eigenvalues of Endomorphisms   \n",
       "189                               14 Eigenvalues of Endomorphisms   \n",
       "190                               14 Eigenvalues of Endomorphisms   \n",
       "191                               14 Eigenvalues of Endomorphisms   \n",
       "192                               14 Eigenvalues of Endomorphisms   \n",
       "193                               14 Eigenvalues of Endomorphisms   \n",
       "194                               14 Eigenvalues of Endomorphisms   \n",
       "195                               14 Eigenvalues of Endomorphisms   \n",
       "196                               14 Eigenvalues of Endomorphisms   \n",
       "197                               14 Eigenvalues of Endomorphisms   \n",
       "198                               14 Eigenvalues of Endomorphisms   \n",
       "199                               14 Eigenvalues of Endomorphisms   \n",
       "200                               14 Eigenvalues of Endomorphisms   \n",
       "201                               14 Eigenvalues of Endomorphisms   \n",
       "202         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "203         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "204         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "205         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "206         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "207         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "208         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "209         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "210         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "211         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "212         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "213         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "214         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "215         15 Polynomials and the Fundamental Theorem of Algebra   \n",
       "216    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "217    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "218    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "219    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "220    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "221    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "222    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "223    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "224    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "225    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "226    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "227    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "228    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "229    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "230    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "231    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "232    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "233    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "234    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "235    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "236    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "237    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "238    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "239    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "240    16 Cyclic Subspaces, Duality and the Jordan Canonical Form   \n",
       "241    17 Matrix Functions and Systems  of Differential Equations   \n",
       "242    17 Matrix Functions and Systems  of Differential Equations   \n",
       "243    17 Matrix Functions and Systems  of Differential Equations   \n",
       "244    17 Matrix Functions and Systems  of Differential Equations   \n",
       "245    17 Matrix Functions and Systems  of Differential Equations   \n",
       "246    17 Matrix Functions and Systems  of Differential Equations   \n",
       "247    17 Matrix Functions and Systems  of Differential Equations   \n",
       "248    17 Matrix Functions and Systems  of Differential Equations   \n",
       "249    17 Matrix Functions and Systems  of Differential Equations   \n",
       "250    17 Matrix Functions and Systems  of Differential Equations   \n",
       "251    17 Matrix Functions and Systems  of Differential Equations   \n",
       "252    17 Matrix Functions and Systems  of Differential Equations   \n",
       "253    17 Matrix Functions and Systems  of Differential Equations   \n",
       "254    17 Matrix Functions and Systems  of Differential Equations   \n",
       "255    17 Matrix Functions and Systems  of Differential Equations   \n",
       "256    17 Matrix Functions and Systems  of Differential Equations   \n",
       "257    17 Matrix Functions and Systems  of Differential Equations   \n",
       "258                           18 Special Classes of Endomorphisms   \n",
       "259                           18 Special Classes of Endomorphisms   \n",
       "260                           18 Special Classes of Endomorphisms   \n",
       "261                           18 Special Classes of Endomorphisms   \n",
       "262                           18 Special Classes of Endomorphisms   \n",
       "263                           18 Special Classes of Endomorphisms   \n",
       "264                           18 Special Classes of Endomorphisms   \n",
       "265                           18 Special Classes of Endomorphisms   \n",
       "266                           18 Special Classes of Endomorphisms   \n",
       "267                           18 Special Classes of Endomorphisms   \n",
       "268                           18 Special Classes of Endomorphisms   \n",
       "269                           18 Special Classes of Endomorphisms   \n",
       "270                           18 Special Classes of Endomorphisms   \n",
       "271                           18 Special Classes of Endomorphisms   \n",
       "272                           18 Special Classes of Endomorphisms   \n",
       "273                           18 Special Classes of Endomorphisms   \n",
       "274                           18 Special Classes of Endomorphisms   \n",
       "275                           18 Special Classes of Endomorphisms   \n",
       "276                           18 Special Classes of Endomorphisms   \n",
       "277                           18 Special Classes of Endomorphisms   \n",
       "278                           18 Special Classes of Endomorphisms   \n",
       "279                           18 Special Classes of Endomorphisms   \n",
       "280                           18 Special Classes of Endomorphisms   \n",
       "281                           19 The Singular Value Decomposition   \n",
       "282                           19 The Singular Value Decomposition   \n",
       "283                           19 The Singular Value Decomposition   \n",
       "284                           19 The Singular Value Decomposition   \n",
       "285                           19 The Singular Value Decomposition   \n",
       "286                           19 The Singular Value Decomposition   \n",
       "287                           19 The Singular Value Decomposition   \n",
       "288                           19 The Singular Value Decomposition   \n",
       "289          20 The Kronecker Product and Linear Matrix Equations   \n",
       "290          20 The Kronecker Product and Linear Matrix Equations   \n",
       "291          20 The Kronecker Product and Linear Matrix Equations   \n",
       "292          20 The Kronecker Product and Linear Matrix Equations   \n",
       "293          20 The Kronecker Product and Linear Matrix Equations   \n",
       "294          20 The Kronecker Product and Linear Matrix Equations   \n",
       "295          20 The Kronecker Product and Linear Matrix Equations   \n",
       "296          20 The Kronecker Product and Linear Matrix Equations   \n",
       "297                                                    Appendix A   \n",
       "298                                                    Appendix A   \n",
       "299                                                    Appendix A   \n",
       "\n",
       "                                                        section_level_2  \\\n",
       "0                                            1.1 The PageRank Algorithm   \n",
       "1                                            1.1 The PageRank Algorithm   \n",
       "2                            1.2 No Claim Discounting in Car Insurances   \n",
       "3                                    1.3 Production Planning in a Plant   \n",
       "4                                         1.4 Predicting Future Profits   \n",
       "5                                                1.5 Circuit Simulation   \n",
       "6                                                1.5 Circuit Simulation   \n",
       "7                                       2.1 Sets and Mathematical Logic   \n",
       "8                                       2.1 Sets and Mathematical Logic   \n",
       "9                                       2.1 Sets and Mathematical Logic   \n",
       "10                                      2.1 Sets and Mathematical Logic   \n",
       "11                                      2.1 Sets and Mathematical Logic   \n",
       "12                                                             2.2 Maps   \n",
       "13                                                             2.2 Maps   \n",
       "14                                                             2.2 Maps   \n",
       "15                                                        2.3 Relations   \n",
       "16                                                        2.3 Relations   \n",
       "17                                                        2.3 Relations   \n",
       "18                                                        2.3 Relations   \n",
       "19                                                        2.3 Relations   \n",
       "20                                                           3.1 Groups   \n",
       "21                                                           3.1 Groups   \n",
       "22                                                           3.1 Groups   \n",
       "23                                                 3.2 Rings and Fields   \n",
       "24                                                 3.2 Rings and Fields   \n",
       "25                                                 3.2 Rings and Fields   \n",
       "26                                                 3.2 Rings and Fields   \n",
       "27                                                 3.2 Rings and Fields   \n",
       "28                                                 3.2 Rings and Fields   \n",
       "29                                                 3.2 Rings and Fields   \n",
       "30                                                 3.2 Rings and Fields   \n",
       "31                                                 3.2 Rings and Fields   \n",
       "32                                                 3.2 Rings and Fields   \n",
       "33                                 4.1 Basic Definitions and Operations   \n",
       "34                                 4.1 Basic Definitions and Operations   \n",
       "35                                 4.1 Basic Definitions and Operations   \n",
       "36                                 4.1 Basic Definitions and Operations   \n",
       "37                                 4.1 Basic Definitions and Operations   \n",
       "38                                 4.1 Basic Definitions and Operations   \n",
       "39                                 4.1 Basic Definitions and Operations   \n",
       "40                                          4.2 Matrix Groups and Rings   \n",
       "41                                          4.2 Matrix Groups and Rings   \n",
       "42                                          4.2 Matrix Groups and Rings   \n",
       "43                                          4.2 Matrix Groups and Rings   \n",
       "44                                          4.2 Matrix Groups and Rings   \n",
       "45                                          4.2 Matrix Groups and Rings   \n",
       "46                                          4.2 Matrix Groups and Rings   \n",
       "47                                          4.2 Matrix Groups and Rings   \n",
       "48                                          4.2 Matrix Groups and Rings   \n",
       "49                                          4.2 Matrix Groups and Rings   \n",
       "50                                              5.1 Elementary Matrices   \n",
       "51                                              5.1 Elementary Matrices   \n",
       "52                        5.2 The Echelon Form and Gaussian Elimination   \n",
       "53                        5.2 The Echelon Form and Gaussian Elimination   \n",
       "54                        5.2 The Echelon Form and Gaussian Elimination   \n",
       "55                        5.2 The Echelon Form and Gaussian Elimination   \n",
       "56                        5.2 The Echelon Form and Gaussian Elimination   \n",
       "57                        5.2 The Echelon Form and Gaussian Elimination   \n",
       "58                        5.2 The Echelon Form and Gaussian Elimination   \n",
       "59                        5.2 The Echelon Form and Gaussian Elimination   \n",
       "60                        5.2 The Echelon Form and Gaussian Elimination   \n",
       "61                                 5.3 Rank and Equivalence of Matrices   \n",
       "62                                 5.3 Rank and Equivalence of Matrices   \n",
       "63                                 5.3 Rank and Equivalence of Matrices   \n",
       "64                                 5.3 Rank and Equivalence of Matrices   \n",
       "65                                 5.3 Rank and Equivalence of Matrices   \n",
       "66                                 5.3 Rank and Equivalence of Matrices   \n",
       "67                                                                  NaN   \n",
       "68                                                                  NaN   \n",
       "69                                                                  NaN   \n",
       "70                                                                  NaN   \n",
       "71                                                                  NaN   \n",
       "72                                                                  NaN   \n",
       "73                                                                  NaN   \n",
       "74                                    7.1 Definition of the Determinant   \n",
       "75                                    7.1 Definition of the Determinant   \n",
       "76                                    7.1 Definition of the Determinant   \n",
       "77                                    7.1 Definition of the Determinant   \n",
       "78                                    7.2 Properties of the Determinant   \n",
       "79                                    7.2 Properties of the Determinant   \n",
       "80                                    7.2 Properties of the Determinant   \n",
       "81                                    7.2 Properties of the Determinant   \n",
       "82                                    7.2 Properties of the Determinant   \n",
       "83                                    7.2 Properties of the Determinant   \n",
       "84                                 7.3 Minors and the Laplace Expansion   \n",
       "85                                 7.3 Minors and the Laplace Expansion   \n",
       "86                                 7.3 Minors and the Laplace Expansion   \n",
       "87                                 7.3 Minors and the Laplace Expansion   \n",
       "88                                 7.3 Minors and the Laplace Expansion   \n",
       "89                                 7.3 Minors and the Laplace Expansion   \n",
       "90                                 7.3 Minors and the Laplace Expansion   \n",
       "91                                 7.3 Minors and the Laplace Expansion   \n",
       "92                                 7.3 Minors and the Laplace Expansion   \n",
       "93   8.1 The Characteristic Polynomial  and the Cayley-Hamilton Theorem   \n",
       "94   8.1 The Characteristic Polynomial  and the Cayley-Hamilton Theorem   \n",
       "95   8.1 The Characteristic Polynomial  and the Cayley-Hamilton Theorem   \n",
       "96   8.1 The Characteristic Polynomial  and the Cayley-Hamilton Theorem   \n",
       "97   8.1 The Characteristic Polynomial  and the Cayley-Hamilton Theorem   \n",
       "98                                     8.2 Eigenvalues and Eigenvectors   \n",
       "99                                     8.2 Eigenvalues and Eigenvectors   \n",
       "100                                    8.2 Eigenvalues and Eigenvectors   \n",
       "101                             8.3 Eigenvectors of Stochastic Matrices   \n",
       "102                             8.3 Eigenvectors of Stochastic Matrices   \n",
       "103                             8.3 Eigenvectors of Stochastic Matrices   \n",
       "104                             8.3 Eigenvectors of Stochastic Matrices   \n",
       "105                             8.3 Eigenvectors of Stochastic Matrices   \n",
       "106               9.1 Basic Definitions and Properties of Vector Spaces   \n",
       "107               9.1 Basic Definitions and Properties of Vector Spaces   \n",
       "108               9.1 Basic Definitions and Properties of Vector Spaces   \n",
       "109                            9.2 Bases and Dimension of Vector Spaces   \n",
       "110                            9.2 Bases and Dimension of Vector Spaces   \n",
       "111                            9.2 Bases and Dimension of Vector Spaces   \n",
       "112                            9.2 Bases and Dimension of Vector Spaces   \n",
       "113                            9.2 Bases and Dimension of Vector Spaces   \n",
       "114                            9.2 Bases and Dimension of Vector Spaces   \n",
       "115                            9.3 Coordinates and Changes of the Basis   \n",
       "116                            9.3 Coordinates and Changes of the Basis   \n",
       "117                            9.3 Coordinates and Changes of the Basis   \n",
       "118                            9.3 Coordinates and Changes of the Basis   \n",
       "119            9.4 Relations Between Vector Spaces and Their Dimensions   \n",
       "120            9.4 Relations Between Vector Spaces and Their Dimensions   \n",
       "121            9.4 Relations Between Vector Spaces and Their Dimensions   \n",
       "122            9.4 Relations Between Vector Spaces and Their Dimensions   \n",
       "123            9.4 Relations Between Vector Spaces and Their Dimensions   \n",
       "124            9.4 Relations Between Vector Spaces and Their Dimensions   \n",
       "125                10.1 Basic Definitions and Properties of Linear Maps   \n",
       "126                10.1 Basic Definitions and Properties of Linear Maps   \n",
       "127                10.1 Basic Definitions and Properties of Linear Maps   \n",
       "128                10.1 Basic Definitions and Properties of Linear Maps   \n",
       "129                10.1 Basic Definitions and Properties of Linear Maps   \n",
       "130                10.1 Basic Definitions and Properties of Linear Maps   \n",
       "131                10.1 Basic Definitions and Properties of Linear Maps   \n",
       "132                10.1 Basic Definitions and Properties of Linear Maps   \n",
       "133                                       10.2 Linear Maps and Matrices   \n",
       "134                                       10.2 Linear Maps and Matrices   \n",
       "135                                       10.2 Linear Maps and Matrices   \n",
       "136                                       10.2 Linear Maps and Matrices   \n",
       "137                                       10.2 Linear Maps and Matrices   \n",
       "138                                       10.2 Linear Maps and Matrices   \n",
       "139                                       10.2 Linear Maps and Matrices   \n",
       "140                                       10.2 Linear Maps and Matrices   \n",
       "141                                       10.2 Linear Maps and Matrices   \n",
       "142                                       10.2 Linear Maps and Matrices   \n",
       "143                                       10.2 Linear Maps and Matrices   \n",
       "144                                       10.2 Linear Maps and Matrices   \n",
       "145                                   11.1 Linear Forms and Dual Spaces   \n",
       "146                                   11.1 Linear Forms and Dual Spaces   \n",
       "147                                   11.1 Linear Forms and Dual Spaces   \n",
       "148                                   11.1 Linear Forms and Dual Spaces   \n",
       "149                                                 11.2 Bilinear Forms   \n",
       "150                                                 11.2 Bilinear Forms   \n",
       "151                                                 11.2 Bilinear Forms   \n",
       "152                                            11.3 Sesquilinear Forms    \n",
       "153                                            11.3 Sesquilinear Forms    \n",
       "154                                            11.3 Sesquilinear Forms    \n",
       "155                                            11.3 Sesquilinear Forms    \n",
       "156                                            11.3 Sesquilinear Forms    \n",
       "157                                      12.1 Scalar Products and Norms   \n",
       "158                                      12.1 Scalar Products and Norms   \n",
       "159                                      12.1 Scalar Products and Norms   \n",
       "160                                      12.1 Scalar Products and Norms   \n",
       "161                                      12.1 Scalar Products and Norms   \n",
       "162                                                  12.2 Orthogonality   \n",
       "163                                                  12.2 Orthogonality   \n",
       "164                                                  12.2 Orthogonality   \n",
       "165                                                  12.2 Orthogonality   \n",
       "166                                                  12.2 Orthogonality   \n",
       "167                                                  12.2 Orthogonality   \n",
       "168                                                  12.2 Orthogonality   \n",
       "169                                                  12.2 Orthogonality   \n",
       "170                                                  12.2 Orthogonality   \n",
       "171                                                  12.2 Orthogonality   \n",
       "172                               12.3 The Vector Product in mathbbR3,1   \n",
       "173                               12.3 The Vector Product in mathbbR3,1   \n",
       "174                               12.3 The Vector Product in mathbbR3,1   \n",
       "175                               12.3 The Vector Product in mathbbR3,1   \n",
       "176                               12.3 The Vector Product in mathbbR3,1   \n",
       "177                               13.1 Basic Definitions and Properties   \n",
       "178                               13.1 Basic Definitions and Properties   \n",
       "179                               13.1 Basic Definitions and Properties   \n",
       "180                               13.1 Basic Definitions and Properties   \n",
       "181                               13.1 Basic Definitions and Properties   \n",
       "182                               13.1 Basic Definitions and Properties   \n",
       "183                               13.1 Basic Definitions and Properties   \n",
       "184                               13.1 Basic Definitions and Properties   \n",
       "185                             13.2 Adjoint Endomorphisms and Matrices   \n",
       "186                             13.2 Adjoint Endomorphisms and Matrices   \n",
       "187                             13.2 Adjoint Endomorphisms and Matrices   \n",
       "188                               14.1 Basic Definitions and Properties   \n",
       "189                               14.1 Basic Definitions and Properties   \n",
       "190                               14.1 Basic Definitions and Properties   \n",
       "191                               14.1 Basic Definitions and Properties   \n",
       "192                                              14.2 Diagonalizability   \n",
       "193                                              14.2 Diagonalizability   \n",
       "194                                              14.2 Diagonalizability   \n",
       "195                                              14.2 Diagonalizability   \n",
       "196                              14.3 Triangulation and Schur's Theorem   \n",
       "197                              14.3 Triangulation and Schur's Theorem   \n",
       "198                              14.3 Triangulation and Schur's Theorem   \n",
       "199                              14.3 Triangulation and Schur's Theorem   \n",
       "200                              14.3 Triangulation and Schur's Theorem   \n",
       "201                              14.3 Triangulation and Schur's Theorem   \n",
       "202                                                    15.1 Polynomials   \n",
       "203                                                    15.1 Polynomials   \n",
       "204                                                    15.1 Polynomials   \n",
       "205                                                    15.1 Polynomials   \n",
       "206                                                    15.1 Polynomials   \n",
       "207                             15.2 The Fundamental Theorem of Algebra   \n",
       "208                             15.2 The Fundamental Theorem of Algebra   \n",
       "209                             15.2 The Fundamental Theorem of Algebra   \n",
       "210                             15.2 The Fundamental Theorem of Algebra   \n",
       "211                             15.2 The Fundamental Theorem of Algebra   \n",
       "212                             15.2 The Fundamental Theorem of Algebra   \n",
       "213                             15.2 The Fundamental Theorem of Algebra   \n",
       "214                             15.2 The Fundamental Theorem of Algebra   \n",
       "215                             15.2 The Fundamental Theorem of Algebra   \n",
       "216                       16.1 Cyclic f-invariant Subspaces and Duality   \n",
       "217                       16.1 Cyclic f-invariant Subspaces and Duality   \n",
       "218                       16.1 Cyclic f-invariant Subspaces and Duality   \n",
       "219                       16.1 Cyclic f-invariant Subspaces and Duality   \n",
       "220                       16.1 Cyclic f-invariant Subspaces and Duality   \n",
       "221                       16.1 Cyclic f-invariant Subspaces and Duality   \n",
       "222                                      16.2 The Jordan Canonical Form   \n",
       "223                                      16.2 The Jordan Canonical Form   \n",
       "224                                      16.2 The Jordan Canonical Form   \n",
       "225                                      16.2 The Jordan Canonical Form   \n",
       "226                                      16.2 The Jordan Canonical Form   \n",
       "227                                      16.2 The Jordan Canonical Form   \n",
       "228                                      16.2 The Jordan Canonical Form   \n",
       "229                                      16.2 The Jordan Canonical Form   \n",
       "230                                      16.2 The Jordan Canonical Form   \n",
       "231                                      16.2 The Jordan Canonical Form   \n",
       "232                       16.3 Computation of the Jordan Canonical Form   \n",
       "233                       16.3 Computation of the Jordan Canonical Form   \n",
       "234                       16.3 Computation of the Jordan Canonical Form   \n",
       "235                       16.3 Computation of the Jordan Canonical Form   \n",
       "236                       16.3 Computation of the Jordan Canonical Form   \n",
       "237                       16.3 Computation of the Jordan Canonical Form   \n",
       "238                       16.3 Computation of the Jordan Canonical Form   \n",
       "239                       16.3 Computation of the Jordan Canonical Form   \n",
       "240                       16.3 Computation of the Jordan Canonical Form   \n",
       "241           17.1 Matrix Functions and the Matrix Exponential Function   \n",
       "242           17.1 Matrix Functions and the Matrix Exponential Function   \n",
       "243           17.1 Matrix Functions and the Matrix Exponential Function   \n",
       "244           17.1 Matrix Functions and the Matrix Exponential Function   \n",
       "245           17.1 Matrix Functions and the Matrix Exponential Function   \n",
       "246           17.1 Matrix Functions and the Matrix Exponential Function   \n",
       "247           17.1 Matrix Functions and the Matrix Exponential Function   \n",
       "248           17.1 Matrix Functions and the Matrix Exponential Function   \n",
       "249              17.2 Systems of Linear Ordinary Differential Equations   \n",
       "250              17.2 Systems of Linear Ordinary Differential Equations   \n",
       "251              17.2 Systems of Linear Ordinary Differential Equations   \n",
       "252              17.2 Systems of Linear Ordinary Differential Equations   \n",
       "253              17.2 Systems of Linear Ordinary Differential Equations   \n",
       "254              17.2 Systems of Linear Ordinary Differential Equations   \n",
       "255              17.2 Systems of Linear Ordinary Differential Equations   \n",
       "256              17.2 Systems of Linear Ordinary Differential Equations   \n",
       "257              17.2 Systems of Linear Ordinary Differential Equations   \n",
       "258                                           18.1 Normal Endomorphisms   \n",
       "259                                           18.1 Normal Endomorphisms   \n",
       "260                                           18.1 Normal Endomorphisms   \n",
       "261                                           18.1 Normal Endomorphisms   \n",
       "262                                           18.1 Normal Endomorphisms   \n",
       "263                           18.2 Orthogonal and Unitary Endomorphisms   \n",
       "264                           18.2 Orthogonal and Unitary Endomorphisms   \n",
       "265                           18.2 Orthogonal and Unitary Endomorphisms   \n",
       "266                           18.2 Orthogonal and Unitary Endomorphisms   \n",
       "267                           18.2 Orthogonal and Unitary Endomorphisms   \n",
       "268                                      18.3 Selfadjoint Endomorphisms   \n",
       "269                                      18.3 Selfadjoint Endomorphisms   \n",
       "270                                      18.3 Selfadjoint Endomorphisms   \n",
       "271                                      18.3 Selfadjoint Endomorphisms   \n",
       "272                                      18.3 Selfadjoint Endomorphisms   \n",
       "273                                      18.3 Selfadjoint Endomorphisms   \n",
       "274                                      18.3 Selfadjoint Endomorphisms   \n",
       "275                                      18.3 Selfadjoint Endomorphisms   \n",
       "276                                      18.3 Selfadjoint Endomorphisms   \n",
       "277                                      18.3 Selfadjoint Endomorphisms   \n",
       "278                                      18.3 Selfadjoint Endomorphisms   \n",
       "279                                      18.3 Selfadjoint Endomorphisms   \n",
       "280                                      18.3 Selfadjoint Endomorphisms   \n",
       "281                                                                 NaN   \n",
       "282                                                                 NaN   \n",
       "283                                                                 NaN   \n",
       "284                                                                 NaN   \n",
       "285                                                                 NaN   \n",
       "286                                                                 NaN   \n",
       "287                                                                 NaN   \n",
       "288                                                                 NaN   \n",
       "289                                                                 NaN   \n",
       "290                                                                 NaN   \n",
       "291                                                                 NaN   \n",
       "292                                                                 NaN   \n",
       "293                                                                 NaN   \n",
       "294                                                                 NaN   \n",
       "295                                                                 NaN   \n",
       "296                                                                 NaN   \n",
       "297                                                                 NaN   \n",
       "298                                                                 NaN   \n",
       "299                                                                 NaN   \n",
       "\n",
       "     section_level_3  \\\n",
       "0                NaN   \n",
       "1                NaN   \n",
       "2                NaN   \n",
       "3                NaN   \n",
       "4                NaN   \n",
       "5                NaN   \n",
       "6                NaN   \n",
       "7                NaN   \n",
       "8                NaN   \n",
       "9                NaN   \n",
       "10               NaN   \n",
       "11               NaN   \n",
       "12               NaN   \n",
       "13               NaN   \n",
       "14               NaN   \n",
       "15               NaN   \n",
       "16               NaN   \n",
       "17               NaN   \n",
       "18               NaN   \n",
       "19               NaN   \n",
       "20               NaN   \n",
       "21               NaN   \n",
       "22               NaN   \n",
       "23               NaN   \n",
       "24               NaN   \n",
       "25               NaN   \n",
       "26               NaN   \n",
       "27               NaN   \n",
       "28               NaN   \n",
       "29               NaN   \n",
       "30               NaN   \n",
       "31               NaN   \n",
       "32               NaN   \n",
       "33               NaN   \n",
       "34               NaN   \n",
       "35               NaN   \n",
       "36               NaN   \n",
       "37               NaN   \n",
       "38               NaN   \n",
       "39               NaN   \n",
       "40               NaN   \n",
       "41               NaN   \n",
       "42               NaN   \n",
       "43               NaN   \n",
       "44               NaN   \n",
       "45               NaN   \n",
       "46               NaN   \n",
       "47               NaN   \n",
       "48               NaN   \n",
       "49               NaN   \n",
       "50               NaN   \n",
       "51               NaN   \n",
       "52               NaN   \n",
       "53               NaN   \n",
       "54               NaN   \n",
       "55               NaN   \n",
       "56               NaN   \n",
       "57               NaN   \n",
       "58               NaN   \n",
       "59               NaN   \n",
       "60               NaN   \n",
       "61               NaN   \n",
       "62               NaN   \n",
       "63               NaN   \n",
       "64               NaN   \n",
       "65               NaN   \n",
       "66               NaN   \n",
       "67               NaN   \n",
       "68               NaN   \n",
       "69               NaN   \n",
       "70               NaN   \n",
       "71               NaN   \n",
       "72               NaN   \n",
       "73               NaN   \n",
       "74               NaN   \n",
       "75               NaN   \n",
       "76               NaN   \n",
       "77               NaN   \n",
       "78               NaN   \n",
       "79               NaN   \n",
       "80               NaN   \n",
       "81               NaN   \n",
       "82               NaN   \n",
       "83               NaN   \n",
       "84               NaN   \n",
       "85               NaN   \n",
       "86               NaN   \n",
       "87               NaN   \n",
       "88               NaN   \n",
       "89               NaN   \n",
       "90               NaN   \n",
       "91               NaN   \n",
       "92               NaN   \n",
       "93               NaN   \n",
       "94               NaN   \n",
       "95               NaN   \n",
       "96               NaN   \n",
       "97               NaN   \n",
       "98               NaN   \n",
       "99               NaN   \n",
       "100              NaN   \n",
       "101              NaN   \n",
       "102              NaN   \n",
       "103              NaN   \n",
       "104              NaN   \n",
       "105              NaN   \n",
       "106              NaN   \n",
       "107              NaN   \n",
       "108              NaN   \n",
       "109              NaN   \n",
       "110              NaN   \n",
       "111              NaN   \n",
       "112              NaN   \n",
       "113              NaN   \n",
       "114              NaN   \n",
       "115              NaN   \n",
       "116              NaN   \n",
       "117              NaN   \n",
       "118              NaN   \n",
       "119              NaN   \n",
       "120              NaN   \n",
       "121              NaN   \n",
       "122              NaN   \n",
       "123              NaN   \n",
       "124              NaN   \n",
       "125              NaN   \n",
       "126              NaN   \n",
       "127              NaN   \n",
       "128              NaN   \n",
       "129              NaN   \n",
       "130              NaN   \n",
       "131              NaN   \n",
       "132              NaN   \n",
       "133              NaN   \n",
       "134              NaN   \n",
       "135              NaN   \n",
       "136              NaN   \n",
       "137              NaN   \n",
       "138              NaN   \n",
       "139              NaN   \n",
       "140              NaN   \n",
       "141              NaN   \n",
       "142              NaN   \n",
       "143              NaN   \n",
       "144              NaN   \n",
       "145              NaN   \n",
       "146              NaN   \n",
       "147              NaN   \n",
       "148              NaN   \n",
       "149              NaN   \n",
       "150              NaN   \n",
       "151              NaN   \n",
       "152              NaN   \n",
       "153              NaN   \n",
       "154              NaN   \n",
       "155              NaN   \n",
       "156              NaN   \n",
       "157              NaN   \n",
       "158              NaN   \n",
       "159              NaN   \n",
       "160              NaN   \n",
       "161              NaN   \n",
       "162              NaN   \n",
       "163              NaN   \n",
       "164              NaN   \n",
       "165              NaN   \n",
       "166              NaN   \n",
       "167              NaN   \n",
       "168              NaN   \n",
       "169              NaN   \n",
       "170              NaN   \n",
       "171              NaN   \n",
       "172              NaN   \n",
       "173              NaN   \n",
       "174              NaN   \n",
       "175              NaN   \n",
       "176              NaN   \n",
       "177              NaN   \n",
       "178              NaN   \n",
       "179              NaN   \n",
       "180              NaN   \n",
       "181              NaN   \n",
       "182              NaN   \n",
       "183              NaN   \n",
       "184              NaN   \n",
       "185              NaN   \n",
       "186              NaN   \n",
       "187              NaN   \n",
       "188              NaN   \n",
       "189              NaN   \n",
       "190              NaN   \n",
       "191              NaN   \n",
       "192              NaN   \n",
       "193              NaN   \n",
       "194              NaN   \n",
       "195              NaN   \n",
       "196              NaN   \n",
       "197              NaN   \n",
       "198              NaN   \n",
       "199              NaN   \n",
       "200              NaN   \n",
       "201              NaN   \n",
       "202              NaN   \n",
       "203              NaN   \n",
       "204              NaN   \n",
       "205              NaN   \n",
       "206              NaN   \n",
       "207              NaN   \n",
       "208              NaN   \n",
       "209              NaN   \n",
       "210              NaN   \n",
       "211              NaN   \n",
       "212              NaN   \n",
       "213              NaN   \n",
       "214              NaN   \n",
       "215              NaN   \n",
       "216              NaN   \n",
       "217              NaN   \n",
       "218              NaN   \n",
       "219              NaN   \n",
       "220              NaN   \n",
       "221              NaN   \n",
       "222              NaN   \n",
       "223              NaN   \n",
       "224              NaN   \n",
       "225              NaN   \n",
       "226              NaN   \n",
       "227              NaN   \n",
       "228              NaN   \n",
       "229              NaN   \n",
       "230              NaN   \n",
       "231              NaN   \n",
       "232              NaN   \n",
       "233              NaN   \n",
       "234              NaN   \n",
       "235              NaN   \n",
       "236              NaN   \n",
       "237              NaN   \n",
       "238              NaN   \n",
       "239              NaN   \n",
       "240              NaN   \n",
       "241              NaN   \n",
       "242              NaN   \n",
       "243              NaN   \n",
       "244              NaN   \n",
       "245              NaN   \n",
       "246              NaN   \n",
       "247              NaN   \n",
       "248              NaN   \n",
       "249              NaN   \n",
       "250              NaN   \n",
       "251              NaN   \n",
       "252              NaN   \n",
       "253              NaN   \n",
       "254              NaN   \n",
       "255              NaN   \n",
       "256              NaN   \n",
       "257              NaN   \n",
       "258              NaN   \n",
       "259              NaN   \n",
       "260              NaN   \n",
       "261              NaN   \n",
       "262              NaN   \n",
       "263              NaN   \n",
       "264              NaN   \n",
       "265              NaN   \n",
       "266              NaN   \n",
       "267              NaN   \n",
       "268              NaN   \n",
       "269              NaN   \n",
       "270              NaN   \n",
       "271              NaN   \n",
       "272              NaN   \n",
       "273              NaN   \n",
       "274              NaN   \n",
       "275              NaN   \n",
       "276              NaN   \n",
       "277              NaN   \n",
       "278              NaN   \n",
       "279              NaN   \n",
       "280              NaN   \n",
       "281              NaN   \n",
       "282              NaN   \n",
       "283              NaN   \n",
       "284              NaN   \n",
       "285              NaN   \n",
       "286              NaN   \n",
       "287              NaN   \n",
       "288              NaN   \n",
       "289              NaN   \n",
       "290              NaN   \n",
       "291              NaN   \n",
       "292              NaN   \n",
       "293              NaN   \n",
       "294              NaN   \n",
       "295              NaN   \n",
       "296              NaN   \n",
       "297              NaN   \n",
       "298              NaN   \n",
       "299              NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       clean_content  \n",
       "0                                                                                                                                                                                                                                                                                                                                                               chapter linear algebra every day life one familiarize student actual question application learns deal real world lothar collatz pagerank algorithm pagerank algorithm method ass importance document mutual link web page basis link structure developed sergei brin larry page founder google stanford university late basic idea algorithm following instead counting link pagerank essentially interprets link page page b vote page page pagerank ass importance page number received vote pagerank also considers importance page cast vote since vote page higher value thus also assign higher value page point important page rated higher thus lead higher position search let u describe model idea mathematically presentation us idea article given set web page every page k assigned importance value xk page k important page j xk x j page k link page j say page j backlink page description backlinks vote example consider following link structure man mus den lernenden mit konkreten fragestellungen au den anwendungen vertraut machen das er lernt konkrete fragen zu text found http translation springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                            linear algebra every day life page link page backlink page easiest approach define importance web page count backlinks vote cast page important page example give importance value page thus important page equally important however intuition also description google suggests backlinks important page important value page le important page idea modeled defining xk sum importance value backlinks page example result four equation satisfied simultaneously disadvantage approach consider number link page thus would possible significantly increase importance page adding link page order avoid importance value backlinks pagerank algorithm divided number link corresponding page creates kind internet democracy every page vote page total cast one vote example give equation four equation four unknown equation unknown occur first power chap see write equation form linear system equation analyzing solving system one important task linear algebra example pagerank algorithm show linear algebra present powerful modeling term linear originates latin word linea mean straight line linearis mean consisting straight line  \n",
       "2    pagerank algorithm tool turned real world problem assessing importance web page problem linear algebra problem examined sect completeness mention solution four unknown computed matlab rounded second significant digit given thus page important one possible multiply solution importance value xk positive constant multiplication scaling often advantageous computational method visual display result example scaling could used give important page value scaling allowed since change ranking page essential information provided pagerank algorithm claim discounting car insurance insurance company compute premium customer basis insured risk higher risk higher premium therefore important identify factor lead higher risk case car insurance factor include number mile driven per year distance home work marital status engine power age driver using information company calculates initial premium usually best indicator future accident hence future insurance claim number accident individual customer past claim history order incorporate information premium rate insurer establish system risk class divide customer homogeneous risk group respect previous claim history customer fewer accident past get discount premium approach called claim discounting scheme mathematical model scheme need set risk class transition rule moving class end policy year customer may move different class depending claim made year discount given percent premium initial class simple example consider four risk class discount following transition rule accident step one class stay  \n",
       "3                                                                                                                                                                                                                                       linear algebra every day life one accident step back one class stay one accident step back class stay next insurance company estimate probability customer class ci year move class c j probability denoted pi j let u assume simplicity probability exactly one accident every customer probability two accident every customer course practice insurance company determine probability dependence class example customer class stay case least one accident happens probability customer accident probability chance move next year way obtain value pi j j arrange matrix follows entry matrix nonnegative real number sum entry row equal matrix called row-stochastic analysis matrix property central topic linear algebra developed throughout book example pagerank algorithm translated practical problem language linear algebra study using linear algebra technique example premium rate discussed example production planning plant production planning plant consider many different factor particular commodity price labor cost available capital order determine production plan consider simple example company produce product xi unit product pi produced pair called production plan suppose raw material labor production one unit product pi cost euro respectively euro available purchase raw material euro payment labor cost production plan must  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   production planning plant satisfy constraint inequality production plan satisfies constraint called feasible let pi profit selling one unit product pi goal determine production plan maximizes profit function find maximum two equation describe straight line coordinate system variable ax two line form boundary line feasible production plan line see figure note also must xi since produce negative unit product planned profit yi equation yi describe parallel straight line coordinate system see dashed line figure satisfy yi yi profit maximization problem solved moving dashed line one reach corner maximal case variable draw simple figure obtain solution graphically general idea finding corner maximum profit still example linear optimization problem formulated real world problem language linear algebra use mathematical method solution predicting future profit prediction profit loss company central planning instrument economics analogous problem arise many area political decision making  \n",
       "5                                                                                           linear algebra every day life example budget planning tax estimate planning new infrastructure consider specific example four quarter year company profit million euro board want predict future profit development basis value evidence suggests profit behave linearly true profit would form straight line αt β connects point coordinate system time profit ax however neither hold example practice therefore one try find straight line deviate little possible given point one possible approach choose parameter α β order minimize sum squared distance given point straight line parameter α β determined resulting line used estimating predicting future profit illustrated following figure determination parameter α β minimize sum square called least square problem solve least square problem using method linear algebra example approach sometimes called parameter identification statistic modeling given data company profit using linear predictor function αt β known linear regression circuit simulation current development electronic device rapid short interval nowadays often le year new model laptop mobile phone issued market achieve continuously new generation computer chip developed typically become smaller powerful naturally use little energy possible important factor development plan simulate chip virtually computer without producing physical prototype model-based planning optimization product central method many high technology area based modern mathematics  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               circuit simulation usually switching behavior chip modeled mathematical system consisting differential algebraic equation describe relation current voltage without going detail consider following circuit circuit description v given input current time characteristic value component r resistor l inductor c capacitor function potential difference three component denoted vr vl vc current applying kirchhoff electrical engineering lead following system linear equation differential equation model dynamic behavior circuit vl dt c vc dt r vr l vl vc vr v example easy solve last two equation vl vr hence obtain system differential equation r vc v dt l l l vc dt c function und vc discus solve system example simple example demonstrates simulation circuit system linear differential equation algebraic equation solved modern computer chip industrial practice require solving system million differential-algebraic equation linear algebra one central tool theoretical analysis system well development efficient solution method gustav robert kirchhoff  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           chapter basic mathematical concept chapter introduce mathematical concept form basis development following chapter begin set basic mathematical logic consider map set important property finally discus relation particular equivalence relation set set mathematical logic begin development concept set use following definition definition set collection well determined distinguishable object x perception thinking object called element object x definition well determined therefore uniquely decide whether x belongs set write x x element set otherwise write x furthermore element distinguishable mean element pairwise distinct two object x equal write x otherwise x mathematical object usually give formal definition equality example consider equality set see definition describe set curly bracket contain either list element example red yellow green georg cantor one founder set theory cantor published definition journal mathematische annalen springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "8                                                                                                                                                                                                                                                                                                                            basic mathematical concept defining property example x x positive even number x x person owning bike well known set number denoted follows n natural number natural number including zero z integer q x x z b n rational number r x x real number real number construction characterization real number r usually done introductory course real analysis describe set via defining property formally write x p x p predicate may hold object x p x assertion p hold x general assertion statement classified either true false instance statement set n infinitely many element true sentence tomorrow weather good assertion since meaning term good weather unclear weather prediction general uncertain negation assertion assertion denote assertion true false false true instance negation true assertion set n infinitely many element given set n infinitely many element set n finitely many element false two assertion b combined via logical composition new assertion following list common logical composition together mathematical short hand notation composition conjunction disjunction implication notation equivalence wording b b implies b b sufficient condition b b necessary condition b equivalent true b true necessary sufficient b b necessary sufficient  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                  set mathematical logic example write assertion x real number x negative x r x whether assertion composed two assertion b true false depends logical value b following table logical value f denote true false respectively f f b f f f f f f f f f example assertion b true b true assertion b false true b false particular false b true independent logical value b thus true since true false since false hand assertion true since false following often prove certain implication b true table logical value show example illustrates prove assumption true assertion b true well instead assume true often write let hold easy see b exercise create table logical value compare table b truth b therefore proved showing truth implies truth b false implies false assertion called contraposition assertion b conclusion b called proof contraposition together assertion also often use so-called quantifier quantifier universal existential notation wording exists return set theory introduce subset equality set definition let n set called subset n denoted n every element also element n write n hold n called equal denoted n n n write n hold  \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     basic mathematical concept called proper subset n denoted n n n hold using notation mathematical logic write definition follows n n n x x x n n n n n assertion right side equivalence read follows object x truth x implies truth x n shorter x x hold x n hold special set set element define formally follows definition set ø x x x called empty set notation mean defined introduced empty set defining property every object x x x element ø hold object hence ø contain element set contains least one element called nonempty theorem every set following assertion hold ø ø ø proof show assertion x x ø x true since x ø assertion x ø false therefore x ø x true every x cp remark implication b let ø know ø hence ø follows definition theorem let n l set following assertion hold subset relation reflexivity n n l l transitivity proof show assertion x x x true x true x x implication two true assertion hence true show assertion x x x l true x true also x n true since n truth x n implies x l true since n hence assertion x x l true  \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        set mathematical logic definition let n set n n x x x n intersection n n x x x n difference n n x x x n n ø set n called disjoint set operation union intersection extended two set ø set set mi mi x x mi mi x x mi set called index set n n write union intersection set mn n mi n mi theorem let n two set n following equivalent n n ø proof show hold since n exists x n x thus x n n ø hold exists x n x hence n since n hold see n hold theorem let n l set following assertion hold n n commutativity n n n n associativity n l n l n l n distributivity n l n l n l n l n n l n l n l n l proof exercise notation n n union intersection set n introduced giuseppe peano one founder formal logic notation smallest common multiple n largest common divisor n set n suggested georg cantor catch  \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               basic mathematical concept definition let set cardinality denoted number element power set denoted p set subset p n n empty set ø cardinality zero p ø ø thus ø cardinality p ø hence one show every set finitely many element finite cardinality hold map section discus map set definition let x nonempty set map f x rule assigns x x exactly one f x write f x x f x instead x f x also write f x set x called domain codomain f two map f x g x called equal f x g x hold x x write f definition assumed x nonempty since otherwise rule assigns element element x one set empty one define empty map however following always assume always explicitly state set given map act nonempty example two map x r r given f x f x x x g x x x analyze property map need terminology  \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        map definition let x nonempty set map id x x x x x called identity x let f x map let x n f f x x called image f f n x x f x n called pre-image n f f x x f x map ø x f x f x called restriction f one note definition f n set hence symbol f mean inverse map f map introduced definition example map domain x r following property f x x r x f f ø g x g g x r x definition let x nonempty set map f x called injective x equality f f implies surjective f x bijective f injective surjective every nonempty set x simplest example bijective map x x id x identity x example let x r x f r r f x x neither injective surjective f r f x x surjective injective f r f x x injective surjective f f x x bijective assertion used continuity map f x x discussed basic course analysis particular used fact continuous function map real interval real interval assertion also show important include domain codomain definition map theorem map f x bijective every exists exactly one x x f x proof let f bijective let since f surjective exists x f x also satisfies f  \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     basic mathematical concept follows injectivity f therefore exists unique x f since exists unique x x f x follows f x thus f surjective let x f f assumption implies f also injective one show two set x finite cardinality exists bijective map lemma set x n exist exactly pairwise distinct bijective map x proof exercise definition let f x x f x g z g map composition f g map g f x z x g f x expression g f read g f stress order composition first f applied x g f x one immediately see f id x f idy f every map f x theorem let f w x g x h z map h g f h g f composition map associative f g g f proof exercise theorem map f x bijective exists map g x g f id x f g idy proof f bijective theorem every exists x x x f x define map g g x g x let given hence f g idy f g f g f hand x x given f x theorem x exists unique x f x g f x g f g f g  \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            map g f id x assumption g f id x thus g f injective thus also f injective see exercise moreover f g idy thus f g surjective hence also f surjective see exercise therefore f bijective map g x characterized theorem unique another map h x h f id x f h idy h id x h g f h g f h g idy lead following definition definition f x bijective map unique map g x theorem called inverse inverse map f denote inverse f f show given map g x unique inverse bijective map f x sufficient show one equation g f id x f g idy indeed f bijective g f id x g g idy g f f g f f id x f f way g f follows assumption f g idy theorem f x g z bijective map following assertion hold f bijective f f g f bijective g f f g proof exercise know theorem g f x z bijective therefore exists unique inverse g f map f g f g g f f g g f f g g f f idy f f f id x hence f g inverse g f relation first introduce cartesian two set named rené descartes founder analytic geometry georg cantor used name connection set n notation n  \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               basic mathematical concept definition n nonempty set set n x x n cartesian product n element x n called ordered pair easily generalize definition n n nonempty set mn mn xn xi mi n element xn mn called ordered n-tuple n-fold cartesian product single nonempty set n xn xi n n time definition least one set empty resulting cartesian product empty set well definition n nonempty set set r n called relation n n r called relation instead x r also write x r x clear relation considered definition least one set n empty every relation n also empty set since n ø instance n n q r x n x relation n expressed r n n n definition relation r set called reflexive x x hold x symmetric x x hold x transitive x z x z hold x z r reflexive transitive symmetric called equivalence relation example let r x x r reflexive since x hold x x also hence r symmetric finally r transitive example x r z r x z r  \n",
       "17                                                                                                                                                                                                                                                                                                                                                                   relation relation r x x reflexive transitive symmetric f r r map r x f x f equivalence relation definition let r equivalence relation set x set x r x r x called equivalence class x respect set equivalence class x r x called quotient set respect equivalence class x r element x never empty set since always x x reflexivity therefore x x r clear equivalence relation r meant often write x instead oft x r also skip additional respect r theorem r equivalence relation set x following equivalent x x ø x proof since x x follows x x x follows x thus x x since x ø exists z x element z x z z thus x z z symmetry therefore x transitivity let x z x x z using symmetry transitivity obtain z hence z mean x analogous way one show x hence x hold theorem show two equivalence class x either x x ø thus every x contained exactly one equivalence class namely x equivalence relation r yield partitioning decomposition mutually disjoint subset every element x called representative equivalence class x useful general approach often use book partition set object set matrix equivalence class find class representative particularly simple structure representative called normal form respect given equivalence relation  \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  basic mathematical concept example given number n n set rn b b divisible n without remainder equivalence relation z since following property hold reflexivity divisible n without remainder symmetry b divisible n without remainder also b transitivity let b b c divisible n without remainder write c b b c summands right divisible n without remainder hence also hold z equivalence class called residue class modulo n nz nz z z equivalence relation rn yield partitioning z n mutually disjoint subset particular n set residue class modulo n quotient set respect rn often denoted thus n set play important role mathematical field number theory exercise let b c assertion show following assertion true associative law b c b c b c b c hold b commutative law b b b b hold c distributive law b c c b c b c c b c hold let b c assertion show following assertion true b b b b b  \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    relation c e f b b b b c c b c c assertion c called de morgan law prove theorem show two set n following hold n let x nonempty set u v nonempty subset let f x map show f u v f u f v let u v x nonempty check whether f u v f u f v hold following map injective surjective bijective f r r x b f r x x c f r x x n n even f n z n n odd show two map f x g z following assertion hold g f surjective g surjective b g f injective f injective let z given show map f z z f x x bijective prove lemma prove theorem prove theorem find two map f g n n simultaneously f surjective b g injective c g f bijective determine equivalence relation set determine symmetric transitive relation set b c reflexive  \n",
       "20                                                                                                                                                                                                                                                               chapter algebraic structure algebraic structure set operation element follow certain rule example structure consider integer operation property addition already elementary school one learns sum b two integer b another integer moreover number every integer every integer exists integer analysis property concrete example lead definition abstract concept built simple axiom integer operation addition lead algebraic structure group principle abstraction concrete example one strength basic working principle mathematics extracting completely exposing mathematical kernel david hilbert also simplify work every proved assertion abstract concept automatically hold concrete example moreover combining defined concept move generalization way extend mathematical theory step step hermann günther graßmann described procedure mathematical method move forward simplest concept combination gain via combination new general group begin set operation specific property definition group set g map called operation g g g b b die mathematische methode hingegen schreitet von den einfachsten begriffen zu den zusammengesetzteren fort gewinnt durch verknüpfung de besonderen neue allgemeinere springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      algebraic structure satisfies following operation associative b c b c hold b c exists element e g called neutral element e g b every g exists g called inverse element b b hold b g group called commutative short hand notation group use g g clear operation used theorem every group g following assertion hold e g neutral element g e also e g neutral element g also e g contains exactly one neutral element every g exists unique inverse element proof let e g neutral element let g satisfy definition exists element g thus e e let e g neutral element let exists g also e follows e e let e g two neutral element e e since neutral element since e also neutral element follows e e second identity used assertion hence e let g two inverse element g let e g unique neutral element follows e e named niels henrik abel founder group theory  \n",
       "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          group example z q r commutative group group neutral element number zero inverse number instead usually write b since operation addition group also called additive group natural number n addition form group since neutral element consider set includes also number zero inverse element hence also addition form group set q r usual multiplication form commutative group multiplicative group neutral element number one inverse element number instead also write ab integer z multiplication form group set z includes number z z inverse element z definition let g group h h group called subgroup g next theorem give alternative characterization subgroup theorem h subgroup group g following property hold ø h b h b h every h also inverse element satisfies proof exercise following definition characterizes map two group compatible respective group operation definition let g g group map ϕ g g g ϕ g called group homomorphism ϕ b ϕ ϕ b b g bijective group homomorphism called group isomorphism  \n",
       "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                  algebraic structure ring field section extend concept group discus mathematical structure characterized two operation motivating example consider integer addition group z multiply element z multiplication associative b b c z furthermore addition multiplication satisfy distributive law b c b c b c c b c integer b property make z addition multiplication ring definition ring set r two operation r r r b b addition r r r b b multiplication satisfy following r commutative group call neutral element group zero write denote inverse element r write b instead multiplication associative b c b c b c distributive law hold b c r b c b c b c c b ring called commutative b b b element r called unit case r called ring unit right hand side two distributive law omitted parenthesis since multiplication supposed bind stronger addition b c b useful illustration purpose nevertheless use parenthesis sometimes write b c instead b c analogous notation group denote ring r r operation clear context ring unit unit element unique e r satisfy e e r particular e e r use following abbreviation sum product element n j n j  \n",
       "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ring field moreover n empty sum r n k define k j ring unit also define k empty product k j theorem every ring r following assertion hold b b b b proof every r adding left right hand side equality obtain way show since b b follows unique additive inverse b b way show b b furthermore b thus b immediately clear z commutative ring unit standard example concept ring modeled example let nonempty set let r set map f r operation r r r r r r f g f g f g f g f g x f x g x f g x f x g x commutative ring unit f x g x f x g x sum product two real number zero ring map r r x unit map r r x real number zero one definition ring additive inverse element occur formally define concept multiplicative inverse  \n",
       "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        algebraic structure definition let r ring unit element b r called inverse r respect b b element r inverse called invertible clear definition b r inverse r r inverse b general however every element ring must invertible element invertible unique inverse shown following theorem theorem let r ring unit r invertible inverse unique denote b r invertible b r invertible b proof b b r inverse r b b b b b b b since b invertible r well defined b b b way show b thus b algebraic point view difference integer one hand rational real number set q r every element except number zero invertible additional structure make q r field definition commutative ring r unit called field every r invertible definition every field commutative ring unit converse hold one also introduce concept field based concept group cp exercise definition field set k two operation k k k k k k b b b b addition multiplication  \n",
       "26                                                                                                                                                                                                                                                                                                                                                                                                                                                     ring field satisfy following k commutative group call neutral element group zero write denote inverse element k write b instead k commutative group call neutral element group unit write denote inverse element k distributive law hold b c k b c b c b c c b show useful property field lemma every field k following assertion hold k least two element k b c imply b c b c k b imply b b k proof follows definition since k already shown ring cp theorem since know exists multiplying side b c left yield b suppose b finished exists multiplying side b left yield b ring r element r called zero b r exists b element called trivial zero divisor property lemma mean field contain trivial zero divisor also ring property hold instance ring integer z later chapter encounter ring matrix contain non-trivial zero divisor see proof theorem following definition analogous concept subgroup cp definition subring cp excercise definition let k field l k l field called subfield k two important example algebraic concept discussed discus field complex number ring polynomial concept zero divisor introduced karl theodor wilhelm weierstraß  \n",
       "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           algebraic structure example set complex number defined c x x r r set define following operation addition multiplication c c c c c c right hand side use addition multiplication field c field neutral element respect addition multiplication given inverse element respect addition multiplication given x x c x x c x x x multiplicative inverse element written ab instead common notation considering subset l x x r c identify every x r element set l via bijective map x x particular thus interpret r subfield c although r really subset c distinguish zero unit element r special complex number imaginary unit satisfies identified real number complex number imaginary unit denoted hence write using identification x r x c write z x c x x x x iy z im z  \n",
       "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ring field last expression z x im z abbreviation real part imaginary part complex number z x since iy yi justified write complex number x iy x yi given complex number z x z x iy number z x respectively z x iy called associated complex conjugate number using real square root modulus absolute value complex number defined zz x iy x iy x ix iyx x simplification omitted multiplication sign two complex number equation show absolute value complex number nonnegative real number property complex number stated exercise end chapter example let r commutative ring unit polynomial r indeterminate variable expression form p αn n αn r coefficient polynomial instead α j j often write α j j set polynomial r denoted r let p αn n q βm two polynomial r n n set β j j n call p q equal written p q α j β j j particular αn n αn n n degree polynomial p αn n denoted deg p defined largest index j α j index exists polynomial zero polynomial p set deg p let p q r degree n respectively n n set β j j define following operation r  \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 algebraic structure p q αn βn n αi β j p q γk operation r commutative ring unit zero given polynomial p unit p r field since every polynomial p r invertible even r field example p polynomial q βm r p q βm hence p invertible polynomial substitute variable object resulting expression evaluated algebraically example may substitute λ r interpret addition multiplication corresponding operation ring defines map r r λ p λ αn λn λk λ λ k n k time r empty product one confuse ring element p λ polynomial p rather think p λ evaluation p λ study property polynomial detail later also evaluate polynomial object matrix endomorphisms exercise determine following whether form group x r x b b b r b ab let b r map f b r r r r x ax ay set g f b b r given show g commutative group operation g g g defined composition two map cp definition let x ø set let x f x x f bijective show x group let g group g denote g unique inverse element show following rule element g b c b b b  \n",
       "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ring field prove theorem let g group fixed g let z g g g g g show z g subgroup subgroup element g commute called centralizer let ϕ g h group homomorphism show following assertion u g subgroup also ϕ u h subgroup furthermore g commutative also ϕ u commutative even h commutative b v h subgroup also v g subgroup let ϕ g h group homomorphism let eg e h neutral element group g h respectively show ϕ eg e h b let ker ϕ g g ϕ g e h show ϕ injective ker ϕ eg show property definition r example order show r commutative ring unit suppose example replace codomain r map commutative ring unit r still commutative ring unit let r ring n show following assertion n even r n n n odd b exists unit r n r invertible element r n n n called nilpotent let r ring unit show r let r ring unit let r denote set invertible element show r group called group unit r b determine set k k k field fixed n n let nz nk k z n example show nz subgroup z b define b b b b b b addition multiplication addition multiplication z show following assertion  \n",
       "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    algebraic structure well defined ii commutative ring unit iii field n prime number let r ring subset r called subring r ring show subring r following property hold r r also r r r also show definition field describe mathematical structure let k field show l subfield k cp definition following property hold l k k b l b l b l l l show field hold let r commutative ring contain non-trivial zero divisor ring called integral domain define r r relation x x x x show equivalence relation b denote equivalence class x xy show following map well defined x x x x x x x x denotes quotient set respect cp definition c show field field called quotient field associated field r z exercise consider r k ring polynomial field k construct way field rational function  \n",
       "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ring field let c b determine b b b ab ba show following rule complex number z z z z z z z z z z b z z z z z c show absolute value complex number satisfies following property z z z b z c equality z c z z z c  \n",
       "33                                                                                                                                                                                                                                                                                                                                                                                                      chapter matrix chapter define matrix important operation study several group ring matrix james joseph sylvester coined term matrix described matrix oblong arrangement term matrix operation defined chapter introduced arthur cayley article memoir theory matrix first consider matrix independent algebraic object book matrix form central approach theory linear algebra basic definition operation begin formal definition matrix definition let r commutative ring unit let n array form ai j anm latin word matrix mean womb sylvester considered matrix object may form various system determinant cp chap interestingly english writer charles lutwidge dodgson better known pen name lewis carroll objected sylvester term wrote aware word matrix already use express meaning use word block surely former word mean rather mould form algebraic quantity may introduced actual assemblage quantity dodgson also objected notation ai j matrix entry space occupied number wholly superfluous important part notation reduced minute subscript alike difficult writer springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      matrix ai j r n j called matrix size n ai j called entry coefficient matrix set matrix denoted r n following usually assume without explicitly mentioning excludes trivial case ring contains zero element cp exercise formally definition n obtain empty matrix size n denote matrix used technical reason proof analyze algebraic property matrix however always consider n zero matrix r n denoted matrix entry equal matrix size n n called square matrix square entry aii n called diagonal entry identity matrix r n n matrix δi j j δi j kronecker delta-function.2 clear n considered write instead n set ith row r n aim r n use comma optical separation entry jth column j j r j j thus row column matrix matrix matrix ai aim r n given combine matrix n r leopold kronecker anm  \n",
       "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   basic definition operation write square bracket around row way combine n matrix j j j r j j matrix n r anm n n ai j r ni j j combine four matrix matrix r n matrix ai j called block block matrix introduce four operation matrix begin addition r n r n r n b b ai j bi j addition r n operates entrywise based addition note addition defined matrix equal size multiplication two matrix defined follows r n r r n b b ci j ci j aik bk j thus entry ci j product b constructed successive multiplication summing entry ith row jth column b clearly order define product b number column must equal number row b definition entry ci j matrix b written multiplication symbol element follows usual convention omitting multiplication sign clear multiplication considered eventually also omit multiplication sign matrix  \n",
       "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  matrix illustrate multiplication rule cij equal ith row time jth column b follows j b b b mj m aim ci j anm important note matrix multiplication general commutative example matrix b hand b although b b defined obviously b b case one recognizes non-commutativity matrix multiplication fact b b different size even b b defined size general b b example yield two product b matrix multiplication however associative distributive respect matrix addition  \n",
       "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         basic definition operation lemma r n b b r c r k following assertion hold b c b b b b b b b b im proof show property others exercise let r n b r c r k well b c di j b c di j definition matrix multiplication using associative distributive law r get di j ait bts c j ait bts c j c j ait bts ait bts c j di j n j k implies b c b c right hand side lemma written parenthesis since use common convention multiplication matrix bind stronger addition r n n define k n ak k time another multiplicative operation matrix multiplication defined follows r r n r n λ λ λai j easily see r n addition scalar multiplication following property lemma b r n c r λ μ r following assertion hold λμ λ μ λ μ λ μ term scalar introduced sir william rowan hamilton originates latin word scale mean ladder  \n",
       "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      matrix λ b λ λ b λ c λ c λ c proof exercise fourth matrix operation introduce transposition r n r n ai j bi j bi j ji example matrix called transpose definition r n n satisfies called symmetric called skew-symmetric transposition following property lemma r n b r λ r following assertion hold λ λ b b proof property exercise proof let b ci j ai j b bi j b ci j ci j aik bk j ci j c ji jk bki ak j bik ak j see b b matlab-minute carry following command order get used matrix operation chapter matlab notation order see matlab output put semicolon end command  \n",
       "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               basic definition operation example consider example car insurance premium chap recall pi j denotes probability customer class ci year move class c j example consists four class probability associated row-stochastic matrix cp denote suppose insurance company following distribution customer four class class class class class matrix describes initial customer distribution using matrix multiplication compute p contains distribution customer next year example consider entry p position computed customer class year move class thus respective initial percentage multiplied probability customer class class probability respectively yield two product continuing way obtain k year distribution pk k k formula also hold k since p insurance company use formula compute revenue payment premium rate coming year assume full premium rate class euro per year rate class euro discount customer initially revenue first year euro  \n",
       "40                                                                                                                                                                                                                                                                                                                                                                                            matrix customer cancel contract model yield revenue year k pk p k example revenue next year rounded full euro number decrease annually rate decrease seems slow exists stationary state state revenue changing significantly property model guarantee existence state important practical question insurance company existence stationary state guarantee significant revenue long-time future since formula depends essentially entry matrix p k reached interesting problem linear algebra analysis property row-stochastic matrix analyze property sect matrix group ring section study algebraic structure formed certain set matrix matrix operation introduced begin addition r n theorem r n commutative group neutral element r n zero matrix ai j r n inverse element j r n write b instead proof using associativity addition r arbitrary b c r n obtain b c ai j bi j ci j ai j bi j ci j ai j bi j ci j ai j bi j ci j b c thus addition r n associative zero matrix r n satisfies ai j ai j ai j given ai j r n j r n j ai j j ai j finally commutativity addition r implies b ai j bi j ai j bi j bi j ai j b note lemma implies transposition homomorphism even isomorphism group r n r n cp definition  \n",
       "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              matrix group ring theorem r n n ring unit given identity matrix ring commutative n proof already shown r n n commutative group cp theorem property ring associativity distributivity existence unit element follow lemma commutativity n hold commutativity multiplication ring example show ring r n n commutative n example proof theorem show n ring r n n non-trivial zero-divisors exist matrix b r n n b exist even r field let u consider invertibility matrix ring r n n respect r n n must satisfy matrix multiplication given matrix r n n inverse cp definition inverse two equation r n n exists invertible inverse unique denoted cp theorem invertible matrix sometimes called non-singular non-invertible matrix called singular show corollary existence inverse already implied one two equation one hold invertible correct check validity equation matrix r n n invertible simple example non-invertible matrix r r another non-invertible matrix however considered element unique inverse given lemma b r n n invertible following assertion hold invertible also write matrix b invertible b b  \n",
       "42                                                                                                                                                                                                                              matrix proof using lemma int int thus inverse already shown theorem general ring unit thus hold particular ring r n n next result show invertible matrix form multiplicative group theorem set invertible matrix r form group respect matrix multiplication denote group g l n r gl abbreviates general linear group proof associativity multiplication g l n r clear shown lemma product two invertible matrix invertible matrix neutral element g l n r identity matrix since every g l n r assumed invertible exists g l n r introduce important class matrix definition let ai j r n n called upper triangular ai j j called lower triangular ai j j upper triangular called diagonal ai j j upper lower triangular write diagonal matrix diag ann next investigate set matrix respect group property beginning invertible upper lower triangular matrix theorem set invertible upper triangular n n matrix invertible lower triangular n n matrix r form subgroup g l n r proof show result upper triangular matrix proof lower triangular matrix analogous order establish subgroup property prove three property theorem since invertible upper triangular matrix set invertible upper triangular matrix nonempty subset g l n r next show two invertible upper triangular matrix b r n n product c b invertible upper triangular matrix invertibility c ci j follows lemma j  \n",
       "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                matrix group ring n ci j aik bk j bk j k j aik bk j aik k j since j j therefore c upper triangular remains prove inverse invertible upper triangular matrix upper triangular matrix n assertion hold trivially assume n let ci j equation written system n equation j j ann cn j δn j j δi j kronecker delta-function defined prove inductively n n diagonal entry aii invertible cii n ci j δi j j formula implies particular ci j j n last row given ann cn j δn j j j n ann cnn cnn ann second equation use cnn commutativity multiplication therefore ann invertible ann thus δn j j cn j ann equivalent note n sum empty thus equal zero particular cn j j n assume assertion hold n k k n particular ci j k n j word row n k upper triangular order prove assertion k consider kth row given  \n",
       "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        matrix akk ck j ak j akn cn j δk j j j k n obtain akk ckk ak k akn cnk induction hypothesis k cn k implies akk ckk ckk akk used commutativity multiplication hence ckk get akk invertible akk δk j ak j akn cn j ck j akk j n hence hold k j δk j j cn j give ck j point represents recursive formula computing entry inverse invertible upper triangular matrix using formula entry computed bottom top right left process sometimes called backward substitution following frequently partition matrix block make use block multiplication every k n write r n n r k k r b r n n partitioned like product b evaluated blockwise particular g l k r g l r g l n r direct computation show  \n",
       "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              matrix group ring matlab-minute create block matrix matlab carrying following command tridiag zero k investigate meaning command full compute product well inverse inv inv b compute inverse b matlab formula corollary set invertible diagonal n n matrix r form commutative subgroup respect matrix multiplication invertible upper lower triangular n n matrix proof since invertible diagonal matrix invertible diagonal n matrix form nonempty subset invertible upper lower triangular n n matrix diag ann b diag bnn invertible b invertible cp lemma diagonal since b diag ann diag bnn diag ann bnn moreover diag ann invertible aii r invertible n cp proof theorem inverse given ann finally commutativity property invertible diagonal matrix diag b b follows directly commutativity definition matrix p r n n called permutation matrix every row every column p exactly one unit entry zero term permutation mean exchange matrix r n n multiplied permutation matrix left right row column respectively exchanged permuted example p  \n",
       "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     matrix p p theorem set n n permutation matrix r form subgroup g l n r particular p r n n permutation matrix p invertible p p proof exercise omit multiplication sign matrix multiplication write ab instead b exercise following exercise r commutative ring unit consider following matrix z b c determine possible matrix c bc b c c b ac c b consider matrix ai j r n x r ym r xn following expression well defined n n x b x c yx yx e x ay f x ay g x ay h x ay x j x k ax l x show following computational rule r n r r  \n",
       "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               matrix group ring prove lemma prove lemma prove lemma let determine n n let p αn n r polynomial cp example r define p r p αn im b fixed matrix r consider map f r r p p show f p q f p f q f pq f p f q p q r map f ring homomorphism ring r r c show f r p p r commutative subring r f r subring r cp exercise multiplication subring commutative map f surjective determine p p z let k field show every matrix k n n written symmetric matrix k n n skew-symmetric matrix k n n also hold field give proof counterexample show binomial formula commuting matrix b r n n k j j k k ab b b j b kj j j n n invertible show let r matrix j hold every let r n n matrix n exists let smallest natural number property investigate whether invertible give particularly simple representation inverse b determine cardinality set ak k n let ai j r n n j j n show subring r n n b show r n n subring property called left ideal r n n c determine analogous subring b r n n b b r n n b b subring property called left ideal r n n examine whether g  \n",
       "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 matrix co α sin α α r sin α co α subgroup g l r generalize block multiplication matrix r n b r determine invertible upper triangular matrix r n n let r n n r n n r n n r n n let b let r n n g l n r show invertible invertible derive case formula g l n r show invertible invertible derive case formula let g l n r u r n v r n show following assertion u v g l n r hold im v u g l r b im v u g l r u v u im v u v last equation called sherman-morrison-woodbury formula named jack sherman winifred morrison max woodbury show set block upper triangular matrix invertible diagonal block set matrix amm aii g l r group respect matrix multiplication prove theorem group permutation matrix commutative show following equivalence relation r n n exists permutation matrix p p b company produce four raw material five intermediate product z z z z z three final product e e e following table show many unit ri z j required producing one unit z k e respectively  \n",
       "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   matrix group ring instance five unit one unit required producing one unit z determine help matrix operation corresponding table show many unit ri required producing one unit e b determine many unit four raw material required producing unit e unit e unit e  \n",
       "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 chapter echelon form rank matrix chapter develop systematic method transforming matrix entry field special form called echelon form transformation consists sequence multiplication left certain elementary matrix invertible echelon form identity matrix inverse product inverse elementary matrix non-invertible matrix echelon form sense closest possible matrix identity matrix form motivates concept rank matrix introduce chapter use frequently later elementary matrix let r commutative ring unit n n j n let r n n identity matrix let ei ith column en define e j ei e tj ei r n n column j entry j e j entry n j define pi j e j e ei e en r n n thus pi j permutation matrix cp definition obtained exchanging column j multiplication r n left pi j mean exchange ofthe row j example springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  echelon form rank matrix λ r define mi λ λei en r n n thus mi λ diagonal matrix obtained replacing ith column λei multiplication r n left mi λ mean multiplication ith row λ example n j λ r define g j λ λe ji ei λe j en r n n thus lower triangular matrix g j λ obtained replacing ith column ei λe j multiplication r n left g j λ mean λ time ith row added jth row similarly multiplication r n left upper triangular matrix g j λ mean λ time jth row added ith row example g g g lemma elementary matrix pi j mi λ invertible λ r g j λ defined respectively invertible following inverse j pi j pi j mi λ mi g j λ g j proof invertibility pi j j pi j already shown theorem symmetry pi j easily seen  \n",
       "52                                                                                                                                                                                                                                                                                elementary matrix since λ r invertible matrix mi well defined straightforward computation show mi mi λ mi λ mi since e tj ei j e ei e tj ei e tj therefore g j λ g j λe ji e ji λe ji e ji e similar computation show g j g j λ echelon form gaussian elimination constructive proof following theorem relies gaussian elimination given matrix k n k field algorithm construct matrix g l n k c quasi-upper triangular obtain special form left-multiplication elementary matrix pi j mi j λ g j λ left-multiplications corresponds application one so-called elementary row operation matrix pi j exchange two row mi λ multiply row invertible scalar g j λ add multiple one row another row assume entry field rather ring proof theorem require nonzero entry invertible generalization result hold certain ring integer z given hermite normal play important role number theory theorem let k field let k n exist invertible matrix st k n n product elementary matrix c st echelon form either c named carl friedrich gauß similar method already described chap rectangular array nine chapter mathematical art text developed ancient china several decade bc stated problem every day life gave practical mathematical solution method detailed commentary analysis written liu hui approx ad around ad charles hermite  \n",
       "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           echelon form rank matrix c denotes arbitrary zero nonzero entry precisely c ci j either zero matrix exists sequence natural number jr called step echelon form jr r min n ci j r j ji ci j r n j ci ji r entry column ji zero n k n n invertible c case st proof set c done let let index first column ai j consist zero let ai first entry column nonzero form proceed follows first permute row finally eliminate normalize new first row multiply ai j nonzero element first entry column permuting normalizing lead  \n",
       "54                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     echelon form gaussian elimination j set order eliminate column left matrix multiply g g n g g n ai j n j keep index larger matrix smaller matrix finished since c echelon form case r least one entry nonzero apply step described matrix k define matrix sk recursively sk k k sk jk matrix constructed analogous first identify first column jk k completely zero well first nonzero entry ai k k jk column permuting normalizing yield matrix k ai k j mk k aik jk pk ik k  \n",
       "55                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     echelon form rank matrix k k set pk k k k k pk ik jk g k jk mk aik jk sk g k n sk indeed product elementary matrix form elementary matrix size n k n k continue procedure inductively end r min n step either r r r step sr construction entry position r jr r echelon form see discussion beginning proof r still eliminate nonzero entry column jr denote matrix r ri j form k r recursively r k ri k j sr r g k sr g k jk jk c st echelon form suppose n c st echelon form invertible c product invertible matrix thus invertible invertible matrix row containing zero r n hence c hand c invertibility elementary matrix  \n",
       "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           echelon form gaussian elimination implies product invertible matrix invertible st literature echelon form sometimes called reduced row echelon form example transformation matrix echelon form via left multiplication elementary matrix g g g g matlab-minute echelon form computed matlab command rref reduced row echelon form apply rref eye order compute inverse matrix gallery tridiag cp exercise formulate conjecture general form prove conjecture proof theorem lead so-called lu square matrix theorem every matrix k n n exists permutation matrix p k n n lower triangular matrix l gln k one diagonal upper triangular matrix u k n n plu matrix u invertible invertible  \n",
       "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      echelon form rank matrix u upper proof k n n eq form sn u triangular r n set sn sr since matrix invertible invertible sn invertible follows u n every matrix si form pi j si si sn ji n pi ji permutation necessary therefore sn sn n sn j j sn j j form permutation matrix k n k implies pk j pk jk k sn sn  \n",
       "58                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         echelon form gaussian elimination hold certain j k j hence sn sn n sn sn n sn invertible lower triangular matrix permutation matrix form group respect matrix multiplication cp theorem thus permutasn l p l invertible lower triangular p lnn invertible tion matrix since l li j invertible also diag l p l u u obtain p lu p p construction diagonal entry l equal one example computation lu matrix g g u hence p l g g thus p p diag  \n",
       "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        echelon form rank matrix l u u gln k lu yield u l p hence computing lu one obtains inverse essentially inverting two triangular matrix since achieved efficient recursive formula lu popular method scientific computing application require inversion matrix solution linear system equation cp chap context however alternative strategy choice permutation matrix used example instead first nonzero entry column one chooses entry large largest absolute value row exchange subsequent elimination strategy influence rounding error computation reduced matlab-minute hilbert matrix ai j qn n entry ai j j j generated matlab command hilb n carry command l u p hilb order compute lu matrix hilb matrix p l u look like compute also lu matrix full gallery tridiag study corresponding matrix p l u show given matrix matrix c theorem uniquely determined certain sense need following definition definition c k n echelon form theorem position r jr called pivot position also need following result lemma z gln k x k z x x proof exercise theorem let b k n echelon form z b matrix z gln k b david hilbert  \n",
       "60                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           echelon form gaussian elimination proof b zero matrix zb hence b let b let b respective column ai bi furthermore let r jr r pivot position b show every matrix z gln k z b form ir z z k since b echelon form entry b row r zero follows b z b since first pivot position b bi k b first column z b implies ai k z b z since z invertible lemma implies k since echelon form b furthermore z z n z z k cp exercise r done r proceed pivot position analogous way since b echelon form kth pivot position give b jk ek jk z b jk invertibility z obtain jk b jk z z z k result yield uniqueness echelon form matrix invariance left-multiplication invertible matrix corollary k n following assertion hold unique matrix c k n echelon form transformed elementary row operation left-multiplication elementary matrix matrix c called echelon form gln k matrix c also echelon form echelon form matrix invariant left-multiplication invertible matrix proof echelon form invertible theorem give gln k echelon form get theorem give  \n",
       "61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  echelon form rank matrix rank equivalence matrix seen corollary echelon form k n unique particular every matrix k n exists unique number pivot position cp definition echelon form justifies following definition definition number r pivot position echelon form k n called denoted rank see immediately k n always rank min n rank moreover theorem show k n n invertible rank property rank summarized following theorem theorem k n following assertion hold exist matrix q gln k z glm k q az ir r rank r q gln k z glm k rank rank q az bc b k n c k rank rank b b rank rank c rank rank exist matrix b k n c k bc rank proof let q g l n k q b echelon form q q bc matrix q bc first rank b row contain nonzero entry corollary echelon form q equal echelon form thus normal echelon form also first rank b row nonzero implies rank rank b rank r ir assertion hold arbitrary matrix q g l n k z g l k r exists matrix q g l n k q echelon form r pivot position exists permutation matrix p k product elementary permutation matrix pi j concept rank introduced context bilinear form first ferdinand georg frobenius  \n",
       "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      rank equivalence matrix p q ir v matrix v k r r v following simplicity omit size zero matrix matrix invertible thus ir ir v k k ir ypa q z p g l k obtain ir q az suppose hold k n matrix q g l n k z g l k obtain rank rank az z rank az rank thus particular rank rank az due invariance echelon form hence rank left-multiplication invertible matrix cp corollary get rank rank az rank q az rank ir k q gln k z glm k invariance rank left-multiplication invertible matrix used showing rank rank q az z rank q az rank az rank hence particular rank rank q az rank r exist matrix q g l n k z g l k q az r therefore  \n",
       "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               echelon form rank matrix rank rank q az rank ir rank ir rank q az rank z q rank using obtain rank rank rank c b rank c rank c let bc b k n c k rank rank bc rank b let hand rank r exist matrix q ir thus obtain g l n k z g l k q az q ir r ir r z bc b k n c k example matrix example echelon form since two pivot position rank multiplying right q yield ab hence rank ab rank assertion theorem motivates following definition  \n",
       "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          rank equivalence matrix definition two matrix b k n called equivalent exist matrix q g l n k z g l k q b z name suggests defines equivalence relation set k n since following property hold reflexivity q az q z im symmetry q b z b q az transitivity q b z b q c z q q c z z equivalence class k n given q az q g l n k z g l k rank r theorem ir r therefore ir ir consequently rank fully determines equivalence class matrix ir k n called equivalence normal form obtain ir k r ir ø r n min n hence min n pairwise distinct equivalence class ir n r min n complete set representative proof theorem know k n n n noncommutative ring unit contains non-trivial zero divisor using equivalence normal form characterized follows k n n invertible zero divisor since ab implies b  \n",
       "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               echelon form rank matrix k n n zero divisor invertible hence rank r n equivalence normal form identity matrix let q z g l n k given q az r every matrix v r r r b z v ab q ir k n n r v b since z invertible exercise following exercise k arbitrary field compute echelon form matrix b c e simplicity element denoted k instead k state elementary matrix carry transformation one matrix invertible compute inverse product elementary matrix αβ let k αδ βγ determine echelon form γ δ k n n k b k show let b g l n k b g l k consider matrix k  \n",
       "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              rank equivalence matrix k field rational function cp exercise examine whether invertible determine possible verify result computing show g l n k echelon form k given inverse invertible matrix thus computed via transformation echelon form two matrix b k n called left equivalent exists matrix q g l n k q b show defines equivalence relation k n determine simple representative equivalence class prove lemma determine lu cp theorem matrix r one matrix invertible determine inverse using lu decomposition let hilbert matrix cp matlab-minute definition determine rank lu theorem p determine rank matrix αβ γ dependence α β γ let b k n n given show rank rank b rank ac b c k n n examine inequality strict let b c determine rank ba b let b ba ab show following assertion b b b c b c c b ii λa μb c λm c μm b c λ μ r iii rank b exist λ μ r λ μ λa μb iv rank b  \n",
       "67                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   chapter linear system equation solving linear system equation central problem linear algebra discus introductory way chapter system arise numerous application engineering natural social science major source linear system equation discretization differential equation linearization nonlinear equation chapter analyze solution set linear system equation characterize number solution using echelon form chap also develop algorithm computation solution definition linear system equation field k n equation unknown xm form xm xm anm xm bn ax b coefficient matrix ai j k n right hand side b bi k given b linear system called homogeneous otherwise x b called solution linear non-homogeneous every x k system x form solution set linear system denote l b next result characterizes solution set l b linear system ax b using solution set l associated homogeneous linear system ax springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   linear system equation lemma let k n b k l b ø given x l b l b x l x z z l proof z l thus x z x l x z x z b b hence x z l b show x l l b z x let l b let x b b z x z x l show l b z l hence x l closer look set l clearly l ø z l λ k z λ z λ hence z l furthermore z z l z z z z z l thus l nonempty subset k hence z closed scalar multiplication addition lemma k n b k k n n l b l sb moreover invertible l b l sb proof x l b also x sb thus x l sb show l b l sb invertible l sb sb multiplying left yield b since l b l sb l b consider linear system equation ax b theorem find matrix g l n k echelon form let b bi sb l b l b lemma linear system ax b take form x bn  \n",
       "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               linear system equation suppose rank r let jr pivot column using rightmultiplication permutation matrix move r pivot column first r column achieved p e e jr e e e e e jr e jr em k yield p ir r k r r permutation lead simplification following presentation usually omitted practical computation b p p x b ay b since p p im write ax form ir yr br yr b r r bn ym apt x b left-multiplication x p mean different ordering unknown xm thus solution ax b easily recovered solution ay b vice versa l b x p l b l b solution determined using extended coefficient matrix b k n note rank obtained attaching b extra column rank b equality br bn rank bn nonzero rank b least one br solution since entry row r n zero rank bn hand rank b br written yr yr ym br  \n",
       "70                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         linear system equation representation implies particular br l b b order determine lemma know l b b l set br yield l ym yr ym arbitrary yr yr ym l thus r b solution ay b uniquely determined example extended coefficient matrix b rank rank b l b ø b written ay b hence b l b l arbitrary summarizing consideration following algorithm solving linear system equation algorithm let k n b k given determine g l n k echelon form define b sb rank rank b l b l b ø p r rank rank b define l b l b l b b l l b determined well l b p rank b rank since rank rank rank b rank b discussion also yield following result different case solvability linear system equation  \n",
       "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                linear system equation corollary k n b k following assertion hold rank rank b l b ø rank rank b b exists unique solution rank rank b exist many solution field k infinitely many element k q k r k c exist infinitely many pairwise distinct solution different case corollary studied example example let k q consider linear system equation ax b b form b apply gaussian elimination algorithm order transform echelon form b rank rank b hence exist solution pivot column ji p p ax b written  \n",
       "72                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                linear system equation consequently b l b l b b l l x arbitrary exercise find field k matrix k n k n n b k l b l sb determine l b following b b b r b let α q bα α determine l l bα dependence α let k n b k n denote bi ith column b show linear system equation ax b least one solution x k rank rank rank rank b find condition solution unique  \n",
       "73                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 linear system equation let n n βn bn αn given βi αi determine recursive formula entry solution linear system ax b  \n",
       "74                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               chapter determinant matrix determinant map assigns every square matrix r n n r commutative ring unit element map interesting important property instance yield necessary sufficient condition invertibility r n n moreover form basis definition characteristic polynomial matrix chap definition determinant several different approach define determinant matrix use constructive approach via permutation definition let n n given bijective map σ n n j σ j called permutation number n denote set map sn permutation σ sn written form σ σ σ n example lemma know n springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   determinant matrix set sn composition map form group cp exercise sometimes called symmetric group neutral element group permutation n commutative group group sn n noncommutative example consider n permutation definition let n σ sn pair σ σ j j n σ σ j called inversion σ k number inversion σ sgn σ k called sign σ n define sgn short inversion permutation σ pair order term inversion confused inverse map σ exists since σ bijective sign permutation sometimes also called signature example permutation inversion sgn permutation inversion sgn define determinant map definition let r commutative ring unit let n map det r n n r ai j det sgn σ n ai σ called determinant ring element det called determinant formula det called signature formula term sgn σ definition interpreted element ring r either sgn σ r sgn σ r r unique additive inverse unit example n thus det sgn n get det det sgn sgn gottfried wilhelm leibniz  \n",
       "76                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   definition determinant n sarrus det order compute det using signature formula leibniz form n product n factor large n costly even modern computer see corollary efficient way computing det signature formula mostly theoretical relevance since represents determinant explicitly term entry considering n entry variable interpret det polynomial variable r r r c standard technique analysis show det continuous function entry study group permutation detail permutation σ inversion sgn σ moreover σ σ σ σ σ σ σ j σ j sgn σ observation generalized follows lemma σ sn sgn σ σ j σ j proof n left hand side empty product defined cp sect hold n let n σ sn sgn σ k k number pair σ σ j j σ σ j σ j σ k j σ k j last equation used fact two product factor except possibly order pierre frédéric sarrus  \n",
       "77                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       determinant matrix theorem sn sgn sgn sgn particular sgn σ sgn σ σ sn proof lemma sgn j j j j σ j σ j j sgn j j j sgn j sgn sgn σ sn sgn n sgn σ σ sgn σ sgn σ sgn σ sgn σ theorem show map sgn homomorphism group sn operation second group standard multiplication integer definition transposition permutation τ sn n exchange exactly two distinct element k n τ k τ k τ j j j n k obviously τ τ every transposition τ sn lemma let τ sn transposition exchange k k τ exactly inversion hence sgn τ proof k j j thus τ given τ k k j k k j k n point denote value τ increasing thus correct order simple counting argument show τ exactly j k inversion  \n",
       "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     property determinant property determinant section prove important property determinant map lemma r n n following assertion hold λ r det λ det λ n λ det ai j upper lower triangular det aii zero row column det n two equal row two equal column det det det proof exercise follows application upper lower triangular matrix zero row column every σ sn least one factor n ai σ equal zero thus det product let row k k ai j equal ak j j let τ sn transposition exchange element k let tn σ sn σ k σ since set tn contains permutation σ sn σ k σ sn tn σ τ σ tn moreover ai σ k ak σ k σ k ak σ σ σ k ak σ k thus using theorem lemma obtain sgn σ n ai σ sgn σ τ ai σ n n ai sgn σ n ai σ  \n",
       "79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     determinant matrix implies det sgn σ n ai σ sgn σ n ai σ sgn σ n ai σ proof case two equal column analogous observe first σ n σ n every σ sn see let n fixed σ j σ j thus σ j element first set j σ j j element second set since σ bijective two set equal let ai j bi j bi j ji det sgn σ bi σ sgn σ n aσ sgn σ n sgn σ n n aσ sgn σ n ai ai σ det used sgn σ cp theorem fact sgn σ n ai factor two product aσ example matrix b c obtain det lemma det b det c lemma may also compute determinant using sarrus rule example item lemma show particular det identity matrix en r n n reason determinant map called normalized  \n",
       "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     property determinant σ sn matrix pσ eσ eσ eσ n called permutation matrix associated σ map group sn group permutation matrix r n n bijective inverse permutation matrix transpose cp theorem easily check pσt r n n j r jth column pσ aσ aσ aσ n right-multiplication pσ exchange column according permutation σ hand ai r n ith row aσ pσt aσ n left-multiplication pσt exchange row according permutation σ next study determinant elementary matrix lemma σ sn associated permutation matrix pσ r n n sgn σ det pσ n pi j defined det pi j mi λ g j λ defined respectively det mi λ λ det g j λ proof σ sn ai j r n n j j j n entry zero hence det σ det σ sgn σ n aσ j j sgn σ n σ σ j j sgn σ σ permutation matrix pi j associated transposition exchange j hence det pi j follows lemma since mi λ g j λ lower triangular matrix assertion follows lemma  \n",
       "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       determinant matrix result lead important computational rule determinant lemma r n n n λ r following assertion hold multiplication row λ lead multiplication det λ det mi λ λ det det mi λ det addition row another row change det det g j λ det det g j λ det det g j λ det det g j λ det exchanging two row change sign det det pi j det det pi j det proof mi λ amk amk amk λamk amk hence det n sgn σ σ sgn σ ai σ n σ σ σ λ det g j λ amk amk amk amk j jk λaik j hence det sgn σ j σ j λai σ j n σ j sgn σ n σ λ sgn σ ai σ j n σ j first term equal det second equal determinant matrix two equal column thus equal zero proof matrix g j λ analogous  \n",
       "82                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             property determinant permutation matrix pi j exchange row j j exchange expressed following four elementary row operation multiply row j add row row j add row j row add row row j therefore pi j g j g j g j j one may verify also carrying matrix multiplication using obtain det pi j det g j g j g j j det g j det g j det g j det j det det since det det cp lemma result lemma row formulated analogously column example consider matrix b simple calculation show det since b obtained exchanging first two column det b det determinant map interpreted map r n r map n column matrix r n n ring ai j r two column ai j det det j ai lemma due property determinant map called alternating map column analogously determinant map alternating map row j form λa μa λ μ r kth row j j n akn r j  \n",
       "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   determinant matrix n det det μa sgn σ λak σ k μak σ k ai σ sgn σ ak σ k n ak σ k μ sgn σ ak σ k λ det μ det n ai σ property called linearity determinant map respect row analogously linearity respect column linear map studied detail later chapter next result called multiplication theorem determinant theorem k field b k n n det ab det det b moreover invertible det det proof theorem know k n n exist invertible elementary st echelon form lemma matrix st det det det det well det ab det ab det det det ab thus also ab zero two case invertible det ab row det implies det hence det ab det det b hand invertible echelon form det give det ab det det b since finally invertible det det det det hence det det since proof relies theorem valid matrix field k formulated theorem b k n n however multiplication theorem determinant also hold matrix commutative ring r unit direct proof based signature formula leibniz found example book advanced linear algebra loehr sect book also contains proof cauchy-binet formula det ab r n b r n n sometimes use det ab det det b  \n",
       "84                                                                                                                                                                                                                                                                                                                                                                                                                                       property determinant hold b r n n although shown result theorem b k n n proof theorem suggests det easily computed transforming k n n echelon form using elementary row operation corollary k n n let st k n n elementary matrix zero row hence st echelon form either det hence det det det st shown theorem every matrix k n n factorized p lu hence det det p det l det u determinant matrix right hand side easily computed since permutation triangular matrix lu matrix therefore yield efficient way compute det matlab-minute look matrix wilkinson n matlab find general formula entry compute n l u p lu cp matlab-minute definition det l det u det p det p l u det permutation associated computed matrix p det integer odd n minor laplace expansion show determinant used deriving formula inverse invertible matrix solution linear system equation formula however theoretical practical relevance definition let r commutative ring unit let r n n n matrix j r obtained deleting jth row ith column called minor matrix adj bi j r n n bi j j det j called adjunct adjunct also called adjungate classical adjoint term introduced james joseph sylvester  \n",
       "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          determinant matrix theorem r n n n adj adj det particular invertible det r invertible case det det det adj proof let b bi j entry bi j j det j c ci j adj satisfies ci j n bik ak j n det k ak j let column let ek r n n k ek kth column identity matrix exist permutation matrix p q perform k row column exchange respectively q p k k using lemma obtain q det p k k det q det p det k det k det det k det k linearity determinant respect column give ci j n ak j det k det j j det j δi j det thus adj det analogously show adj det  \n",
       "86                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             minor laplace expansion det r invertible det adj det adj invertible det adj hand invertible det det det det det det used multiplication theorem determinant r cp comment following proof theorem thus det invertible det det det adj example det thus invertible invertible considered element since case det det z det matrix invertible since z invertible note r n n invertible theorem show obtained inverting one ring element det use theorem multiplication theorem matrix commutative ring unit prove result already announced sect order r n n unique inverse r n n one two show need checked equation r n n exists corollary let r n n matrix invertible multiplication theorem determinant yield proof det det det det det det thus also invertible det r invertible det det unique inverse n obvious n shown right theorem multiply equation get analogous proof starting  \n",
       "87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       determinant matrix let u summarize invertibility criterion square matrix field shown far g l n k theorem echelon form identity matrix definition clear rank n rank rank b n b k algorithm b b k theorem det alternatively obtain g l n k theorem echelon form least one zero row definition clear rank n rank n algorithm l theorem det field q r c usual absolute value number formulate following useful invertibility criterion matrix theorem k n n k q r c diagonally dominant n j n j det proof prove assertion contraposition showing det implies diagonally dominant det l homogeneous linear system xn let xm equation ax least one solution x entry x maximal absolute value xm x j j x given particular xm mth row amn xn amm xm n j xj j take absolute value side use triangle inequality yield  \n",
       "88                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           minor laplace expansion xm n j j n j xm hence n j j j diagonally dominant converse theorem hold example matrix det diagonally dominant theorem obtain laplace determinant particularly useful contains many zero entry cp example corollary r n n n following assertion hold n det n j ai j det j laplace expansion det respect ith row j n det n j ai j det j laplace expansion det respect jth column proof two expansion det follow immediately comparison diagonal entry matrix equation det adj det adj laplace expansion allows recursive definition determinant r n n n let det defined corollary choose arbitrary row column formula det contains matrix size use laplace expansion expressing determinant term determinant n n matrix recursively matrix remain r define det finally state cramer give explicit formula solution linear system form determinant rule theoretical value order compute n component solution requires evaluation n determinant n n matrix pierre-simon gabriel laplace published expansion cramer  \n",
       "89                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            determinant matrix corollary let k field g l n k b k unique solution linear system equation ax b given xn b det adj b x xi det b det example consider q b laplace expansion respect last column yield det det det thus invertible ax b unique solution x b cramer rule following entry det det det det det det det det  \n",
       "90                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    minor laplace expansion exercise permutation σ sn called r exists subset ir n r element σ k k r σ ir σ ir write r σ ir particular transposition τ sn let n given compute b let n σ determine σ j j c show inverse cycle ir given ir show two cycle disjoint element ir j ir j ø commute e show every permutation σ sn written product disjoint cycle except order uniquely determined σ prove lemma using show group homomorphism sgn sn satisfies following assertion set σ sn sgn σ subgroup sn cp exercise b σ π sn π σ π compute determinant following matrix en zn n ei ith column identity matrix b b bi j zn n bi j c c eπ π e π π πe e  \n",
       "91                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               determinant matrix wilkinson matrix cp matlab-minute end sect construct matrix b rn n n det b det det b let r commutative ring unit n r n n show following assertion hold b c e f g adj adj ab adj b adj b r n n invertible adj λa adj λ adj adj det adj det invertible adj adj det adj adj invertible one drop requirement invertibility b e let n ai j rn n ai j xi xn j yn hence particular xi j j matrix called cauchy show det x j x j x j yi b use derive formula determinant n n hilbert matrix cp matlab-minute definition let r commutative ring unit αn r n n n r αn vn called vandermonde show det vn james hardy wilkinson louis cauchy alexandre-théophile vandermonde augustin α j αi  \n",
       "92                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 minor laplace expansion b let k field let k set polynomial variable degree n show two polynomial p q k equal exist pairwise distinct βn k p β j q β j show following assertion let k field let k n n n odd det b g l n r det let k field k n n k n n k n n k n n show following assertion g l n k det det det b g l n k det det det c det det det show also matrix defined commutative ring unit construct matrix rn n n det det det det det let ai j g l n r ai j z j show following assertion hold qn n b zn n det c linear system equation ax b unique solution x every b det show g zn n det subgroup g l n q  \n",
       "93                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              chapter characteristic polynomial eigenvalue matrix already characterized matrix using rank determinant chapter use determinant map order assign every square matrix unique polynomial called characteristic polynomial matrix polynomial contains important information matrix example one read determinant thus see whether matrix invertible even important root characteristic polynomial called eigenvalue matrix characteristic polynomial cayley-hamilton theorem let r commutative ring unit let r corresponding ring polynomial cp example ai j r n n set r n n n ann entry matrix element commutative ring unit r diagonal entry polynomial degree entry constant polynomial using definition form determinant matrix element r springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "94                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     characteristic polynomial eigenvalue matrix definition let r commutative ring unit r n n pa det r called characteristic polynomial example n pa det det n obtain pa det using definition see general form pa matrix r n n given n sgn σ δi σ ai σ pa following lemma present basic property characteristic polynomial lemma r n n pa pat pa n n aii det proof using lemma obtain pa det det det pat using pa see n pa n aii sgn σ n δi σ ai σ  \n",
       "95                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       characteristic polynomial cayley-hamilton theorem first term right hand side form n aii polynomial degree n n second term polynomial degree n thus claimed moreover definition yield aii pa det n det det lemma show characteristic polynomial r n n always degree coefficient n polynomial called monic coefficient given sum diagonal entry quantity called trace n trace aii following lemma show every monic polynomial p r degree n exists matrix r n n pa lemma n n p n r p characteristic polynomial matrix r n n n matrix called companion matrix proof prove assertion induction n p pa det let assertion hold n consider p βn n r  \n",
       "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       characteristic polynomial eigenvalue matrix using laplace expansion respect first row cp corollary induction hypothesis get pa det det βn det det βn n βn βn n example polynomial p z companion matrix identity matrix characteristic polynomial det pa thus different matrix may characteristic polynomial example seen evaluate polynomial p r scalar λ analogously evaluate p matrix r cp exercise p βn n r define p βn n im r multiplication right hand side scalar multiplication β j r j r j recall im evaluating given polynomial matrix r therefore defines map r r  \n",
       "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       characteristic polynomial cayley-hamilton theorem particular using characteristic polynomial pa r n n satisfies n pa sgn σ δi σ ai σ im r note r n n pa det obvious equation pa det wrong definition pa r n n det r two expression even n following result called cayley-hamilton theorem every matrix r n n characteristic polynomial pa r pa r n n proof n pa pa let n let ei ith column identity matrix r n n aei ani en n equivalent n aii ei ji e j last n equation written bε ann en hence b r n n r p p r r n n set r form commutative ring unit given identity matrix cp exercise using theorem obtain adj b b det b claimed verified n feel necessary give proof general sir william rowan hamilton proved theorem case n context investigation quaternion one first proof general n given ferdinand georg frobenius james joseph sylvester coined name theorem calling no-little-marvelous hamilton-cayley theorem arthur cayley showed theorem n  \n",
       "98                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          characteristic polynomial eigenvalue matrix det b r identity matrix r n n matrix n time identity matrix diagonal multiplying equation right ε yield adj b bε det b ε implies det b r n n finally using lemma give n det b δi σ aσ sgn σ n δσ aσ sgn σ pat pa completes proof eigenvalue eigenvectors section present introduction topic eigenvalue eigenvectors square matrix field k concept studied detail later chapter definition let k n n λ k v k satisfy av λv λ called eigenvalue v called eigenvector corresponding λ definition v never eigenvector matrix λ may eigenvalue example v eigenvector corresponding eigenvalue λ α k αv αv α av α λv λ αv thus also αv eigenvector corresponding λ  \n",
       "99                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    eigenvalue eigenvectors theorem k n n following assertion hold λ eigenvalue λ root characteristic polynomial pa λ k λ eigenvalue det λ eigenvalue λ eigenvalue proof equation pa λ det λin hold matrix λin invertible cp equivalent l λin x however mean exists vector x λin x x λ eigenvalue pa assertion follows pa n det cp lemma follows pa pat cp lemma whether matrix k n n eigenvalue may depend field k considered example matrix characteristic polynomial pa r polynomial root since equation real solution consider element pa c root two complex number eigenvalue item theorem show eigenvalue eigenvector however may eigenvector example matrix characteristic polynomial pa hence eigenvalue λ  \n",
       "100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      characteristic polynomial eigenvalue matrix λ thus eigenvector corresponding eigenvalue eigenvector hand λ λ thus eigenvector corresponding eigenvalue eigenvector theorem implies criterion invertibility k n n cp g l n k eigenvalue root pa definition two matrix b k n n called similar exists matrix z g l n k z b z one easily show defines equivalence relation set k n n cp proof following definition theorem two matrix b k n n similar pa pb proof z b z multiplication theorem determinant yield pa det det z b z det z b z det z det b det z det b det z z pb cp remark theorem theorem theorem show two similar matrix eigenvalue condition b similar sufficient necessary pa pb example let pa pb every matrix z g l n k z b z thus pa pb although b similar cp also example  \n",
       "101                                                                                                                                                                                                                                                                                                                                     eigenvalue eigenvectors matlab-minute root polynomial p αn n computed approximated matlab using command root p p n matrix entry p n compute root p monic polynomial p r display output using format long exact root p large numerical error computation root using root p form matrix p compare structure one companion matrix lemma transfer proof lemma structure matrix compute eigenvalue command eig compare output one root p observe eigenvectors stochastic matrix consider eigenvalue problem presented sect context pagerank algorithm mathematical modeling lead equation written form ax x ai j rn n n number document satisfies n ai j ai j j matrix called column-stochastic note column-stochastic row-stochastic matrix also occurred car insurance application considered sect example want determine x xn ax x entry xi describes importance document importance value nonnegative xi thus want determine entrywise nonnegative eigenvector corresponding eigenvalue λ first check whether problem solution study whether solution unique presentation based article lemma column-stochastic matrix rn n eigenvector corresponding eigenvalue proof since column-stochastic eigenvalue theorem show also eigenvalue hence exists corresponding eigenvector  \n",
       "102                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    characteristic polynomial eigenvalue matrix matrix real entry called positive entry positive lemma rn n positive column-stochastic x eigenvector corresponding eigenvalue either x positive proof x xn eigenvector ai j corresponding eigenvalue n xi ai j x j suppose entry x positive entry x negative exists least one index k n ak j x j n ak j j implies n n n n n ai j j n n n j ai j j ai j j impossible indeed x must positive prove following uniqueness result theorem rn n positive column-stochastic exists xi ax unique positive x xn proof lemma least one positive eigenvector corresponding eigenvalue suppose x xn x xn j two eigenvectors suppose normalized xi j assumption made without loss generality since every nonzero multiple eigenvector still eigenvector show x x α r define x α x αx ax α ax α ax x αx x α α equal zero thus α first entry x lemma x α eigenvector corresponding eigenvalue ax α x α implies x α hence α xi xi  \n",
       "103                                                                                                                                                                                                                                                                                                                                                                       eigenvectors stochastic matrix summing n equation yield n xi α n xi α get xi xi n therefore x x unique positive eigenvector x theorem called perron eigenvector positive matrix theory eigenvalue eigenvectors positive general nonnegative matrix important area matrix theory since matrix arise many application construction matrix rn n pagerank algorithm columnstochastic positive since usually many entry ai j order obtain uniquely solvable problem one use following trick let si j rn n si j obviously positive columnstochastic real number α define matrix α α αs matrix positive column-stochastic hence unique positive eigenvector u corresponding eigenvalue thus α u α u α u u α u n large number document entire internet number small α u u therefore solution eigenvalue problem u α u small α potentially give good approximation u satisfies au u practical solution eigenvalue problem matrix α topic field numerical linear algebra matrix represents link structure document mutually linked thus document equally important matrix α α αs therefore model following internet surfing behavior user follows proposed link probability arbitrary link probability α originally google used value α oskar perron  \n",
       "104                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       characteristic polynomial eigenvalue matrix exercise following exercise k arbitrary field determine characteristic polynomial following matrix q verify cayley-hamilton theorem case direct computation two matrix b c similar let r commutative ring unit n show every g l n r exists polynomial p r degree n adj p conclude q hold polynomial q r degree n b let r n n apply theorem matrix r n n derive alternative proof cayley-hamilton theorem formula det adj let k n n matrix ak k matrix called nilpotent show λ eigenvalue b determine pa show n hint may assume pa form λn k c show μin invertible μ k show determine eigenvalue corresponding eigenvectors following matrix r b c difference consider b c matrix c let n ε consider matrix ε ε  \n",
       "105                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              eigenvectors stochastic matrix element cn n determine eigenvalue dependence ε many pairwise distinct eigenvalue ε determine eigenvalue corresponding eigenvectors b simplicity element denoted k instead k let k n n b k n c k n rank c ac c b show every eigenvalue b eigenvalue show following assertion trace λ μb λ trace μ trace b hold λ μ k b k n n b trace ab trace b hold b k n n c b k n n similar trace trace b prove disprove following statement exist matrix b k n n trace ab trace trace b b exist matrix b k n n ab b suppose matrix ai j cn n real entry ai j show λ eigenvalue corresponding eigenvector v νn also λ eigenvalue corresponding eigenvector v ν ν n  \n",
       "106                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     chapter vector space previous chapter focussed matrix property defined algebraic operation matrix derived important concept associated including rank determinant characteristic polynomial eigenvalue chapter place concept abstract framework introducing idea vector space matrix form one important example vector space property certain namely finite dimensional vector space studied transparent way using matrix next chapter study linear map vector space connection matrix play central role well basic definition property vector space begin definition vector space field k definition let k field vector space k shortly k space set v two operation v v v v w v w addition k v v λ v λ v scalar multiplication satisfy following v commutative group v w v λ μ k following assertion hold b c λ μ v λμ v λ v w λ v λ λ μ v λ v μ springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "107                                                                                                                                                                                                                     vector space element v v called element λ k called scalar usually omit sign scalar multiplication usually write λv instead λ clear context important field using often omit explicit reference k simply write vector space instead k space example set k n matrix addition scalar multiplication form k space obvious reason element k k sometimes called column row vector respectively set k form k space addition defined example usual addition polynomial scalar multiplication p αn n k defined λ p λ λ λ αn n continuous real valued function defined real interval α β pointwise addition scalar multiplication f g x f x g x λ f x λ f x form r-vector space shown using addition two continuous function well multiplication continuous function real number yield continuous function since definition v commutative group already know vector space property theory group cp chap particular every vector space contains unique neutral element respect addition called null vector every vector v v unique additive inverse v v v v usual write v w instead v lemma let v k space k neutral null element k v respectively following assertion hold k v v λ λ k λ v v λ v v λ k term introduced sir william rowan hamilton context quaternion motivated latin verb vehi vehor vectus sum mean ride drive also term scalar introduced hamilton see footnote scalar multiplication  \n",
       "108                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         basic definition property vector space proof v v k v k k v k v k adding k v side identity give k λ k λ λ λ λ adding λ side identity give λ λ k v v λ v v λ λ v k v well λ v λ λ v v λ following write instead k clear null element meant group ring field identify substructure vector space vector space definition let v k space let u u k space called subspace v substructure must closed respect given operation addition scalar multiplication lemma u subspace k space v ø u v following assertion hold v w u v w u λv u λ k v u proof exercise example every vector space v trivial subspace u v u let k n u l k u solution set homogeneous linear system ax u u empty v w u v w av aw v w u furthermore λ k λ v λ av λ λv u hence u subspace k every n set k p k deg p n subspace k definition let v k space n n vn vector form n λi vi v λn vn  \n",
       "109                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       vector space called linear combination vn coefficient λn k linear span vn set span vn n λi vi λn k let set suppose every vector vm let set vector called system vector denoted vm linear span system vm denoted span vm defined set vector v v linear combination finitely many vector system definition consistently extended case n case vn list length zero empty list define empty sum vector v obtain span vn span ø following consider list vector vn set vector vn usually mean n case empty list associated zero vector space v sometimes discussed separately example vector space k k spanned vector set k form subspace k spanned vector lemma v vector space vn v span vn subspace proof clear ø span vn furthermore span vn definition closed respect addition scalar multiplication lemma satisfied base dimension vector space discus central theory base dimension vector space start concept linear independence definition let v k space vector vn v called linearly independent equation n λi vi λn k λi vi always implies λn otherwise hold scalar λn k equal zero vector vn called linearly dependent  \n",
       "110                                                                                                                                                                                                                                                                                                                                                                                                     base dimension vector space empty list linear independent set every vector vm v corresponding system vm called linearly independent finitely many vector system always linearly independent sense otherwise system called linearly dependent vector vn linearly independent zero vector linearly combined trivial way vn consequently one vector zero vector vn linearly dependent single vector v linearly independent v following result give useful characterization linear independence finitely many least two given vector lemma vector vn n linearly independent vector vi n written linear combination others proof prove assertion contraposition vector vn linearly dependent n λi vi least one scalar λ j equivalently vj n j λi vi j v j linear combination vector using concept linear independence define concept basis vector space definition let v vector space set vn v called basis v vn linearly independent span vn set ø basis zero vector space v let set suppose every vector vm set vm called basis v corresponding system vm linearly independent span vm short basis linearly independent spanning set vector space example let e j k n matrix entry position j entry cp sect set  \n",
       "111                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    vector space e j n j basis vector space k n cp example matrix e j k n n j linearly independent since n λi j e j λi j implies λi j n j ai j k n n ai j e j hence span e j n j k n basis called canonical standard basis vector space k n denote canonical basis vector k en vector also called unit vector n column identity matrix basis vector space k cp example given set since corresponding system linearly independent every polynomial p k linear combination finitely many vector system next result called basis extension theorem theorem let v vector space let vr v r vr linearly independent span vr v set vr extended basis v using vector set proof note r list vr empty hence linearly independent due definition prove assertion induction span vr v linear independence vr show set basis v  \n",
       "112                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  base dimension vector space let assertion hold suppose vr v given vr linearly independent span vr vr already basis v done suppose therefore span vr exists least one j j span vr particular w j w j λw j r λi vi implies λ otherwise would w j span vr therefore λr due linear independence vr thus vr w j linearly independent induction hypothesis extend set vr w j basis v using vector set w j contains element example consider vector space v k cp example vector vector linearly independent basis v since span example vector element v span span extend get linearly independent vector indeed span thus basis basis extension theorem every vector space spanned finitely many vector basis consisting finitely many element central result theory vector space every basis number element order show result first prove following exchange lemma λi vi lemma let v vector space let vm v let w v span w vm span vm proof assumption λi vi span vm say γi vi λi vi w γi vi γi λi vi span w vm hand w αi vi span w vm  \n",
       "113                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          vector space λi vi αi vi λi αi vi span vm thus span w vm span vm using lemma prove exchange theorem theorem let w wn u u u finite subset vector space let wn linearly independent w span u u n n element u numbered appropriately element u u n exchanged n element w way span wn u u span u u n u u λi u scalar λm proof assumption zero otherwise contradicts linear independence wn appropriate renumbering lemma yield span u u span u u u suppose r r n exchanged vector u u r wr span wr u r u span u u r u r u clear r assumption wr span u u thus wr r λi wi λi u scalar λm one scalar λr λm must nonzero otherwise wr span wr contradicts linear independence wm appropriate renumbering λr lemma yield span wr u r u span wr u r u continue construction r n obtain literature theorem sometimes called steinitz exchange theorem ernst steinitz result first proved hermann günther graßmann  \n",
       "114                                                                                                                                                                                                                                                                                       base dimension vector space span wn u u span u u n u u particular n using fundamental theorem following result unique number basis element simple corollary corollary vector space v spanned finitely many vector v basis consisting finitely many element two base v number element proof assertion clear v cp definition let v span vm theorem extend span using element vm basis thus v basis finitely many element let u u u w wk two base w v span u u theorem k u v span wk theorem k thus define dimension vector space definition exists basis k space v consists finitely many element v called finite dimensional unique number basis element called dimension denote dimension dim k v dim v clear field meant v spanned finitely many vector v called infinite dimensional write dim k v note zero vector space v basis ø thus dimension zero cp definition v finite dimensional vector space vm v dim v vector vm must linearly dependent vector linearly independent could extend via theorem basis v would contain dim v element example set form basis vector space k n basis n element hence dim k n n hand vector space k spanned finitely many vector cp example hence infinite dimensional example let v vector space continuous real valued function real interval cp example define n function f n v  \n",
       "115                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         vector space f n x j x n n x n x every linear combination k x n n x λ j f j continuous function value λ j thus equation k λ j f j v implies λ j must zero f f k v linearly independent k consequently dim v coordinate change basis study linear combination basis vector finite dimensional vector space particular study happens linear combination change another basis vector space lemma vn basis k space v every v v exist uniquely determined scalar λn k v λn vn scalar called coordinate v respect basis vn proof let v λi vi μi vi scalar λi μi k n n λi μi vi  \n",
       "116                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             coordinate change basis linear independence vn implies λi μi definition coordinate vector depend given basis particular depend ordering numbering basis vector author distinguish basis set collection element without particular ordering ordered basis book keep set notation basis vn index indicate ordering basis vector let v k space vn v need linearly independent v λn vn coefficient λn k let u write vn λn vn λn vn n-tuple v vn v n v n time n v skip parenthesis write v instead v notation formally defines multiplication map v n k α k α v α α λn vn vn αλn μn k u μn vn vn μn v u λn μn vn vn λn μn  \n",
       "117                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     vector space show vector given linear combination operation scalar multiplication addition correspond operation coefficient vector respect linear combination extend notation let ai j k n let j u j vn j j write linear combination u u system u u vn side equation element v right-multiplication arbitrary n-tuple vn v n matrix k n thus corresponds forming linear combination vector vn corresponding coefficient given entry formally defines multiplication map v n k n v lemma let v k space let vn v linearly independent let k n let u u vn vector u u linearly independent rank proof exercise consider also matrix b bi j k using obtain u u b vn lemma previous notation vn b vn ab proof exercise let vn wn base v let v lemma exist unique coordinate λn μn respectively v vn wn λn μn describe method transforming coordinate λn respect basis vn coordinate μn respect basis wn vice versa  \n",
       "118                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  coordinate change basis every basis vector v j j n exist unique coordinate pi j n j v j wn j pn j defining p pi j k n n write n equation vector v j analogous vn wn way every basis vector w j j n exist unique coordinate qi j n j w j vn j qn j set q qi j k n n analogously get wn vn q thus wn vn q wn p q wn p q implies wn p q mean n linear combination basis vector wn corresponding coordinate given entry n column p q equal zero vector since basis vector linearly independent coordinate must zero hence p q k n n p q analogously obtain equation q p therefore matrix p k n n invertible p q furthermore v vn wn p wn p λn λn λn  \n",
       "119                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       vector space due uniqueness coordinate v respect basis wn obtain p p μn λn λn μn hence multiplication matrix p transforms coordinate v respect basis vn respect basis wn multiplication p yield inverse transformation therefore p p called coordinate transformation matrix summarize result obtained follows theorem let vn wn base k space uniquely determined matrix p k n n invertible yield coordinate transformation vn wn v vn vn λn μn p μn λn example consider vector space v r entrywise addition scalar multiplication basis v given set another basis v set corresponding coordinate transformation matrix obtained defining equation p q relation vector space dimension first result describes relation vector space subspace lemma v finite dimensional vector space u v subspace dim u dim v equality u v  \n",
       "120                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          relation vector space dimension proof let u v let u u basis u u u ø u using theorem extend set basis u proper subset v least one basis vector need added hence dim u dim v u v every basis v also basis u thus dim u dim v subspace vector space v intersection given u v u u cp definition sum two subspace defined u u v u u lemma subspace vector space v following assertion hold subspace equality proof exercise important result following dimension formula subspace theorem finite dimensional subspace vector space v dim dim dim dim proof let vr basis extend set basis vr basis vr xk assume r k one list empty following argument easily modified suffices show vr xk basis obviously span vr xk hence suffices show vr xk linearly independent let r k λi vi μi wi γi xi  \n",
       "121                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            vector space k γi xi r λi vi μi wi left hand side equation definition vector γi xi construction right hand side vector therefore vector vr however vr basis μi wi implies linearly independent therefore also r λi vi k γi xi hence λr γk due linear independence vr xk least one subspace theorem infinite dimensional assertion still formally correct since case dim dim dim example subspace k k k dim dim k dim k dim definition sum extended arbitrary finite number subspace uk k subspace vector space v define uk k u j k u j u j u j j k sum called direct ui k u j k case write direct sum  \n",
       "122                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 relation vector space dimension uk k uj particular sum two subspace v direct following theorem present two equivalent characterization direct sum subspace theorem u uk sum k subspace vector space v following assertion equivalent sum u direct ui u j every vector u u representation form u u j uniquely determined u j u j j u j u j u j j k implies u j j proof let u u j u j u j u j u j j every k ui ui u j u j ui uj u hence u u ui u j implies u obvious u j u given let u u j u particular implies u j u j j hence j u thus ui u j exercise following exercise k arbitrary field following set usual addition scalar multiplication r-vector space determine possible basis dimension determine basis r-vector space c dimr c determine basis c-vector space c dimc c show k linearly independent det  \n",
       "123                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       vector space let v k space nonempty set map v set map show map v operation map v map v map v f g f g f g x f x g x x k map v map v λ f λ f λ f x λ f x x k space show function sin co map r r linearly independent let v vector space n dim v n let vn show following statement equivalent vn linearly independent span vn vn basis show k n k space cp example find subspace k space show k k space cp example show k subspace k cp example determine dim k show polynomial linearly independent q extend basis q let n n n j αi j αi j k k element k called bivariate polynomial k unknown define scalar multiplication addition k becomes vector space determine basis k show lemma let k n b k solution set l b ax b subspace k let k n n let λ k eigenvalue show set v k av λv subspace k let k n n let two eigenvalue show two associated eigenvectors linearly independent show b c  \n",
       "124                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 relation vector space dimension base vector space k determine corresponding coordinate transformation matrix examine element following set linear independence vector space k determine dimension subspace spanned element one set basis k show set sequence αi q n entrywise addition scalar multiplication form infinite dimensional vector space determine basis system prove lemma prove lemma prove lemma let finite dimensional subspace vector space show sum direct dim dim dim let uk k finite dimensional subspace vector space suppose ui u j j sum uk direct let u subspace finite dimensional vector space show u u subspace u called exists another subspace u complement u determine three subspace v v subspace v uniquely determined complement  \n",
       "125                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          chapter linear map chapter study map vector space compatible two vector space operation addition scalar multiplication map called linear map homomorphism first investigate important property show case finite dimensional vector space every linear map represented matrix base respective space chosen base chosen clever way read important property linear map matrix representation central idea arise frequently later chapter basic definition property linear map start investigation definition linear map vector space definition let v w k space map f v w called linear f λv λ f v f v w f v f w hold v w v λ k set map denoted l v w linear map f v w also called linear transformation vector space homomorphism bijective linear map called isomorphism exists isomorphism v w space v w called isomorphic denote map f l v v called endomorphism bijective endomorphism called automorphism springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "126                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             linear map easy exercise show condition definition hold f λv μw λ f v μ f w hold λ μ k v w example every matrix k n defines map k k x ax map linear since λx λax x k λ k x ax ay x k cp lemma aii linear cp map trace k n n k ai j trace exercise map f q q linear show exercise map g q q linear example g g g set linear map vector space form vector space lemma let v w k space f g l v w λ k define f g λ f f g v f v g v λ f v λ f v v l v w k space proof cp exercise next result deal existence uniqueness linear map theorem let v w k space let vm basis v let wm exists unique linear map f l v w f vi wi  \n",
       "127                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    basic definition property linear map v proof every v v exist unique coordinate λ v λm v v λi vi cp lemma define map f v w f v λi v wi v definition f vi wi λ λi v vi next show f linear every λ k λv hence λ λi v wi λ λi v wi λ f v f λv u λi u vi v v u f v u λi v λi u wi v λi λi v wi λi u vi hence λi u wi f v f u thus f l v w suppose g l v w also satisfies g vi wi v every v λi vi f v f λi v vi λi v f vi λi v wi λi v g vi g λi v vi g v hence f g f indeed uniquely determined theorem show map f l v w uniquely determined image f given basis vector note image vector wm w may linearly dependent w may infinite dimensional definition introduced image pre-image map next recall definition completeness introduce kernel linear map definition v w k space f l v w kernel image f defined ker f v v f v im f f v v v w w pre-image w space v defined f w f w v v f v w kernel linear map sometimes called null space nullspace map author use notation null f instead ker f  \n",
       "128                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        linear map note pre-image f w set f mean inverse map f cp definition particular f ker f w im f f w ø example k n corresponding map l k k example ker x k ax im ax x k note ker l cp definition let j k denote jth column j x xm k write ax xjaj clearly ker moreover see representation ax ker column linearly independent set im given linear combination column im span lemma v w k space every f l v w following assertion hold f f f v v f isomorphism f l w v ker f subspace v im f subspace f surjective im f f injective ker f f injective vm v linearly independent f f vm w linearly independent vm v linearly dependent f f vm w linearly dependent equivalently f f vm w linearly independent vm v linearly independent w im f u f w arbitrary f w u ker f u v v ker f proof f f k k f well f v f f v f v existence inverse map f w v guaranteed theorem show f linear w exist uniquely determined v f f hence f f f f f f f f  \n",
       "129                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           basic definition property linear map moreover every λ k f f λ f f f λ f obvious corresponding definition let f injective v ker f f v know f since f v f injectivity f yield v suppose ker f let u v v f u f v f u v u v ker f implies u v u λi f vi linearity f yield let f λi vi λi vi ker f λi vi hence since f injective λm due linear independence vm thus f f vm linearly independent λi vi λm vm linearly dependent k equal zero applying f side using linearity λi f vi hence f f vm linearly dependent yield let w im f u f w v f w f v f u thus f v u v u ker f v u ker f show f w u ker f hand v u f f v f u w v f w show u ker f f w example consider matrix k n corresponding map l k k example given b k b l b b im l b ø case corollary suppose b im let x l b arbitrary lemma yield l b x ker assertion lemma ker column linearly independent b case corollary ker column linearly dependent b case corollary basis ker λi wi k l b thus solution ax b depend parameter  \n",
       "130                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           linear map following result give important dimension formula linear map also known rank-nullity theorem dimension image f equal rank matrix associated f cp theorem dimension kernel null space f sometimes called f theorem let v w k space let v finite dimensional every f l v w dimension formula dim v dim im f dim ker f proof let vn f f vn w linearly independent lemma also vn linearly independent thus dim im f dim v since ker f v dim ker f dim v im f ker f finite dimensional let wr vk base im f ker f respectively let u f u r f wr show u u r vk basis v implies assertion v v lemma unique coordinate μr k f v μi wi let v μi u f v f v hence v v ker f give v v λi vi unique coordinate λk k therefore k r λi vi μi u k λi vi thus v span u u r vk since u u r vk v v span u u r vk remains show u u r vk linearly independent r αi u k βi vi f f r αi u k βi vi r αi f u r αi wi thus αr wr linearly independent finally linear independence vk implies βk term introduced james joseph sylvester  \n",
       "131                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           basic definition property linear map example linear map f α q α q ker f im f α hence dim im f dim ker f indeed dim im f dim ker f dim k n l k k example dim k dim ker dim im thus dim im dim ker hold ker column linearly independent cp example hand dim im dim ker dim im thus ker case column linearly dependent since exists x k ax corollary v w k space dim v dim w n f l v w following statement equivalent f injective f surjective f bijective proof hold hold definition show implied well f injective ker f cp lemma dimension formula theorem yield dim w dim v dim im f thus im f w cp lemma f also surjective f surjective im f w dimension formula dim w dim v yield dim ker f dim v dim im f dim w dim im f thus ker f f also injective using theorem also characterize two finite dimensional vector space isomorphic  \n",
       "132                                                                                                                                                linear map corollary two finite dimensional k space v w isomorphic dim v dim w proof v w exists bijective map f l v w lemma im f w ker f dimension formula theorem yield dim v dim im f dim ker f dim w dim dim w let dim v dim w need show exists bijective f l v w let vn wn base v theorem exists unique f l v w f vi wi v λn vn ker f f v f λn vn f λn f vn λ λn wn since wn linearly independent λn hence v ker f thus f injective moreover dimension formula yield dim v dim im f dim w therefore im f w cp lemma f also surjective example vector space k n k n dimension therefore isomorphic isomorphism given linear map r-vector space c x iy x r dimension therefore isomorphic isomorphism given linear map x x iy vector space q dimension therefore isomorphic isomorphism given linear map although mathematics formal exact science smallest detail matter one sometimes us abuse notation order simplify presentation used example inductive existence proof echelon form theorem kept simplicity index larger matrix smaller matrix ai course entry position j matrix keeping index entry denoted rather induction made argument much le technical proof remained formally correct abuse notation always justified confused misuse notation field linear algebra justification often given isomorphism identifies vector space example constant polynomial field k polynomial form αt α k often written simply α element field justified since  \n",
       "133                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              basic definition property linear map k k isomorphic k space dimension already used identification similarly identified vector space v v written v instead v sect another common example literature notation k n text denotes set n-tuples element k often used matrix set column vector k row vector k n actual meaning clear context attentive reader significantly benefit simplification due abuse notation linear map matrix let v w finite dimensional k space base vm wn respectively let f l v w lemma every f v j w j exist unique coordinate ai j k n f v j j j wn define ai j k n write similarly equation vector f v j f f vm wn matrix determined uniquely f given base v v λm vm v f v f λm vm f λm f vm f f vm λm wn λm wn λm coordinate f v respect given basis w therefore given λm  \n",
       "134                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       linear map thus compute coordinate f v simply multiplying coordinate v motivates following definition definition uniquely determined matrix called matrix representation f l v w respect base vm v wn denote matrix f construction matrix representation definition consistently extended case least one k space dimension zero instance dim v n w f v j every basis vector v j thus every vector f v j empty linear combination vector basis ø matrix representation f empty matrix size also v matrix representation f empty matrix size many different notation matrix representation linear map literature notation reflect matrix depends linear map f given base example alternative notation f f mean matrix important special case obtained v w hence particular n f idv identity obtain vn wn idv idv exactly matrix p coordinate transformation matrix theorem hand wn vn idv thus idv idv example consider vector space q base linear map f q q matrix representation f f f vector space k basis b n linear map  \n",
       "135                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   linear map matrix f k k αn n n αn f j j j n f b b k thus f b b permutation matrix theorem let v w finite dimensional k space base vm wn respectively map l v w k n f f isomorphism hence l v w k n dim l v w dim k n n proof proof denote map f f mat mat f f first show map linear let f g l v w mat f f j mat g gi j j f g v j f v j g v j n f j wi n gi j wi n f j gi j wi thus mat f g f j gi j f j gi j mat f mat g λ k j λ f v j λ f v j λ n f j wi n λ f j wi thus mat λ f λ f j λ f j λ mat f remains show mat bijective f ker mat mat f k n f v j j thus f v v v f zero map mat injective cp lemma hand ai j k n arbitrary define linear map f v w via f v j n ai j wi j cp proof theorem mat f hence mat also surjective cp lemma corollary show dim l v w dim k n n cp also example  \n",
       "136                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      linear map theorem show particular f g l v w satisfy f g f g hold given base v thus prove equality linear map via equality matrix representation consider map element finite dimensional vector space coordinate respect given basis lemma b vn basis k space v map v k v λn vn b v λn isomorphism called coordinate map v respect basis b proof linearity b clear moreover obviously b v k b surjective v ker b λn v ker b b also injective cp lemma example vector space k basis b n b αn n k αn hand basis b n yield αn b αn n k base finite dimensional vector space v w respectively illustrate meaning construction matrix representation f f l v w following commutative diagram v f f k k see different composition map yield result particular f f  \n",
       "137                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            linear map matrix matrix f k n interpreted linear map k k use coordinate map bijective hence invertible way obtain f f f v f v v word coordinate f v respect basis w given product f coordinate v respect basis next show consecutive application linear map corresponds multiplication matrix representation theorem let v w x k space f l v w g l w x g f l v x moreover v w x finite dimensional respective base g f g f proof let h g f show first h l v x u v v λ μ k h λu μv g f λu μv g λ f u μ f v λg f u μg f v λh u μh v let vm wn x f f j g gi j j h v j g f v j g n f k j wk n f k j gik xi n n f k j g wk gik f k j h j n fk j gik xi xi thus h h j gi j f j g f using theorem study change base affect matrix representation linear map  \n",
       "138                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        linear map corollary let v w finite dimensional k space base v f l v w f idw f idv particular matrix f f equivalent proof applying theorem twice identity f idw f idv yield f idw f idv idw f idv idw f idv matrix f f equivalent since idw idv invertible v w becomes f idv f idv idv f idv thus matrix representation f f endomorphism f l v v similar cp definition following commutative diagram illustrates corollary f k bee ee ee ee idv b yv yy k k yyy f w f b yy yy idw b ee ee ee ee k analogously f f b f example following base vector space  \n",
       "139                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    linear map matrix coordinate transformation matrix idv idv idv coordinate map one easily verify idv theorem let v w k space dim v dim w n respectively exist base v w f ir k n r dim im f min n furthermore r rank f f matrix representation f respect arbitrary base v w define rank f rank f dim im f proof let vm wn two arbitrary base v w respectively let r rank f theorem exist invertible matrix q k n n z k q f z ir  \n",
       "140                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     linear map r rank f min n let u introduce two new base vm wn v w via vm vm z wn wn q hence wn wn q construction z idv q idw corollary obtain ir idw f idv f thus found base yield desired matrix representation f every choice base lead corollary equivalent matrix therefore also rank r remains show r dim im f structure matrix f show f v j w j j r r j therefore vr vm ker f implies dim ker f r hand w j im f thus dim im f r theorem yield dim v dim im f dim ker f hence dim ker f r dim im f r example k n corresponding map l k k example im span thus rank equal number linearly independent column since rank rank cp theorem number equal number linearly independent row theorem first example general strategy use several time following chapter choosing appropriate base matrix representation reveal desired information linear map efficient way theorem information rank linear map f dimension image dimension formula linear map generalized composition map follows  \n",
       "141                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   linear map matrix theorem v w x finite dimensional k space f l v w g l w x dim im g f dim im f dim im f ker g proof let g f restriction g image f map g l im f x v g v applying theorem g yield dim im f dim im g dim ker g im g g v x v im f im g f ker g v im f g v im f ker g imply assertion note theorem v w f idv g l v x give dim im g dim v dim ker g equivalent theorem interpret matrix k n b k n linear map theorem implies equation rank b rank dim im ker b special case k r b following result corollary rn rank rank proof let w ωn im ker w ay vector multiplying equation left using w ker obtain w ay implies ay w w n ω since hold w im ker  \n",
       "142                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      linear map exercise following exercise k arbitrary field consider linear map given matrix determine ker dim ker dim im construct map f l v w linearly independent vector vr v image f f vr w linearly dependent map f r r αn n nαn n called derivative polynomial p r respect variable show f determine ker f im f base let f l r r matrix representation f determine f base b determine coordinate f respect basis construct map f l k k following property f pq f p q p f q p q k f map uniquely determined property map property let α k k n n show map k k p p α k k p p linear justify name evaluation homomorphism map let g l n k show map f k n n k n n isomorphism  \n",
       "143                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         linear map matrix let k field let k n n consider map f k k x x ax f linear map show f let v q-vector space basis vn let f l v v defined f v j v j v j n j vn determine f b let wn w j j j show basis determine coordinate transformation matrix idv idv well matrix representation f f extend theorem consistently case w property matrix g f g f consider map f r r αn n αn n n show f linear determine ker f im f b choose base two vector space verify choice rank f dim im f hold let αn r n pairwise distinct number let n polynomial r defined pj n j αk α j αk j show set b pn basis r basis called lagrange r b show corresponding coordinate map given joseph-louis de lagrange  \n",
       "144                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       linear map b r p p p αn hint use exercise b verify different path commutative diagram vector space base linear map f q q f  \n",
       "145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          chapter linear form bilinear form chapter study different class map one two k space one dimensional k space defined field k map play important role many area mathematics including analysis functional analysis solution differential equation also essential development book using bilinear sesquilinear form introduced chapter define study euclidean unitary vector space chap linear form dual space used existence proof jordan canonical form chap linear form dual space start set linear map k space vector space k definition v k space f l v k called linear form k space v l v k called dual space linear form sometimes called linear functional one-form stress linearly map one dimensional vector space example v r-vector space continuous real valued function real interval α β γ α β two map f v r g g γ β f v r g g x x α linear form springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "146                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   linear form bilinear form dim v n dim v n theorem let vn basis v let basis k space k f v f vi αi αi k n f αn k n element v n λi vi v f v f n λi vi λi f vi λi αi αn λn n n n f v identified isomorphic vector space k k given basis finite dimensional vector space v construct special uniquely determined basis dual space v theorem v k space basis b vn exists unique basis b v v j δi j j n called dual basis b proof theorem unique linear map v k constructed prescribing image given basis b thus n exists unique map l v k v j δi j j remains show b basis v λn k n λi v v j n λi v j λ j j thus linearly independent dim v n implies b basis v cp exercise example consider v k canonical basis b en en dual basis b ei e j δi j show ei b eit k n n  \n",
       "147                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          linear form dual space definition let v w k space respective dual space v w let f l v w f w v h f h h f called dual map f next derive property dual map lemma v w x k space following assertion hold f l v w dual map f linear hence f l w v f l v w g l w x g f l x v g f f f l v w bijective f l w v bijective f f proof h h w k f h h h h f h f h f h f h f f h f h exercise following theorem show concept dual map transposed matrix closely related theorem let v w finite dimensional k space base respectively let corresponding dual base f l v w f f proof vm wn let let f ai j k n f v j n ai j wi j f bi j k n f w bi j j n  \n",
       "148                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      linear form bilinear form every pair k k n n wi n wi f f bik bik used definition dual map well wi δki close relationship transposed matrix dual map author call dual map f transpose linear map f applied matrix lemma theorem yield following rule known chap ab b k n b k g l n k example two base element corresponding dual base given r r r r matrix representation map linear map α f  \n",
       "149                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        linear form dual space f f bilinear form consider special map pair k space k space definition let v w k space map β v w k called bilinear form v w β w β w β w β v β v β v β λv w β v λw λβ v w hold v v w w λ k bilinear form β called non-degenerate first variable β v w w w implies v analogously called non-degenerate second variable β v w v v implies w β non-degenerate variable β called non-degenerate space v w called dual pair respect β v w β called bilinear form additionally β v w β w v hold v w v β called symmetric otherwise β called nonsymmetric example k n β k k k v w w av bilinear form k k non-degenerate n g l n k cp exercise bilinear form β r x x degenerate variable x β x β x x set r x β x x equal solution set quadratic equation two variable geometrically set given two straight line cartesian coordinate system  \n",
       "150                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         linear form bilinear form v k space β v v k v f f v bilinear form v v since β f f f f β f β f β v f f f f v f v f v β v f β v f β λv f f λv λ f v λβ v f λ f v β v λ f hold v v f f f v λ k bilinear form non-degenerate thus v v dual pair respect β cp exercise case dim v n definition let v w k space base vm wn respectively β bilinear form v w β bi j k n bi j β v j wi called matrix representation β respect base v λ j v j v w μi wi w β v w n λ j μi β v j wi n μi bi j λ j w β v used coordinate map lemma example em n en n canonical base k k respectively β bilinear form example ai j k n β bi j n ai j ei n ae bi j β e j ei j hence β following result show symmetric bilinear form symmetric matrix representation lemma bilinear form β finite dimensional vector space v following statement equivalent β symmetric every basis b v matrix β symmetric exists basis b v β symmetric  \n",
       "151                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          bilinear form proof exercise analyze effect basis change matrix representation bilinear form theorem let v w finite dimensional k space base v β bilinear form v w β idw β idv proof let vm vm wn n w vm p p pi j idv vm wn w n q q qi j idw β vj w bi j bi j β β v j wi β pk j vk n w n n β vk w pk j pk j j β qni pm j implies β q β p hence assertion follows v w two base v obtain following special case theorem β idv β idv two matrix representation β β β case congruent formally define follows definition two matrix b k n n exists matrix z g l n k b z az b called congruent lemma congruence equivalence relation set k n n proof exercise  \n",
       "152                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    linear form bilinear form sesquilinear form complex vector space introduce another special class form definition let v w c-vector space map v w c called sesquilinear form v w w w w λv w λs v w v v v v λw λs v w hold v v w w λ v w called sesquilinear form additionally v w w v hold v w v called prefix sesqui latin mean one half note sesquilinear form linear first variable semilinear half linear second variable following result characterizes hermitian sesquilinear form lemma sesquilinear form c-vector space v hermitian v v r v proof hermitian particular v v v v v v thus v v hand v w v definition v w v w v v v w w v w w v iw v iw v v w v v w w w first equation implies v w w v r since v w v w v v w w r assumption second equation implies analogously w v v w therefore v w w v v w w v v w w v v w w v multiplying second equation adding resulting equation first obtain v w w v corollary sesquilinear form c-vector space v v w v w v w v iw v iw v v w w v w charles hermite  \n",
       "153                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            sesquilinear form proof result follows multiplication adding result corollary show sesquilinear form c-vector space v uniquely determined value v v v definition hermitian transpose ai j cn matrix h j cm n h called hermitian matrix real entry obviously h thus real symmetric matrix also hermitian ai j cn n hermitian particular aii ii n hermitian matrix real diagonal entry hermitian transposition satisfies similar rule usual transposition cp lemma cn b cm λ c following assertion lemma hold h h h ah λ h λ h ab h b h h proof exercise example cn map c v w w h av sesquilinear form matrix representation sesquilinear form defined analogously matrix representation bilinear form cp definition definition let v w c-vector space base vm wn respectively sesquilinear form v w bi j cn bi j v j wi called matrix representation respect base example em n en n canonical base respectively sesquilinear form example ai j cn bi j  \n",
       "154                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               linear form bilinear form n bi j e ei n ae ai j ei n ae j ei j j hence exercise following exercise k arbitrary field let v finite dimensional k space v show f v f v v consider basis b vector space r compute dual basis b b let v n-dimensional k space let basis v prove disprove exists unique basis vn v v j δi j let v finite dimensional k space let f g v f show g λ f λ k hold ker f ker g possible omit assumption f let v k space let u subspace set u f v f u u u called annihilator u show following assertion u subspace v b subspace v c w k space f l v w ker f im f prove lemma let v w k space show set bilinear form v w operation v w v w v w λ β v w λ β v w k space let v w k space base vm wn corresponding dual base respectively j n let βi j v w k v w v w w show βi j bilinear form v w  \n",
       "155                                                                                                                                                                                                                                                                                                                                                                                                                                                                               sesquilinear form b show set βi j j n basis k space bilinear form v w cp exercise determine dimension space let v r-vector space continuous real valued function real interval α β show β f x g x x β v v r f g α symmetric bilinear form β degenerate show map β example bilinear form show non-degenerate n g l n k let v finite dimensional k space show v v dual pair respect bilinear form β example bilinear form β non-degenerate let v finite dimensional k space let u v w v subspace dim u dim w prove disprove space u w form dual pair respect bilinear form β u w k v h h v let v w finite dimensional k space base respectively let β bilinear form v show following statement equivalent β invertible β degenerate second variable β degenerate first variable b conclude β non-degenerate β invertible prove lemma prove lemma bilinear form β k space v map qβ v k v β v v called quadratic form induced β show following assertion k β symmetric β v w qβ v v qβ w hold v w show sesquilinear form c-vector space v satisfies polarization identity v w v v v v v v v v v w consider following map c x x b x c x  \n",
       "156                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         linear form bilinear form x bilinear form sesquilinear form test whether bilinear form symmetric sesquilinear form hermitian derive corresponding matrix representation respect canonical basis basis prove lemma let cn n hermitian show v w w h av hermitian sesquilinear form let v finite dimensional c-vector space basis b let sesquilinear form show hermitian hermitian show following assertion b cn n h eigenvalue purely imaginary b h trace trace c h b h b trace ab trace b  \n",
       "157                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   chapter euclidean unitary vector space chapter study vector space field r using definition bilinear sesquilinear form introduce scalar product vector space scalar product allow extension well-known concept elementary geometry length angle abstract real complex vector space particular lead idea orthogonality orthonormal base vector space example importance concept many application study least-squares approximation scalar product norm start definition scalar product euclidean unitary vector space definition let v k space either k r k map v v k v w called scalar product v following property hold k r symmetric bilinear form k c hermitian sesquilinear form positive definite hold v v equality v r-vector space scalar product called euclidean vector c-vector space scalar product called unitary vector space scalar product sometimes called inner product note nonnegative real also v c-vector space easy see subspace u euclid alexandria approx bc springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "158                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                euclidean unitary vector space euclidean unitary vector space v euclidean unitary vector space respectively scalar product space v restricted subspace u example scalar product given w called standard scalar product scalar product given w h called standard scalar product k r k c spur b h scalar product k n scalar product vector space continuous real valued function real interval α β given f α β f x g x x show use euclidean unitary structure vector space order introduce geometric concept length vector angle vector motivation general concept length absolute value real number map r r x map following property λ x x r equality x x property generalized real complex vector space follows definition let v k space either k r k map v r v v  \n",
       "159                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                scalar product norm called norm v v w v λ k following property hold λv v v equality v v w v w triangle inequality k space norm defined called normed space example standard scalar product v v v defines norm called euclidean norm standard scalar product v v h v defines norm called euclidean norm common terminology although space unitary euclidean k r k c f trace h n j norm k n called frobenius k n frobenius norm equal euclidean norm k moreover frobenius norm k n equal euclidean norm k k nm identify vector space via isomorphism obviously f f h f k n v vector space continuous real valued function real interval α β β f x x f f f α norm v called l let k r k c let p r p given v νn k p-norm k defined v p n ferdinand georg frobenius p p  \n",
       "160                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           euclidean unitary vector space p euclidean norm k norm typically omit index write instead taking limit p obtain k given v max following figure illustrate unit circle respect p-norm set v v p p p p k r k c p-norm k n defined p sup av p v p use p-norm k denominator p-norm k numerator notation sup mean supremum least upper bound known analysis one show supremum attained vector v thus may write max instead sup definition particular ai j k n max max n j j norm called maximum column sum maximum row sum norm k n respectively easily see h h however matrix thus matrix satisfies matrix considered chap  \n",
       "161                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    scalar product norm norm example form v given scalar product show map v always defines norm proof based following theorem theorem v euclidean unitary vector space scalar product v w v equality v w linearly dependent proof inequality trivial w thus let w let λ λw v λ implies v w linearly dependent v λw scalar λ hence λλ hand let w v w linearly dependent w define λ get λw v since scalar product positive definite v λw thus v w linearly dependent inequality called cauchy-schwarz important tool analysis particular estimation approximation interpolation error augustin louis cauchy hermann amandus schwarz  \n",
       "162                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               euclidean unitary vector space corollary v euclidean unitary vector space scalar product map v r v v norm v called norm induced scalar product proof prove three defining property norm since positive definite v equality v v v λ k euclidean case k r unitary case k c λv hence λv v order show triangle inequality use cauchy-schwarz inequality fact z every complex number z v w v w v v w v w v v w w v w thus v w v w orthogonality use scalar product introduce angle vector motivation consider euclidean vector space standard scalar product induced euclidean norm v cauchy-schwarz inequality show v w v w v w angle v w uniquely determined real number ϕ π co ϕ v w  \n",
       "163                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            orthogonality vector v w orthogonal ϕ co ϕ thus v w orthogonal elementary calculation lead cosine theorem triangle w v v w v w co ϕ v w orthogonal cosine theorem implies pythagorean v w following figure illustrate cosine theorem pythagorean theorem vector following definition generalize idea angle orthogonality definition let v euclidean unitary vector space scalar product euclidean case angle two vector v w v uniquely determined real number ϕ π co ϕ v w two vector v w v called orthogonal basis vn v called orthogonal basis v j j n j furthermore vi n v norm induced scalar product vn called orthonormal basis orthonormal basis therefore v j δi j pythagoras samos approx bc  \n",
       "164                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        euclidean unitary vector space note term defined respect given scalar product different scalar product yield different angle vector particular orthogonality two given vector may lost consider different scalar product example standard basis vector orthogonal orthonormal basis respect standard scalar product cp example consider symmetric invertible matrix defines symmetric non-degenerate bilinear form v w w av cp example bilinear form positive definite since v v av bilinear form therefore scalar product denote denote induced norm respect scalar product vector satisfy clearly orthonormal basis r respect also note hand vector satisfy hence orthonormal basis respect scalar product show every finite dimensional euclidean unitary vector space orthonormal basis theorem let v euclidean unitary vector space basis vn exists orthonormal basis u u n v span u u k span vk k n  \n",
       "165                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               orthogonality proof give proof induction dim v n set u u u orthonormal basis v span u span let assertion hold n let dim v n let basis vn span vn n-dimensional subspace induction hypothesis exists orthonormal basis u u n vn span u u k span vk k define u n u k k u u u since vn span u u n must u lemma yield span u u span j n u u j u u j u u n u k k u j u j u j u j u finally u u u completes proof proof theorem show given basis vn orthonormalized transformed orthonormal basis u u n span u u k span vk k resulting algorithm called gram-schmidt method algorithm given basis vn set u j n set u v u u jørgen j u k k u pedersen gram erhard schmidt  \n",
       "166                                                                                                                                                                                                                                                                                                                  euclidean unitary vector space slight reordering combination step gram-schmidt method yield u u vn u u u n u n n n un upper triangular matrix right hand side coordinate transformation matrix basis vn basis u u n v cp theorem thus shown following result theorem v finite dimensional euclidean unitary vector space given basis gram-schmidt method applied yield orthonormal basis v idv invertible upper triangular matrix consider m-dimensional subspace standard scalar product write vector orthonormal basis qm column matrix q qm obtain real case q q qit q j j qi δ ji im analogously complex case q h q qih q j j qi δ ji im hand q q im q h q im matrix q rn q cn respectively column q form orthonormal basis respect standard scalar product m-dimensional subspace respectively matrix version theorem therefore formulated follows corollary let k r k c let vm k linearly independent exists matrix q k n column orthonormal respect standard scalar product k q q im k r q h q im k c upper triangular matrix r g l k vm q factorization called q r-decomposition matrix vm q r-decomposition many application numerical mathematics cp example lemma let k r k c let q k n matrix orthonormal column respect standard scalar product k v qv hold v k euclidean norm k k  \n",
       "167                                                                                                                                                                                                                                                                              orthogonality proof k c v v h v v h q h q v qv proof k r analogous introduce two important class matrix definition matrix q rn n whose column form orthonormal basis respect standard scalar product called orthogonal matrix q cn n whose column form orthonormal basis respect standard scalar product called unitary matrix q qn rn n therefore orthogonal q q qit q j j qi δ ji particular orthogonal matrix q invertible q q cp corollary equation q q mean n row q form orthonormal basis n respect scalar product wv analogously unitary matrix q cn n invertible q q h h q q q q h n column q form orthonormal basis n lemma set n orthogonal u n unitary n matrix form subgroup g l n r g l n c respectively proof consider n proof u n analogous since every orthogonal matrix invertible n g l n r identity matrix orthogonal hence n ø q n also q q n since q q q q finally q q n q q q q q q q q q q thus q q n example many application measurement sample lead data set represented tuples τi μi τm pairwise distinct measurement point μm corresponding measurement order approximate given data set simple model one try construct polynomial p small degree value p p τm close possible measurement μm simplest case real polynomial degree geometrically corresponds construction straight line minimal distance  \n",
       "168                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               euclidean unitary vector space given point shown figure cp sect many possibility measure distance following describe one detail use gram-schmidt method q r-decomposition construction straight line statistic method called linear regression real polynomial degree form p αt β looking coefficient α β r p τi ατi β μi using matrix write problem α v v α β β μm τm mentioned different possibility interpreting symbol particular different norm measure distance given value μm polynomial value p p τm use euclidean norm consider minimization problem α min β α vector linearly independent since entry pairwise distinct entry equal let r q r-decomposition extend vector orthonormal basis qm q qm rm orthogonal matrix  \n",
       "169                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          orthogonality α min v min v α β α r α β r α min q β α r α q min q β α r α β min α qt used q q im qv v v upper triangular matrix r invertible thus minimization problem solved q α r β using definition euclidean norm write minimizing property polynomial p αt β α p τi μi β min α ατi β μi since polynomial p minimizes sum square distance measurement μi polynomial value p τi polynomial yield least square approximation measurement value consider example sect four quarter year company profit million euro assumption profit grows linearly like straight line goal estimate profit last quarter following year given data lead approximation problem α α β β  \n",
       "170                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       euclidean unitary vector space numerical computation q r-decomposition yield α β resulting profit estimate last quarter following year p million euro matlab-minute example one could imagine profit grows quadratically instead linearly determine analogously procedure example polynomial p αt βt γ solves least square problem p τi μi min α β βτi γ μi use matlab command qr computing q r-decomposition determine estimated profit last quarter following year analyze property orthonormal base detail lemma v euclidean unitary vector space scalar product orthonormal basis u u n n u v proof every v v exist uniquely determined coordinate λn n λi u every j n u j λi u j v λj coordinate u n v respect orthonormal basis u u n often called n fourier coefficient v respect basis representation v u called abstract fourier expansion v given orthonormal basis jean baptiste joseph fourier  \n",
       "171                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      orthogonality corollary v euclidean unitary vector space scalar product orthonormal basis u u n following assertion hold n n u u u v w v parseval n u v v bessel proof v n u thus n n n u w u u u special case v bessel identity every vector v v satisfies v n u max u norm induced scalar product absolute value coordinate v respect orthonormal basis v therefore bounded norm property hold general basis example consider v standard scalar product euclidean norm every real ε set ε basis every vector v v ε ε ε moderate number small large numerical algorithm situation lead significant problem due roundoff error avoided orthonormal base used marc-antoine friedrich parseval wilhelm bessel  \n",
       "172                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                euclidean unitary vector space definition let v euclidean unitary vector space scalar product let u v subspace u v v u u called orthogonal complement u v lemma orthogonal complement u subspace proof exercise lemma v n-dimensional euclidean unitary vector space u v m-dimensional subspace dim u n v u u proof know n cp lemma n u v thus u v v v u v assertion trivial thus let n let u u orthonormal basis u extend basis basis v apply gram-schmidt method order obtain orthonormal basis u u u u n span u u n u therefore v u u w u u hence w since scalar product positive definite thus u u implies v u u dim u n cp theorem particular u span u u n vector product section consider product vector space frequently used physic electrical engineering definition vector product cross product map v w v w contrast scalar product vector product two element vector space scalar vector using canonical basis vector  \n",
       "173                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          vector product write vector product ω ω det det v w det ω ω ω lemma vector product linear component v w following property hold v w v vector product anti commutative alternating v w v w standard scalar product euclidean norm v v standard scalar product proof exercise cauchy-schwarz inequality follows v w hold v w linearly dependent obtain μw v v v arbitrary λ μ v w linearly independent product v w orthogonal plane origin spanned v w v w λv μw λ μ r geometrically two possibility position three vector v w v left side figure correspond right-handed orientation usual coordinate system canonical basis vector associated thumb index finger middle finger right hand motivates name right-hand rule order explain detail one need introduce concept orientation omit ϕ π angle vector v w v w co ϕ  \n",
       "174                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         euclidean unitary vector space cp definition write lemma v w v w ϕ v w ϕ v w sin ϕ geometric interpretation equation following norm vector product v w equal area parallelogram spanned v interpretation illustrated following figure exercise let v finite dimensional real complex vector space show exists scalar product show map defined example scalar product corresponding vector space let arbitrary scalar product show exists matrix rn n w av v w let v finite dimensional c-vector space let scalar product v following property v w v satisfy v w also v w prove disprove exists real scalar λ v w v w v w show map defined example norm corresponding vector space show n max j max j ai j k n k r k c cp example sketch matrix example p set av v v p let v euclidean unitary vector space let norm induced scalar product show satisfies parallelogram identity v w v w  \n",
       "175                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           vector product let v k space k r k c scalar product induced norm show v w v orthogonal respect v λw v λw λ k exist scalar product cp example induced norm scalar product show inequality n αi βi n γi αi n βi γi hold arbitrary real number αn βn positive real number γn let v finite dimensional euclidean unitary vector space scalar product let f v v map f v f w v w show f isomorphism let v unitary vector space suppose f l v v satisfies f v v prove disprove f statement also hold euclidean vector space let diag dn rn n dn show w dv scalar product analyze property scalar product violated least one di zero di nonzero different sign orthonormalize following basis vector space respect scalar product trace b h let q rn n orthogonal let q cn n unitary matrix possible value det q let u let h u ut u uu rn n show n column h u form orthonormal basis respect standard scalar product matrix form called householder study detail example prove lemma alston scott householder pioneer numerical linear algebra  \n",
       "176                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 euclidean unitary vector space let analyze whether vector orthonormal respect standard scalar product compute orthogonal complement span let v euclidean unitary vector space scalar product let u u k v let u span u u k show v v v u u j j unitary vector space standard scalar product let given determine orthonormal basis span prove lemma  \n",
       "177                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     chapter adjoints linear map chapter introduce adjoints linear map sense represent generalization hermitian transpose matrix matrix symmetric hermitian equal hermitian transpose analogous way endomorphism selfadjoint equal adjoint endomorphism set symmetric hermitian matrix selfadjoint endomorphisms form certain vector space play key role proof fundamental theorem algebra chap special property selfadjoint endomorphisms studied chap basic definition property chap considered euclidean unitary vector space hence vector space field r let v w vector space general field k let β bilinear form v every fixed vector v v map βv w k w β v w linear form thus assign every v v vector βv w defines map β v w v βv analogously define map β w v w βw βw v k defined v β v w every w springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "178                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       adjoints linear map lemma map β β defined respectively linear β l v w β l w v dim v dim w n β non-degenerate cp definition β β bijective thus isomorphism proof prove assertion map β proof β analogous first show linearity let v k every w w β w β w β w β w β w β w β β w hence β β β therefore β l v w let dim v dim w n let β non-degenerate show β l v w injective lemma hold ker β v ker β β v βv w thus βv w β v w w since β non-degenerate v finally dim v dim w dim w dim w imply dim v dim w β bijective cp corollary next discus existence adjoint map theorem v w k space dim v dim w n β non-degenerate bilinear form v w following assertion hold every f l v v exists uniquely determined g l w w β f v w β v g w v v w map g called right adjoint f respect β every h l w w exists uniquely determined k l v v β v h w β k v w v v w map k called left adjoint h respect β proof show proof analogous let v dual space v let f l v v dual map f let β l w v since β non-degenerate β bijective lemma define g β f β l w w  \n",
       "179                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         basic definition property v v w w β v g w β v β f β w β β f β w v β β f β w v β β β w f v β w f v β f v w recall dual map satisfies f β w β w f remains show uniqueness let g l w w β v g w β f v w v v w β v g w β v g w hence β v g g w v v w since β non-degenerate second variable g g w w w g example let v w k β v w w bv matrix b g l n k β non-degenerate cp example consider linear map f v v v fv matrix f k n n linear map h w w w h w matrix h k n n βv w k w w bv β v w v bv β w v w w b identified isomorphic vector space w k n respectively v k n g l w w right adjoint f respect β β f v w w b f v w b fv β v g w g w bv v v w represent linear map g via multiplication matrix g k n n g w gw w b fv w g bv v w k hence b f g b since b invertible unique right adjoint given g b f b b f b analogously left adjoint k l v v h respect β obtain equation β v h w h w bv w h bv β k v w w bk v v v w k v lv matrix l k n n obtain h b b l hence l b h b  \n",
       "180                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    adjoints linear map v finite dimensional β non-degenerate bilinear form v theorem every f l v v unique right adjoint g unique left adjoint k β f v w β v g w β v f w β k v w v w β symmetric β v w β w v hold v w v yield β v g w β f v w β w f v β k w v β v k w therefore β v g k w v w v hence g k since β non-degenerate thus proved following result corollary β symmetric non-degenerate bilinear form finite dimensional k space v every f l v v exists unique g l v v β f v w β v g w β v f w β g v w v w definition scalar product euclidean vector space symmetric nondegenerate bilinear form cp definition lead following corollary corollary v finite dimensional euclidean vector space scalar product every f l v v exists unique f ad l v v f v w v f ad w v f w f ad v w v w map f ad called adjoint f respect order determine whether given map g l v v unique adjoint f l v v one two condition verified f g l v v equation f v w v g w hold v w v also v f w f w v w g v g v w v w v used symmetry scalar product similarly v f w g v w hold v w v also f v w v g w v w v  \n",
       "181                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       basic definition property example consider euclidean vector space scalar product v w w dv linear map f v fv f v w f v w w fv w f dv f w dv v f ad w thus f ad v f dv v used symmetric show uniquely determined adjoint map also exist unitary case however conclude directly corollary since scalar product c-vector space symmetric bilinear form hermitian sesquilinear form order show existence adjoint map unitary case construct explicitly construction work also euclidean case let v unitary vector space scalar product let u u n orthonormal basis given f l v v define map n v f u u g v v v v w v λ μ c n g λv μw n λv μw f u u λg v μg w λ v f u u μ v f u u  \n",
       "182                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         adjoints linear map hence g l v v let v n λi u v w v n n w f u j u j λi u v g w n n λi w f u λi f u w f v w furthermore v f w f w v w g v g v w v w g l v v satisfies f v w v g w v w v g g since scalar product positive definite therefore formulate following result analogously corollary corollary v finite dimensional unitary vector space scalar product every f l v v exists unique f ad l v v f v w v f ad w v f w f ad v w v w map f ad called adjoint f respect euclidean case validity one two equation v w v implies validity v w example consider unitary vector space scalar product v w w h dv linear map f v fv f v w f v w w h fv w h f dv f h h w h dv v f ad w thus f ad v f h dv v  \n",
       "183                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       basic definition property used real symmetric next investigate property adjoint map lemma let v finite dimensional euclidean unitary vector space f f l v v k k r euclidean k c unitary case f f ad f f euclidean case map f f ad therefore linear unity case semilinear idv ad idv every f l v v f ad ad f f f l v v f f ad f f proof v w v k f f v w f v w f v w v f w v f w v f w f w v f f w thus f f ad f f v w v idv v w v w v idv w thus idv ad idv v w v f ad v w v f w thus f ad ad f v w v f f v w f f v w f v f w v f f w v f f w thus f f ad f f following result show relation image kernel endomorphism adjoint theorem v finite dimensional euclidean unitary vector space f l v v following assertion hold  \n",
       "184                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        adjoints linear map ker f ad im f ker f im f ad proof w ker f ad f ad w v f ad w f v w v v hence w im f hand w im f f v w v f ad w v since non-degenerate f ad w hence w ker f ad using f ad ad f get ker f ker f ad ad im f ad example consider unitary vector space standard scalar product linear map f v fv f f ad v f h v f h matrix f f h rank therefore dim ker f dim ker f ad simple calculation show ker f span ker f ad span dimension formula linear map implies dim im f dim im f ad matrix f f h see im f span im f ad span equation ker f ad im f ker f im f ad verified direct computation  \n",
       "185                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           adjoint endomorphisms matrix adjoint endomorphisms matrix study relation matrix representation endomorphism adjoint let v finite dimensional unitary vector space scalar product let f l v v orthonormal basis b u u n v let f b b ai j cn n n f u j ak j u k j n hence n ak j u k u ai j j f u j u f ad b b bi j cn n n f ad u j bk j u k j n bi j f ad u j u u j f u f u u j ji thus f ad b b f b b h hold finite dimensional euclidean vector space omit complex conjugation therefore shown following result theorem v finite dimensional euclidean unitary vector space orthonormal basis b f l v v f ad b b f b b h euclidean case f b b h f b b important special class selfadjoint endomorphisms definition let v finite dimensional euclidean unitary vector space endomorphism f l v v called selfadjoint f f ad trivial example selfadjoint endomorphism l v v f idv corollary v finite dimensional euclidean vector space f l v v selfadjoint b orthonormal basis v f b b symmetric matrix  \n",
       "186                                                                                                                                                                                                                                                                                                                                                                adjoints linear map v finite dimensional unitary vector space f l v v selfadjoint b orthonormal basis v f b b hermitian matrix selfadjoint endomorphisms form vector space however one careful use appropriate field vector space defined particular set selfadjoint endomorphisms unitary vector space v form c-vector space f f ad l v v f ad f ad f f cp lemma similarly hermitian matrix cn n form c-vector space h cn n hermitian h h lemma v n-dimensional euclidean vector space set selfadjoint endomorphisms f l v v f f ad form r-vector space dimension n n v n-dimensional unitary vector space set selfadjoint endomorphisms f l v v f f ad form r-vector space dimension proof exercise matrix cn n called complex symmetric unlike hermitian matrix complex symmetric matrix form c-vector space lemma set complex symmetric matrix cn n form c-vector space dimension n n proof exercise lemma used chap proof fundamental theorem algebra exercise let β v w w bv b diag defined v w consider linear map f v fv h w h w h determine βv β β well right adjoint f left adjoint h respect β let v v w w two finite dimensional euclidean vector space let f l v w show exists unique g l w v f v w w v g w v v v w let v w w bv v w  \n",
       "187                                                                                                                                                                                                                                                                                                                  adjoint endomorphisms matrix show v w w bv scalar product b using scalar product determine adjoint map f ad f v fv f c investigate property f need satisfy f selfadjoint let n f xn determine adjoint f ad f respect standard scalar product let v finite dimensional euclidean unitary vector space let f l v v show ker f ad f ker f im f ad f im f ad let v finite dimensional euclidean unitary vector space let u v subspace let f l v v f u u show f ad u u let v finite dimensional euclidean unitary vector space let f l v v v show v im f v ker f ad matrix version cn n b linear system equation ax b solution b l h let v finite dimensional euclidean unitary vector space let f g l v v selfadjoint show f g selfadjoint f g commute f g g f let v finite dimensional unitary vector space let f l v v show f selfadjoint f v v r hold v let v finite dimensional euclidean unitary vector space let f l v v projection f satisfies f f show f selfadjoint ker f im f v w hold v ker f w im f let v finite dimensional euclidean unitary vector space let f g l v v show g ad f l v v v w hold v im f w im g two polynomial p q r let p q dt p q show defines scalar product r b consider map n f r r n αi iαi determine f ad ker f ad im f ker f ad im f prove lemma prove lemma  \n",
       "188                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        chapter eigenvalue endomorphisms previous chapter already studied eigenvalue eigenvectors matrix chapter generalize concept endomorphisms investigate endomorphisms finite dimensional vector space represented diagonal matrix upper triangular matrix representation easily read important information endomorphism particular eigenvalue basic definition property first consider arbitrary vector space concentrate finite dimensional case definition let v k space f l v v λ k v v satisfy f v λv λ called eigenvalue f v called eigenvector f corresponding λ definition v eigenvector eigenvalue λ may occur cp example following definition equation f v λv written λv f v λidv f v hence λ k eigenvalue f ker λidv f springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "189                                                                                                                                                                                                                                                                                                                                                                                                                eigenvalue endomorphisms already know kernel endomorphism v form subspace v cp lemma hold particular ker λidv f definition v k space λ k eigenvalue f l v v subspace v f λ ker λidv f called eigenspace f corresponding λ g λ f dim v f λ called geometric multiplicity eigenvalue λ definition eigenspace v f λ spanned eigenvectors f corresponding eigenvalue λ v f λ finite dimensional g λ f dim v f λ equal maximal number linearly independent eigenvectors f corresponding λ definition let v k space let u v subspace let f l v v f u u f u u hold u u u called f subspace important example f subspace eigenspaces f lemma v k space λ k eigenvalue f l v v v f λ f subspace proof every v v f λ f v λv v f λ consider finite dimensional vector space discus relationship eigenvalue f eigenvalue matrix representation f respect given basis lemma v finite dimensional k space f l v v following statement equivalent λ k eigenvalue f λ k eigenvalue matrix f b b every basis b proof let λ k eigenvalue f let b vn arbitrary basis v v eigenvector f corresponding eigenvalue λ f v λv exist unique coordinate μn k equal zero v μ j v j using obtain f b b b f v b λv b v λ μn μn  \n",
       "190                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      basic definition property thus λ eigenvalue f b b hand f b b μn λ μn given arbitrary basis b vn v set μn v μ j v j v f v μ j f v j f f vn vn f b b μn μn vn λv n μn λ eigenvalue f lemma implies eigenvalue f root characteristic polynomial matrix f b b cp theorem however hold general matrix representation form f b b b b two different base general two matrix f b b idv b b f b b f b b eigenvalue example consider vector space base endomorphism f v fv f matrix representation f b b f b b det f b b thus f eigenvalue hand characteristic polynomial f b b matrix eigenvalue two different base b b v matrix f b b f b b similar cp discussion following corollary theorem shown  \n",
       "191                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                eigenvalue endomorphisms similar matrix characteristic polynomial justifies following definition definition n n v n-dimensional k space basis b f l v v p f det f b b k called characteristic polynomial f characteristic polynomial p f always monic polynomial deg p f n dim v discussed p f independent choice basis scalar λ k eigenvalue f λ root p f p f λ shown example real vector space dimension least two exist endomorphisms eigenvalue λ root p f p f λ q monic polynomial q k linear factor λ divide polynomial p f show formally corollary also q λ q λ q monic polynomial q continue p f λ g q k thus p f λ g k g λ lead following definition definition let v finite dimensional k space let f l v v eigenvalue λ k characteristic polynomial f form p f λ g g k g λ called algebraic multiplicity eigenvalue λ f denoted λ f λk pairwise distinct eigenvalue f corresponding algebraic multiplicity f λk f dim v n f λk f n since deg p f dim v example endomorphism f v fv f characteristic polynomial p f real root p f f dim  \n",
       "192                                                                                                                                                                                                                                                                                                                                                                                      basic definition property lemma v finite dimensional k space f l v v g λ f λ f every eigenvalue λ f proof let λ k eigenvalue f geometric multiplicity g λ f exist linear independent eigenvectors vm v f corresponding eigenvalue λ dim v eigenvectors form basis b dim v n extend eigenvectors basis b vm vn f v j λv j j therefore f b b λim z two matrix z k z k using lemma obtain p f det f b b λ det z implies λ f g λ f following try find basis v eigenvalue given endomorphism f read easily matrix representation easiest form matrix sense diagonal triangular matrix since eigenvalue diagonal entry diagonalizability section analyze given endomorphism diagonal matrix representation formally define property follows definition let v finite dimensional k space endomorphism f l v v called diagonalizable exists basis b v f b b diagonal matrix accordingly matrix k n n diagonalizable exists matrix g l n k d diagonal matrix k n n order analyze diagonalizablility begin sufficient condition linear independence eigenvectors condition also hold v infinite dimensional lemma let v k space f l v v λk k k pairwise distinct eigenvalue f corresponding eigenvectors vk v vk linearly independent  \n",
       "193                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  eigenvalue endomorphisms proof prove assertion induction let k let eigenvectors f corresponding eigenvalue let k applying f side equation well multiplying equation yield two equation subtracting second equation first get since also obtain since thus linearly independent proof inductive step analogous assume assertion hold k let pairwise distinct eigenvalue f corresponding eigenvectors let k satisfy μk vk applying f equation yield μk λk vk multiplication give μk vk subtracting equation previous one get μk λk vk since pairwise distinct vk linearly independent induction hypothesis obtain μk implies also linearly independent using result next show sum eigenspaces corresponding pairwise distinct eigenvalue direct cp theorem lemma let v k space f l v v λk k k pairwise distinct eigenvalue f corresponding eigenspaces satisfy k v f λi v f λ j k  \n",
       "194                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          diagonalizability proof let fixed let k v v f λi v f λ j v j v j v f λ j j particular v v linear independence eigenvectors corresponding pairwise j distinct eigenvalue cp lemma implies v following theorem give necessary sufficient condition diagonalizability endomorphism finite dimensional vector space theorem v finite dimensional k space f l v v following statement equivalent f diagonalizable exists basis v consisting eigenvectors f characteristic polynomial p f decomposes n dim v linear factor k p f λn eigenvalue λn k f every eigenvalue λ j g λ j f λ j f proof f l v v diagonalizable exists basis b vn v scalar λn k f b b λn hence f v j λ j v j j scalar λn thus eigenvalue f corresponding eigenvectors vn hand exists basis b vn v consisting eigenvectors f f v j λ j v j j n scalar λn k corresponding eigenvalue hence f b b form let b vn basis v consisting eigenvectors f let λn k corresponding eigenvalue f b b form hence p f λn p f decomposes linear factor k  \n",
       "195                                                                                                                                                                                                                                                                                                                                                                                                                                                                 eigenvalue endomorphisms still show g λ j f λ j f every eigenvalue λ j eigenvalue λ j algebraic multiplicity j λ j f λ j occurs j time diagonal diagonal matrix f b b hold exactly j vector basis b eigenvectors f corresponding eigenvalue λ j j linearly independent vector element eigenspace v f λ j hence dim v f λ j g λ j f j λ j f lemma know g λ j f λ j f thus g λ j f λ j f λk pairwise distinct eigenvalue f correspond let λ j f j k ing geometric algebraic multiplicity g λ j f respectively since p f decomposes linear factor k λ j f n dim v g λ j f λ j f j k implies k g λ j f n dim v lemma obtain cp also theorem v f v f λk λ j j k select base respective eigenspaces v f get basis v consists eigenvectors f theorem lemma imply important sufficient condition diagonalizability corollary v n-dimensional k space f l v v n pairwise distinct eigenvalue f diagonalizable condition n dim v pairwise distinct eigenvalue however necessary diagonalizability endomorphism simple counterexample identity idv n-fold eigenvalue idv b b hold every basis b hand exist endomorphisms multiple eigenvalue diagonalizable  \n",
       "196                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                diagonalizability example endomorphism f r r v fv f characteristic polynomial thus eigenvalue ker v f span thus g f f theorem f diagonalizable triangulation schur theorem property g λ j f λ j f hold every eigenvalue λ j f f diagonalizable however long characteristic polynomial p f decomposes linear factor find special basis b f b b triangular matrix theorem v finite dimensional k space f l v v following statement equivalent characteristic polynomial p f decomposes linear factor k exists basis b v f b b upper triangular f triangulated proof n dim v f b b ri j k n n upper triangular p f rnn show assertion induction n dim v case n trivial since f b b k suppose assertion hold n let dim v n assumption p f k eigenvalue f let v eigenvector corresponding eigenvalue extend vector basis b bw w span bw v span w f b b  \n",
       "197                                                                                                                                                                                                                                                                                                                                                                                                                                                        eigenvalue endomorphisms define h l w span g l w w h w j j g w j k j wk j n f w h w g w w w f b b h bw g bw bw consequently pg p f hence pg dim w n characteristic polynomial g l w w decomposes linear factor w w induction hypothesis exists basis bw upper triangular thus basis b v w g w bw bw matrix f upper triangular matrix version theorem read follows characteristic polynomial pa k n n decomposes linear factor k triangulated exists matrix g l n k r upper triangular matrix r k n n corollary let v finite dimensional euclidian unitary vector space f l v v p f decomposes r euclidian case case c unitary case linear factor exists orthonormal basis b v f b b upper triangular proof p f decomposes linear factor theorem exists basis v f upper triangular applying gram-schmidt method basis obtain orthonormal basis v idv upper triangular cp theorem f idv f idv idv f idv invertible upper triangular matrix form group respect matrix multiplication cp theorem thus matrix product right hand side upper triangular hence f upper triangular example consider euclidian vector space r scalar product p p q dt endomorphism  \n",
       "198                                                                                                                                                                                                                                                                                                                                                                                                                                             triangulation schur theorem f r r f f polynomial eigenvectors f corresponding distinct eigenvalue thus b basis diagonal matrix note b orthonormal basis r f b b since particular since p f decomposes linear factor corollary guarantee existence orthonormal basis b f b b upper triangular proof implication theorem one chooses eigenvector f proceeds inductively order obtain triangulation f example let u use first vector vector eigenvector f norm corresponding eigenvalue r vector norm b orthonormal basis f b b upper triangular matrix construct vector orthogonalizing using gram-schmidt method lead triangulation f b b could also choose eigenvector f norm corresponding eigenvalue orthogonalizing vector lead second basis vector corresponding basis obtain triangulation f example show triangulation f element diagonal different different orthonormal base diagonal element except order uniquely determined since eigenvalue f detailed statement uniqueness given lemma next chapter prove fundamental theorem algebra state every non-constant polynomial c decomposes linear factor result following corollary known schur issai schur  \n",
       "199                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           eigenvalue endomorphisms corollary v finite dimensional unitary vector space every endomorphism v unitarily triangulated f l v v exists orthonormal basis b v f b b upper triangular matrix f b b called schur form f v unitary vector space standard scalar product obtain following matrix version corollary corollary cn n exists unitary matrix q cn n q r q h upper triangular matrix r cn n matrix r called schur form following result show schur form matrix cn n n pairwise distinct eigenvalue almost unique lemma let cn n n pairwise distinct eigenvalue let cn n two schur form diagonal equal u u h unitary diagonal matrix u proof exercise survey result unitary similarity matrix found article matlab-minute consider n matrix n n n cn n n n compute schur form using command u r schur n eigenvalue formulate conjecture rank general prove conjecture exercise following exercise k arbitrary field let v vector space let f l v v eigenvalue λ show im λidv f f subspace let v finite dimensional vector space let f l v v bijective show f f invariant subspace  \n",
       "200                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             triangulation schur theorem let v n-dimensional k space let f l v v let u m-dimensional f subspace show basis b v exists f b b matrix k k k let k r c f k k v fv f compute p f determine k r k c eigenvalue f algebraic geometric multiplicity well associated eigenspaces consider vector space r standard basis n endomorphism n f r r n αi αi dt compute p f eigenvalue f algebraic geometric multiplicity examine whether f diagonalizable change one considers map kth derivative k n examine whether following matrix b q c diagonalizable set diagonalizable invertible matrix subgroup g l n k let n consider r-vector space r map f r r p p p show f linear n f diagonalizable let v r-vector space basis vn examine whether following endomorphisms diagonalizable f v j v j v j n f vn vn  \n",
       "201                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     eigenvalue endomorphisms b f v j jv j v j n f vn nvn let v finite dimensional euclidian vector space let f l v v f f ad l v v show f f diagonalizable let v c-vector space let f l v v f determine possible eigenvalue f let v finite dimensional vector space f l v v show p f f l v v let v finite dimensional k space let f l v v p μm k show p f bijective μm eigenvalue determine condition entry matrix αβ γ δ diagonalizable triangulated determine endomorphism r diagonalizable triangulated let v vector space dim v show f l v v triangulated exist subspace vn v v j v j n b dim v j j j n c v j f j prove lemma  \n",
       "202                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              chapter polynomial fundamental theorem algebra chapter discus polynomial detail consider division polynomial derive classical result polynomial algebra including factorization irreducible factor also prove fundamental theorem algebra state every non-constant polynomial complex number least one complex root implies every complex matrix every endomorphism finite dimensional complex vector space least one eigenvalue polynomial let u recall important term context polynomial k field p αn n n αn k polynomial k variable set k polynomial form commutative ring unit cp example αn deg p n called degree αn p called monic p deg p deg p p called constant lemma two polynomial p q k following assertion hold deg p q max deg p deg q deg p q deg p deg q proof exercise introduce concept associated division polynomial springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "203                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      polynomial fundamental theorem algebra definition let k field two polynomial p k exists polynomial q k p q called divisor p write p read divide p two polynomial p k called coprime p q k always imply q constant non-constant polynomial p k called irreducible k p q two polynomial q k implies q constant exist two non-constant polynomial q k p q p called reducible k note property irreducibility defined polynomial degree least polynomial degree always irreducible whether polynomial degree least irreducible may depend underlying field example polynomial q irreducible factorization show r reducible polynomial r irreducible using imaginary unit c reducible next result concern division remainder polynomial theorem p k k exist uniquely defined polynomial q r k p q r deg r deg proof show first existence polynomial q r k hold deg k follows q p r deg r deg assume deg deg p deg set q r p q r deg r deg let n deg p deg prove induction n hence p therefore p q r q r deg r deg  \n",
       "204                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    polynomial suppose assertion hold n let two polynomial p n deg p deg given let sm highest coefficient p h p k deg h deg p n induction hypothesis exist polynomial q r k h q r deg r deg follows p q r q q deg r deg remains show uniqueness suppose hold exist polynomial q r k p q r deg r deg r r q q r r q q thus deg r r deg q q deg deg q q deg hand also deg r r max deg r deg r deg contradiction show indeed r r q q theorem important consequence root polynomial first known theorem corollary λ k root p k p λ exists uniquely determined polynomial q k p λ q proof apply theorem polynomial p λ get uniquely determined polynomial q r deg r deg p λ q polynomial r constant evaluating λ give p λ λ λ q λ r λ r λ paolo ruffini  \n",
       "205                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         polynomial fundamental theorem algebra yield r p λ q polynomial p k least degree root λ k linear factor λ divisor p particular p reducible converse statement hold instance polynomial q reducible root corollary motivates following definition definition λ k root p k multiplicity uniquely determined nonnegative integer p λ q polynomial q k q λ recursive application corollary given polynomial p k lead following result corollary λk k pairwise distinct root p k corresponding multiplicity k exists unique polynomial q k p λk k q q λ j j particular sum multiplicity pairwise distinct root p deg p next result known lemma lemma p k coprime exist polynomial k p proof may assume without loss generality deg p deg proceed induction deg deg k thus p suppose assertion hold polynomial p k deg n n let p k deg p deg n given theorem exist polynomial q r p q r deg r deg r since assumption p coprime suppose exists non-constant polynomial h k divide r h also divide p contradiction assumption p coprime thus polynomial r coprime since deg r deg étienne bézout  \n",
       "206                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                polynomial apply induction hypothesis polynomial r k hence k exist polynomial r r p q get p q p q completes proof using lemma bézout easily prove following result lemma p k irreducible divisor product h two polynomial h k p divide least one factor proof every polynomial divisor zero polynomial p divisor p coprime since p irreducible lemma exist polynomial k p hence h h h p h polynomial p divide term right hand side thus also recursive application lemma obtain euclidean theorem describes prime factor decomposition ring polynomial theorem every polynomial p αn n k unique ordering factor decomposition p μ pk μ k monic irreducible polynomial pk k proof deg p thus p assertion hold k μ let deg p p irreducible assertion hold p μ αn p reducible p two non-constant polynomial either irreducible decompose every multiplicative decomposition p obtained way deg p n non-constant factor suppose p μ pk β q q k k n μ β k well monic irreducible polynomial pk k p hence j j since polynomial q j irreducible must q j  \n",
       "207                                                                                                                                                                polynomial fundamental theorem algebra may assume without loss generality j cancel polynomial identity give μ pk β q q proceeding analogously polynomial pk finally obtain k μ β p j q j j fundamental theorem algebra seen existence root polynomial depends field considered field c special sense since fundamental theorem guarantee every non-constant polynomial root order use theorem context first present equivalent formulation language linear algebra theorem following statement equivalent every non-constant polynomial p c root v finite dimensional c-vector space every endomorphism f l v v eigenvector proof v f l v v characteristic polynomial p f c non-constant since deg p f dim v thus p f root c eigenvalue f f indeed eigenvector let p αn n c non-constant polynomial αn root p equal root monic polynomial p pa p cp p let cn n companion matrix lemma v n-dimensional c-vector space b arbitrary basis v exists uniquely determined f l v v f b b cp theorem assumption f eigenvector hence also eigenvalue p pa root fundamental theorem algebra proven without tool analysis particular one need polynomial continuous use following standard result based continuity polynomial lemma every polynomial p r odd degree real root numerous proof important result exist carl friedrich gauß alone gave four different proof starting one dissertation contained however gap history result described detail book  \n",
       "208                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        fundamental theorem algebra proof let highest coefficient p positive lim p lim p since real function p continuous intermediate value theorem analysis implies existence root argument case negative leading coefficient analogous proof fundamental theorem algebra follows presentation article proof induction dimension however use usual consecutive order dim v order based set j j odd n j instance odd lemma v r-vector space dim v odd dim v every f l v v eigenvector let k field j every k space v dim v j every f l v v eigenvector two commuting f f l v v common eigenvector f f f f exists vector v v two scalar k f v v f v v r-vector space dim v odd two commuting f f l v v common eigenvector proof every f l v v degree p f r odd hence lemma implies p f root therefore f eigenvector proceed induction dim v dim v run element j increasing order set j proper subset n consisting natural number divisible j particular smallest element j dim v j assumption two arbitrary f f l v v eigenvector f f since dim v α k thus f f α f  \n",
       "209                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          polynomial fundamental theorem algebra common eigenvector f f let dim v j let assertion proven k space whose dimension element j smaller dim v let f f l v v f f f f assumption f eigenvector corresponding eigenvalue let u im idv f w v ker idv f subspace u w v f f u u f w space w shown lemma space u easily shown well cp exercise subspace u w also f u u u idv f v v since f f commute f u f idv f v idv f f v idv f f v u w w idv f f w idv f f w f idv f w f idv f w f hence f w dim v dim u dim w since dim v divisible j either dim u dim w divisible j hence either dim u j dim w j corresponding subspace proper subspace v dimension element j smaller dim v induction hypothesis f f common eigenvector subspace thus f f common eigenvector corresponding subspace equal v must subspace w since dim w v w every vector v eigenvector f assumption also f eigenvector exists least one common eigenvector f f follows assumption hold k r j mean hold well prove fundamental theorem algebra formulation theorem theorem v finite dimensional c-vector space every f l v v eigenvector  \n",
       "210                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    fundamental theorem algebra proof prove assertion induction j dim v j start j thus showing assertion c-vector space odd dimension let v arbitrary c-vector space n dim v let f l v v consider arbitrary scalar product v scalar product always exists cp exercise well set self-adjoint map respect scalar product h g l v v g g ad lemma set h form r-vector space dimension n define h h l h h h g f g g f ad h g f g g f ad g h h h h h cp exercise since n odd also n odd lemma h h common eigenvector hence exists g h g g h g g h h ih g f g g h therefore particular g f g h ih since g exists v v g v g v f g v show g v v eigenvector f proof j complete assume j every c-vector space v dim v j every f l v v eigenvector lemma implies every two commuting f f l v v common eigenvector show every c-vector space v dim v every f l v v eigenvector since j j q q odd prove c-vector space v n dim v j q odd q let v vector space let f l v v given choose arbitrary basis v denote matrix representation f respect basis cn n let  \n",
       "211                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         polynomial fundamental theorem algebra b cn n b b set complex symmetric n n matrix define h h l h b ab b h b ab b h h h h cp exercise lemma set form c-vector space dimension n n n j q odd natural number q thus n n j q j q q j q j induction hypothesis commuting endomorphisms h h common eigenvector hence exists b b b b h b particular b b multiplying equation left yield b b b b b b b b n factorize α β used every complex number square root αin β b bv β bv bv since b exists v bv eigenvector corresponding eigenvalue β β bv eigenvector corresponding eigenvalue α since β eigenvector also f eigenvector  \n",
       "212                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    fundamental theorem algebra matlab-minute compute eigenvalue matrix using command eig definition real matrix real eigenvalue reason occurrence complex eigenvalue matlab interprets every matrix complex matrix mean within matlab every matrix unitarily triangulated since every complex polynomial degree least decomposes linear factor direct corollary fundamental theorem algebra lemma following result corollary v finite dimensional c-vector space two commuting f f l v v common eigenvector example two complex matrix b commute eigenvalue b hence b common eigenvalue common eigenvectors b using corollary schur theorem corollary generalized follows theorem v finite dimensional unitary vector space f f l v v commute f f simultaneously unitarily triangulated exists orthonormal basis b v f b b f b b upper triangular proof exercise  \n",
       "213                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               polynomial fundamental theorem algebra exercise following exercise k arbitrary field prove lemma show following assertion k b c imply imply exists c k examine whether following polynomial irreducible q r c q r c determine decomposition irreducible factor decompose polynomial irreducible factor field k q k r k show following assertion p k deg p p irreducible b deg p p root p irreducible c deg p p irreducible p root let g l n c n let adj cn n adjunct show exist n matrix j cn n det j det j n aj adj hint use pa construct polynomial p c adj p express p product linear factor show two polynomial p q c common root exist polynomial c deg deg p deg deg q p q let v finite dimensional unitary vector space f l v v h g l v v g g ad let h h l v v g f g g f ad  \n",
       "214                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       fundamental theorem algebra h h l v v g f g g f ad show h h l h h h h h h let cn n b cn n b b let h cn n b ab b h cn n b ab show h h l h h h h let v c-vector space f l v v let u finite dimensional f subspace show u contains least one eigenvector f let v k space let f l v v show following statement k c exists f subspace u v dim u b k r exists f subspace u v dim u prove theorem construct example showing condition f g g f theorem sufficient necessary simultaneous unitary triangulation f let k n n diagonal matrix pairwise distinct diagonal entry b k n n ab b show case b diagonal matrix say b diagonal entry pairwise distinct show matrix commute determine unitary matrix q q h aq q h b q upper triangular show following statement p k k n n g l n k p sp b b c k n n ab c ap b p c c k c cn n exists unitary matrix q q h aq q h p q upper triangular let v finite dimensional unitary vector space let f l v v normal f satisfies f f ad f ad f  \n",
       "215                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  polynomial fundamental theorem algebra show λ c eigenvalue f v f λ f subspace b show using f diagonalizable hint show induction dim v v direct sum eigenspaces f c show using b f even unitarily diagonalizable exists orthonormal basis b v f b b diagonal matrix let g l v v unitarily diagonalizable show g normal show endomorphism finite dimensional unitary vector space normal unitarily diagonalizable give different proof result theorem let v finite dimensional k space f l v v v f subspace let furthermore f j f j l u j u j j every v v exist unique u u v u show also f v f u f u f u f u write f f f call f direct sum f f respect decomposition v b show rank f rank f rank f p f p p c show λ f λ f λ f λ k set λ h λ eigenvalue h l v v show g λ f g λ f g λ f λ k set g λ h dim ker λidv even λ eigenvalue h l v v e show p f p f p f p k  \n",
       "216                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                chapter cyclic subspace duality jordan canonical form chapter use duality theory analyze property endomorphism f finite dimensional vector space v detail particularly interested algebraic geometric multiplicity eigenvalue f characterization corresponding eigenspaces strategy analysis decompose vector space v direct sum f subspace appropriately chosen base essential property f obvious matrix representation matrix representation derive called jordan canonical form f great importance many different derivation form using different mathematical tool approach using duality theory based article vlastimil pták cyclic f subspace duality let v finite dimensional k space f l v v v exists uniquely defined smallest number n vector f f linearly independent vector f f f linearly dependent obviously dim v since dim v vector v linearly independent number called grade respect f denote grade f vector linearly dependent thus grade respect f springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "217                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     cyclic subspace duality jordan canonical form f vector f linearly dependent hold eigenvector f eigenvector f f every j n define subspace k j f span f f space k j f called jth krylov f lemma v finite dimensional k space f l v v v following assertion hold f km f f subspace v span f f km f j f j f u v f subspace contains vector km f u thus among f subspace v contain vector krylov subspace km f one smallest dimension f f n dim k j f j j proof exercise assertion trivial thus let f let u v f subspace contains u also contains vector f f km f u particular dim u dim km f let k f apply f side f thus since f apply inductively f k obtain thus vector f linearly independent implies dim k j f j j vector f f form construction basis krylov subspace km f application f vector f k basis yield next basis vector f k application f last vector f yield linear combination basis vector since f km f due special structure subspace km f called cyclic f subspace aleksey nikolaevich krylov  \n",
       "218                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 cyclic f subspace duality definition let v k space endomorphism f l v v called nilpotent f hold time f f called nilpotent index zero map f nilpotent endomorphism index v zero map endomorphism map nilpotent index case omit requirement f f f nilpotent index v vector f v f f v f v f v hence f v eigenvector f corresponding eigenvalue construction sect show eigenvalue nilpotent endomorphism also cp exercise lemma v k space f l v v nilpotent index dim v proof f nilpotent index exists v f f lemma vector f linearly independent implies dim v example vector space k endomorphism f k k nilpotent index since f f f u f subspace v f l u u f u u u f u restriction f subspace u cp definition theorem let v finite dimensional k space f l v v exist f subspace v v v f l bijective f l nilpotent proof v ker f f v f f v f thus v ker f therefore ker f ker f proceeding inductively see ker f ker f ker f since v finite dimensional exists smallest number ker f ker f j j number let im f ker f  \n",
       "219                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          cyclic subspace duality jordan canonical form f bijective v show space satisfy assertion first observe f v v f w w v therefore f v f f w f f w v f f v f f v f therefore f v application dimension formula linear map cp theorem f give dim v dim dim v v f w w v since v hence f v f f w f w first equation hold since v definition ker f ker f implies f w therefore v f w obtain v let v ker f given since v exists vector w v v f w implies f v f f w f w definition ker f ker f thus w ker f therefore v f w implies ker f f injective thus also bijective cp corollary hand v definition f v f v thus f zero map l f nilpotent development recall term result chap let v finite dimensional k space let v dual space u v w v two subspace bilinear form β u w k v h h v non-degenerate u w called dual pair respect β requires dim u dim w f l u u dual map f l u u defined f u u h h v u h u f h v h f v furthermore f k f k k set u h v h u u u called annihilator u set subspace v cp exercise analogously set w v v h v h w called annihilator set subspace v  \n",
       "220                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   cyclic f subspace duality lemma let v finite dimensional k space f l v v v dual space v f l v v dual map f let u v w v two subspace following assertion hold dim v dim w dim w dim u dim u f nilpotent index f nilpotent index w v f subspace w v f subspace u w dual pair respect bilinear form defined v u w proof exercise v v f v hence h f v f h v f h v every h v v v f nilpotent index f f h h v therefore f h v h f v v implies f contradiction assumption f nilpotent index thus f nilpotent index let w w every h w f h w thus f h w h f w hence f w w u u w h u h w since u w since u w dual pair respect bilinear form defined u moreover dim u dim w using obtain dim v dim w dim w dim u dim w u w obtain v u w example consider vector space v canonical basis b subspace v u span w h v h b α α α r v u h v h b α α r v w span  \n",
       "221                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                cyclic subspace duality jordan canonical form example easily see dim v dim w dim w dim u dim u u w form dual pair respect bilinear form defined k moreover v u w following theorem present given nilpotent f decomposition v f subspace idea decomposition construct dual pair subspace u v w v u f w f lemma w f lemma follows v u w theorem let v finite dimensional k space let f l v v nilpotent index let v satisfy f let h v satisfy h f f f h f f subspace km f v km f h v respectively dual pair respect bilinear form defined furthermore v km f km f h km f h f subspace proof let v vector f since f space km f m-dimensional f subspace v cp lemma let h v vector h f f h particular f h l v v since f nilpotent index also f nilpotent index cp lemma f h l v v therefore km f h m-dimensional f subspace v cp lemma remains show km f km f h dual pair let γ j f j km f vector h β h h km f h show inductively thus  \n",
       "222                                                                                                                                                                                                                                                                                                                          cyclic f subspace duality using f h km f h assumption vector yield f h h f γ j h f j h f last equation hold since f j j f h f obtain suppose k k using f h km f h assumption vector yield f h h f γ j h f γk h f last equation hold since γ j j k f j k asserted therefore bilinear form defined space km f km f h non-degenerate first variable analogously bilinear form non-degenerate second variable hence km f km f h dual pair using lemma v km f km f h space km f h lemma f subspace jordan canonical form let v finite dimensional k space f l v v exists basis b v consisting eigenvectors f f b b diagonal matrix f diagonalizable necessary sufficient condition characteristic polynomial p f decomposes linear factor k addition g f λ j f λ j every eigenvalue λ j cp theorem p f decomposes linear factor g f λ j f λ j hold least one eigenvalue λ j f diagonalizable still triangulated exists basis b v f b b upper triangular matrix cp theorem triangular matrix read algebraic usually geometric multiplicity eigenvalue goal following construction determine basis b v f b b upper triangular addition algebraic also reveals geometric multiplicity eigenvalue assumption p f decomposes linear factor k construct basis b v f b b block diagonal matrix form  \n",
       "223                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    cyclic subspace duality jordan canonical form f b b jdm λm diagonal block form λj jd j λ j k j j λj λ j k j n j matrix form called jordan block size j corresponding eigenvalue λ j following construction first assume p f decomposes linear factor assume existence single eigenvalue k f using eigenvalue define endomorphism g f idv l v v theorem exist g-invariant subspace u v w v v u w nilpotent bijective u since otherwise w v g would bijective contradicts assumption eigenvalue f let nilpotent index construction dim u let u vector since vector eigenvector corresponding eigenvalue lemma vector linearly independent subspace u consider basis matrix representation respect basis given k  \n",
       "224                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             jordan canonical form show particular characteristic polynomial given monomial hence eigenvalue moreover construction dim u construction complete moment hand dim u applying theorem l u u show u u consider exists subspace u map nilpotent index carry construction g g determine vector u eigenvector subspace u basis u k construction k dim u step procedure terminates found decomposition u form u kdk gk wk g kdk g wk second equation used kd j g j w j kd j g wk j combine constructed base bk basis b u b b bk bk jdk thus nilpotent endomorphism characteristic polynomial eigenvalue transfer result f g idv every g-invariant subspace f one observes easily kd j f w j kd j g w j j k  \n",
       "225                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  cyclic subspace duality jordan canonical form cp exercise hence follows u f kdk f wk every j k j f g w j g g w j g w j g w j g w j g j w j matrix representation f respect basis b u therefore given f b b f f bk bk jdk map f idw bijective construction eigenvalue f therefore f dim u dk order determine g f let v u arbitrary vector exist scalar α j k j k α j g w j using obtain k j f v k j k j α j f g w j α j g w j α j g w j k j v α j g w j vector last sum linearly independent hence f v v α j j k j show every eigenvector f corresponding eigenvalue form k α j g j w j least one α j nonzero v f span g g dk wk  \n",
       "226                                                                                                                                                                                                                                                                jordan canonical form since g g dk wk linearly independent follows g f geometric multiplicity eigenvalue therefore equal number jordan block corresponding eigenvalue matrix representation furthermore observe every subspace kd j f w j endomorphism f exactly one linear independent eigenvector corresponding eigenvalue summarize result following theorem theorem let v finite dimensional k space let f l v v k eigenvalue f following assertion hold exist f subspace u v w v v u map f idu nilpotent map f idw bijective particular eigenvalue f subspace u written u f kdk f wk vector wk u kd j f w j j f invariant subspace v j called cyclic decomposition u exists basis b u f b b jdk f dk g f f eigenvalue eigenvalue restriction f l w w apply theorem f vector space w direct sum form w x f idx nilpotent f idy bijective space x cyclic decomposition analogous theorem exists matrix representation f analogous construction carried eigenvalue f characteristic polynomial p f decomposes linear factor k finally obtain cyclic decomposition entire space v give following theorem theorem let v finite dimensional k space let f l v v characteristic polynomial p f decomposes linear factor k exists basis b v f b b jdm λm λm k necessarily pairwise distinct eigenvalue f every eigenvalue λ j f f λ j equal sum size  \n",
       "227                                                                                                                                                                                                                                                                                                                                                                                                                                                                       cyclic subspace duality jordan canonical form jordan block corresponding λ j g f λ j equal number jordan block corresponding λ j matrix representation form called jordan canonical f theorem know f l v v diagonalizable p f decomposes linear factor k g f λ j f λ j hold every eigenvalue λ j f p f decomposes linear factor jordan canonical form show g f λ j f λ j every jordan block corresponding λ j size fundamental theorem algebra yield following corollary theorem corollary v finite dimensional c-vector space every f l v v jordan canonical form following uniqueness result justifies name canonical form theorem let v finite dimensional k space f l v v jordan canonical form unique order jordan block diagonal proof let dim v n let two base v f well f k n n jdm λm k n n jck μk given eigenvalue λ j j define r λ j rank λ j λ j r λ j d λ j equal number jordan block λ j k diagonal number jordan block corresponding eigenvalue λ j exact size therefore given λ j λ j λ j λ j d λ j marie ennemond camille jordan derived form two year earlier karl weierstraß proved result implies jordan canonical form  \n",
       "228                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      jordan canonical form cp example matrix similar therefore eigenvalue λm μk furthermore rank αin rank αin α k particular every λ j exists μi μk μi λ j μi matrix get r μi rank μi r λ j show matrix reordering jordan block diagonal matrix example example illustrates construction proof theorem get r d d  \n",
       "229                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      cyclic subspace duality jordan canonical form consider power jordan block jd λ k since id jd commute k j jd j j k jd λ k λid jd k k p j λ jd j j every k p j jth derivative polynomial p k respect p k k p j k j k k k j j j easily show following result lemma p k polynomial degree k k p jd λ p j λ jd j j proof exercise considered linear map k k matrix jd represents upshift since jd αd clearly k αd jd jd hence linear map jd nilpotent index sum right hand side therefore term even deg p moreover right hand side show p jd λ upper triangular matrix constant entry diagonal matrix constant diagonal called toeplitz particular main diagonal entry p λ see p jd λ hold p λ p λ p λ thus shown following result otto toeplitz  \n",
       "230                                                                                                                                            jordan canonical form lemma let p k polynomial jd λ k jordan block matrix p jd λ invertible λ root p jd λ k λ d-fold root p linear factor λ divisor let v finite dimensional k space let f l v v assume p f decomposes linear factor cayley-hamilton theorem theorem know p f f l v v exists monic polynomial degree dim v annihilates endomorphism f let k two monic polynomial smallest possible degree f f f since monic k polynomial deg deg deg minimality assumption deg deg implies thus every f l v v exists uniquely determined monic polynomial minimal degree annihilates f justifies following definition definition v finite dimensional k space f l v v uniquely determined monic polynomial minimal degree annihilates f called minimal polynomial f denote polynomial f construction always deg f deg p f dim v lemma v finite dimensional k space f l v v minimal polynomial f divide every polynomial annihilates f particular divisor characteristic polynomial p f proof p p f f divide p k polynomial p f deg f deg p using division remainder cp theorem exist uniquely determined polynomial q r k p q f r deg r deg f thus p f q f f f r f r f minimality deg f implies r hence f divide p f decomposes linear factor explicitly construct f using jordan canonical form f lemma let v finite dimensional k space f l v v jordan canonical form pairwise distinct eigenvalue λk respective maximal size corresponding jordan block mf k λ j j  \n",
       "231                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             cyclic subspace duality jordan canonical form proof know lemma f divisor p f therefore mf k λ j j exponent jdm λm jordan canonical form f f f l v v equivalent f k n n n dim v f f jd j λ j j necessary sufficient f λ j j lemma hold every linear factor λ j j j k divisor f therefore f desired form example f endomorphism jordan canonical form p f f f show f f f l v v jordan canonical form great importance theoretical linear algebra practical application however usually matrix k r k c considered relevant since numerically stable method computing jordan canonical form general matrix finite precision arithmetic reason lack method entry jordan canonical form depend continuously entry given matrix example consider matrix ε ε  \n",
       "232                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               jordan canonical form every given ε matrix ε two distinct eigenvalue ε hence diagonal matrix j ε jordan canonical form ε however ε obtain ε j ε thus j ε converge jordan canonical form ε similar example given matrix exercise jordan block size n corresponding eigenvalue every ε obtain diagonalizable matrix ε cn n n pairwise distinct eigenvalue matlab-minute let random matrix constructed command rand construct several matrix always compute eigenvalue using command eig display eigenvalue format long one observes two eigenvalue real complex conjugate always error starting digit decimal point error order happen chance due behavior eigenvalue perturbation arise rounding error computer computation jordan canonical form derive method computation jordan canonical form endomorphism f finite dimensional k space assume p f decomposes linear factor k root p f eigenvalue f known construction follows important step existence proof jordan canonical form sect suppose λ eigenvalue f f corresponding jordan block size exist linearly independent vector t f b b j λ  \n",
       "233                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            cyclic subspace duality jordan canonical form b t writing id instead idv simplicity notation f λid f λid f λid t hence j f λid j t j vector t form sequence one constructed context krylov subspace span t k f λid t reverse sequence called jordan chain f corresponding eigenvalue λ vector eigenvector f corresponding λ vector f f λid f λid hence ker f λid ker f λid general j ker f λid j ker f λid j motivates following definition definition let v finite dimensional k space let f l v v eigenvalue λ k let k vector v v v ker f λid k ker f λid called principal vector level k f corresponding eigenvalue λ principal vector level one eigenvectors principal vector higher level considered generalization eigenvectors therefore sometimes called generalized eigenvectors computation jordan canonical form f thus need know number length jordan chain corresponding different eigenvalue f correspond number size jordan block f f matrix representation f respect arbitrary basis cp proof theorem  \n",
       "234                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              computation jordan canonical form d λ rank f λi rank f λi dim im f λid dim im f λid dim v dim ker f λid dim v dim ker f λid dim ker f λid dim ker f λid number jordan block corresponding λ size least implies particular d λ λ d λ λ number jordan block exact size corresponding λ exists smallest number n ker f λid ker f λid ker f λid ker f λid hence d λ jordan block corresponding λ size larger order compute jordan canonical form therefore proceed follows determine eigenvalue f every eigenvalue λ f carry following step determine smallest number n ker f ker f ker f ker f dim ker f λid λ f b determine d λ dim ker f λid dim ker f λid d λ λ dim ker f λid g λ f number jordan block corresponding λ c simplify notation write d d λ determine jordan chain follows since dm dm exist dm jordan block size block determine jordan chain dm principal vector level vector tdm ker f λid ker f λid  \n",
       "235                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             cyclic subspace duality jordan canonical form following property dm αdm k αi ti ker f λid αdm first index ti j indicates number chain second indicates level principal vector ker f λid j ker f λid ii j proceed follows determined j principal vector level j say j j td j j apply f λid vector hence ti f λid ti j j order determine principal vector level j dj αi ti ker f λid αd j k dj f λid dj dj αi ti f λid thus αi ti j αi ti j ker f λid giving αd j j exist j jordan block size j need jordan chain length j thus extend already computed td j ker f λid ker f λid principal vector level j j via td ker f λid ker f λid following must hold αd k αi ti ker f λid αd completing step j obtained linearly independent vector ker f λid since dim ker f λid found basis ker f λid way determined different jordan chain combine follows tλ  \n",
       "236                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       computation jordan canonical form chain begin eigenvector followed principal vector increasing level use convention chain ordered decreasingly according length jordan chain linearly independent first vector eigenvectors linearly independent show exercise thus pairwise distinct eigenvalue f basis f jordan canonical form example interpret matrix f endomorphism eigenvalue f root pf particular pf decomposes linear factor f jordan canonical form consider different eigenvalue f eigenvalue obtain ker f ker span dim ker f f eigenvalue obtain ker f ker span  \n",
       "237                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 cyclic subspace duality jordan canonical form ker f ker span dim ker f f b dim ker f dim ker f dim ker f dim ker f c computation jordan chain principal vector level one choose form basis ker f r finished choose principal vector level two say vector r span compute f since add another principal vector level one choose since vector linearly independent ker f implies way get coordinate transformation matrix jordan canonical form f f  \n",
       "238                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        computation jordan canonical form exercise following exercise k arbitrary field prove lemma prove lemma let v k space f l v v λ k prove disprove subspace u v f f λidv let v finite dimensional k space f l v v v v λ k show k j f v k j f λidv v j conclude grade v respect f equal grade v respect f λidv prove lemma let v finite dimensional euclidean unitary vector space let f l v v selfadjoint nilpotent show f let v finite dimensional k space let f l v v nilpotent index suppose p f decomposes linear factor show following assertion p f n n dim v b f c exists vector v v grade respect f every λ k f λ let v finite dimensional k space f l v v show following assertion ker f j ker f j exists ker f ker f ker f ker f j j b im f j im f j exists im f im f im f im f j j c minimal ker f ker f im f im f theorem implies v ker f im f decomposition v f subspace let v finite dimensional k space let f l v v projection cp exercise show following assertion v im f implies f v b v im f ker f c exists basis b v f b b ik k dim im f n dim v particular p f k λ every eigenvalue λ f  \n",
       "239                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 cyclic subspace duality jordan canonical form map g idv f projection ker g im f im g ker f let v finite dimensional k space let u w v two subspace v u show exists uniquely determined projection f l v v im f u ker f determine jordan canonical form matrix r using method presented sect determine also minimal polynomial determine jordan canonical form minimal polynomial linear map f determine order block matrix j jordan canonical form pj j let v finite dimensional k space f l v v suppose p f decomposes linear factor show following assertion p f f hold g λ f eigenvalue λ f b f diagonalizable f simple root root multiplicity one c root λ k f simple ker f λidv ker f λidv let v k space dimension let f l v v p f decomposing linear factor show jordan canonical form f uniquely determined p f f hold longer dim v let k n n matrix characteristic polynomial decomposes linear factor show exists diagonalizable matrix nilpotent matrix n n n n let k n n matrix jordan canonical form define λ inr δi j jnr λ k n n λ  \n",
       "240                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               computation jordan canonical form show following assertion b c inr jn λ inr jn λ similar jn λ inr jnr λ written product two symmetric matrix determine matrix two symmetric matrix  \n",
       "241                                                                                                                                                                                                                                                                                                                                        chapter matrix function system differential equation chapter give introduction area matrix function first define general matrix function derive important property using example network analysis chemical reaction illustrate matrix function arise naturally application network analysis example involves exponential function matrix study property important function detail analysis chemical reaction kinetics lead system ordinary differential equation whose solution based matrix exponential function matrix function matrix exponential function following study function yield given n n matrix n n matrix possible definition function given entrywise application one could define scalar function matrix instance ai j cn n function sin sin sin ai j however definition compatible matrix multiplication since general already following definition primary matrix function definition turn consistent matrix multiplication since definition based jordan canonical form assume simplicity cn n consideration also apply square matrix r long jordan canonical form definition let cn n jordan canonical form j diag jdm λm springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "242                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      matrix function system differential equation let c λm function f c said defined spectrum value f j λi j di exist f j λi j di jth derivative function f λ respect λ evaluated λi λi r real derivative λi c r complex derivative moreover assume equal eigenvalue occur different jordan block mapped value f defined spectrum primary matrix function f defined f f j f j diag f f jdm λm f jdi λi di λi f di f λi f λi f λi f λi f λi f λi f λi f λi note definition f existence value required example let let f z z square root function set f f definition choose branch square root function f f matrix primary square root taking different branch function different jordan block corresponding eigenvalue incompatible definition instance matrix x incompatible definition despite fact x x solution x cn n matrix equation x called square root matrix cn n example show may primary square root according definition following f always mean primary matrix function according definition usually omit term primary shown polynomial p c degree k  \n",
       "243                                                                                                                                                                                                                                                                                                                                                                                                                                                           matrix function matrix exponential function p jdi λi k p j λi jdi j j simple comparison show formula agrees f mean computation p jdi λi lead result definition p jdi λi generally following result hold lemma let cn n p αk k c f p yield matrix function f satisfies f αk ak proof exercise consider particular polynomial f resulting f equal product show definition primary matrix function f consistent matrix multiplication following theorem great practical theoretical importance show matrix f always written polynomial theorem let cn n minimal polynomial let f definition exists uniquely determined polynomial p c degree deg f p particular f f f f well f v av v f v v g l n c proof present proof since requires advanced result interpolation theory detail found chap using theorem show primary matrix function f definition independent choice jordan canonical form already know theorem jordan canonical form unique order jordan block j diag jdm λm λm diag two jordan canonical form p j p permutation matrix p rn n matrix j order diagonal block hence f j diag f f jdm λm p p diag f f jdm λm p p p diag f f λm p p f p  \n",
       "244                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        matrix function system differential equation theorem applied matrix j yield existence polynomial p f j p j thus get f f j sp j p p p p j p p f j p f let u consider exponential function f z e z infinitely often complex differentiable throughout particular e z defined sense definition spectrum every given matrix sdiag jdm λm cn n c arbitrary fixed derivative function et z respect variable z given j tz e j et z dz j j use notation exp instead e exponential function matrix every jordan block jd λ f z e z exp jd λ etλ tλ jd k e k matrix exponential function exp given exp sdiag exp exp jdm λm parameter used next section context linear differential equation analysis shown every z c function e z represented absolutely convergent series ez zj j using series equation jd obtain  \n",
       "245                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       matrix function matrix exponential function j tλ tλ exp jd λ e jd jd j j tλ jd j j tj j λi jd j tj λid jd j j jd λ j j derivation used absolute convergence exponential series finiteness series matrix jd allows application cauchy product absolutely convergent series also proven analysis lemma cn n c exp matrix exponential function j exp j proof shown already jordan block assertion follows j j j j j j representation matrix exponential function immediately see lemma matrix rn n every real matrix exponential function exp real matrix following result present important property matrix exponential function lemma two matrix b cn n commute exp b exp exp b every matrix cn n exp g l n c exp exp augustin louis cauchy  \n",
       "246                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         matrix function system differential equation proof b commute cauchy product formula yield j j exp exp b b b j j j j b b j j j exp b used binomial formula commuting matrix cp exercise since commute exp exp exp exp j j hence exp g l n c exp exp non-commuting matrix statement lemma general hold cp exercise matlab-minute compute matrix exponential function exp matrix r using command look help expm also compute diagonalization using command form matrix exponential function exp compare matrix compute relative error norm look help norm example let ai j cn n symmetric matrix aii ai j j identify matrix graph g v e consisting set n vertex v n set edge e v v n row identified vertex e every entry ai j identified edge j e due symmetry ai j ji therefore consider following element  \n",
       "247                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     matrix function matrix exponential function e unordered pair j j following example illustrates identification identified g v e e v graph g displayed follows path length vertex vertex ordered list vertex ki v closed path length example path given length respectively mathematical field graph theory one usually assumes vertex path pairwise distinct deviation convention motivated following interpretation matrix power entry ai j matrix mean exists path length vertex vertex j vertex j adjacent ai j path exists matrix therefore called adjacency matrix graph g square adjacency matrix entry j position given j n sum right hand side obtain given e j e sum right side therefore equal number vertex adjacent j hence j entry equal number pairwise distinct path j j pairwise distinct closed path length g generally one show following cp exercise let ai j cn n symmetric adjacency matrix aii ai j j n let g graph identified n j entry equal number pairwise distinct path j j pairwise distinct closed path length g  \n",
       "248                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         matrix function system differential equation matrix obtain pairwise distinct closed path length pairwise distinct path length numerous real world application involve network modeled mathematically using graph example include social biological telecommunication airline network property network studied interdisciplinary area network science important task identify participant network central sense functionality significant impact entire network network modeled graph study centrality vertex example vertex considered central connected large part graph via many short closed path longer connection usually le important thus path scaled according length use scaling factor path length vertex graph g adjacency matrix obtain centrality measure form ii relative ordering vertex according formula changed add constant obtain centrality vertex exp ii ii another important quantity so-called communicability vertex j j given weighted sum pairwise distinct path j exp j ij matrix matlab function expm yield  \n",
       "249                                                                                                                                                                                                                                                                              matrix function matrix exponential function exp vertex largest centrality followed would define centrality vertex number adjacent vertex example could distinguish vertex largest communicability example exists vertex information concerning analysis network using adjacency matrix matrix function found article system linear ordinary differential equation differential equation describes relationship desired function derivative equation used area science engineering modeling physical phenomenon ordinary differential equation involve function one variable derivative partial differential equation involve function several variable partial derivative section focus ordinary differential equation first order function first derivative occur simple example modeling ordinary differential equation first order increase decrease biological population bacteria petri dish let size population time enough food external condition temperature pressure constant population grows real rate k proportional current number individual described equation dt clearly one also take k population shrink looking function r r satisfies general solution given exponential function cetk c r arbitrary constant unique solution need know size population given initial time way obtain initial value problem ky  \n",
       "250                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          matrix function system differential equation show solved uniquely function e k example chemical reaction certain initial substance called educts reactant transformed substance called product reaction distinguished concerning order discus reaction first order reaction rate determined one educt reaction second higher order one typically obtains nonlinear differential equation beyond focus chapter example educt transformed product rate write reaction symbolically model mathematically ordinary differential equation value concentration substance time concentration product grows rate corresponding equation may happen reaction first order develops direction transforms rate transforms rate model reaction mathematically system linear ordinary differential equation combining function vector valued function write system ay  \n",
       "251                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 system linear ordinary differential equation derivative function always considered entrywise reaction also several step example reaction form lead differential equation thus system ay sum entry column equal zero since every decrease substance certain rate substance increase rate summary chemical reaction first order lead system linear ordinary differential equation first order written ay real square matrix derive general theory system linear real complex ordinary differential equation first order form ay g k n n given matrix given positive real number g k given function k desired solution assume k r k g k system called homogeneous otherwise called non-homogeneous given system form system ay called associated homogeneous system  \n",
       "252                                                                                                                                                                                                                                                                                                                                                                                                                       matrix function system differential equation lemma solution homogeneous system form subspace infinite dimensional k space continuously differentiable function interval k proof show required property according lemma function w continuously differentiable solves homogeneous system thus solution set system empty k continuously differentiable solution k w continuously differentiable aw function w solution homogeneous system following characterization solution non-homogeneous system analogous characterization solution set non-homogeneous linear system equation lemma also cp lemma lemma k solution non-homogeneous system every solution written solution associated homogeneous system proof solution ay g g difference thus solution associated homogeneous system order describe solution system ordinary differential equation consider given matrix k n n matrix exponential function exp lemma consider real variable power series matrix exponential function lemma converges differentiated termwise respect variable derivative matrix respect variable considered entrywise yield exp dt dt exp result obtained entrywise differentiation matrix exp respect  \n",
       "253                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  system linear ordinary differential equation obtain tλ exp jd λ e dt dt λetλ etλ λetλ etλ jd λid jd etλ jd λ exp jd λ also give dt exp exp theorem unique solution homogeneous differential equation system given initial condition k given function exp set solution homogeneous differential equation system form n-dimensional k space basis exp exp en proof exp exp exp exp dt dt exp ay exp hence solution satisfies initial condition w another solution u exp w exp w exp w exp dt exp aw k show function u constant entry particular u u w w exp used exp exp cp lemma  \n",
       "254                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       matrix function system differential equation function exp e j exp en k j n solves homogeneous system ay since matrix exp k n n invertible every cp lemma function linearly independent arbitrary solution ay k unique solution initial value problem linear combination function exp consequence exp exp en describe solution non-homogeneous system need integral function form w k wn every fixed define w d d k wn d apply integral entrywise function definition dt w d w determine explicit solution formula system linear differential equation based so-called duhamel theorem unique solution non-homogeneous differential equation system initial condition k given exp exp exp g d proof derivative function defined exp exp g d exp g d exp exp g exp exp exp dt dt jean-marie constant duhamel  \n",
       "255                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          system linear ordinary differential equation exp exp exp g d g ay furthermore exp exp exp g d also satisfies initial condition let another solution satisfies initial condition lemma w w solves homogeneous system therefore w exp c c k cp theorem obtain c c hence theorem shown explicit solution system linear ordinary differential equation first order compute matrix exponential function introduced function using jordan canonical form given matrix numerical computation based jordan canonical form advisable cp example significant practical relevance numerous different algorithm computing matrix exponential function proposed shown article existing algorithm completely satisfactory example example circuit simulation presented sect lead system ordinary differential equation r vc v dt l l l vc dt c using initial value vc obtain solution exp vc v exp d example let u also consider example mechanic weight mass attached spring spring constant μ let distance weight equilibrium position illustrated following figure  \n",
       "256                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                matrix function system differential equation want determine position x weight time x extension spring described hooke corresponding ordinary differential equation second order μ x x dt initial condition x initial velocity weight write differential equation second order x system first order introducing velocity v new variable velocity given derivative position respect time v thus acceleration yield system ay mμ x v initial condition theorem unique solution homogeneous initial value problem given function exp consider element eigenvalue two complex non-real number iρ ρ mμ corresponding eigenvectors iρ thus exp sir itρ e iρ robert hooke  \n",
       "257                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            system linear ordinary differential equation exercise construct matrix ai j determine solution x matrix equation x classify solution primary square root determine matrix x real entry x prove lemma prove following assertion cn n det exp exp trace b h exp unitary c exp e e let diag jdm λm cn n rank determine primary matrix function f f z z function also exist rank n let log z r eiϕ r ϕ π c r eiϕ ln r iϕ principle branch complex logarithm ln denotes real natural logarithm show function defined spectrum compute log well exp log compute exp exp sin π construct two matrix b exp b exp exp b prove assertion entry ad example let compute exp r solve homogeneous system differential equation ay initial condition compute matrix exp example explicitly thus show exp r despite fact eigenvalue eigenvectors real  \n",
       "258                                                                                                                                                                                                                                                                                                                                                                      chapter special class endomorphisms chapter discus class endomorphisms square matrix whose eigenvalue eigenvectors special property property exist assumption chapter assumption concern relationship given endomorphism adjoint endomorphism thus focus euclidean unitary vector space lead class normal orthogonal unitary selfadjoint endomorphisms class natural counterpart set square real complex matrix normal endomorphisms start definition endomorphism matrix definition let v finite dimensional euclidean unitary vector space endomorphism f l v v called normal f f ad f ad f matrix rn n cn n called normal h h respectively z c zz zz property normality therefore interpreted generalization property complex number first study property normal endomorphisms finite dimensional unitary vector space recall following result b orthonormal basis v f l v v f b b h f ad b b cp theorem every f l v v unitarily triangulated cp corollary schur theorem hold general euclidean case since every real polynomial decomposes linear factor term introduced otto toeplitz context bilinear form springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "259                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            special class endomorphisms using result obtain following characterization normal endomorphisms unitary vector space theorem v finite dimensional unitary vector space f l v v normal exists orthonormal basis b v f b b diagonal matrix f unitarily diagonalizable proof let f l v v normal let b orthonormal basis v r f b b upper triangular matrix r h f ad b b f f ad f ad f obtain r r h f f ad b b f ad f b b r h show induction n dim v r diagonal obvious n let assertion hold n let r upper triangular r r h r h write r r cn n upper triangular rrh rh r obtain hence induction hypothesis cn n diagonal therefore diagonal well conversely suppose exists orthonormal basis b v f b b diagonal f ad b b f b b h diagonal since diagonal matrix commute f f ad b b f b b f ad b b f ad b b f b b f ad f b b implies f f ad f ad f hence f normal application theorem unitary vector space v standard scalar product matrix cn n viewed element l v v yield following matrix version corollary matrix cn n normal exists orthonormal basis consisting eigenvectors unitarily diagonalizable  \n",
       "260                                                                                                                                                                                                                                                                                                                                          normal endomorphisms following theorem present another characterization normal endomorphisms unitary vector space theorem v finite dimensional unitary vector space f l v v normal exists polynomial p c p f f ad proof p f f ad polynomial p c f f ad f p f p f f f ad f hence f normal conversely f normal exists orthonormal basis b v f b b diag λn furthermore f ad b b f b b h diag λn let p c polynomial p λ j λ j j polynomial explicitly constructed using lagrange basis c cp exercise f ad b b diag λn diag p p λn p diag λn p f b b p f b b hence also f ad p f several characterization normal endomorphisms finite dimensional unitary vector space normal matrix cn n found article see also exercise consider euclidean case focus real square matrix result formulated analogously normal endomorphisms finite dimensional euclidean vector space let rn n normal also satisfies h h considered element cn n unitarily diagonalizable d h hold unitary matrix cn n diagonal matrix cn n despite fact real entry neither real general since element rn n may diagonalizable instance normal matrix diagonalizable r considered element eigenvalue unitarily diagonalizable discus case real normal matrix detail first prove real version schur theorem  \n",
       "261                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  special class endomorphisms theorem every matrix rn n exists orthogonal matrix u rn n n n u au r r rmm every j either r j j rjj j j j j j r second case r j j considered complex matrix pair complex conjugate eigenvalue form α j iβ j α j r β j r matrix r called real schur form proof proceed via induction n r u suppose assertion hold n let given consider element eigenvalue λ c α β r corresponding eigenvector v x iy x av λv dividing equation real imaginary part obtain two real equation ax αx β ay βx αy two case case β two equation ax αx ay αy thus least one real vector x eigenvector corresponding real eigenvalue α without loss generality assume vector x x extend x vector orthonormal basis respect standard scalar product matrix x orthogonal satisfies α matrix rn n induction hypothesis exists orthogonal matrix rn n desired form matrix u orthogonal satisfies  \n",
       "262                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      normal endomorphisms u au α r r desired form case β first show x linearly independent x using β first equation implies also possible since eigenvector v x iy must nonzero thus x using β second equation implies also x linearly dependent exists μ r x μy two equation written ax α βμ x ax β αμ x μ implies β since μ r implies β contradicts assumption β consequently x linearly independent combine two equation system αβ x x α rank x applying gram-schmidt method respect standard scalar product matrix x yield x q q q g l r follows aq x x αβ αβ q α α real matrix αβ α considered element pair complex conjugate eigenvalue α iβ β particular nonzero since otherwise would two real eigenvalue extend vector orthonormal basis respect standard scalar product n list empty q orthogonal aq q  \n",
       "263                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     special class endomorphisms matrix analogously first case application induction hypothesis matrix yield desired matrix r u theorem implies following result real normal matrix corollary matrix rn n normal exists orthogonal matrix u rn n u au diag rm every j either r j rj αj βj j α j β j second case matrix r j considered complex matrix pair complex conjugate eigenvalue form α j iβ j proof exercise example matrix considered complex matrix eigenvalue therefore neither diagonalizable triangulated orthogonal matrix u transformed matrix u au real schur form orthogonal unitary endomorphisms section extend concept orthogonal unitary matrix endomorphisms  \n",
       "264                                                                                                                                      orthogonal unitary endomorphisms definition let v finite dimensional euclidean unitary vector space endomorphism f l v v called orthogonal unitary respectively f ad f idv f ad f idv f ad f bijective hence f injective cp exercise corollary implies f bijective hence f ad unique inverse f also f f ad idv cp remark following definition note orthogonal unitary endomorphism f normal therefore result previous section also apply f lemma let v finite dimensional euclidean unitary vector space let f l v v orthogonal unitary respectively b orthonormal basis v f b b orthogonal unitary matrix respectively proof let dim v every orthonormal basis b v idv b b f ad f b b f ad b b f b b f b b h f b b thus f b b orthogonal unitary respectively euclidean case f b b h f b b following theorem show orthogonal unitary endomorphism characterized fact change scalar product arbitrary vector lemma let v finite dimensional euclidean unitary vector space scalar product f l v v orthogonal unitary respectively f v f w v w v w proof f orthogonal unitary v w v v w idv v w f ad f v w f v f w hand suppose v w f v f w v w v w f v f w v w v f ad f w v idv f ad f w since scalar product non-degenerate v chosen arbitrarily idv f ad f w w v hence idv f ad f following corollary cp lemma corollary v finite dimensional euclidean unitary vector space scalar product f l v v orthogonal unitary respectively norm induced scalar product f v v v v  \n",
       "265                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             special class endomorphisms vector space v standard scalar product induced norm v v h v well unitary matrix cn n av v v thus av sup v cp example hold analogously orthogonal matrix rn n study eigenvalue eigenvectors orthogonal unitary endomorphisms lemma let v finite dimensional euclidean unitary vector space let f l v v orthogonal unitary respectively λ eigenvalue f proof let scalar product f v λv v v v idv v v f ad f v v f v f v λv λv v v v v implies statement lemma hold particular unitary orthogonal matrix however one keep mind orthogonal matrix orthogonal endomorphism may eigenvalue example orthogonal matrix characteristic polynomial pa real root considered element matrix eigenvalue theorem cn n unitary exists unitary matrix u cn n u h au diag λn j j rn n orthogonal exists orthogonal matrix u rn n u au diag rm every j either r j λ j λ j cj sj rj j c j j  \n",
       "266                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       orthogonal unitary endomorphisms proof unitary matrix cn n normal hence unitarily diagonalizable cp corollary lemma eigenvalue absolute value orthogonal matrix normal hence corollary exists orthogonal matrix u rn n u au diag rm either r j αj βj rj j α j β j first case r j λ j j lemma since u orthogonal also u au orthogonal hence every diagonal block r j orthogonal well r tj r j obtain r j desired form study two important class orthogonal matrix example let j n n j n let α define ri j α j sin α co α sin α co α j matrix ri j α ri j rn n equal identity matrix except entry rii co α ri j sin α r ji sin α r j j co α n matrix α co α sin α sin α co α  \n",
       "267                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        special class endomorphisms satisfies α α co α sin α sin α co α co α sin α sin α co α co α α α α α α one easily see matrix ri j α rn n orthogonal multiplication vector v matrix ri j α result counterclockwise rotation v angle α j plane numerical mathematics matrix ri j α called given illustrated figure vector v matrix represent rotation degree respectively example u define householder matrix h u ut u uu rn n u set h every u h u orthogonal matrix cp exercise multiplication vector v matrix h u describes reflection v hyperplane span u u hyperplane vector orthogonal u respect standard scalar product illustrated figure vector v householder matrix h u corresponds u wallace given pioneer numerical linear algebra  \n",
       "268                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       orthogonal unitary endomorphisms matlab-minute let u apply command norm u compute euclidean norm u form householder matrix u check orthogonality h via computation norm h form vector compare euclidean norm u selfadjoint endomorphisms already studied selfadjoint endomorphisms f finite dimensional euclidean unitary vector space defining property class endomorphisms f f ad cp definition obviously selfadjoint endomorphisms normal hence result sect hold strengthen result lemma finite dimensional euclidean unitary vector space v f l v v following statement equivalent f selfadjoint every orthonormal basis b v f b b f b b h exists orthonormal basis b v f b b f b b h euclidean case f b b h f b b proof corollary already shown implies obviously implies hold f b b f b b h f ad b b cp theorem hence f f ad hold following strong result diagonalizability selfadjoint endomorphisms euclidean unitary case theorem v finite dimensional euclidean unitary vector space f l v v selfadjoint exists orthonormal basis b v f b b real diagonal matrix  \n",
       "269                                                                                                                                                                                                                                                                                                                                                                                                                                                          special class endomorphisms proof consider first unitary case f selfadjoint f normal hence unitarily diagonalizable cp theorem let b orthonormal basis v f b b diagonal matrix f b b f ad b b f b b h implies diagonal entry f b b eigenvalue f real let v n-dimensional euclidean vector space b vn symmetric particular normal corolorthonormal basis v f b b lary exists orthogonal matrix u u j rn n u f b b u diag rm j either r j αj βj rj j α j β j since u f b b u symmetric block r j β j occur thus u real diagonal matrix u f b b define basis b wn v wn vn idv construction u idv b b hence u u b b therefore u f scalar product v vi v j δi j u f b b b b j u u get wi w j n u ki vk n n n n u u ki u vk u ki u k j δi j hence b orthonormal basis theorem following matrix version corollary rn n symmetric exist orthogonal matrix u rn n diagonal matrix rn n u du cn n hermitian exist unitary matrix u cn n diagonal matrix rn n u du h statement corollary known principal ax transformation briefly discus background name theory bilinear form application geometry symmetric matrix ai j rn n defines symmetric bilinear form via  \n",
       "270                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           selfadjoint endomorphisms β r x ax n n ai j xi j map q r x β x x x ax called quadratic form associated symmetric bilinear form since symmetric exists orthogonal matrix u u u n u au real diagonal matrix en β set u u n form orthonormal basis respect standard scalar product u u n en u hence u change base obtain β β u au cp theorem thus real diagonal matrix represents bilinear form β defined respect basis quadratic form q associated β also transformed simpler form change base since analogously q x x ax x u du x dy n λi q u yn thus quadratic form q turned sum square defined quadratic form q principal ax transformation given change base canonical basis basis given pairwise orthonormal eigenvectors n pairwise orthogonal subspace span u j j n form n principal ax geometric interpretation term illustrated following example example symmetric matrix u au  \n",
       "271                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      special class endomorphisms orthogonal matrix u u u c c number rounded fourth significant digit associated quadratic form q x define set e x q x described principal ax transformation consists transformation canonical coordinate system coordinate system given orthonormal basis eigenvectors carry transformation replace q quadratic form q get set ed r q set form ellipse centered origin two dimensional cartesian coordinate system spanned canonical basis vector ax length illustrated left part following figure element x e given x u e orthogonal matrix c c  \n",
       "272                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         selfadjoint endomorphisms given rotation rotates ellipse e counterclockwise angle c approximately degree hence e rotated version e right part figure show ellipse e cartesian coordinate system dashed line indicate respective span vector u u eigenvectors principal ax ellipse e let rn n symmetric given vector v scalar α r q x x ax v x α x quadratic function n variable entry vector x set zero function set x q x called hypersurface degree quadric example already seen quadric case n v next give example example let n v α corresponding quadric surface ball radius around origin v α corresponding quadric let n parabola  \n",
       "273                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        special class endomorphisms let n v α corresponding quadric parabolic cylinder corollary motivates following definition definition rn n symmetric cn n hermitian n positive n negative n zero eigenvalue counted corresponding multiplicity triple n n n called inertia let u first consider simplicity case real symmetric matrix lemma rn n symmetric inertia n n n diag congruent proof let rn n symmetric let u orthogonal matrix u rn n diag λn rn n inertia n n n assume without loss generality diag diagonal matrix contain positive negative eigenvalue respectively rn n diag rn n diag g l n r  \n",
       "274                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  selfadjoint endomorphisms diag μm diag μm thus u u u u result used proof sylvester law theorem inertia symmetric matrix rn n invariant congruence every matrix g g l n r matrix g ag inertia proof assertion trivial let inertia n n n n n equal zero assume without loss generality n n following argument applied n lemma exist g g l n r diag g g let g g l n r arbitrary set b g ag b n n therefore b g b g b symmetric inertia n n diag matrix g g l n r show n n also n bg g b g g g g b g g g g g g l n r implies rank rank b rank b hence n set g u u n vn wn w w g w w since n let span u u n span dim x α j u j g αn αn r zero implies x ax james joseph sylvester proved result quadratic form also coined name law inertia according expressing fact existence invariable number inseparably attached bilinear form  \n",
       "275                                                                                                                                                                                                                                                                                                                          special class endomorphisms hand x analogous argument show x ax hence dimension formula subspace cp theorem yield dim v dim dim dim dim n n repeat construction interchanging role thus n n n n thus n n proof complete n following result transfer lemma theorem complex hermitian matrix theorem let cn n hermitian inertia n n n exists matrix g g l n c g h diag moreover every matrix g g l n c matrix g h ag inertia proof exercise finally discus special class symmetric hermitian matrix definition real symmetric complex hermitian n n matrix called positive semidefinite v h av v resp v positive definite v h av v resp v reverse inequality hold corresponding matrix called negative semidefinite negative definite respectively selfadjoint endomorphisms define analogously v finite dimensional euclidean unitary vector space scalar product f l v v selfadjoint f called positive semidefinite positive definite f v v v v resp f v v v v following theorem characterizes symmetric positive definite matrix see exercise exercise transfer result positive semidefinite matrix resp positive definite endomorphisms theorem rn n symmetric following statement equivalent positive definite eigenvalue real positive exists lower triangular matrix l g l n r l l  \n",
       "276                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         selfadjoint endomorphisms proof symmetric matrix diagonalizable real eigenvalue cp corollary λ eigenvalue associated eigenvector v av λv λv v v av v v implies λ let u diag λn u diagonalization orthogonal matrix u rn n cp corollary λ j j let v arbitrary let w u w v u w v av u w u diag λn u u w w diag λn w n λ j w l l l g l n r every v v av v l l v l v since l invertible note need l lower triangular let u diag λn u diagonalization orthogonal matrix u rn n cp corollary since positive definite know λ j j set diag λn u u b b let b q r q rdecomposition invertible matrix b cp corollary q rn n orthogonal r rn n invertible upper triangular matrix b b q r q r l l l r one easily see analogous result hold complex hermitian matrix cn n case assertion lower triangular matrix l g l n c l l h factorization l l called cholesky special case lu theorem fact theorem show lu real symmetric positive definite matrix computed without row permutation order compute cholesky factorization symmetric positive definite matrix ai j rn n consider equation cholesky  \n",
       "277                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                special class endomorphisms l l lnn lnn first row obtain j l j j analogously row n obtain aii li j li j lii aii ai j n lik l jk lik l jk lik l jk lii l ji ai j lik l jk j lii l ji symmetric hermitian positive definite matrix closely related positive definite bilinear form euclidian unitary vector space theorem v finite dimensional euclidian unitary vector space β symmetric hermitian bilinear form v respectively following statement equivalent β positive definite β v v v v every basis b v matrix representation β symmetric hermitian positive definite exists basis b v matrix representation β symmetric hermitian positive definite proof exercise exercise let rn n normal show α every α r ak every k p every p r normal let b rn n normal b ab normal well let normal symmetric show αβ α  \n",
       "278                                                                                                                                                                                                                                                                                                              selfadjoint endomorphisms α r β r prove corollary using theorem show real skew-symmetric matrix matrix rn n complex skew-hermitian matrix matrix h cn n normal let v finite dimensional unitary vector space let f l v v normal show following assertion f f f selfadjoint b f f f f c f nilpotent f let v finite dimensional real complex vector space let f l v v diagonalizable show exists scalar product v f normal respect scalar product let cn n show following assertion normal exists normal matrix b n distinct eigenvalue commute b normal normal every c let h h hermitian h skew-hermitian part show h h h h h show furthermore normal h commute show cn n normal f z ad bc defined spectrum f bi c map f z called möbius transformation play important role function theory many area mathematics let v finite dimensional euclidian unitary vector space let f l v v orthogonal unitary respectively show f exists orthogonal unitary respectively let u let householder matrix h u defined show following assertion u matrix h u en orthogonally similar exists orthogonal matrix q rn n q h u q en implies h u eigenvalue algebraic multiplicity n respectively b every orthogonal matrix rn n written product n householder matrix exist u u n h u h u n august ferdinand möbius  \n",
       "279                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           special class endomorphisms let v satisfy v v show exists orthogonal matrix u rn n u v transfer proof lemma theorem complex hermitian matrix thus show theorem determine symmetric matrix orthogonal matrix u u au diagonal positive definite let k r c let vn basis k prove disprove matrix h k n n positive definite v hj av j j use definition test whether symmetric matrix positive definite determine case inertia let rn n g l r rm called schur complement matrix r show positive definite positive definite schur complement see also exercise show cn n hermitian positive definite x h ax defines scalar product prove following version theorem positive semidefinite matrix rn n symmetric following statement equivalent positive semidefinite eigenvalue real nonnegative exists upper triangular matrix l rn n l l let v finite dimensional euclidian unitary vector space let f l v v selfadjoint show f positive definite eigenvalue f real positive let rn n matrix x rn n x called square root cp sect issai schur  \n",
       "280                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    selfadjoint endomorphisms show symmetric positive definite matrix rn n symmetric positive definite square root b show matrix symmetric positive definite compute symmetric positive definite square root c show matrix jn n square root show matrix positive definite compute cholesky factorization using let b cn n hermitian let b furthermore positive definite show polynomial det b c exactly n real root prove theorem  \n",
       "281                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              chapter singular value decomposition matrix decomposition introduced chapter important many practical application since yield best possible approximation certain sense given matrix matrix low rank low rank approximation considered compression data represented given matrix illustrate example image processing first prove existence decomposition theorem let cn n given exist unitary matrix v cn n w cm h rn diag σr v r σr r rank proof set v cn w im finished let r rank since n r since h cm hermitian exists unitary matrix w wm cm w h h w diag λm rm cp corollary without loss generality assume λm every j h aw j λ j w j hence λ j w hj w j w hj h aw j j λ j j rank h rank r see modify proof lemma complex case therefore matrix h exactly r positive eigenvalue λr r time eigenvalue springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "282                                                                                                                                                                                                                                                                                                                                                                                                                         singular value decomposition define σ j λ j j r σr let g l r x xm aw vr xr z xr xm vr vrh vr vrh z ir h h h v z x x w aw r zh z h vr z h z implies particular z vrh vr ir extend vector xr xn respect xr orthonormal basis xr standard scalar product matrix xr xn cn n v vr unitary x aw x vr z vr finally obtain vr dw h v h proof show theorem formulated analogously real matrix rn n case two matrix v w orthogonal n apply theorem h resp real case definition decomposition form called singular value decomposition short matrix diagonal entry matrix called singular value column v resp w called left resp right singular vector obtain unitary diagonalization matrix h ah h h h v w v singular value therefore uniquely determined positive square root positive eigenvalue h h unitary matrix v w singular value decomposition however eigenvectors general uniquely determined development decomposition special case middle century current general form many important player history linear algebra played role historical note concerning singular value decomposition one find contribution jordan sylvester schmidt current form shown carl henry eckart gale young  \n",
       "283                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       singular value decomposition write svd form im wh w w h u p v h v u cn orthonormal column u h u im p p h cm positive semidefinite inertia r r factorization u p called polar decomposition viewed generalization polar representation complex number z eiϕ lemma suppose matrix cn rank r svd form v vn w wm considering element l im span vr ker span wr wm proof j r aw j v h w j v j σ j v j since σ j hence r linear independent vector satisfy vr im r rank dim im implies im span vr j r aw j hence linear independent vector satisfy wr wm ker dim ker dim im r implies ker span wr wm svd form written r σ j v j w hj thus written sum r matrix form σ j v j w hj rank σ j v j w hj let ak k σ j v j w hj k k rank ak k using matrix unitarily invariant cp exercise get ak σr hence approximated matrix ak rank approximating matrix approximation error matrix explicitly known singular value decomposition furthermore yield best possible approximation matrix rank k respect matrix theorem ak ak every matrix b cn rank b k  \n",
       "284                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    singular value decomposition proof assertion clear k rank since ak ak let k rank let b cn rank b k given dim ker b k consider b element l wm right singular vector u span dimension k since ker b u subspace dim ker b dim u ker b u let v ker b u given exist c v α j w j j hence b v av α j aw j αjσjvj therefore α j σ j v j max b b j σ j since pairwise orthonormal j since ak completes proof matlab-minute command n generates n n n matrix entry n row column diagonal sum equal entry therefore magic square compute svd using command v w said singular value rank form ak k rank verify numerically equation svd one important practical mathematical tool almost area science engineering social science medicine even psychology great importance due fact svd allows distinguish important non-important information given data practice latter  \n",
       "285                                                                                                                            singular value decomposition corresponds measurement error noise transmission data fine detail signal image play important role often important information corresponds large singular value non-important information small one many application one see furthermore singular value given matrix decay rapidly exist large many small singular value case matrix approximated well matrix low rank since already small k approximation error ak small low rank approximation ak requires little storage capacity computer k scalar vector stored make svd powerful tool application data compression interest example illustrate use svd image compression picture obtained research center matheon mathematics key greyscale picture shown left figure consists pixel pixel given value value stored real matrix full rank compute svd v using command v w matlab diagonal entry matrix singular value ordered decreasingly matlab theorem k compute matrix ak rank k using command k k k matrix represent approximation original picture based k largest singular value corresponding singular vector three approximation shown next original picture quality approximation decrease decreasing k even approximation k show essential feature matheon bear another important application svd arises solution linear system equation cn svd form define matrix w v h n rm n thank falk ebert help original bear seen front mathematics building tu berlin information matheon found  \n",
       "286                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       singular value decomposition one easily see ir w h rm r n invertible right hand side equation equal identity matrix case matrix therefore viewed generalized inverse case invertible matrix equal inverse definition matrix called moore-penrose pseudoinverse let cn b given linear system equation ax b x close possible solution try find x b using moore-penrose inverse obtain best possible approximation respect euclidean norm theorem let cn n b given v h x b satisfies svd x r h v b j x σj x proof let given let z ξm w h v v h b h b r n h h b σ j ξ j j j n h b j equality hold ξ j v hj b j j r satisfied z w h v h b last equation hold w v h b b eliakim hastings moore sir roger penrose  \n",
       "287                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            singular value decomposition vector x therefore attains lower bound equation r h vj x σj easily checked every vector attains lower bound must form h vh b b r yr ym σr yr ym c implies x minimization problem vector x written x min τm pairwise distinct τm r minimization problem corresponds problem linear regression least square approximation example solved q r-decomposition q r decomposition h h cp exercise r h q h q r r h q h r r h r h q h r q h thus solution least-squares approximation example identical solution minimization problem using svd exercise show frobenius norm matrix unitarily invariant f f cn unitary matrix p cn n q cm hint frobenius norm one use trace h use result exercise show f σr singular value cn show h h cn show cn  \n",
       "288                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                singular value decomposition let cn let moore-penrose inverse show following assertion rank h h b matrix x uniquely determined matrix satisfies following four matrix equation ax x ax x ax h ax x h x let b compute moore-penrose inverse vector x x x b x prove following theorem let cn b n h b h b b u matrix u n u h u b real u also chosen real hint one direction trivial direction consider unitary diagonalization h b h b yield matrix w svd b show assertion using two decomposition theorem application found article  \n",
       "289                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          chapter kronecker product linear matrix equation many application particular stability analysis differential equation lead linear matrix equation ax x b matrix b c given goal determine matrix x solves equation give formal definition description solution equation kronecker another product matrix useful chapter develop important property product study application context linear matrix equation many result topic found book definition k field ai j k b k n n b b b ai j b b amm b called kronecker product b kronecker product sometimes called tensor product matrix product defines map k k n n k mn mn definition extended non-square matrix simplicity consider case square matrix following lemma present basic computational rule kronecker product lemma square matrix b c k following computational rule hold b c b leopold kronecker said used product lecture berlin defined formally first time johann georg zehfuss springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "290                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                kronecker product linear matrix equation μa b μb μ b μ k b c c b c whenever b defined b c b c whenever b c defined b b therefore kronecker product two symmetric matrix symmetric proof exercise particular contrast standard matrix multiplication order factor kronecker product change transposition following result describes matrix multiplication two kronecker product lemma c k b k n n b c ac b hence particular b im b im b b b b invertible proof since b ai j b c ci j block fi j k n n block matrix fi j b c given fi j aik b ck j aik ck j b aik ck j b block matrix g j ac b g j k n n obtain g j gi j b gi j aik ck j show b c ac b easily follow equation general kronecker product non-commutative cp exercise following relationship b b lemma k b k n n exists permutation matrix p k mn mn p b p b proof exercise computation determinant trace rank kronecker product exist simple formula  \n",
       "291                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     kronecker product linear matrix equation theorem k b k n n following rule hold det b det n det b det b trace b trace trace b trace b rank b rank rank b rank b proof lemma multiplication theorem determinant cp theorem get det b det im b det det im b lemma exists permutation matrix p p p implies det det p p det det n since det im b det b follows det b det n det b therefore also det b det b b ai j b obtain trace b n aii b j j aii n b j j trace trace b trace b trace trace b exercise matrix k n column j k j n define vec k application vec turn matrix column vector thus vectorizes lemma map vec k n k isomorphism particular ak k n linearly independent vec vec ak k linearly independent proof exercise consider relationship kronecker product vec map  \n",
       "292                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             kronecker product linear matrix equation theorem k b k n n c k n vec ac b b vec c hence particular vec ac vec c vec c b b im vec c vec ac c b b im vec c proof j n jth column ac b given ac b e j ac j n bk j ac ek n bk j cek j j bn j vec c implies vec ac b b vec c b resp im obtain linearity vec yield order study relationship eigenvalue matrix b kronecker product use bivariate polynomial polynomial two variable cp exercise p l j αi j k polynomial k b k n n define matrix p b l αi j ai b j careful order factor since general ai b j b j ai cp exercise example rm b rn n p r get matrix p b b following result known stephanos named cyparissos stephanos showed besides result also assertion lemma  \n",
       "293                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        kronecker product linear matrix equation theorem let k b k n n two matrix jordan normal form eigenvalue λm k μn k respectively p b defined following assertion hold eigenvalue p b p λk k eigenvalue b λk k eigenvalue λk k proof let g l k g l n k j bt j b jordan canonical form matrix j j b upper triangular thus j j j matrix j ai j b j ai j b upper triangular eigenvalue j j j j j b λm μn respectively thus p λk k n diagonal entry matrix p j j b using lemma obtain p b l αi j j j b j l l αi j j ai j b j αi j j ai j b j l αi j j ai j b j l αi j j ai j b j p j j b implies assertion follow p p respectively following result matrix exponential function kronecker product helpful application involve system linear differential equation lemma cm b cn n c im b exp c exp exp b proof lemma know matrix im b commute using lemma obtain  \n",
       "294                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          kronecker product linear matrix equation exp c exp im b exp exp im b j im b j j im b j j b j exp exp b used property matrix exponential series cp sect given matrix j k b j k n n j q c k n equation form x x aq x bq c called linear matrix equation unknown matrix x k n theorem matrix x k n solves x vec x solves linear system equation k gx vec c g q b tj j proof exercise consider two special case theorem cm b cn n c cm n sylvester ax x b c unique solution common eigenvalue eigenvalue b negative real part unique solution given x exp c exp b dt sect integral defined entrywise james joseph sylvester  \n",
       "295                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  kronecker product linear matrix equation proof analogous representation theorem write sylvester equation b im x vec c b eigenvalue λm μn respectively g b im theorem eigenvalue λk k thus g invertible sylvester equation uniquely solvable λk k let b matrix eigenvalue negative real part common eigenvalue unique solution let j j b bt jordan canonical form b consider linear differential equation dz az z b z c dt solved function z cm n z exp c exp b cp exercise function satisfies lim z lim exp c exp b lim exp j exp j b constant integration equation yield z lim z z z dt z dt b use without proof existence infinite integral implies x z dt exp c exp b dt unique solution theorem also give solution another important matrix equation corollary c cn n lyapunov ax x h alexandr mikhailovich lyapunov also ljapunov liapunov  \n",
       "296                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           kronecker product linear matrix equation unique solution x cn n eigenvalue negative real part furthermore c hermitian positive definite also x hermitian positive definite proof since assumption h common eigenvalue unique solvability follows theorem solution given matrix x exp exp h dt exp c exp h dt c hermitian positive definite x hermitian x h x xx x exp c exp dt x x h exp c exp h x dt last inequality follows monotonicity integral fact x also exp h x since exp h invertible every real exercise prove lemma construct two square matrix b b b prove lemma prove theorem prove lemma show b normal cm b cn n normal true b unitary b unitary use singular value decomposition v w ah cm b vb b w bh cn n derive singular value decomposition b show cm b cn n matrix equation b b hold prove theorem let cm b cn n c cm n show z exp c exp b solution matrix differential equation ddtz az z b initial condition z c  \n",
       "297                                                                                                                                                                                                                                                                                                                                                                                                                                                                            appendix short introduction matlab interactive software system numerical computation simulation visualization contains large number predefined function allows user implement program so-called m-files name matlab originates matrix laboratory indicates matrix orientation software indeed matrix major object matlab due simple intuitive use matrix consider matlab well suited teaching field linear algebra short introduction explain important way enter operate matrix matlab one learn essential matrix operation well important algorithm concept context matrix linear algebra general actively using matlab-minutes book use predefined function matrix matlab entered form list entry enclosed square bracket entry list ordered row natural order index top bottom left right new row start every semicolon example matrix entered matlab typing semicolon matrix suppresses output matlab omitted matlab writes entered computed quantity example entering registered trademark mathworks springer international publishing switzerland liesen mehrmann linear algebra springer undergraduate mathematics series doi  \n",
       "298                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             appendix short introduction matlab matlab give output one access part matrix corresponding index list index k abbreviated k colon mean row given column index column given row index example matrix matrix matrix several predefined function produce matrix particular given positive integer n eye n identity matrix zero n one n n matrix zero n matrix one rand n n random matrix several matrix appropriate size combined new matrix example command b c lead e help function matlab started command help order get information specific function one add name function example  \n",
       "299                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            appendix short introduction matlab input help ops information operation operator matlab particular addition multiplication transposition help matfun matlab function operate matrix help gallery collection example matrix help det determinant help expm matrix exponential function  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data['by_page_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(raw_indexes_list, updated_by_line_index) = get_raw_indexes_list(\n",
    "  df_cann_lines_index=split_data['by_line_index']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_indexes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_raw_indexes_list(\n",
    "  processed_data_dir_path=PROCESSED_DATA_DIR_PATH,\n",
    "  pdf_filepath=file_path,\n",
    "  raw_indexes_list=raw_indexes_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_indexes = load_raw_indexes_list(\n",
    "  processed_data_dir_path=PROCESSED_DATA_DIR_PATH,\n",
    "  pdf_filepath=file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_is_in_index = add_is_in_index(\n",
    "  candidates_df=candidates_df,\n",
    "  indexes_list=clean_indexes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_is_in_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_by_candidate(\n",
    "  candidates_df=with_is_in_index  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates_df[candidates_df['candidate_keyword'] == 'socio-cultural']\n",
    "candidates_df[candidates_df['candidate_keyword'] == 'socio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_frequencies = add_frequencies_column(\n",
    "  by_pages_body_df=split_data['by_page_body'],\n",
    "  candidates_df=candidates_df,\n",
    "  freq_ngrams=freq_ngrams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_frequencies[500:510]\n",
    "# len(freq_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_is_in_toc = add_is_in_toc(\n",
    "  candidates_df=with_frequencies, \n",
    "  by_line_toc=split_data['by_line_toc']\n",
    ")\n",
    "\n",
    "with_is_in_toc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_importance = add_importance(candidates_df=with_is_in_toc)\n",
    "\n",
    "with_importance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_position_in_context = add_position_in_context(candidates_df=with_importance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_position_in_context[500:505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_is_named_entity = add_is_named_entity(\n",
    "  candidates_df=candidates_df,\n",
    "  df_pages_body=split_data['by_page_body']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_is_named_entity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_length_or_word = add_length_of_word(candidates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_length_or_word[500:503]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_is_named_author = add_is_named_author(\n",
    "  candidates_df=with_length_or_word,\n",
    "  df_cann_pages_biblio=split_data['by_page_biblio']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_is_named_author.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_tfidf = add_tfidf(\n",
    "  candidates_df=with_is_named_author,\n",
    "  df_cann_pages_body=split_data['by_page_body']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(raw_indexes_list, updated_by_line_index) = get_raw_indexes_list(\n",
    "  df_cann_lines_index=split_data['by_line_index']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_indexes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_raw_indexes_list(\n",
    "  processed_data_dir_path=PROCESSED_DATA_DIR_PATH,\n",
    "  pdf_filepath=file_path,\n",
    "  raw_indexes_list=raw_indexes_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_indexes = load_raw_indexes_list(\n",
    "  processed_data_dir_path=PROCESSED_DATA_DIR_PATH,\n",
    "  pdf_filepath=file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_is_in_index = add_is_in_index(\n",
    "  candidates_df=candidates_df,\n",
    "  indexes_list=clean_indexes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_is_in_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_by_candidate(\n",
    "  candidates_df=with_is_in_index  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01cb0e940888771f45517ede6c557d5279c3bc08138d07b2a04ba54b7480cc42"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
