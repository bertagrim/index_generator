{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2ecfe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66e110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "# from pdfminer.converter import TextConverter\n",
    "# from pdfminer.layout import LAParams\n",
    "# from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "# from sentence_splitter import SentenceSplitter\n",
    "# splitter = SentenceSplitter(language=\"en\")\n",
    "\n",
    "from typing import Dict\n",
    "import sys\n",
    "\n",
    "from pdf_reader import process_pages, get_number_translator, get_sections_level_adder\n",
    "from data_loader import get_pdf_filepaths, load_data_frames\n",
    "from data_cleaner import clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4bcc556",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a859c",
   "metadata": {},
   "source": [
    "## 1. Get bookmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05237c58",
   "metadata": {},
   "source": [
    "### Get real pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d90da63",
   "metadata": {},
   "source": [
    "## 2. Create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87ca6bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/aadroher/DevProjects/bootcamp_berta/final_project/data/pdf/zimmermann_13.pdf',\n",
       " '/Users/aadroher/DevProjects/bootcamp_berta/final_project/data/pdf/winter_16.pdf',\n",
       " '/Users/aadroher/DevProjects/bootcamp_berta/final_project/data/pdf/williamson_94.pdf',\n",
       " '/Users/aadroher/DevProjects/bootcamp_berta/final_project/data/pdf/cann_93.pdf']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = get_pdf_filepaths(\"../data/pdf/\")\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fbf732e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>page_number</th>\n",
       "      <th>real_page_num</th>\n",
       "      <th>section_level_1</th>\n",
       "      <th>section_level_2</th>\n",
       "      <th>section_level_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yoad Winter\\n\\nElements of\\nFormal Semantics\\nAn Introduction to the Mathematical\\nTheory of Meaning in Natural Language\\n\\nE d i n b u r g h A d vA n c E d T E x T b o o k s i n L i n g u i s t i c s</td>\n",
       "      <td>0</td>\n",
       "      <td>Cover</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elements of Formal Semantics</td>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Edinburgh Advanced Textbooks in Linguistics\\nSeries Editors\\nPeter Ackema, Reader in Linguistics (University of Edinburgh)\\nMitsuhiko Ota, Reader in the Phonetics and Phonology of Language\\nAcquisition (University of Edinburgh)\\nEditorial Advisory Board\\nRicardo Bermudez-Otero (University of Manchester)\\nKersti Börjars (University of Manchester)\\nGreville Corbett (University of Surrey)\\nAnastasia Giannakidou (University of Chicago)\\nCaroline Heycock (University of Edinburgh)\\nJack Hoeksema (University of Groningen)\\nMiriam Meyerhoff (University of Edinburgh)\\nGeoffrey Pullum (University of Edinburgh)\\nAndrew Spencer (University of Essex)\\nDonca Steriade (MIT)\\nSusi Wurmbrand (University of Connecticut)\\n\\nTITLES IN THE SERIES INCLUDE\\nEssential Programming for Linguistics\\nMartin Weisser\\nMorphology: From Data to Theories\\nAntonio Fábregas and Sergio Scalise\\nLanguage and Logics: An Introduction to the Logical Foundations of Language\\nHoward Gregory\\nElements of Formal Semantics: An Introduction to the Mathematical\\nTheory of Meaning in Natural Language\\nYoad Winter\\nVisit the Edinburgh Advanced Textbooks in Linguistics website at www.euppublishing.com/series/EATL</td>\n",
       "      <td>2</td>\n",
       "      <td>II</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elements of Formal Semantics\\nAn Introduction to the Mathematical Theory of Meaning in Natural Language\\nYoad Winter</td>\n",
       "      <td>3</td>\n",
       "      <td>III</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In memory of Paula Frank-Boas\\n1916 Amsterdam–2015 Haifa\\n\\nEdinburgh University Press is one of the leading university presses in the UK.\\nWe publish academic books and journals in our selected subject areas across the humanities and social sciences, combining cutting-edge scholarship with high editorial and production values to produce academic works of lasting importance.\\nFor more information visit our website: www.edinburghuniversitypress.com c\\n\u0002Yoad\\nWinter, 2016\\nEdinburgh University Press Ltd\\nThe Tun - Holyrood Road, 12(2f) Jackson’s Entry, Edinburgh EH8 8PJ\\nTypeset in 12/14 Minion by by Nova Techset Pvt Ltd, Bangalore, India, and printed and bound in Great Britain by\\nCPI Group (UK) Ltd, Croydon CR0 4YY\\nA CIP record for this book is available from the British Library\\nISBN 978 0 7486 4044 7 (hardback)\\nISBN 978 0 7486 7777 1 (webready PDF)\\nISBN 978 0 7486 4043 0 (paperback)\\nISBN 978 0 7486 7779 5 (epub)\\nThe right of Yoad Winter to be identified as the author of this work has been asserted in accordance with the Copyright, Designs and Patents Act 1988, and the Copyright and\\nRelated Rights Regulations 2003 (SI No. 2498).</td>\n",
       "      <td>4</td>\n",
       "      <td>IV</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          content  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Yoad Winter\\n\\nElements of\\nFormal Semantics\\nAn Introduction to the Mathematical\\nTheory of Meaning in Natural Language\\n\\nE d i n b u r g h A d vA n c E d T E x T b o o k s i n L i n g u i s t i c s   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Elements of Formal Semantics   \n",
       "2  Edinburgh Advanced Textbooks in Linguistics\\nSeries Editors\\nPeter Ackema, Reader in Linguistics (University of Edinburgh)\\nMitsuhiko Ota, Reader in the Phonetics and Phonology of Language\\nAcquisition (University of Edinburgh)\\nEditorial Advisory Board\\nRicardo Bermudez-Otero (University of Manchester)\\nKersti Börjars (University of Manchester)\\nGreville Corbett (University of Surrey)\\nAnastasia Giannakidou (University of Chicago)\\nCaroline Heycock (University of Edinburgh)\\nJack Hoeksema (University of Groningen)\\nMiriam Meyerhoff (University of Edinburgh)\\nGeoffrey Pullum (University of Edinburgh)\\nAndrew Spencer (University of Essex)\\nDonca Steriade (MIT)\\nSusi Wurmbrand (University of Connecticut)\\n\\nTITLES IN THE SERIES INCLUDE\\nEssential Programming for Linguistics\\nMartin Weisser\\nMorphology: From Data to Theories\\nAntonio Fábregas and Sergio Scalise\\nLanguage and Logics: An Introduction to the Logical Foundations of Language\\nHoward Gregory\\nElements of Formal Semantics: An Introduction to the Mathematical\\nTheory of Meaning in Natural Language\\nYoad Winter\\nVisit the Edinburgh Advanced Textbooks in Linguistics website at www.euppublishing.com/series/EATL   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Elements of Formal Semantics\\nAn Introduction to the Mathematical Theory of Meaning in Natural Language\\nYoad Winter   \n",
       "4                               In memory of Paula Frank-Boas\\n1916 Amsterdam–2015 Haifa\\n\\nEdinburgh University Press is one of the leading university presses in the UK.\\nWe publish academic books and journals in our selected subject areas across the humanities and social sciences, combining cutting-edge scholarship with high editorial and production values to produce academic works of lasting importance.\\nFor more information visit our website: www.edinburghuniversitypress.com c\\n\u0002Yoad\\nWinter, 2016\\nEdinburgh University Press Ltd\\nThe Tun - Holyrood Road, 12(2f) Jackson’s Entry, Edinburgh EH8 8PJ\\nTypeset in 12/14 Minion by by Nova Techset Pvt Ltd, Bangalore, India, and printed and bound in Great Britain by\\nCPI Group (UK) Ltd, Croydon CR0 4YY\\nA CIP record for this book is available from the British Library\\nISBN 978 0 7486 4044 7 (hardback)\\nISBN 978 0 7486 7777 1 (webready PDF)\\nISBN 978 0 7486 4043 0 (paperback)\\nISBN 978 0 7486 7779 5 (epub)\\nThe right of Yoad Winter to be identified as the author of this work has been asserted in accordance with the Copyright, Designs and Patents Act 1988, and the Copyright and\\nRelated Rights Regulations 2003 (SI No. 2498).   \n",
       "\n",
       "   page_number real_page_num section_level_1 section_level_2 section_level_3  \n",
       "0            0         Cover                            None            None  \n",
       "1            1             I                            None            None  \n",
       "2            2            II                            None            None  \n",
       "3            3           III                            None            None  \n",
       "4            4            IV                            None            None  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = file_paths[1]\n",
    "\n",
    "\n",
    "data_frames = load_data_frames(file_path)\n",
    "data_frames['by_page'].head()\n",
    "# clean_data_frames = clean_data(data_frames)\n",
    "\n",
    "# clean_data_frames['by_line'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb071c5",
   "metadata": {},
   "source": [
    "### Add columns with real page numbers and sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e04220",
   "metadata": {},
   "source": [
    "#### Page by page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f79b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cann_pages['real_page_num']=df_cann_pages['page_number'].apply(get_number_translator(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cann_pages['section_level_1']=df_cann_pages['page_number'].apply(get_sections_level_adder(file_path, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cann_pages['section_level_2']=df_cann_pages['page_number'].apply(get_sections_level_adder(file_path, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cann_pages['section_level_3']=df_cann_pages['page_number'].apply(get_sections_level_adder(file_path, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b3341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_cann_pages[df_cann_pages.section_level_2 != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def list_lines(df_by_pages):\n",
    "#     list_pages=df_by_pages.values.tolist()\n",
    "#     list_lines=[]\n",
    "#     for item in list_pages:\n",
    "#         if type(item[0])==str:\n",
    "#             split_file = splitter.split(text=item[0])\n",
    "#             for line in split_file:\n",
    "#                 list_lines.append([line, item[1], item[2], item[3], item[4], item[5]])\n",
    "#         else:\n",
    "#             list_lines.append([item[0], item[1], item[2], item[3], item[4], item[5]])\n",
    "#     return list_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c901ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list_lines(df_cann_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47619eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cann_lines = pd.DataFrame(list_lines(df_cann_pages), columns = df_cann_pages.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cann_lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd16a18",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a467c3",
   "metadata": {},
   "source": [
    "### Drop lines/pages that include the phrase \"This page was intentionally left blank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ed2ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_cann_lines[df_cann_lines.content.str.match(\"This page intentionally left blank\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cann_lines[df_cann_lines.content.str.match(\"This page was intentionally left blank\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cann_lines=df_cann_lines[~df_cann_lines.content.str.match(\"This page intentionally left blank\", na=False)]\n",
    "# df_cann_lines=df_cann_lines[~df_cann_lines.content.str.match(\"This page was intentionally left blank\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8649f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cann_pages=df_cann_pages[~df_cann_pages.content.str.match(\"This page intentionally left blank\", na=False)]\n",
    "# df_cann_pages=df_cann_pages[~df_cann_pages.content.str.match(\"This page was intentionally left blank\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cann_pages.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e3511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_cann_lines.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09800a54",
   "metadata": {},
   "source": [
    "### Drop null lines/pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b413680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cann_pages.content.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf4e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines.content.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd52e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines.content.dropna(inplace=True)\n",
    "df_cann_pages.content.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a23d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cann_lines.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd7be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cann_pages.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e62f42",
   "metadata": {},
   "source": [
    "### Drop lines/pages that contain only numbers (they are page numbers, numbers of sections, etc), only punctuation or are empty (with whitespace character or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef36af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finds lines that contain only numbers \n",
    "#PROBLEM: needed for index???\n",
    "df_cann_lines[df_cann_lines.content.str.match(\"^[^a-zA-Z]*\\d+[^a-zA-Z]*$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03765c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#finds lines that are empty (they don't have a whitespace character)\n",
    "df_cann_lines[df_cann_lines.content.str.match(\"^$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd091e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this finds the lines that are just punctuation\n",
    "df_cann_lines[df_cann_lines.content.str.match(\"^[^\\w]+$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0053257",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines=df_cann_lines[~df_cann_lines.content.str.match(\"^[^a-zA-Z]*\\d+[^a-zA-Z]*$\")]\n",
    "df_cann_pages=df_cann_pages[~df_cann_pages.content.str.match(\"^[^a-zA-Z]*\\d+[^a-zA-Z]*$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines=df_cann_lines[~df_cann_lines.content.str.match(\"^[^\\w]+$\")]\n",
    "df_cann_pages=df_cann_pages[~df_cann_pages.content.str.match(\"^[^\\w]+$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines=df_cann_lines[~df_cann_lines.content.str.match(\"^$\")]\n",
    "df_cann_pages=df_cann_pages[~df_cann_pages.content.str.match(\"^$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d437667",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines.reset_index(drop=True, inplace=True)\n",
    "df_cann_pages.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c3f9c",
   "metadata": {},
   "source": [
    "### Split dataset into 3: ToC, Body of text, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d594e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines.to_csv(\"cann_info_lines.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages.to_csv(\"cann_info_pages.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b5bfa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'cann_info_pages.csv'\n",
    "df_cann_pages = pd.read_csv(path,index_col=0)\n",
    "df_cann_pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'cann_info_lines.csv'\n",
    "df_cann_lines = pd.read_csv(path,index_col=0)\n",
    "df_cann_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf5cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages.section_level_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cca9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages.section_level_1.fillna(\"No info\", inplace=True)\n",
    "df_cann_lines.section_level_1.fillna(\"No info\", inplace=True)\n",
    "df_cann_pages.section_level_2.fillna(\"No info\", inplace=True)\n",
    "df_cann_lines.section_level_2.fillna(\"No info\", inplace=True)\n",
    "df_cann_pages.section_level_3.fillna(\"No info\", inplace=True)\n",
    "df_cann_lines.section_level_3.fillna(\"No info\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d201506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages.section_level_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bca627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FALTA: remove references (maybe don't do it for compendiums)\n",
    "\n",
    "#toc\n",
    "df_cann_pages_toc=df_cann_pages[df_cann_pages.section_level_1.str.match(\"CONTENTS|Contents\")]#generalise more\n",
    "df_cann_lines_toc=df_cann_lines[df_cann_lines.section_level_1.str.match(\"CONTENTS|Contents\")]\n",
    "\n",
    "#index\n",
    "df_cann_pages_index=df_cann_pages[df_cann_pages.section_level_1.str.match(\"INDEX|Index\")]#generalise more\n",
    "df_cann_lines_index=df_cann_lines[df_cann_lines.section_level_1.str.match(\"INDEX|Index\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#body: probably needs to be done manually\n",
    "df_cann_pages_body=df_cann_pages[(df_cann_pages.page_number>=19) & (df_cann_pages.page_number<=340)]\n",
    "df_cann_lines_body=df_cann_lines[(df_cann_lines.page_number>=19) & (df_cann_lines.page_number<=340)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe13156",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages_toc.reset_index(drop=True, inplace=True)\n",
    "df_cann_lines_toc.reset_index(drop=True, inplace=True)\n",
    "df_cann_pages_body.reset_index(drop=True, inplace=True)\n",
    "df_cann_lines_body.reset_index(drop=True, inplace=True)\n",
    "df_cann_pages_index.reset_index(drop=True, inplace=True)\n",
    "df_cann_lines_body.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe7fa94",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a07bd",
   "metadata": {},
   "source": [
    "### STEPS: \n",
    "1. Remove stopwords\n",
    "2. Remove all numbers\n",
    "3. Stem\n",
    "4. Look for phrases (use gensim phraser)\n",
    "5. Do wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "porter = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from collections import defaultdict\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim import corpora\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7329e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep updating\n",
    "textbook_words=['thus', 'today', 'nowadays', 'actually', 'section', 'indeed', 'every', 'any', 'some', 'example', 'therefore', 'definition', 'introduction', 'conclusion', 'chapter', 'appendix', 'otherwise', 'thing', 'rather', 'instead', 'like', 'since', 'given', 'case', 'hence', 'iff', 'see', 'beyond', 'below', 'above', 'postscript', 'index', 'ensure', 'generally', 'anything', 'something', 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_data):\n",
    "    tokens=word_tokenize(text_data)\n",
    "    \n",
    "    lowercased=[w.lower() for w in tokens]\n",
    "    \n",
    "    no_punct=[word for word in lowercased if word.isalpha()]\n",
    "    \n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    no_sw=[w for w in no_punct if not w in stop_words]\n",
    "    \n",
    "    no_tw=[w for w in no_sw if not w in textbook_words]\n",
    "    \n",
    "    long_words=[w for w in no_tw if len(w)>2]\n",
    "    \n",
    "    clean_tokens=[lemmatizer.lemmatize(word) for word in long_words]\n",
    "    \n",
    "    return (\" \").join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68420403",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_body[\"clean_content\"] = df_cann_lines_body[\"content\"].apply(lambda x: clean_text(x))\n",
    "df_cann_lines_toc[\"clean_content\"] = df_cann_lines_toc[\"content\"].apply(lambda x: clean_text(x))\n",
    "#df_cann_lines_index[\"clean_content\"] = df_cann_lines_index[\"content\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages_body[\"clean_content\"] = df_cann_pages_body[\"content\"].apply(lambda x: clean_text(x))\n",
    "df_cann_pages_toc[\"clean_content\"] = df_cann_pages_toc[\"content\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42684fa1",
   "metadata": {},
   "source": [
    "### FIltering by POS to get only NPs as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e32c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def keep_NPs(sent):\n",
    "    list_tokens=[]\n",
    "    sent = nlp(sent)\n",
    "    for word in sent:\n",
    "        if word.tag_== \"NN\" or word.tag_==\"NNP\" or word.tag_==\"NNPS\" or word.tag_==\"NNS\" or word.tag_==\"JJ\" or word.tag_==\"JJR\" or word.tag_==\"JJS\":\n",
    "            list_tokens.append(word) \n",
    "    clean_sent=(\" \").join([i.text for i in list_tokens])\n",
    "    return clean_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dabacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_body[\"clean_content\"] = df_cann_lines_body[\"clean_content\"].apply(lambda x: keep_NPs(x))\n",
    "#df_cann_lines_toc[\"clean_content\"] = df_cann_lines_toc[\"content\"].apply(lambda x: keep_NPs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages_body[\"clean_content\"] = df_cann_pages_body[\"clean_content\"].apply(lambda x: keep_NPs(x))\n",
    "#df_cann_pages_toc[\"clean_content\"] = df_cann_pages_toc[\"content\"].apply(lambda x: keep_NPs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_body.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_body.to_csv(\"cann_lines_body_clean.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages_body.to_csv(\"cann_pages_body_clean.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'cann_lines_body_clean.csv'\n",
    "df_cann_lines_body = pd.read_csv(path,index_col=0)\n",
    "df_cann_lines_body.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbfc95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'cann_pages_body_clean.csv'\n",
    "df_cann_pages_body = pd.read_csv(path,index_col=0)\n",
    "df_cann_pages_body.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bc0aa",
   "metadata": {},
   "source": [
    "### Using gensim to extract n-grams and most common terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da414af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_body[\"clean_content\"] = df_cann_lines_body[\"clean_content\"].fillna('')\n",
    "df_cann_pages_body[\"clean_content\"] = df_cann_pages_body[\"clean_content\"].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unclear to me what to use as documents -- lines or pages:\n",
    "#sent = [page.split() for page in df_cann_pages_body[\"clean_content\"]]\n",
    "sent = [sentence.split() for sentence in df_cann_lines_body[\"clean_content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a6dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram models\n",
    "bigram = gensim.models.phrases.Phrases(sent, min_count=3, threshold=10)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f1adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list=list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31984672",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = corpora.Dictionary([sent for sent in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b990ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1336b2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same but easier, probably use this. Falta: add it to df (maybe normalise it to 0-1)\n",
    "mydict.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_frequency={}\n",
    "for item in mydict.most_common():\n",
    "    map_frequency[item[0]]=item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87aa4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_frequency(x):\n",
    "    return map_frequency[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next: \n",
    "#(1) get score if they are in sections or subsections, DONE\n",
    "#(2) get score for their position in the sentence, LEAVE IT OUT\n",
    "#(3) get importance of context score\n",
    "\n",
    "#Finally,\n",
    "#(4) do NER and add all names of people, institutions... what else?\n",
    "\n",
    "#If I finish, calculate the length of index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9838f33",
   "metadata": {},
   "source": [
    "### Create a dataframe of all the candidate keywords, with a column consisting of its context and a column consisting of each relevant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff13577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_sent = [sentence for sentence in df_cann_lines_body[\"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(raw_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdffecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidates_list(sentences):\n",
    "    candidates=[]\n",
    "    for item in sentences:\n",
    "        for w in item:\n",
    "            candidates.append([w, (' ').join(item)])\n",
    "    return candidates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_list=create_candidates_list(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c20976",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df=pd.DataFrame(candidates_list, columns=['candidate_keyword', 'context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855513a0",
   "metadata": {},
   "source": [
    "#### Add frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdadb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df['frequency']=candidates_df.candidate_keyword.apply(assign_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61985eb9",
   "metadata": {},
   "source": [
    "#### Add appearance in title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a638eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#falta netejar toc\n",
    "def clean_toc(text_data):\n",
    "    tokens=word_tokenize(text_data)\n",
    "    \n",
    "    lowercased=[w.lower() for w in tokens]\n",
    "    \n",
    "    no_punct=[word for word in lowercased if word.isalpha()]\n",
    "    \n",
    "    clean_tokens=[lemmatizer.lemmatize(word) for word in no_punct]\n",
    "    \n",
    "    return (\" \").join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5977ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cann_lines_toc['content'].apply(clean_toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_toc=[]\n",
    "for line in df_cann_lines_toc.content.apply(clean_toc):\n",
    "    words_toc+=line.split()\n",
    "#print(words_toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfcb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_toc(x):\n",
    "    if \"_\" in x:\n",
    "        if x.split(\"_\")[0] in words_toc or x.split(\"_\")[1] in words_toc:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if x in words_toc:\n",
    "            return 1\n",
    "        else: \n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df['is_in_toc']=candidates_df.candidate_keyword.apply(is_in_toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69872d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a02862",
   "metadata": {},
   "source": [
    "### Adding the importance of the context for the whole document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a811e49",
   "metadata": {},
   "source": [
    "This seems to work! Using sentence embeddings and book embedding and compute cosine similarity between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e8548",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a41d0d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "doc=(' ').join(candidates_df.context.unique())\n",
    "candidates=candidates_df.context.unique()\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e458ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 30\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f976866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3b63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9b390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1c1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a30535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0e12ae9",
   "metadata": {},
   "source": [
    "### USELESS STUFF FOR NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa72cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do it with tf-idf?: get a tf-idf vector per sentence and do cosine similarity with the tf-idf vectore of the whole book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48a810",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfdif_vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "#Unclear to me what to use as documents -- sentences or pages\n",
    "tfidf = tfdif_vectorizer.fit_transform(candidates_df[\"context\"])\n",
    "#tfidf = tfdif_vectorizer.fit_transform(df_cann_pages_body[\"clean_content\"])\n",
    "\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a723b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf[2].toarray()\n",
    "#print(tfdif_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdif_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80fa314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 embed sentences using sent2vec\n",
    "#2 embed whole document using doc2vec\n",
    "# rank sentence embeddings based on similarity to whole document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install sent2vec\n",
    "#from sent2vec.vectorizer import Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d11153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#candidates_df.context.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = candidates_df.context.unique()\n",
    "\n",
    "#vectorizer = Vectorizer()\n",
    "#vectorizer.bert(sentences)\n",
    "#vectors = vectorizer.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff2110",
   "metadata": {},
   "source": [
    "### Let's try it with word2vec (not pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5253e8df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from time import time \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d469652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8457ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdb3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"function\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343dab6d",
   "metadata": {},
   "source": [
    "### Let's try it with pretrained model from spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_sm')#maybe try with bigger models later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518985e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in candidates_df.context.unique():\n",
    "    doc = model(line)\n",
    "    print(line, doc.vector, len(doc.vector))\n",
    "#    for token in doc:\n",
    "#        print(token.vector)\n",
    "#        print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba0cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in candidates_df.context.unique():\n",
    "    \n",
    "    doc = model(line)\n",
    "    print(line, doc.vector, len(doc.vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad473a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[89])\n",
    "print(doc[3])\n",
    "doc[1].similarity(doc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94282c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = corpora.Dictionary([sent for sent in sentences])\n",
    "corpus = [mydict.doc2bow(sent) for sent in sentences]\n",
    "\n",
    "# Show the Word Weights in Corpus\n",
    "#for doc in corpus:\n",
    "#    print([[mydict[id], freq] for id, freq in doc])\n",
    "\n",
    "# Create the TF-IDF model\n",
    "tfidf = models.TfidfModel(corpus, smartirs='ntc')\n",
    "\n",
    "# Show the TF-IDF weights\n",
    "for doc in tfidf[corpus]:\n",
    "    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474bf40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this gives you the words that get highest tfidf score in any sentence\n",
    "#problem: gives you weird examples that only appear in one place\n",
    "topWords = {}\n",
    "for doc in corpus_tfidf:\n",
    "    for iWord, tf_idf in doc:\n",
    "        if iWord not in topWords:\n",
    "            topWords[iWord] = 0\n",
    "\n",
    "        if tf_idf > topWords[iWord]:\n",
    "            topWords[iWord] = tf_idf\n",
    "\n",
    "for i, item in enumerate(sorted(topWords.items(), key=lambda x: x[1], reverse=True), 1):\n",
    "    print(\"%2s: %-13s %s\" % (i, mydict[item[0]], item[1]))\n",
    "    if i == 1000: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk import ngrams\n",
    "\n",
    "#sent = [line for line in df_cann_lines[\"clean_content\"]]\n",
    "\n",
    "#ngram_counts = Counter(ngrams(sent, 2))\n",
    "#ngram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def most_frequent_words(docs, vocabulary, top_words):\n",
    "#    vocab = Counter()\n",
    "#    \n",
    "#    for doc in docs:\n",
    "#        for word in doc.split(' '):\n",
    "#            if word in vocabulary.keys():\n",
    "#                vocab[word] += 1\n",
    "#    return vocab.most_common(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035eb9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#most_frequent_words(df_cann_pages_body[\"clean_content\"], tfdif_vectorizer.vocabulary_, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
