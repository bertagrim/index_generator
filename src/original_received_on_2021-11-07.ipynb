{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import pdftotext\n",
    "\n",
    "import re\n",
    "from sentence_splitter import SentenceSplitter\n",
    "splitter = SentenceSplitter(language=\"en\")\n",
    "\n",
    "import string\n",
    "\n",
    "from typing import Dict\n",
    "import fitz\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f763e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bcc556",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a859c",
   "metadata": {},
   "source": [
    "## 1. Get bookmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11fd3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_count(filepath):\n",
    "    # WARNING! One page can have multiple bookmarks!\n",
    "    with fitz.open(filepath) as doc:\n",
    "        num_pages = doc.page_count  # [[lvl, title, page, …], …]\n",
    "    return num_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab52a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bookmarks(filepath: str) -> Dict[int, str]:\n",
    "    # WARNING! One page can have multiple bookmarks!\n",
    "    bookmarks = {}\n",
    "    with fitz.open(filepath) as doc:\n",
    "        toc = doc.get_toc()  # [[lvl, title, page, …], …]\n",
    "        for level, title, page in toc:\n",
    "            bookmarks[page-1] = [level, title]\n",
    "    return bookmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c7bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sections(filepath):\n",
    "    num_pages=get_page_count(filepath)\n",
    "    dict_sections=get_bookmarks(filepath)\n",
    "    \n",
    "    complete_sections_dict={}\n",
    "    section_lev_1=''\n",
    "    section_lev_2=''\n",
    "    section_lev_3=''\n",
    "    abs_num=0\n",
    "    \n",
    "    depth2=False\n",
    "    depth3=False\n",
    "    \n",
    "    while abs_num < num_pages:\n",
    "        if abs_num not in dict_sections.keys():\n",
    "            complete_sections_dict[abs_num]=[[1, section_lev_1]]\n",
    "            if depth2:\n",
    "                complete_sections_dict[abs_num].append([2, section_lev_2])\n",
    "            if depth3:\n",
    "                complete_sections_dict[abs_num].append([3, section_lev_3])\n",
    "            abs_num+=1\n",
    "        else:\n",
    "            if dict_sections[abs_num][0]==1:\n",
    "                complete_sections_dict[abs_num]=[[1, dict_sections[abs_num][1]]]\n",
    "                section_lev_1=dict_sections[abs_num][1]\n",
    "                abs_num+=1\n",
    "            elif dict_sections[abs_num][0]==2:\n",
    "                complete_sections_dict[abs_num]=[[1, section_lev_1]]\n",
    "                complete_sections_dict[abs_num].append([2, dict_sections[abs_num][1]])\n",
    "                section_lev_2=dict_sections[abs_num][1]\n",
    "                abs_num+=1\n",
    "                depth2=True\n",
    "            elif dict_sections[abs_num][0]==3:\n",
    "                complete_sections_dict[abs_num]=[[1, section_lev_1]]\n",
    "                complete_sections_dict[abs_num]=[[2, section_lev_2]]\n",
    "                complete_sections_dict[abs_num].append([3, dict_sections[abs_num][1]])\n",
    "                section_lev_3=dict_sections[abs_num][1]\n",
    "                abs_num+=1\n",
    "                depth3=True\n",
    "            else:\n",
    "                continue\n",
    "                    \n",
    "    return complete_sections_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d66c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(filepath):\n",
    "    dict_sections=get_all_sections(filepath)\n",
    "    abs_num=0\n",
    "    depth3=False\n",
    "    depth2=False\n",
    "    \n",
    "    list_values=[]\n",
    "    for value in dict_sections.values():\n",
    "        if len(value)==3:\n",
    "            depth3=True\n",
    "        if len(value)==2:\n",
    "            depth2=True\n",
    "    \n",
    "    if depth3==True:\n",
    "        return 3\n",
    "    elif depth2==True:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sections_level_1(x):\n",
    "    complete_sections_dict=get_all_sections(filepath)\n",
    "    sections=complete_sections_dict[x]\n",
    "    section_level_1=sections[0][1]\n",
    "    return section_level_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec27fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sections_level_2(x):\n",
    "    if get_depth(filepath)==2:\n",
    "        complete_sections_dict=get_all_sections(filepath)\n",
    "        sections=complete_sections_dict[x]\n",
    "        section_level_2=sections[x][1][1]\n",
    "        return section_level_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4eb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sections_level_3(x):\n",
    "    if get_depth(filepath)==3:\n",
    "        complete_sections_dict=get_all_sections(filepath)\n",
    "        sections=complete_sections_dict[x]\n",
    "        section_level_3=sections[x][2][1]\n",
    "        return section_level_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05237c58",
   "metadata": {},
   "source": [
    "### Get real pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_map = [(1000, 'M'), (900, 'CM'), (500, 'D'), (400, 'CD'), (100, 'C'), (90, 'XC'), (50, 'L'), (40, 'XL'), (10, 'X'), (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')]\n",
    "\n",
    "def num2roman(num):\n",
    "    roman = ''\n",
    "    while num > 0:\n",
    "        for i, r in num_map:\n",
    "            while num >= i:\n",
    "                roman = roman +  r\n",
    "                num = num - i\n",
    "    return roman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d4382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_pages(filepath):\n",
    "    dict_pages={}\n",
    "    with fitz.open(filepath) as doc:\n",
    "        page_num=doc.get_page_labels()\n",
    "    for item in page_num:\n",
    "        abs_page=item['startpage']\n",
    "        if item['prefix']!='':\n",
    "            real_page=item['prefix']\n",
    "        else:\n",
    "            if item['style']=='r':\n",
    "                #transform to arabic numeral\n",
    "                real_page=[item['firstpagenum'], 'r']\n",
    "            elif item['style']=='D':\n",
    "                real_page=[item['firstpagenum'], 'd']\n",
    "        dict_pages[abs_page]=real_page\n",
    "    return dict_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48932b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(filepath):\n",
    "    num_pages=get_page_count(filepath)\n",
    "    dict_pages=get_dict_pages(filepath)\n",
    "    \n",
    "    trans_dict={}\n",
    "    real_num=0\n",
    "    abs_num=0\n",
    "    is_roman=False\n",
    "    \n",
    "    while abs_num < num_pages:\n",
    "        if abs_num not in dict_pages.keys():\n",
    "            if is_roman:\n",
    "                trans_dict[abs_num]=num2roman(real_num)\n",
    "            else:\n",
    "                trans_dict[abs_num]=real_num\n",
    "            abs_num+=1\n",
    "            real_num+=1\n",
    "        else:\n",
    "            if type(dict_pages[abs_num])==str:\n",
    "                trans_dict[abs_num]=dict_pages[abs_num]\n",
    "                real_num+=1\n",
    "                abs_num+=1\n",
    "            else:\n",
    "                if dict_pages[abs_num][1]=='r':  \n",
    "                    is_roman=True\n",
    "                    trans_dict[abs_num]=num2roman(dict_pages[abs_num][0])\n",
    "                else:\n",
    "                    trans_dict[abs_num]=dict_pages[abs_num][0]\n",
    "                    is_roman=False\n",
    "                real_num=dict_pages[abs_num][0]+1\n",
    "                abs_num+=1\n",
    "                    \n",
    "    return trans_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_number(x):\n",
    "    trans_dict=translate(filepath)\n",
    "    return trans_dict[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194356cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not needed\n",
    "def get_list_real_pages(filepath):\n",
    "    trans_dict=translate(filepath)\n",
    "    return list(trans_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d90da63",
   "metadata": {},
   "source": [
    "## 2. Create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a list of lists that have as first element the content of one page and as the second element its page number (starting from 0)\n",
    "def process_pages(name):\n",
    "    path=\"books/processed/\"+name+\".pdf\"\n",
    "    read_file = open(path,'rb')\n",
    "    content_per_page = pdftotext.PDF(read_file)\n",
    "    \n",
    "    list_pages=[]\n",
    "    page_count=0\n",
    "    for page in content_per_page:\n",
    "        content=page\n",
    "\n",
    "        # recover hyphen-splitter sentences\n",
    "        pattern_line_break_1 = re.compile(r\"-\\n\")\n",
    "        # recover sentences split not preceded by a period and followed by lowercase character. I added upper case character, number, parentheses or quotation marks. Problem is that I was losing the first sentences of each page, because they were attached to the header\n",
    "        #pattern_line_break_2 = re.compile(r\"(?<![.?¿!¡º])\\s*\\n(?=\\s*[a-zA-Z0-9\\(\\)\\'\\\"])\")\n",
    "        pattern_line_break_2 = re.compile(r\"(?<![.?¿!¡º])\\s*\\n(?=\\s*[a-z])\")\n",
    "\n",
    "        content_processed = pattern_line_break_1.sub(\"\",content)\n",
    "        content_processed = pattern_line_break_2.sub(\" \", content_processed)\n",
    "\n",
    "        # Split lines\n",
    "        splitter = SentenceSplitter(language=\"en\")\n",
    "        split_file = splitter.split(text=content_processed)\n",
    "        page_content=\"\\n\".join(split_file)\n",
    "        \n",
    "        list_pages.append([page_content, page_count])\n",
    "        page_count+=1\n",
    "\n",
    "    return list_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cann_pages=process_pages(\"(Cambridge Textbooks in Linguistics) Ronnie Cann - Formal Semantics_ An Introduction-Cambridge University Press (1993)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0781ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages = pd.DataFrame(list_cann_pages, columns = ['content', 'page_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a188088",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb071c5",
   "metadata": {},
   "source": [
    "### Add columns with real page numbers and sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e04220",
   "metadata": {},
   "source": [
    "#### Page by page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"books/processed/(Cambridge Textbooks in Linguistics) Ronnie Cann - Formal Semantics_ An Introduction-Cambridge University Press (1993).pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f79b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages['real_page_num']=df_cann_pages['page_number'].apply(translate_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages['section_level_1']=df_cann_pages['page_number'].apply(add_sections_level_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages['section_level_2']=df_cann_pages['page_number'].apply(add_sections_level_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages['section_level_3']=df_cann_pages['page_number'].apply(add_sections_level_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_lines(df_by_pages):\n",
    "    list_pages=df_by_pages.values.tolist()\n",
    "    list_lines=[]\n",
    "    for item in list_pages:\n",
    "        if type(item[0])==str:\n",
    "            split_file = splitter.split(text=item[0])\n",
    "            for line in split_file:\n",
    "                list_lines.append([line, item[1], item[2], item[3], item[4], item[5]])\n",
    "        else:\n",
    "            list_lines.append([item[0], item[1], item[2], item[3], item[4], item[5]])\n",
    "    return list_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c901ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lines(df_cann_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47619eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines = pd.DataFrame(list_lines(df_cann_pages), columns = df_cann_pages.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd16a18",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a467c3",
   "metadata": {},
   "source": [
    "### Drop lines/pages that include the phrase \"This page was intentionally left blank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ed2ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_cann_lines[df_cann_lines.content.str.match(\"This page intentionally left blank\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cann_lines[df_cann_lines.content.str.match(\"This page was intentionally left blank\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines=df_cann_lines[~df_cann_lines.content.str.match(\"This page intentionally left blank\", na=False)]\n",
    "df_cann_lines=df_cann_lines[~df_cann_lines.content.str.match(\"This page was intentionally left blank\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8649f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages=df_cann_pages[~df_cann_pages.content.str.match(\"This page intentionally left blank\", na=False)]\n",
    "df_cann_pages=df_cann_pages[~df_cann_pages.content.str.match(\"This page was intentionally left blank\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e3511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cann_lines.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09800a54",
   "metadata": {},
   "source": [
    "### Drop null lines/pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b413680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cann_pages.content.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf4e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines.content.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd52e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines.content.dropna(inplace=True)\n",
    "df_cann_pages.content.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a23d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cann_lines.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd7be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cann_pages.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e62f42",
   "metadata": {},
   "source": [
    "### Drop lines/pages that contain only numbers (they are page numbers, numbers of sections, etc), only punctuation or are empty (with whitespace character or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef36af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finds lines that contain only numbers \n",
    "#PROBLEM: needed for index???\n",
    "df_cann_lines[df_cann_lines.content.str.match(\"^[^a-zA-Z]*\\d+[^a-zA-Z]*$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03765c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds lines that are empty (they don't have a whitespace character)\n",
    "df_cann_lines[df_cann_lines.content.str.match(\"^$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd091e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this finds the lines that are just punctuation\n",
    "df_cann_lines[df_cann_lines.content.str.match(\"^[^\\w]+$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0053257",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines=df_cann_lines[~df_cann_lines.content.str.match(\"^[^a-zA-Z]*\\d+[^a-zA-Z]*$\")]\n",
    "df_cann_pages=df_cann_pages[~df_cann_pages.content.str.match(\"^[^a-zA-Z]*\\d+[^a-zA-Z]*$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines=df_cann_lines[~df_cann_lines.content.str.match(\"^[^\\w]+$\")]\n",
    "df_cann_pages=df_cann_pages[~df_cann_pages.content.str.match(\"^[^\\w]+$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines=df_cann_lines[~df_cann_lines.content.str.match(\"^$\")]\n",
    "df_cann_pages=df_cann_pages[~df_cann_pages.content.str.match(\"^$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d437667",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines.reset_index(drop=True, inplace=True)\n",
    "df_cann_pages.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c3f9c",
   "metadata": {},
   "source": [
    "### Split dataset into 3: ToC, Body of text, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d594e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines.to_csv(\"cann_info_lines.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages.to_csv(\"cann_info_pages.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b5bfa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'cann_info_pages.csv'\n",
    "df_cann_pages = pd.read_csv(path,index_col=0)\n",
    "df_cann_pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9cd7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = 'cann_info_lines.csv'\n",
    "df_cann_lines = pd.read_csv(path,index_col=0)\n",
    "df_cann_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf5cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages.section_level_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cca9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages.section_level_1.fillna(\"No info\", inplace=True)\n",
    "df_cann_lines.section_level_1.fillna(\"No info\", inplace=True)\n",
    "df_cann_pages.section_level_2.fillna(\"No info\", inplace=True)\n",
    "df_cann_lines.section_level_2.fillna(\"No info\", inplace=True)\n",
    "df_cann_pages.section_level_3.fillna(\"No info\", inplace=True)\n",
    "df_cann_lines.section_level_3.fillna(\"No info\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d201506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages.section_level_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bca627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FALTA: remove references (maybe don't do it for compendiums)\n",
    "\n",
    "#toc\n",
    "df_cann_pages_toc=df_cann_pages[df_cann_pages.section_level_1.str.match(\"CONTENTS|Contents\")]#generalise more\n",
    "df_cann_lines_toc=df_cann_lines[df_cann_lines.section_level_1.str.match(\"CONTENTS|Contents\")]\n",
    "\n",
    "#index\n",
    "df_cann_pages_index=df_cann_pages[df_cann_pages.section_level_1.str.match(\"INDEX|Index\")]#generalise more\n",
    "df_cann_lines_index=df_cann_lines[df_cann_lines.section_level_1.str.match(\"INDEX|Index\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#body: probably needs to be done manually\n",
    "df_cann_pages_body=df_cann_pages[(df_cann_pages.page_number>=19) & (df_cann_pages.page_number<=340)]\n",
    "df_cann_lines_body=df_cann_lines[(df_cann_lines.page_number>=19) & (df_cann_lines.page_number<=340)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe13156",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages_toc.reset_index(drop=True, inplace=True)\n",
    "df_cann_lines_toc.reset_index(drop=True, inplace=True)\n",
    "df_cann_pages_body.reset_index(drop=True, inplace=True)\n",
    "df_cann_lines_body.reset_index(drop=True, inplace=True)\n",
    "df_cann_pages_index.reset_index(drop=True, inplace=True)\n",
    "df_cann_lines_body.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe7fa94",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a07bd",
   "metadata": {},
   "source": [
    "### STEPS: \n",
    "1. Remove stopwords\n",
    "2. Remove all numbers\n",
    "3. Stem\n",
    "4. Look for phrases (use gensim phraser)\n",
    "5. Do wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "porter = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from collections import defaultdict\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim import corpora\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7329e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep updating\n",
    "textbook_words=['thus', 'today', 'nowadays', 'actually', 'section', 'indeed', 'every', 'any', 'some', 'example', 'therefore', 'definition', 'introduction', 'conclusion', 'chapter', 'appendix', 'otherwise', 'thing', 'rather', 'instead', 'like', 'since', 'given', 'case', 'hence', 'iff', 'see', 'beyond', 'below', 'above', 'postscript', 'index', 'ensure', 'generally', 'anything', 'something', 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_data):\n",
    "    tokens=word_tokenize(text_data)\n",
    "    \n",
    "    lowercased=[w.lower() for w in tokens]\n",
    "    \n",
    "    no_punct=[word for word in lowercased if word.isalpha()]\n",
    "    \n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    no_sw=[w for w in no_punct if not w in stop_words]\n",
    "    \n",
    "    no_tw=[w for w in no_sw if not w in textbook_words]\n",
    "    \n",
    "    long_words=[w for w in no_tw if len(w)>2]\n",
    "    \n",
    "    clean_tokens=[lemmatizer.lemmatize(word) for word in long_words]\n",
    "    \n",
    "    return (\" \").join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68420403",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_body[\"clean_content\"] = df_cann_lines_body[\"content\"].apply(lambda x: clean_text(x))\n",
    "df_cann_lines_toc[\"clean_content\"] = df_cann_lines_toc[\"content\"].apply(lambda x: clean_text(x))\n",
    "#df_cann_lines_index[\"clean_content\"] = df_cann_lines_index[\"content\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages_body[\"clean_content\"] = df_cann_pages_body[\"content\"].apply(lambda x: clean_text(x))\n",
    "df_cann_pages_toc[\"clean_content\"] = df_cann_pages_toc[\"content\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42684fa1",
   "metadata": {},
   "source": [
    "### FIltering by POS to get only NPs as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e32c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def keep_NPs(sent):\n",
    "    list_tokens=[]\n",
    "    sent = nlp(sent)\n",
    "    for word in sent:\n",
    "        if word.tag_== \"NN\" or word.tag_==\"NNP\" or word.tag_==\"NNPS\" or word.tag_==\"NNS\" or word.tag_==\"JJ\" or word.tag_==\"JJR\" or word.tag_==\"JJS\":\n",
    "            list_tokens.append(word) \n",
    "    clean_sent=(\" \").join([i.text for i in list_tokens])\n",
    "    return clean_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dabacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_body[\"clean_content\"] = df_cann_lines_body[\"clean_content\"].apply(lambda x: keep_NPs(x))\n",
    "#df_cann_lines_toc[\"clean_content\"] = df_cann_lines_toc[\"content\"].apply(lambda x: keep_NPs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages_body[\"clean_content\"] = df_cann_pages_body[\"clean_content\"].apply(lambda x: keep_NPs(x))\n",
    "#df_cann_pages_toc[\"clean_content\"] = df_cann_pages_toc[\"content\"].apply(lambda x: keep_NPs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_body.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_body.to_csv(\"cann_lines_body_clean.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_pages_body.to_csv(\"cann_pages_body_clean.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'cann_lines_body_clean.csv'\n",
    "df_cann_lines_body = pd.read_csv(path,index_col=0)\n",
    "df_cann_lines_body.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbfc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'cann_pages_body_clean.csv'\n",
    "df_cann_pages_body = pd.read_csv(path,index_col=0)\n",
    "df_cann_pages_body.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bc0aa",
   "metadata": {},
   "source": [
    "### Using gensim to extract n-grams and most common terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da414af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_body[\"clean_content\"] = df_cann_lines_body[\"clean_content\"].fillna('')\n",
    "df_cann_pages_body[\"clean_content\"] = df_cann_pages_body[\"clean_content\"].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unclear to me what to use as documents -- lines or pages:\n",
    "#sent = [page.split() for page in df_cann_pages_body[\"clean_content\"]]\n",
    "sent = [sentence.split() for sentence in df_cann_lines_body[\"clean_content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a6dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram models\n",
    "bigram = gensim.models.phrases.Phrases(sent, min_count=3, threshold=10)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f1adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list=list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31984672",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = corpora.Dictionary([sent for sent in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b990ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1336b2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#same but easier, probably use this. Falta: add it to df (maybe normalise it to 0-1)\n",
    "mydict.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_frequency={}\n",
    "for item in mydict.most_common():\n",
    "    map_frequency[item[0]]=item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87aa4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_frequency(x):\n",
    "    return map_frequency[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next: \n",
    "#(1) get score if they are in sections or subsections, DONE\n",
    "#(2) get score for their position in the sentence, LEAVE IT OUT\n",
    "#(3) get importance of context score\n",
    "\n",
    "#Finally,\n",
    "#(4) do NER and add all names of people, institutions... what else?\n",
    "\n",
    "#If I finish, calculate the length of index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9838f33",
   "metadata": {},
   "source": [
    "### Create a dataframe of all the candidate keywords, with a column consisting of its context and a column consisting of each relevant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff13577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_sent = [sentence for sentence in df_cann_lines_body[\"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(raw_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdffecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidates_list(sentences):\n",
    "    candidates=[]\n",
    "    for item in sentences:\n",
    "        for w in item:\n",
    "            candidates.append([w, (' ').join(item)])\n",
    "    return candidates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_list=create_candidates_list(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c20976",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df=pd.DataFrame(candidates_list, columns=['candidate_keyword', 'context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fec46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "candidates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855513a0",
   "metadata": {},
   "source": [
    "#### Add frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdadb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df['frequency']=candidates_df.candidate_keyword.apply(assign_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734c94a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "candidates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61985eb9",
   "metadata": {},
   "source": [
    "#### Add appearance in title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a638eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#falta netejar toc\n",
    "def clean_toc(text_data):\n",
    "    tokens=word_tokenize(text_data)\n",
    "    \n",
    "    lowercased=[w.lower() for w in tokens]\n",
    "    \n",
    "    no_punct=[word for word in lowercased if word.isalpha()]\n",
    "    \n",
    "    clean_tokens=[lemmatizer.lemmatize(word) for word in no_punct]\n",
    "    \n",
    "    return (\" \").join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5977ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_toc['content'].apply(clean_toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_toc=[]\n",
    "for line in df_cann_lines_toc.content.apply(clean_toc):\n",
    "    words_toc+=line.split()\n",
    "#print(words_toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfcb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_toc(x):\n",
    "    if \"_\" in x:\n",
    "        if x.split(\"_\")[0] in words_toc or x.split(\"_\")[1] in words_toc:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if x in words_toc:\n",
    "            return 1\n",
    "        else: \n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df['is_in_toc']=candidates_df.candidate_keyword.apply(is_in_toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69872d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a02862",
   "metadata": {},
   "source": [
    "### Adding the importance of the context for the whole document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a811e49",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "This seems to work! Using sentence embeddings and book embedding and compute cosine similarity between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e8548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a41d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "doc=(' ').join(candidates_df.context.unique())\n",
    "candidates=candidates_df.context.unique()\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e458ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 30\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f976866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3b63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9b390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1c1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a30535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0e12ae9",
   "metadata": {},
   "source": [
    "### USELESS STUFF FOR NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa72cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do it with tf-idf?: get a tf-idf vector per sentence and do cosine similarity with the tf-idf vectore of the whole book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48a810",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfdif_vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "#Unclear to me what to use as documents -- sentences or pages\n",
    "tfidf = tfdif_vectorizer.fit_transform(candidates_df[\"context\"])\n",
    "#tfidf = tfdif_vectorizer.fit_transform(df_cann_pages_body[\"clean_content\"])\n",
    "\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a723b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf[2].toarray()\n",
    "#print(tfdif_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdif_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80fa314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 embed sentences using sent2vec\n",
    "#2 embed whole document using doc2vec\n",
    "# rank sentence embeddings based on similarity to whole document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install sent2vec\n",
    "#from sent2vec.vectorizer import Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d11153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#candidates_df.context.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = candidates_df.context.unique()\n",
    "\n",
    "#vectorizer = Vectorizer()\n",
    "#vectorizer.bert(sentences)\n",
    "#vectors = vectorizer.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff2110",
   "metadata": {},
   "source": [
    "### Let's try it with word2vec (not pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5253e8df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from time import time \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d469652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8457ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdb3b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232590e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"function\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343dab6d",
   "metadata": {},
   "source": [
    "### Let's try it with pretrained model from spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_sm')#maybe try with bigger models later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518985e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in candidates_df.context.unique():\n",
    "    doc = model(line)\n",
    "    print(line, doc.vector, len(doc.vector))\n",
    "#    for token in doc:\n",
    "#        print(token.vector)\n",
    "#        print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba0cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in candidates_df.context.unique():\n",
    "    \n",
    "    doc = model(line)\n",
    "    print(line, doc.vector, len(doc.vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad473a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[89])\n",
    "print(doc[3])\n",
    "doc[1].similarity(doc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94282c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = corpora.Dictionary([sent for sent in sentences])\n",
    "corpus = [mydict.doc2bow(sent) for sent in sentences]\n",
    "\n",
    "# Show the Word Weights in Corpus\n",
    "#for doc in corpus:\n",
    "#    print([[mydict[id], freq] for id, freq in doc])\n",
    "\n",
    "# Create the TF-IDF model\n",
    "tfidf = models.TfidfModel(corpus, smartirs='ntc')\n",
    "\n",
    "# Show the TF-IDF weights\n",
    "for doc in tfidf[corpus]:\n",
    "    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474bf40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this gives you the words that get highest tfidf score in any sentence\n",
    "#problem: gives you weird examples that only appear in one place\n",
    "topWords = {}\n",
    "for doc in corpus_tfidf:\n",
    "    for iWord, tf_idf in doc:\n",
    "        if iWord not in topWords:\n",
    "            topWords[iWord] = 0\n",
    "\n",
    "        if tf_idf > topWords[iWord]:\n",
    "            topWords[iWord] = tf_idf\n",
    "\n",
    "for i, item in enumerate(sorted(topWords.items(), key=lambda x: x[1], reverse=True), 1):\n",
    "    print(\"%2s: %-13s %s\" % (i, mydict[item[0]], item[1]))\n",
    "    if i == 1000: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk import ngrams\n",
    "\n",
    "#sent = [line for line in df_cann_lines[\"clean_content\"]]\n",
    "\n",
    "#ngram_counts = Counter(ngrams(sent, 2))\n",
    "#ngram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def most_frequent_words(docs, vocabulary, top_words):\n",
    "#    vocab = Counter()\n",
    "#    \n",
    "#    for doc in docs:\n",
    "#        for word in doc.split(' '):\n",
    "#            if word in vocabulary.keys():\n",
    "#                vocab[word] += 1\n",
    "#    return vocab.most_common(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035eb9fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#most_frequent_words(df_cann_pages_body[\"clean_content\"], tfdif_vectorizer.vocabulary_, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
