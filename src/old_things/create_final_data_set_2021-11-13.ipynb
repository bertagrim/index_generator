{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87455714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import pdftotext\n",
    "\n",
    "import re\n",
    "from sentence_splitter import SentenceSplitter\n",
    "splitter = SentenceSplitter(language=\"en\")\n",
    "\n",
    "import string\n",
    "\n",
    "from typing import Dict\n",
    "import fitz\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c267a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4d9408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3591bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>page_number</th>\n",
       "      <th>real_page_num</th>\n",
       "      <th>section_level_1</th>\n",
       "      <th>section_level_2</th>\n",
       "      <th>section_level_3</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Introduction</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1 Introduction</td>\n",
       "      <td>No info</td>\n",
       "      <td>No info</td>\n",
       "      <td>introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Semantics and semantic theory</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1 Introduction</td>\n",
       "      <td>No info</td>\n",
       "      <td>No info</td>\n",
       "      <td>semantics semantic theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In its broadest sense, semantics is the study ...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1 Introduction</td>\n",
       "      <td>No info</td>\n",
       "      <td>No info</td>\n",
       "      <td>broadest sense semantics study meaning linguis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is, however, more usual within linguistics ...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1 Introduction</td>\n",
       "      <td>No info</td>\n",
       "      <td>No info</td>\n",
       "      <td>however usual within linguistics interpret ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In other words, semantics is the study of mean...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1 Introduction</td>\n",
       "      <td>No info</td>\n",
       "      <td>No info</td>\n",
       "      <td>word semantics study meaning abstracted away a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  page_number  \\\n",
       "0                                       Introduction           19   \n",
       "1                      Semantics and semantic theory           19   \n",
       "2  In its broadest sense, semantics is the study ...           19   \n",
       "3  It is, however, more usual within linguistics ...           19   \n",
       "4  In other words, semantics is the study of mean...           19   \n",
       "\n",
       "   real_page_num section_level_1 section_level_2 section_level_3  \\\n",
       "0              1  1 Introduction         No info         No info   \n",
       "1              1  1 Introduction         No info         No info   \n",
       "2              1  1 Introduction         No info         No info   \n",
       "3              1  1 Introduction         No info         No info   \n",
       "4              1  1 Introduction         No info         No info   \n",
       "\n",
       "                                       clean_content  \n",
       "0                                       introduction  \n",
       "1                          semantics semantic theory  \n",
       "2  broadest sense semantics study meaning linguis...  \n",
       "3  however usual within linguistics interpret ter...  \n",
       "4  word semantics study meaning abstracted away a...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../data/processed/cann_93/by_line_body.csv'\n",
    "df_cann_lines_body = pd.read_csv(path,index_col=0)\n",
    "df_cann_lines_body.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4558e8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>page_number</th>\n",
       "      <th>real_page_num</th>\n",
       "      <th>section_level_1</th>\n",
       "      <th>section_level_2</th>\n",
       "      <th>section_level_3</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1\\n\\nIntroduction\\n\\n1.1\\n\\nSemantics and sema...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1 Introduction</td>\n",
       "      <td>No info</td>\n",
       "      <td>No info</td>\n",
       "      <td>introduction semantics semantic theory broades...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 Introduction semantics over the last two dec...</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>1 Introduction</td>\n",
       "      <td>No info</td>\n",
       "      <td>No info</td>\n",
       "      <td>introduction semantics last two decade theory ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Semantics and semantic theory controversial ma...</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>1 Introduction</td>\n",
       "      <td>No info</td>\n",
       "      <td>No info</td>\n",
       "      <td>semantics semantic theory controversial matter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 Introduction the principle (3) minimally req...</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>1 Introduction</td>\n",
       "      <td>No info</td>\n",
       "      <td>No info</td>\n",
       "      <td>introduction principle minimally requires mean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Semantics and semantic theory\\nOne way in whic...</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>1 Introduction</td>\n",
       "      <td>No info</td>\n",
       "      <td>No info</td>\n",
       "      <td>semantics semantic theory one way may achieved...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  page_number  \\\n",
       "0  1\\n\\nIntroduction\\n\\n1.1\\n\\nSemantics and sema...           19   \n",
       "1  1 Introduction semantics over the last two dec...           20   \n",
       "2  Semantics and semantic theory controversial ma...           21   \n",
       "3  1 Introduction the principle (3) minimally req...           22   \n",
       "4  Semantics and semantic theory\\nOne way in whic...           23   \n",
       "\n",
       "   real_page_num section_level_1 section_level_2 section_level_3  \\\n",
       "0              1  1 Introduction         No info         No info   \n",
       "1              2  1 Introduction         No info         No info   \n",
       "2              3  1 Introduction         No info         No info   \n",
       "3              4  1 Introduction         No info         No info   \n",
       "4              5  1 Introduction         No info         No info   \n",
       "\n",
       "                                       clean_content  \n",
       "0  introduction semantics semantic theory broades...  \n",
       "1  introduction semantics last two decade theory ...  \n",
       "2  semantics semantic theory controversial matter...  \n",
       "3  introduction principle minimally requires mean...  \n",
       "4  semantics semantic theory one way may achieved...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../data/processed/cann_93/by_page_body.csv'\n",
    "df_cann_pages_body = pd.read_csv(path,index_col=0)\n",
    "df_cann_pages_body.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d9d7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'cann_lines_toc_clean.csv'\n",
    "df_cann_lines_toc = pd.read_csv(path,index_col=0)\n",
    "df_cann_lines_toc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251711c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"cann_lines_index.csv\"\n",
    "df_cann_lines_index = pd.read_csv(path,index_col=0)\n",
    "df_cann_lines_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638edfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"cann_pages_biblio.csv\"\n",
    "df_cann_pages_biblio = pd.read_csv(path,index_col=0)\n",
    "df_cann_pages_biblio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b91265",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_body[\"clean_content\"] = df_cann_lines_body[\"clean_content\"].fillna('')\n",
    "df_cann_pages_body[\"clean_content\"] = df_cann_pages_body[\"clean_content\"].fillna('')\n",
    "df_cann_lines_toc[\"content\"] = df_cann_lines_toc[\"content\"].fillna('')\n",
    "df_cann_lines_index[\"content\"] = df_cann_lines_index[\"content\"].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b34a0",
   "metadata": {},
   "source": [
    "### 1: Get candidates plus frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent = [page.split() for page in df_cann_pages_body[\"clean_content\"]]\n",
    "sentences = [sentence.split() for sentence in df_cann_lines_body[\"clean_content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_text=''\n",
    "for sentence in df_cann_lines_body[\"clean_content\"]:\n",
    "    whole_text+=' '\n",
    "    whole_text+=sentence\n",
    "whole_text\n",
    "\n",
    "Counter(ngrams(whole_text.split(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_bigrams_tuples=dict(Counter(ngrams(whole_text.split(), 2)))\n",
    "freq_bigrams={}\n",
    "for item in freq_bigrams_tuples.items():\n",
    "    pair=item[0]\n",
    "    string=pair[0]+' '+pair[1]\n",
    "    freq_bigrams[string]=item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7695803",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_unigrams=dict(Counter(whole_text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f113a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_ngrams=dict(freq_unigrams, **freq_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2811709",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bigrams_lines=[list(ngrams(sentence.split(), 2)) for sentence in df_cann_lines_body[\"clean_content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4638ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bigrams_str=[]\n",
    "for item in list_bigrams_lines:\n",
    "    sublist=[]\n",
    "    for pair in item:\n",
    "        string=pair[0]+' '+pair[1]\n",
    "        sublist.append(string)\n",
    "    list_bigrams_str.append(sublist)   \n",
    "list_bigrams_str "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_sentences(raw_sent):\n",
    "    tokens=word_tokenize(raw_sent)\n",
    "    lowercased=[w.lower() for w in tokens]\n",
    "    no_punct=[word for word in lowercased if (word.isalpha() or re.match(\"[a-z]+-[a-z]+\", word))]\n",
    "    clean_raw_words=[lemmatizer.lemmatize(w) for w in no_punct]\n",
    "    return (\" \").join(clean_raw_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30439f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sent=[]\n",
    "for sent in df_cann_lines_body[\"content\"]:\n",
    "    raw_sent.append(get_raw_sentences(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_and_contexts=list(zip(list_bigrams_str, sentences, raw_sent))\n",
    "bigrams_and_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec46920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidates_list(bigrams_contexts):\n",
    "    candidates=[]\n",
    "    for item in bigrams_contexts:\n",
    "        doc=nlp((' ').join(item[1]))\n",
    "        for w in item[1]:\n",
    "            if re.match(\"[a-z]+-[a-z]+\", w):\n",
    "                #print(w)\n",
    "                candidates.append([w, item[1], item[2], 'NOUN'])\n",
    "            else:\n",
    "                for w in doc:\n",
    "                    candidates.append([str(w), item[1], item[2], w.pos_])\n",
    "        for w in item[0]:\n",
    "            candidates.append([w, item[1], item[2], 'CHUNK'])\n",
    "    return candidates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_list=create_candidates_list(bigrams_and_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c793054",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df=pd.DataFrame(candidates_list, columns=['candidate_keyword', 'clean_context', 'raw_context', 'POS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b52bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2a2e164",
   "metadata": {},
   "source": [
    "### 2. Column frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44cd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_length(df_book_pages_body):\n",
    "    all_clean_content=[]\n",
    "    for page in df_book_pages_body[\"clean_content\"]:\n",
    "        all_clean_content+=page.split(' ')\n",
    "    return len(all_clean_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43822b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_length=get_book_length(df_cann_pages_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823553e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_frequency(x, mf):\n",
    "    value=(mf.get(x)/book_length)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a8b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df['freq']=candidates_df.candidate_keyword.apply(lambda x:assign_frequency(x,freq_ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dde5e79",
   "metadata": {},
   "source": [
    "### 3. Column: is in toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4da7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_toc(text_data):\n",
    "    tokens=word_tokenize(text_data)\n",
    "    \n",
    "    lowercased=[w.lower() for w in tokens]\n",
    "    \n",
    "    no_punct=[word for word in lowercased if (word.isalpha() or re.match(\"[a-z]+-[a-z]+\", word))]\n",
    "    \n",
    "    clean_tokens=[lemmatizer.lemmatize(word) for word in no_punct]\n",
    "    \n",
    "    return (\" \").join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ec9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_toc['content'].apply(clean_toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5363a0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a83ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_toc=[]\n",
    "for line in df_cann_lines_toc.content.apply(clean_toc):\n",
    "    words_toc+=line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_toc(x):\n",
    "    if \"_\" in x:\n",
    "        if x.split(\"_\")[0] in words_toc and x.split(\"_\")[1] in words_toc:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if x in words_toc:\n",
    "            return 1\n",
    "        else: \n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934edd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df['is_in_toc']=candidates_df.candidate_keyword.apply(is_in_toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606e6d4",
   "metadata": {},
   "source": [
    "### 4. Column: Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4cdd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_sentence(x):\n",
    "    return (' ').join(x)\n",
    "\n",
    "candidates_df.clean_context=candidates_df.clean_context.apply(form_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d7617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=(' ').join(candidates_df.clean_context.unique())\n",
    "contexts=candidates_df.clean_context.unique()\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "context_embeddings = model.encode(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba5771",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = cosine_similarity(doc_embedding, context_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed40005",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities=dict(zip(candidates_df.clean_context.unique(),list(distances[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_similarities(x):\n",
    "    return similarities[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df['importance']=candidates_df.clean_context.apply(assign_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e675188",
   "metadata": {},
   "source": [
    "### 5. Column: position in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pos(row):\n",
    "    list_words=row['raw_context'].split(' ')\n",
    "    word=row['candidate_keyword']\n",
    "    if len(list_words)==1:\n",
    "        return 0\n",
    "    else:\n",
    "        if len(word)>1:\n",
    "            word=word.split(' ')[0]\n",
    "            return list_words.index(word)/(len(list_words)-1)\n",
    "        else:\n",
    "            return list_words.index(word)/(len(list_words)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df['position_in_context']=candidates_df.apply(return_pos, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a295d25",
   "metadata": {},
   "source": [
    "### 6. Column: is a named entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list_names(x):\n",
    "    tokens=word_tokenize(x)\n",
    "    lowercased=[w.lower() for w in tokens]\n",
    "    no_punct=[word for word in lowercased if (word.isalpha() or re.match(\"[a-z]+-[a-z]+\", word))]\n",
    "    clean_tokens=[w for w in no_punct if len(w)>2]\n",
    "    return (\" \").join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_named_entities(df_pages_body):\n",
    "    named_entities=[]\n",
    "    for page in df_pages_body.content:\n",
    "        page_named_entities=re.findall('(?<=[a-zA-Z] )[A-Z]+[a-z]+', page)\n",
    "        for item in page_named_entities:\n",
    "            named_entities.append(clean_list_names(item))\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a9e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities=find_named_entities(df_cann_pages_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff9bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_named_entity(x):\n",
    "    if x in named_entities:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c24d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df['is_named_entity']=candidates_df.candidate_keyword.apply(is_named_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b66adc",
   "metadata": {},
   "source": [
    "### 7. Column: length of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff164a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dict={}\n",
    "for keyword in candidates_df.candidate_keyword.unique():\n",
    "    len_dict[keyword]=len(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb81582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_len(x):\n",
    "    return len_dict[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c82ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df['length']=candidates_df.candidate_keyword.apply(assign_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8159f9d9",
   "metadata": {},
   "source": [
    "### 8. Column: is a named author "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list_names(x):\n",
    "    tokens=word_tokenize(x)\n",
    "    lowercased=[w.lower() for w in tokens]\n",
    "    no_punct=[word for word in lowercased if word.isalpha()]\n",
    "    clean_tokens=[w for w in no_punct if len(w)>2]\n",
    "    return (\" \").join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c14a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_authors_in_biblio(df_pages_biblio):\n",
    "    clean_list_authors=[]\n",
    "    for page in df_pages_biblio.content:\n",
    "        list_names=re.findall('[A-Z]\\.\\s[A-Za-z]+|[A-Za-z]+\\,\\s*[A-Z]\\.', page)\n",
    "        for name in list_names:\n",
    "            if clean_list_names(name)!='and' and clean_list_names(name)!='&':\n",
    "                if clean_list_names(name)!='':\n",
    "                    clean_list_authors.append(clean_list_names(name))\n",
    "    return clean_list_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f87418",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_authors=set(find_authors_in_biblio(df_cann_pages_biblio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0950aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_named_author(x, unique_authors):\n",
    "    if x in unique_authors:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e63cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df['is_named_author']=candidates_df.candidate_keyword.apply(lambda x: is_named_author(x, unique_authors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d349bbb",
   "metadata": {},
   "source": [
    "### 9. Target column: is in index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd9be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_words=['page', 'see', 'also', 'index', 'bold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams_index(text_data):\n",
    "    list_bigrams=[]\n",
    "    for ngram in re.findall('[a-zA-z]+\\s[a-zA-z]+\\s[a-zA-z]+|[a-zA-z]+\\s[a-zA-z]+|[a-zA-Z]+', text_data):\n",
    "        list_bigrams.append(ngram)\n",
    "    for hw in re.findall(\"[a-z]+-[a-z]+\", text_data):\n",
    "        list_bigrams.append(hw)\n",
    "    return list_bigrams    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2132ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_index['ngrams']=df_cann_lines_index.content.apply(find_ngrams_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cbfcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_index(list_ngrams):\n",
    "    clean_list_ngrams=[]\n",
    "    for ngram in list_ngrams:\n",
    "        clean_ngram=[]\n",
    "        for w in ngram.split():\n",
    "            clean_w=w.lower()\n",
    "            clean_w=lemmatizer.lemmatize(clean_w)\n",
    "            if clean_w not in stop_words and clean_w not in index_words:\n",
    "                clean_ngram.append(clean_w)\n",
    "        clean_list_ngrams.append((' ').join(clean_ngram))     \n",
    "    \n",
    "    return clean_list_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228eee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cann_lines_index.ngrams=df_cann_lines_index.ngrams.apply(clean_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbee9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_list_indexes=df_cann_lines_index.ngrams.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e78aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('cann_indexes.txt', 'w')\n",
    "lines=[(\",\").join(sublist)+\"\\n\" for sublist in raw_list_indexes]\n",
    "f.writelines(lines)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick manual cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('cann_indexes.txt', 'r')\n",
    "cann_clean_indexes=f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df48959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_indexes(indexes_txt):\n",
    "    cann_clean_indexes_nosep=[]\n",
    "    for item in indexes_txt:\n",
    "        item=item.strip('\\n')\n",
    "        item=item.split(\",\")\n",
    "        cann_clean_indexes_nosep.append(item)\n",
    "    final_indexes=[(item, 1) for sublist in cann_clean_indexes_nosep for item in sublist if item!='']\n",
    "    return dict((set(final_indexes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a89c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_indexes=get_final_indexes(cann_clean_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee65c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target_col(x):\n",
    "    if x in dict_indexes:\n",
    "        return dict_indexes[x]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e0ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df['is_in_index']=candidates_df.candidate_keyword.apply(add_target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c533eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how many ngrams are left out of cndidates to begin with:\n",
    "list_kw=list(set(candidates_df.candidate_keyword.tolist()))\n",
    "count_yes=0\n",
    "count_no=0\n",
    "for item in get_final_indexes(cann_clean_indexes):\n",
    "    if item in list_kw:\n",
    "        print(item+' 1')\n",
    "        count_yes+=1\n",
    "    else:\n",
    "        print(item+ ' 0')\n",
    "        count_no+=1\n",
    "\n",
    "print('YES: '+str(count_yes)+', NO:'+ str(count_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda5cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed. Conclusion: words that don't appear are either not extracted or trigrams or badly processed from pdf. I could do a visualisation about this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e38e8",
   "metadata": {},
   "source": [
    "### 10. Aggregate lines with duplicated candidate_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df.drop(columns=['clean_context', 'raw_context'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Save as CSV. \n",
    "candidates_df.groupby(['candidate_keyword', 'length', 'POS', 'is_named_entity', 'is_named_author', 'is_in_toc', 'importance', 'is_in_index'], as_index = False).agg({'freq':np.mean, 'position_in_context':np.mean}, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
